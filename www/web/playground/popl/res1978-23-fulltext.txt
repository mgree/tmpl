efficient computation of expressions with common subexpressions extended abstract computer science department and sethi bell hill new introduction background after deciding on the order in which operations in an expression like a must be per formed a compiler determines the exact computer in or machine code that will carry out the opera tions our theoretical understanding of code generation can be summarized by a optimal code from simple expressions like in which all subexpressions are distinct can be generated very efficiently over a broad class of machines and b if we wish to avoid redundant computation of common subexpressions then generation of optimal code becomes very difficult even for quite simple machines on b above and sethi show that the problem of generating optimal code for a one register machine is npcomplete aho et al add that even when all common subexpressions have exactly one operation optimal code generation is npcomplete for one register machines and also for infinite register machines expressions will be represented by dags directed acyclic graphs as in figure if no common subexpressions occur then the dag representation is a tree real machines have many features not found in models of machines so any theoretical results can at best provide insight into the design of code generators johnson describes the application of theoretical results on code generation from trees to the design of a portable code generator see also the discussion in et al common subexpressions do indeed occur in practice so heuristics are invoked to accommodate them we would like to develop theoretical results that indicate how common subexpressions might be handled in this paper we study a class of from which optimal code can be generated efficiently we study a class of graphs which we call graphs that include trees and are general enough to permit large common subexpressions but from which optimal code can be generated in polynomial time figure shows a graph taken from an algorithm in the ca cl our interest in series parallel graphs was by and who consider a scheduling problem that is related to code generation their results imply that the minimal number of registers needed to compute a graph can be found in o n log n time much remains to be done since their model does not permit store instructions further information on code generation may be found in the machine model as in and sethi we consider a hardware stack which can hold at most d elements we say the stack has depth d coupled with the stack is a random access memory large enough to hold all initial and computed values a machine is the infinite register machine has two kinds of rr ops and rs where r and s are registers since an operation a register the problem is one of moves bell inc figure a dag represents the result of a sequence of assignments the dag for t t a shown specified by selecting at least one binary operation in addition to load destructive store removes the top element from the stack and store leaves the stack unchanged arithmetic operations are divided into four categories depending on whether the left or right operand is on top of the stack and on whether one or both operands are on the stack for example in the o denotes an operation the r denotes that the right operand is on top of the stack and ss denotes that the two operands the top two positions on the stack in the left operand is on top of the stack and the right is in memory the remaining two combinations are and each category like is really a class of operations since is and is division and so on this model also other addressing modes like indirection and immediate operands see the model in if we allow the cost of say to be different from associated with each instruction is a cost ci the cost of a sequence of instructions is determined by the costs of the instructions in the sequence relation with register models we consider stack machines for two reasons a the results from dags for machines carry over to stack machines since a stack of depth is the same as a machine finding a polynomial algorithm for optimal code generation from graphs for stack machines is therefore significant b the second reason is pragmatic stack machines are easier to analyze since access to values on the stack is restricted the nature of a stack machine guarantees and normal form results of the type that aho and johnson had to prove for trees on register machines on stack machines between subtrees needed for optimality on register machines in just cannot occur since a register machine can simulate a stack machine any algorithms we develop can be used as heuristics on register machines consider a register machine with instructions of the form r r ops where r and s are registers suppose that a value a is computed into r and a value b is computed into s before the instruction r r ops k performed the order in which a and b are computed does not matter on a stack machine if a is computed before b then a will be below b in the stack to the order of computation of a and b is achieved if both and instructions are available ie either the left or the right operand may be on top of the stack for this reason we will pay special attention to stack machines with both and instructions it will be convenient to consider machines which have all four kinds of operations ie we call such machines commutative stack machines the results proved for commutative machines can often be carried over to the case when one or both of the operations are not available graphs all graphs in this paper are directed and acyclic the number of edges entering resp leaving a node is called the resp of x node x is called a shared node if a dag is a graph if it can be reduced to a graph consisting of a single node by applying a sequence of the following transformations delete edge xy and node y if d yl and delete edges xy and node y and add edge xz if if there are two edges from node x to node y then delete one of them transformations and by themselves define graphs formulated by and transformation allows subgraphs an example of a graph is given in figure results we will an idea of the results in this paper in the context of an example suppose the graph in figure a is computed on a machine with all four kinds of operations the last node to be computed is u if no unnecessary stores occur then we expect that one of nodes v and w will be computed its value being in the stack while the other of v and w is computed once both v and w are on the stack then either an or an will be the last instruction that computes u traversals suppose the last instruction is an since w must then be below v on the stack w is computed before v all values in the for w must therefore be computed before any value in the for v is computed the exception being the shared node of course since there are no copy instructions the first time the value of the shared node x is on top of the stack it must be stored so that it can be reused the instructions of a program to compute the graph define a tree traversal of the graph as in figure b when w is computed before v the of the shared node x will be traversed when w is computed we will talk of the subtree of x to w stack positions during a tree traversal of a graph there will in general be a number of intermediate results that will be used later for example in figure b the value of w is computed and held on the stack while the subtree for v is computed since an intermediate result a stack position associated with each tree traversal is the number of stack positions needed to make that particular traversal for the graph in figure a the order of computation indicated in figure c requires one less stack position than the order in b in section we will determine a traversal of a graph that the number of stack positions needed on a commutative machine testing a linear algorithm is given in section to test of a binary dag each node has two or no sons this algorithm provides the framework for the in sections and optimal code the number of stack positions used and the cost of a program may sometimes be goals in figure the subtree at x to v as in c the number of stack positions but requires one more instruction than the in b the reason is that in b once the vg figure a graph from oct page the symbol indicates an indexing operation with representing the address since no advantage can be taken of leaves on stack machines leaves with the same label have not been identified j d a b c figure on a stack machine a program to compute a graph corresponds to a tree traversal of the graph the problem is one of finding the best traversal nv i j i figure think of there being an unknown subtree at node z the subtree requires a stack positions figure the decision of which to y may depend on which z is attached to and vice versa a operation in c a load will be needed for one of the sons of w in the number of instructions we need to a stack positions b loads and c stores another issue to be dealt with is sketched in figure in figure deciding on the right to the subtree for y may depend on where z is attached and the best place to z may depend on the requirements for u and v where the requirement for u depends on where y is attached these difficulties will be resolved in section by using dynamic program we adapt the dynamic programming approach of to construct an o d n algorithm note that the instruction set can be restricted by making the costs of some of the instructions infinite stack requirements central to the optimal algorithms for trees in is an estimate of the resource requirements in number of registers of an expression this estimate evaluation order and also the choice of values to be stored this estimate also plays a role in the practical code generators described by johnson and et al the purpose of this section is to indicate how such an estimate can be for graphs evaluated on commutative stack machines in the process we will gain some insights into evaluation order in the presence of common subexpressions as the example in figure indicates a program to compute a graph defines a tree traversal of the graph in which each shared node is attached to one of its since tree traversals play an important role in computing graphs on stack machines we will first consider some properties of tree traversals classical of trees suppose that the given graph is a tree k then as proved in the following labelling gives the minimal number of stack positions required to compute t recall that we are considering commutative machines which have all four kinds of operations ­ let u be a node in a tree the label ru is defined as follows ru r r if u has sons v and w and r rw ru rl if u has sons v and w and ry rw an auxiliary problem the first extension of the simple tree case is the following problem problem ap consider a tree t with a distinguished leaf z the computation of z requires an unknown number a of stack positions see figure express the minimal number of stack positions required to compute the tree without stores as a function of a and the tree u if we ignore the fact that the leaf z is distinguished then the minimal number of stack positions required to compute each node without stores can be determined using the classical labelling above for node a this label is r let r represent the number of stack positions required for u if z requires a positions if node u is not an ancestor of z then clearly ru ru we will show how another number can be determined so that the requirement ru of an ancestor u of z is given by the conditional r a r a in the above expression we write pa b for if p then a else b determine su as follows r consider an ancestor u of z let the sons of u be v and w and without loss of generality let z be in the subtree for v if r r then if r r then ru if rv then u if r rl then u rl theorem consider an instance of problem ap with a distinguished leaf z that requires a positions for each ancestor u of z node u requires ru positions as ex figure we can decide on whether the subtree for z should be attached to x or y without looking at the structure of the subtree for z in equation we prove the theorem by induction on the height of node u basis height o then u being an ancestor of z must be z since r r simplifies to a which is given as the requirement of z inductive step height let v and w be the sons of u and without loss of generality let z be in the subtree for v case this case follows from the inductive hypothesis since and u so r is exactly r moreover if v is computed before w then rr positions are enough to compute u case rr in this case so r simplifies to a which is clearly a lower bound since computing v and then w requires no more than ru registers case and hence from the definition of we get ru a the only part to about is when ie cy and under these conditions using and then a r a a rw since the subtrees for both v and w require at least rw positions ru must be at least rw again rw positions are enough since we can hold w on the stack and then compute v using rv r positions case r and hence and yielding ru a again the part of interest is when r which occurs when a and a ru the only possible value for a is ru thus since the subtrees of both v and w require at least rw positions under these conditions r must be at least lr as in case rw positions are enough u one shared node consider node u as shown in figure the number of positions required to compute u depend on the order in which nodes v and w are comput a leaf is at height o a x is at height plus the maximum over the heights of the sons of x ed if w is computed before v then we need max rw r positions for u where r is determined relative to node z as in theorem interestingly enough we can show the following lemma lr if proof we prove the lemma by considering two cases case i a in this case since we are given that r rw we get lr lr case r r we get in this case a ix since a a a lr ir the lemma is therefore true u the implication of lemma is that in order to decide if the subtree for z should be attached to node x or y it suffices to treat z as just another leaf use the classical scheme on the subtrees created for v and w and then the subtree for z to node x or y depending on which of x and y would be computed first if z was a leaf in contrast consider the discussion in p where a similar problem is the following auxiliary problem is a generalization of these observations to the case where the shared node has k problem ap consider a tree t with a set of dis leaves z during any computation of the tree exactly one leaf in z the first to be computed requires an unknown number a of stack positions the remaining elements of z are treated as ordinary leaves find an ele ment z in z such that assigning the requirement a to z the total number of positions required for the tree t theorem problem ap can be solved as follows traverse the tree in order ie at node u with sons procedure begin mark u old for each son v of u do if v is marked new then if v is shared then if then su v then begin if then su sv end else error then null s uo then so s u su s v then error else begin if then su sz end end end search root if then error figure a binary dag is if and only if the above algorithm halts without error note that v represents the of v v and w traverse v before w if rv rw assign the requirement a to the first distinguished leaf encountered in this traversal let t be a tree with k distinguished leaves we prove the lemma by induction on k basis k if k is then the problem is trivial and if k is then the basis follows from lemma inductive step k let u with sons v and w be such that at least one distinguished leaf is in the subtrees for each of v and w we will concentrate on node u since the requirement of node u the requirement for the whole tree from the inductive hypothesis we can find distinguished leaves x in the subtree for v and y in the subtree for w such that x resp y is the first distinguished leaf to be reached in an order traversal of the subtree for v resp w if r r then from lemma the requirement of node u is by assigning a to node z thereby proving the theorem u discussion starting with figure we have referred several times to the fact that the computation of a graph defines a tree traversal that each shared node to one of its a precise definition of can be given by showing that a program p in which all stores are defines a depth first traversal d of a graph g the key idea is that if a node w is below v in the stack then all nodes reachable from w must be traversed before v and the nodes reachable from v are traversed associated with traversal d is an integer sd which gives the number of stack positions used by d the minimum over all traversals gives the number of stack positions needed to compute g theorem can be applied to determine the minimal stack requirement and a corresponding evaluation order of a graph with more than one shared node just find a subgraph with one shared node determine the local evaluation order for this subgraph then the shared node to one of its this leaves the graph with one less shared node and the process can be repeated in fact this process can be implemented very easily in the context of the algorithm in section to test if a dag is algorithm to binary dags figure gives an algorithm to test of a binary dag each node has two or no sons the purpose of sx at node x is as follows if s uo when we from node u then there are no edges entering a node in the for u from outside the for u if su is nonzero then su represents the node u that has an edge entering it form outside the for u any other nodes in the for u that have edges entering from the outside must be dominated by u for the entire dag to be when a son v of u is processed we first check if v is shared if v is indeed shared then either v becomes su or v was already su and the newly found edge u v represents a duplicate edge that will be removed by transformation in the definition of dags if v is not a shared node then s vo says that the for v to a single node so v into u by transformation otherwise if then we will eventually get a chain u vs u where v will be eliminated by transformation finally if s us v then we have found two paths from u to su that will eventually by transformation we will now show that the algorithm in figure works as lemma let g and g be any dags such that g transforms to g in one step under transformations then g is accepted by the algorithm in figure if and only if g is accepted by the algorithm proofi the idea of the proof is to show that given an accepting computation of g we can construct an accepting computation of g and vice versa there are three cases one for each transformation case suppose edge u v and node v are eliminated by transformation then v is a leaf and leaves s vo if the call is eliminated then we get a computation of g that accepts exactly when the computation of g accepts case node v is eliminated by transformation this case is very similar to case case this is the interesting case if v is not shared in g then v must be the only son of u in g recall that we start with binary dags and the number of sons of a node is under the transformations in this case when we from v in g we will set su sv to dag g node v in g has two edges entering it both from node u when the first edge from u to v is traversed su will be set to v when the next edge u v is traversed su will be set to sv as in the computation of g the case when v in g has more than one edge entering it is similar since the are appropriately u since the graph consisting of a single node is accepted it follows immediately from lemma that the algorithm in figure accepts all graphs lemma if the two points in the algorithm where ele ments of are assigned to are never reached but the binary dag is accepted then the dag is a tree when a shared node v occurs its u has su set to v the only way an ancestor x of v can have sx is if the points mentioned in the statement of the lemma are reached since these points are never reached and the graph must be a tree u theorem a binary dag is if and only if the algorithm in figure terminates normally moreover the algorithm takes time at most linear in the size of the dag proof it is immediate from lemma that if the dag is then the algorithm will terminate normally we will show by contradiction that all dags accepted by the algorithm are suppose the theorem is false and there exists a dag that is accepted by the algorithm but is not let g be the smallest in number of nodes and edges dag that is not but is accepted by the algorithm we show that then there exists a smaller dag than g that is not but is accepted by the algorithm there by deriving a contradiction consider the first time that one of the points where ele ments of are assigned to is reached from lemma and the fact that g is not one of these points must be reached let the point be reached during an execution of search w and let x be the son of w that is being processed if x is shared then there must be two edges from w to x by deleting one of these edges we get a smaller graph than g that is ac from lemma it remains to consider the case when x is not shared consider the execution of since sx there must be exactly one son say y of z such that y has a shared node in its from the algorithm for any other sons y of x s y but then from lemma the for y must be a tree elimination of y and its subtree yields a smaller graph than g so y cannot exist and y is the only son of x but then x has one edge entering it from w and one edge leaving it to y so x itself can be eliminated yielding a smaller graph it is evident form the algorithm that the time taken is linear in the size of the dag the theorem must therefore be true u the algorithm in figure provides a framework in which the number of stack positions needed for a graph as in section can be determined while su is being computed the r values can also be computed similarly the algorithm provides the framework in which the dynamic programming approach of section yields optimal code from graphs optimal code in this section we consider a machine with a fixed number d of stack positions and a cost c associated with each instruction i we will show how least cost programs for this machine can be generated from dags if a dag is large enough it will not be possible to compute the dag in one piece ie it may be necessary to start by computing some node storing the value of z thereby the stack and then working on the remaining nodes changing our viewpoint consider a node u in a dag we are interested in the part of the computation that leaves the value of u on top of the stack since the for u may be very large it may be necessary to empty the stack one or more times during the computation of u let us represent the computation of u by pq where at the end of p the stack is empty and at all times during q at least one value is on the stack there are other nodes besides u that have to be computed and when we the instructions for the entire dag we need to ensure that there enough stack positions to compute u since p leaves the stack empty the instructions p can appear right at the beginning when all d positions are available so it is only the stack requirement of q that when all instructions are put together when we talk of the number of positions used to compute u we will refer to the number of positions used during q aho and johnson make a similar point consider the subgraph nodes w and y in figure suppose there are no shared nodes in one source of uncertainty is the number of stack positions used to compute the for y let k be a parameter representing the number of positions used to compute y given a value of k we can determine the minimal cost of computing cw this minimal cost is not enough to solve the problem however in figure the number of instructions used to compute the dag in a uses one more stack position than necessary so we will use another parameter j which represents the number of stack positions with which y is computed at each node u in we will determine jk the minimal cost of computing node u using at most j positions during which y requires k positions note that it makes sense to consider k j since node y can be and stored as discussed above in the process of computing k jk we will use another k which represents the cost of computing u subject to the condition that at least one store occurs on the path from u to the shared node y the cost of computing and storing y is not included in either kj k or k if then the value of u must ap in memory at the end of the computation otherwise it must be on top of the stack similarly if k then then the value of y appears free of cost on top of the stack if y is reached with k positions available if k o positions are not available then y has to be loaded if ko then y is taken from memory the first we consider is jk if k j or if then j k is o otherwise it is the cost of load ing y since the stack must be after y is computed and before u appears on the stack in k it follows that k is o if and is the cost of load ing y otherwise for all nodes u if y is not in the for u then j jk and k is ignored let u have left son v and right son w and let y appear in the of both v and w there are four cases depending on the instruction z that computes u case i is an kj o cz o ci k case i is an o c c kj o ci kl k and k correspond to and instructions respectively using these costs we get cj c cj cj cal for all other values of j load the computation of s j k is similar returning to figure suppose that j and kw ij have been determined relative to node y we will identify node y by writing s i j and k ij in order to proceed we need to determine ki k and ki k is the minimal value of kj k plus k ij as j between o and d again the computation of j k is similar note that it takes o d time to determine k theorem given a machine m with d stack positions and a binary dag d with n nodes optimal code can be generated from d for m in o d n time the overall approach is similar to that in the cost matrices and su are determined this computation can be embedded in the algorithm to recognize dags in figure once the minimal cost is found the exact sequence of instructions that led to the minimal cost can be found · discussion we have studied a class of dags from which optimal code can be generated in polynomial time for stack machines the results can probably be extended in a number of directions heuristics in section we considered commutative machines which have all four kinds of operations the major distinction between these machines and register machines is that the values of common subexpressions can be and later reused without having to store them one possible heuristic would be to determine the evaluation order for a dag with a stack machine in mind and then determine code for the register machine with this evaluation order in mind a related problem is given a fixed order of evaluation for a dag how difficult is it generate optimal code for a register machine general graphs in the evaluation order is assumed to be given and the number of registers used in the presence of control flow is optimized an obvious next step is integrate known results and treat the full problem of code generation references h m and t scheduling to minimize maximum cost subject to precedence constraints operations research to appear a v aho and s c johnson optimal code generation for expression trees july a v aho s c johnson and j d unman code generation for expressions with common subexpressions jan a v aho s c johnson and j d unman generation for machines with tions fourth acm symposium on principles gramming languages jan code pro a v aho and sethi how hard is compiler code generation lecture notes in computer science a v aho and j d unman principles of compiler design reading massachusetts j c register assignment algorithm for generation of highly optimized object code ibm j res develop jan john and t the generation of optimal code for stack machines july john and sethi code generation for a machine jacm july a p on programming of arithmetic opera tions cacm aug william a class of register allocation algorithms rc ibm thomas j watson research center heights new york s c johnson a portable compiler theory and practice fh acm symposium on principles of programming languages jan and c j register assignment algorithm ii rc ibm thomas j watson research center heights new york on compiling algorithms for arithmetic expressions aug and sethi a comparison of instruction sets for stack machines th annual acm symposium on theory of computing may r r on arithmetic expressions and trees cacm j and c e the number of two terminal series parallel networks j math sethi and j d unman the generation of optimal code for arithmetic expressions oct w m optimization in and eds an advanced course springer verlag w a et al the design of an compiler e f general register assignment in the presence of data flow rc ibm thomas j watson research center heights new york a v aho j e hopcroft and j d unman the design and analysis of computer algorithms r e tarjan depth first search and linear graph algorithms siam j computing june o 