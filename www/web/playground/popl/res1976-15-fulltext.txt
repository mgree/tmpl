mathematical semantics and data flow programming paul r k o s i n s k i project mac massachusetts institute of technology cambridge massachusetts and ibm t h o m a s j w a t s o n r e s e a r c h c e n t e r heights new york abstract a data flow program is a flowchart like network of operators which compute concurrently dependent only on the availability of the data which flow along the paths each operator has only a local effect transforming input data to output data although operators may exhibit memory and thus not be functional from an input to an output all operators are functions from input sequences to output sequences this plus the strong locality of effect allows of semantics more readily than traditional languages which are with storage and this paper proves the semantic behavior of some elementary data flow programs and proves that certain optimization transformations preserve other behaviors background in the past several years the math specification of programming language semantics has been much investigated there have been two main lines of attack on this problem the axiomatic app r o a c h t a k e n by h o a r e and the i o n a l a p p r o a c h t a k e n by s c o t t and in the axiomatic approach each primitive operation in the programming language has assigned to t one or more axioms which formally specify the effect that the operation has upon the state of the abstract machine when that operation is executed that is the axioms describe the mathematical relationship between the before state and the after state a sequence of operations have an effect which is the composition of the individual relations for the component operations thus given a program together with a set of axioms one can determine by theorem proving manual or automatic whether the program does indeed satisfy its axiomatization alternatively one can derive a theorem which describes the programs behavior in the functional approach each primitive operation is assumed to compute a particular function thus a sequence of operations computes the function which is the composition of the component operations functions if the operations are performed repeatedly as in a while loop the composite function is not so easily determined in the axiomatic approach an inductive proof is needed setting up the functional equation corresponding to the loop one gets fx if then else x where test is the predicate of the while body is the function computed by the body of the loop and f is the function computed by the loop as a whole this is a recursive definition but it is hard to solve because the unknown is the function f such e q u a t i o n s can be solved in certain by means of the y or fixed point operator contribution has been to show that there exist lattices called reflexive domains in which the y operator can always apply to give the unique minimal fixedpoint solution of such equations and that such domains characterize programming languages this approach can be applied to applicative languages with relative ease since such languages are based on the idea of functions and function composition unfortunately applicative languages are used for programming even lisp has operators such as the go setq and the effect of such operators is to make the functional characterization of the program considerably from the syntactic structure of the program this occurs for two reasons first since some operators such as a s s i g n m e n t eg setq or worse change the state of the whole abstract machine the function corresponding to such an operator must transform states into states then in order to be composable all operators must transform states whereas the program is written as if most operators transform variables second control flow operators of which go is a example can cause both the conditional and the loop structure of the program to become arbitrarily complicated structured programming with i n s i s t e n c e on a l i m i t e d d i s c i p l i n e d set of control operators ifthenelse and prevents the second problem from occurring that is one recursive equation corresponds to one loop the first problem remains however since most existing languages have state transforming assignment operators data flow semantics a data flow programming language has the basic mathematical simplicity of applicative languages without most of their operators in transform their inputs to their outputs without ever the state of the rest of the program since there is no control flow there is no goto in of this loops may be as well as recursion most significant though is the fact that unlike ordinary applicative languages programs may exhibit memory behavior that is the current output may depend on past inputs as well as the current input memory in is not primitive but is like other operators thus its effects are local like those of other operators and it does not the semantics of programs a program is a directed graph whose nodes are operators and whose arcs are data paths data in are pure values either simple like numbers or compound like arrays or records there are no addresses in although certain operators may be to interpret input values in a manner of addresses an operator when its required inputs are available on its incoming paths after a variable amount of time it sends its outputs on its outgoing paths it is not necessary that all inputs be present before an operator it depends on the particular operator similarly not all outputs may be produced by a given synchronous operators only when all their inputs are present and produce their outputs all at once they are to subroutines some operators produce a time sequence of output values from one input value or conversely they are analogous to the operators in a program thus operate in parallel with one another subject only to the availability of data on the paths an operator may either be primitive or defined an operator is defined as a network of other operators which are connected by data paths such that certain paths are connected on one end only these paths are the parameters of the defined operator a defined operator operates as if its node were replaced by the network which defines it and the parameter paths to the paths which were connected to that node thus recursive operators may be defined sufficient synchronization signals are passed with the data on the paths so that operators do not and so that the operation of the program as a whole is independent of the of the component operators at least in basic full allows timing dependent programs in order to with the real world but it is not yet possible to the semantics fortunately the synchronization mechanism is implicit in the presented here there are six primitive operators in d f p l s h o w n in f i g u r e i of these three are simple in their behavior the constant the fork and the primitive computational function pcf this latter is really a whole class of including the usual arithmetic logical and aggregate operators eg construct and select these three operators all have the property that they demand all their inputs to they produce all their outputs the constant is a case having no inputs furthermore each is independent of any past history that is the operator is a function from current input to current output the functional equations for the operators in figure are thus x c for the primitive constant x y for the pcf f x u y u z u for the fork the next most complicated operators are the switch operators also shown in f i g u r e i t h e s e two o p e r a t o r s also have the property that each is independent of previous but not all are upon each the switch for example demands c and u as inputs for each but only one of x y and z receives output which one is determined by the value received on input c the output value is just the value of u the switch operates conversely only one of the inputs x y and z is accepted upon c is d e m a n d e d and its v a l u e is always sent out on u since these operators sometimes do not we can not describe their functional behavior by such simple equations as before not producing an output is not the same as producing a null output but we can describe their behavior if we view them as functions from sequences of inputs to sequences of outputs now the functional equations for both kinds of switches are one origin indexing is assumed u x y and z where x k u if c k i j l c i l ym uj if cj uj if cj k i j the notation q means the number of times the length j prefix of c takes on the value q thus roughly speaking the switch merges two or more sequences into one sequence the same length as the control sequence conversely the switch splits a data sequence into two or more sequences dependent on the values in the control sequence in all cases the order of the input sequences is preserved in the output sequences the most complicated primitive operator is the loop shown in figure also the loop provides the of the standard leading test while loop of ordinary programming languages the loop operator also has the property that it does not all of its i n p u t s outputs each time it its however is a two stage process that introduces a phase shift of one unit in mapping input sequences to output sequences thus allowing construction of iterative loops and even an to memory or storage in conventional languages the four paths connecting to the loop in figure can be characterized as follows x is the initialization value y is the current iteration value z is the feedback value which becomes current on the next iteration and c is the control value which tells the loop whether to stop or take another iteration although other loops can be such as one having a final output value they can all be from this minimal loop plus the primitives above the precise functional equation for this loop is y where y xl yk if kl yk if kl j ik ic i viewed over time the columns the loop operates as follows the value carried on a path appears under its name if appropriate x x y y y y y x z z x z · z z z z z · c c c c c i i i i now the first three operators can be as functions from sequences of inputs to sequences of outputs x c for the primitive constant xi f x u i v i w i yi for the pcf f x u y u z u for the fork a synchronous operator s is defined as one whose function is such that y sx where x first j elements of x ie there is one output for each input but that output may depend on past inputs also this property of synchronous operators allows us to avoid the of using a separate index for the sequence of values on each data path all paths in a of synchronous operators may share the same sequence index since that behaves like a single synchronous operator in general any operator constructed entirely out of synchronous operators is itself synchronous and the fork and all pcf operators are synchronous all primitive operators are in the sense that an output cannot be affected by future inputs that is once an output is produced it cannot be changed more precisely if y fx x fx yz fx ji t h e n optimization one can prove that natural of o p t i m i z a t i o n t r a n s f o r m a t i o n s p r e s e r v e the functionality of certain programs for example in figure we see the application of common subexpression elimination the before and after program compute the same function for any operator f referring to the before operator d e f i n i t i o n in f i g u r e we see that x i xi xl yi yi yi z i z zl by the d e f i n i t i o n of the fork operator hence x x x y y y and z z z so v w v and w therefore v v and w w by similar reasoning in the after operator d e f i n i t i o n of f i g u r e v v and w w thus the two operators are equivalent for any operator f since have such simple functional properties we will omit them as explicit operators in our proofs and just label all paths connected to a fork with the same symbol in f i g u r e we see the a p p l i c a t i o n of that is moving a computation out of a conditional expression the operator f is moved to the front of the conditional and the operator g is moved to the for this optimization to apply it is sufficient for f and g to be s i m p l e f u n c t i o n s of t h e i r inputs eg that is vi h i f a i b i zl to prove this we shall assume that d and e are synchronous operators and that a b c and m are mutually synchronized input paths so that we can use the same index for all of them if these assumptions were not valid the network would up the proof consists of three parts first s h o w that r r s s s e c o n d s h o w v v w w using the obvious result that u u t t and third show that z z we will prove the first part in fair detail the second part is obvious and the third is just like the first i by a s s u m p t i o n r if c j l k i s j l c i l by definition of r if ci by and above pk bj if k i s j c i l n k aj if c j i both by definition of r k if k i by assumption for f and above r k r qed s i m i l a r l y we can p r o v e s k s kv thus the first part of the proof that preserves the semantics the first and third parts of this proof stand as separate theorems in themselves they would not often be used however b e c a u s e u n b a l a n c e d s w i t c h o p e r a t o r s ie an without an or would be used in programs the most interesting kind of operator is one which behaves like a memory cell a trivial kind of memory cell which serves as the building block for f a n c i e r ones is shown in f i g u r e it is just a holding that is the output is what the input was on the previous more precisely it can be shown to satisfy the following equations y q yi the proof is straightforward i w q by definition of the primitive constant y w by d e f i n i t i o n of the loop yk if z k l l by definition of the loop operator t r u e x k yk by definition of the pcf true y q by and above yk x k i y k l by and above qed a memory cell is shown in f i g u r e w h e n a value is p r e s e n t e d on the control path c the current contents is read out on path y when a value is presented on c and a data value is presented on the input path x the cell is updated to contain that new value the cell has an initial contents of q viewed over time the mem operator behaves as follows c c c c c c c i i i y y y q q r y t x x x r s t the precise formulation of this behavior may be proved to be y q if cz yj x k if ki the proof of this follows a q by d e f i n i t i o n of hold bz by d e f i n i t i o n of hold a yj if c j i £ c i by definition of wk if k i s £ c i l by definition of b z yj if c j i £ c i by definition of z k if k i £ c i l by definition of x k by d e f i n i t i o n of operator if by ° and above if by and above i yj q if ci by i n d u c t i o n on i and above ii yj bj k if by above bj k m if by induction on above yj m if by ii and above m x k if by and above · yj x k if c k m i v i m by and above yj x k if from above by simplifying the if condition making use of the fact that m is arbitrary in the range to yj x k if from above since implies that k i j k l c i l j i j k l c i steps i and above are the desired results for the behavior of the memory cell more complicated memories may be by substituting other operators for the fork and o p e r a t o r s in f i g u r e for example by replacing the fork by a operator and the i by an a queue memory results to program a random access memory another input path to carry the address must be added as well as replacing the operators to make our domains and of data value sequences into lattices we have to define a partial order on them following g kahn we say that a sequence a is than a sequence b if and only if b is a prefix of a the set of sequences including infinite ones form a lattice under this partial order the bottom of this lattice is the empty sequence this lattice does not the scott notion of value approximation that requires further investigation the operator obtained by connecting the output of the hold operator to a two way fork connecting one fork output back to the hold input and making the second fork output the output parameter of the defined operator is the constant operator it is characterized by the equation x i q i that it satisfies this equation can be proved inductively as above another way of proving it is to use the lattice fixedpoint approach to do this we note that our earlier notion of causality exactly corresponds to monotonicity in the lattice we make the further assumption of continuity which corresponds to the reasonable assumption that an operator will produce output after a finite sequence of inputs or not at all then referring to our previous description of the hold operator as a function which transforms any input sequence to an output sequence which is the initial constant to that input sequence we see that the minimal fixedpoint of this function is the infinite sequence of that constant value a more detailed proof of an almost identical situation appears in g k a h n currently does not allow operator valued data and thus does not require the existence of reflexive domains in of this allows iterative and recursive programs both in the practical and mathematical it is that can be extended to allow operator valued data in the future and that this extension can be with techniques conclusions we have shown that it is possible to develop a mathematical semantics of in terms of functions from sequences of inputs to sequences of outputs this is not complicated by the of memory because memory is local like all other operators nor by the presence of control flow which leads to continuations the for dealing with input and output sequences is not all bad many programs such as database sys are inherently nonterminating and cannot be reasonable viewed as simple functions from an input to an output as it currently stands has a primitive operator which is or timing dependent in its operation it would be extremely desirable if it could be characterized as a mathematical function also to do this would probably require the functions to take pairs as values thus the entire system of axioms theorems and proofs the theorems of particular interest in this new system would be those which show that certain defined operators although in their internal operation are completely determinate when considered as atomic operators then those parts of a program which have to be in order to deal with the outside world could be so whereas other parts of the program could be determinate and thus simpler to analyze this duality of determinate operators and operators suggests the need for a convenient transformation between them if programs are viewed as an algebra then morphisms between such algebras might be defined in fact the process of compiling one program into another with simpler operators can be analyzed as a particular morphism the approaches set in this paper will yield a practical applicability of mathematical semantics to more realistic programs than possible references i first v e r s i o n of a d a t a flow procedure language mit project mac computation structures group memo pr a data flow programming language ibm research report rc march car hoare an axiomatic basis for computer programming comm acm pp october d scott outline of a m a t h e m a t i c a l theory of computation proceedings of the fourth annual princeton conference on information sciences and systems pp fe allen and j a of optimizing transformations ibm research report rc september g kahn a p r e l i m i n a r y theory for parallel programs laboratory report january primitive constant u y fork u uv w x sy primitive computational function j loop xy z xyz switch switch figure o i e jc i j ill ii ii li l i i i i i l i fill l i i i i b i i i i i i i li li zd l ii i i d i ii d i i i i i w j ml j i f z z o i w i ii i if c o f j ii i z ill m iz iz i 