conference record of he fifth annual acm symposium on principles of programming languages monoids for data flow analysis k rosen computer sciences department research center heights introduction the data flow analysis research dealt with concrete problems such as detection of available expressions and with low level representations of control flow with one large graph each of whose nodes represents a basic block several recent papers have introduced an abstract approach dealing with any problem ble in terms of a semilattice l and a monoid m of maps from various algebraic constraints examples include cc gw ki ta ta we several other recent papers have introduced a high level representation with many small graphs each of which represents a small portion of the control flow information in a program the hierarchy of small graphs is explicit in and implicit in papers that deal with syntax directed analysis of programs written within the of classical struc programming sec examples include tk the abstract papers have the low level tions while the high level papers have the concrete problems of the work this paper studies abstract conditions on l and lead to data flow analysis with on high level representations unlike some methods oriented toward structured programming tk wu our method the ability to with arbitrary escape and jump statements while it exploits the control flow information in the parse tree the general algebraic framework for data flow analysis with is presented in section along with some preliminary lemmas monoids properly include the gw section relates data flow problems hierarchies of small graphs introduced in high level analysis begins with local information expressed by mapping the arcs of a large graph into the monoid m much as in low level analysis but each small graphs represents a set often an infinite set of paths in the underlying large graph appropriate members of m are associated with these arcs this local information is used to solve global flow problems in section the fundamental theorem of section is applied to programs with the control struc tures of classical structured programming in section for a given monoid m the time required to solve any global data flow problem is linear in the number of statements in the program for varying m the time is linear in the product of thk number by t where t is a parameter of m introduced in the definition of ty for reasons sketched at the beginning of section we feel to with source level escape and jump statements as well as with classical structured programming section shows how to apply the fundamental theorem of section to programs with escapes and jumps the explicit time bound is only derived for programs jumps between obtained by section which also contains examples of monoids in the full paper finally section proofs of are omitted to save space the full paper will to a journal we proceed from the general to the particular except in some places where the rule a little makes a significant improvement in the flow common mathematical notation is used to avoid parentheses the value of a function f at an argument than fx if fx is itself a the result of applying y the usual s and symbols for arbitrary partial orders as well as order among integers to is x y implies fx fy maps are sometimes called monotonic in the literature with a binary operation a such that x a the greatest lower bound of the set tx y a meet semilattice every subset has a greatest lower bound is complete in particular the empty subset has a greatest lower bound t so a complete meet semilattice maximum element is a set together with an associative binary operation o that has a unit element m m o m for all m in all our examples the monoid m will be a monoid of functions every member of m is a function from itself the operation o f o gx and the unit is the identity function with x x for all x two considerations the notational choices first we in ways that are common in mathematics and are convenient here second we try to facilitate comparisons with gw to the extent that the among these works permit is between gw ki and the join of ta we where least upper bounds are considered instead of greatest lower bounds in applications that are intuitively stated in terms of what must happen on all paths in some class of paths in a program while to of joins is more natural in applications that stated in terms of what can happen on some paths whether there the relevant class and by using that is equivalent to v join oriented applications can be reduced to meet oriented ones and vice versa a general theory should in one way or the other and we have chosen meets for us strong assertions about a programs data flow are high in the semilattice algebraic framework throughout this paper l will be a complete meet a maximum element t the greatest lower bound of a set x will be denoted a x in addition to l there is given a monoid m of maps f l l such that m k closed under meets f a g k the map such that f a gx fx a gx for all x in l this framework k a natural generalization of the information propagation space gw p l is the set of all subsets of a given finite set and s k set inclusion with f s g iff fx s gx for all x in l m k a meet semilattice with the pointwise meet operation we will further assume that m has a maximum t which with the maximum in the complete semilattice of all maps on l t x t for all x in l this assumption can be we say that m k a closed monoid of maps on l the monoid m is idempotent iff each f in m is idempotent in the sense monoid m is ast gw p iff each f in m satisfies which is a weaker condition than idempotence on the other hand every fast monoid that arises in gw is actually idempotent as well there are important data flow monoids that are not idempotent and they are also not fast for example neither form of the constant propagation monoid cp in p is fast to deal with such monoids we begin by the trick used to deal with them in gw p the closure f of any f in m k the map f af al n where n is the set of all natural numbers more generally the following definition considers monoids certain expressions involving greatest lower bounds like can be approximated definition the monoid m is iff there is given a binary operation on m and a positive integer t such that for all f g in m and g f can be computed from g and f at most t steps where any a or o operation is counted as a single step in particular we can define g f to be g o f whenever f is in m and we know how to find it in t steps thus the monoids considered in gw are all in monoids where g o f g f we will sometimes be able to get data flow information than in gw as will be seen in section a simple but important example of is provided by any idempotent monoid we have g f g a g f with t the monoids for traditional global flow problems like available expressions u are for this reason by letting be a binary operation and by putting after composition with g in def we avoid to assume a y a gy the assumption will be called does not always hold p it data flow problems arise when members of m are associated with the arcs of a graph only finite graphs will be considered here our view of graphs is more general than is usual in data flow research a graph g consists of finite sets ng the nodes and ag the arcs together with maps sg and tg from ag to ng these are the source and target maps in arcs are drawn as arrows from sources to targets of course the g subscripts are sometimes omitted if there k no as to which graph is intended a path is a finite sequence c c ck of arcs such that i whenever k k the null sequence with k o is allowed and is denoted l a nonnull path is from the node sc to the node we map notation with c sc and of sources and targets for nonnull paths as well as for arcs definition given a graph g and a map f ag m the extension of f to paths is also denoted f and is defined by fc if c x and ck if c a when a program is represented by a graph g we have a map f a m with fc l l for each arc c this local information tells how an assertion x in l associated with sc is propagated to tc as a transformed assertion y when control flows along c there is also a set e c n of designated entry nodes and an initial assignment e e l of entry assertions with en being true whenever control enters the program at n note that en need not be true whenever control reaches n from within the program itself knowing what is true at entry nodes at the time of we want to determine what is true at all nodes at all times definition a global flow problem ij is a g f e e where f ag m and e el a for p is any i ng l such that whenever m is in e and c m n is a path in g in def generalizes the definition of a safe assignment gw p in several ways that are crucial here we do not assume that e consists of a single node m and that all nodes in g are reachable from m we do not assume that l has a minimum or that em j for m in e if l does have a minimum then we can solve any problem by letting in l for all n this solution k a maximum solution would be ideal but for some choices of m there can be no to find one as in acceptable assignments gw p we therefore consider solutions that are large enough to be interesting but that may be computable with a reasonable amount of effort definition a fixpoint for a global flow problem p g f e e is any j ng l such that for all m in e and c in ag jm s em and s a good solution for p is any solution i such that for each fixpoint and each n in ng j in jn comparing def with def it is clear that any fixpoint is a solution in many examples l is well there are no chains in this case a maximum fixpoint can be found by the obvious fixpoint algorithm when distributivity holds the maximum fixpoint k the only good solution however fails for some important problems and here there are good solutions that are not fixpoints examples are in gw fig like the methods of gw ta our method of finding a good solution will sometimes find one that is better than the maximum fixpoint instead of a global flow problem directly high level analysis considers a hierarchy of problems with smaller graphs each of which represents a small portion of the control flow information about a program good solutions for the auxiliary problems are combined in a good solution for the original problem because g and e will vary in much smaller ranges than f and e as we move through the hierarchy of auxiliary problems we will find it helpful to consider global flow schemes pairs g e such that g is a graph and e is a set of nodes in g thus a scheme can be out to a problem by adding f ag m and e e l perhaps we can simultaneously solve all the problems derived from a given scheme by working with formal expressions definition a flow expression for a given scheme g e is any formal expression built with set of operators a using paths in g as variables and a symbol t as a constant given a flow expression x and f ag m the value x f in m k only defined if m k or x is free of in that case x f t x in a o now we want to assign flow expressions to nodes in a scheme so that any problem derived by out the scheme with f and e can be solved by evaluating expressions definition a solution for a scheme g e is any map x assigning a flow expression xm n to each m n in e x such that whenever m is in e and c m n is a path in g xm n f fc for all f ag m definition a flow cover for a scheme g e is any solution x such that whenever j is a fixpoint for a global flow prob lem g f e e derived from g e jn s f jm for all m n in e x ng in the terminology of ta p the value xm n f is a tag for the triple m n p where p is the set of all paths from m to n in g the technical realization of the similar ta is quite unlike what happens here behind p lemma let g f e e be a global flow problem suppose g e has a flow cover x then a good solution for is obtained by setting for each node n in g n f e lemmas ­ in the full paper show that any scheme g e has a flow cover if m is the flow covers constructed in proving the lemmas are optimized for the most common schemes in sections and lemma let x be a flow cover for g e and let h be the result of deleting some arcs from g for all m n hr e x let ym n be like xm n except for replacing each path c that involves a deleted arc with the corresponding expression d o t where d is the largest suffix of c as a string of arcs that is free of deleted arcs then y is a flow cover for h e hierarchies of graphs in the control flow in a program is represented by a hierarchy of small graphs rather than by one large graph the relevant abstract definitions from sec will be here for ease of reference as in it is convenient to assume that no two arcs have the same sources and targets so that the arc c may be identified with the pair sc tc of nodes a nesting for g is a finite partially ordered set z with a maximum m together with a set na of nodes and a set aa of arcs for each a in z the following properties are required ng and an ag f a in implies n na and a q act c in ag has sc tc in na c is in as a nesting structure with and exits consists of g and as in together with for each a in sets of designated and designated exits na and q na such that whenever a in fl n and fl n and exit sets are defined by n given a nesting structure with and exits consider any a in n in and p in let ha n p if there is a path from n to p in g only nodes of na and n p o otherwise these path bits can be computed bottomup beginning with choices of a that are minimal in z path bits for a can be determined from previously computed path bits for parts of a where al y a as in sec we construct in the set of nodes in ga is the induced graph ga for each a where no u u part u no na ­ u the set of real arcs of ga k cc for each in there is a set of arcs n p i n c p p n p the total set of arcs k u u with the obvious definitions both real and of sources and targets some arcs are definition a nesting structure with and exits is locally covered by assigning a flow cover to each induced flow scheme ga one can solve any global flow problem p g f e e without computing on g provided that g has a locally covered nesting structure and e the algorithm does compute with z and the induced graphs and it uses a given flow cover xa for each induced scheme ga in section we will exploit structured programming to optimize away most of the work with induced graphs the algorithm requires some preprocessing that the local information in f ag m instead of only what must happen when control flows along an arc of g we ask what must happen when control flows along any of the paths summarized by an arc in an induced graph ga for each a in there are maps fa m and f x m such that for all c in and all m p in x if c c then fc else tm a i c c p p fa if a is minimal in z then and defines fa to agree with f if a is then f still with f on real arcs that are not but on arcs fa is determined by the various fp maps for in these will be available when we try to compute fa provided we begin at the bottom in a linearization of the partial order on now consider the maximum n in because e s is a global flow problem p e e and lemma yields l a good solution to po for each part of n we can now specify a global flow problem where g is the graph whose set of nodes is nd and whose set of arcs is ad rl n x nj with sources and targets as in g this is not the induced graph g and will generally be much larger the map f is the restriction of f to arcs in gj for entry information we use e and ej restricted to e so that depends on the choice of in it can be shown that g has a nesting structure with a in i a so that induction on the size of should lead to good solutions of for each p in combining these with should yield a good solution to the original problem p several technical points must be checked because m may not be distributive we do not have as simple as the induced graph theorem rb sec to the of the auxiliary problems p to the original problem p the first step is to note that p does indeed inherit a nesting structure with induced maps as well as en and exits from p moreover the construction applied p pj yields the same induced maps as to fa fa for all a s these remarks are formulated more precisely as lemma in the full paper we can therefore use induction on i in proving the following three lemmas on the of the auxiliary problems lemma if j is a fixpoint for ij then the restriction to nodes in is a fixpoint for po of j lemma suppose is in part and c m p in g where m is in and p is in exit then fm p s fc lemma extend from to have domain ng by setting ion t whenever n is not in suppose that is a good solution to pj for each in then i ng l defined by is a good solution to p high level data flow analysis given a global flow problem p g f e e with e such that g is locally covered by a flow cover xa for each induced scheme g a we can use the results of the previous section to find a good solution for p we begin by computing the local information of through x recursively we solve the auxiliary problems of and combine the results as in lemma lemmas and lead to a correctness proof by induction on i i but how much does all this cost can the method be optimized to exploit in well structured programs to address such questions we program the algorithm more formally but still at a very high level we also impose an additional condition on the nesting structure for all a in x ar n this is true in all the examples of interest here and it greatly simplifies the analysis of computational complexity a global flow problem satisfying all the conditions imposed so far is said to be nested these conditions are in stating theorem below the following procedure takes as argument some y in the well nested problem p is accessed by solve as a global variable the purpose of calling is to have a side effect on i a global variable whose values are maps from the nodes of g into l the effect of call is to change in for each n in ny so that i on ny becomes a solution to a problem p py defined as in but with arbitrary y in in place of restricted to be a part of m to this effect re begins by calling to find a good tion for p defined as in but with y in place of n the procedures do not construct p and p but we do in the correct ness proof proc y member of the poset z call on nodes in gy i is now a good solution to p for y for all in party do call note that does not explicitly combine the solutions to the p as might be expected from lemma we have already to optimize the method in light of using as a for implicit procedure bodies such as the body of above we write the program solve that gets p as input and puts out a good solution solve p well nested global flow problem i map from ng into l fa map from into m for all a in x fa map from x into m for all a in proc this procedure sets up f and f for each a in has access to induced graphs and to f lo member of thk procedure finds a good solution as in lemma h member of z get p call for all n in ng do in intl call for we assume an arbitrary listing a a s rr of such that ai s j implies i j thus a variable ranging over can be from to m like an integer variable the programming for is omitted here because it is the obvious implementation of in light of if c is in then c is in for unique finally is as suggested by lemma lemma let stand for or for each a in is called with actual parameter a exactly once in the course of solves tion when control passes through call in solve any node n such that in changes must be in na lemma whenever control reaches call in solve each m in entry has im whenever control leaves call in solve i restricted to is a good solution to py to study the correctness and computational complexity of solve it will be appropriate to assume that a and o always require one step while of a member of m to a member of l also takes one step see gw p for more on the reasonable ness of this assumption for the time bound hr the following theorem we assume that each iterator for all var in set is defined by step var through set in some fixed order theorem let p g f e e be a well nested global flow problem there k given a nesting structure for g with and exits such that e and holds and there is given a flow cover xa for each induced global flow scheme ga then solve with input p with output i such that i is a good solution for p moreover let all iterators be defined by through their index sets then the time required apart from inputoutput is ta where al is a x and each a in has ta i i where is i x i and each is the time re to evaluate p fa by lo m p in x for the jth pair proof correctness follows from lemmas and by induction on the depth of recursion in the time required by for each a is i i the call on while the time required by with y a is o by lemma the calls on and in solve require time tal ta i i finally the i ng term comes from i s control structures a special case the input to a compiler is a program not a global flow problem while constructing a problem from a program for the sake of a compiler can also a nesting structure with and exits when the graph and nesting structure are chosen in the way described in the relation between the parse tree and the hierarchy of small graphs is so transparent that most tions with graphs can be optimized away here we are optimizing the compiler itself before using it to optimize programs for of this section a program is always a single statement we use syntax most types of statement are complex built up from smaller statements by control operators like if or do the simple statements contain no smaller statements this section explains how g is constructed but h is see any one of r for a more discussion with examples we a statement node in the parse tree with the corresponding fragment of program text the set z of all statements in a program is partially ordered by s a iff is a descendant of a in the parse tree the maximum n in x is the whole program while the minimal elements are the simple statements the graph g and the hierarchy of induced graphs can both be con in a bottomup pass through the parse tree for g itself each statement nodes and arcs given by up ab where the new nodes and arcs are determined by the control operator used to form a when a is complex we will allow control operators like case that take a varying number of arguments and then a phrase like the control operator used to form should be interpreted as the control operator and number of arguments used to form for all statements considered here has two distinguished nodes entering a and leaving a and the designated and exits of are given by entering a and leaving a the three operators in contribute beyond entering a and leaving a to g they are no new nodes a else a k p conditional statements while statements sequential compound statements figure is a more concise version of figures in giving names to the arcs in and indicating their sources and targets for each of these three control operators the arcs named q or qk for integers k are not in they are used to how we expect the induced graph ga to look when a is a conditional or a while or a sequential compound statement conditions under which these are will be studied but first we consider four more control operators they are a one part conditional statements a case of l jk a do until case statements until statements a for from to by do iteration statements with varying and choices of keywords the until and iteration statements are very the concrete syntax above reflects the authors for short keywords and clean algollike these two statements add new nodes testing a as well as entering a and leaving a to g the new arcs are as indicated in figure as in figure the arcs named q or qk are not in a but are expected to be in the induced graph ga in contrast to the commonly drawn in discussions of structured programming or graph grammars for generating classes of sec lm figures and have no or arcs and no conventions for joining graphs along such dangling arcs the advantages of our greater be come apparent when control structures other than those of classical structured programming are as in r a the control operators considered here will be called csp operators for classical structured programming in addition to the important properties of csp operators are as follows associated with each statement a built from a csp operator is a graph the expected induced graph for a that is determined apart from the names of the nodes and arcs by the operator used to form a thus and for any two while statements a and a are as in figure but with a and a in place of a and with names of the loop bodies of a and a in place of apart from renaming and variations in the number k of statement arguments taken by each control operator like case or sequential composition there are only finitely many expected induced graphs one for each csp operator in our language and each such graph eg has at most one cycle in eg and are ok there is a path from entering a to leaving a in eg no arc has source leaving a in eg the functions of k in will change if the family of csp operators is changed we have omitted more elaborate loop building operators because the escape statements in the next section are sim and more powerful means to the same ends the importance will in lemma of to compose structured programs in the classical sense of sec one builds all complex statements with csp operators each simple statement has no effect on the flow of control control enters the statement something happens and control leaves the statement this the following definition definition the expected induced graph eg for a simple statement has two nodes entering and leaving and one arc entering leaving a simple statement is classical iff the nodes and arcs by to g are given by n entering leaving j entering leaving a program is classical iff every simple statement in it is classical and every complex statement is built by a csp operator lemma let g be the graph for a classical program each statement a has then entering a and leaving a and ga we can now do much of the work of data flow analysis for cal programs at the time of language definition long before any specific programs are compiled with each expected scheme entering a we can associate an expected flow cover as soon as like those in figure and figure are available by lemma will indeed be a flow cover for the actual in scheme ga when we a specific statement a in a specific program by theorem solve can find a good solution to a global flow problem by moving through the parse tree and evaluating formal expressions by the expected flow covers having fixed the set of csp operators we can estimate the costs of evaluating tbe expressions rather the elaborate time bound in theorem will reduce to the assertion that the cost of data flow analysis is i t the expected flow covers are displayed in the following series of lemmas beginning with the sim ones since m is always entering a in exam p it will be con to write rather than entering a lemma let a be a simple statement cover for the expected flow scheme where then is a flow k and leaving a c for the arc c from to leaving a in we omit lemma for one part conditional statements lemma let a be a two part conditional or case statement with parts pk then is a flow cover for the expected flow scheme where a entering bk leaving bk qk for k k q a a qk we omit lemma for sequential compound statements lemma let a be a while statement and let be b q a in figure for a then is a flow cover for the expected flow scheme where a and entering b leaving b q leaving a e we omit lemma for until statements and lemma for iteration statements the y are much like lemma but use expressions o i as needed theorem let f be a global flow problem derived from a classical program then p is well nested using the given flow cover for each statement a solve finds a good solution the time required apart from inputoutput is i z i t proof that p is well nested follows from lemmas with holding because the parse tree is indeed a tree by rem solve finds a good solution all that remains is to special the time bound in the theorem when the pairs m p in x are ordered in the obvious ways suggested by the of in the lemmas the jth exam p evaluation in requires steps with s t except k in lemma therefore ta in theorem is i i i t i i by t and ta is kt for k i i therefore tal is i z i t but x by and the total time is i i t s and s so for example consider the syntax directed available expressions analysis in tk p the stated system of equations is because case uses x for x ins but the four cases only define x for x ins after the obvious it is clear that the method of tk is a special case of solve that exploits the idempotence of the monoid m control structures escapes and jumps complex statements in programs commonly have goals as when one writes a statement that searches a file for the record with a given key for a large file with a complicated indexing structure the search statement could be quite complicated for example a find the record might be in a topdown manner as a b look in fast memory p look in slow if necessary if part of the file is in fast memory and part is in slow if does find the record then there is no need to proceed to z the goal of a has been accomplished and the programmer wants to ensure that control will leave a without anything else in a language like wu this intention can be expressed directly by writing leave anywhere within a perhaps as the then part of a conditional statement of course is the identifier used to label a in the program any statement important enough to be left is important enough to be labelled as an operator leave takes the name of a statement as an argument rather than the statement itself with we have n entering is leaving as in def l but a entering leaving a instead of def the expected induced graph from def will not be relevant for leave designated and exits are still as in but now entering is also in exit for any f with a by of course one can avoid and within classical structured programming by writing something like a true look in fast memory if then look in slow memory and being careful to put enough assignments false into the elaboration of tests of will also be needed what useful purpose is by this exercise one can agree with and of some of the literature escapes or jumps lm p while that escapes and perhaps jumps should nevertheless be provided note that clarity and reliability are the main reasons for prefer ring to the greater efficiency of before the on page of lm is just a by product of expressing programmer as directly as possible no good is done when the programmer codes as and then the compiler back to finally note that leave is quite unlike the escape statements in lm and its main references unlike from the en closing loop or to the next iteration of the enclosing loops body leave escapes from whatever the programmer wants to escape from referring to it by whatever name the programmer wants to use there is no counting and no of escapes with loops if simple statements are to be allowed at all there is no point in allowing statements that are simultaneously less powerful less readable and more to implement than is leave one sometimes wants more power than leave provides the event driven case statement kn za addresses this need but less than the introduced in sec to save space we will only deal explicitly with the simpler escape statement leave in applying theorem those who wish to handle more powerful escapes will find it easy to generalize this section in light of sec definition a complex statement by a csp operator and satisfies a is iff it is entering a u up q part ec leaving a a simple statement is iff it is either classical or a leave statement a program is iff each simple statement is and each complex statement is built by a csp operator equations and above are as in and where is only applied to complex statements the other applications have been added to bring out the between programs and the classical programs of def we omit lemma which says that all complex statements in a program are and goes on to show how to derive the actual induced graphs for such statements from their expected in graphs lemma let a be a simple statement then xa is a flow cover for the induced flow scheme ga where xa if a is classical xa a and leaving a t if a is a leave statement lemma let a be a complex statement in a program let ya be the result of the expected flow cover to avoid using any arc of that is not in ga as in lemma let be the set of all e in ­ ­ such that iff entering se where is the part of a with e e al then xa is a flow cover for the induced flow scheme ga where each node p in has p determined by one of the following cases p yl p if p is in but p leaving a xa p t if p exit ­ and fi entering p xa p i o entering if p c exit ­ and i p is in xa p ya p a a ae e o se if p leaving a we can now generalize theorem to allow escape statements the new time bound in the following theorem is similar to the bound for the method of graham and wegman as applied to programs gw and theorem let p be a global flow problem derived from a program then p is well nested using the flow cover xa from lemma or lemma for each statement a solve finds a good solution the time required apart from inputoutput is i x t where is the sum over all a in z of the numbers i i ­ proof we proceed much as in the proof of theorem using lemmas ­ and ­ the p evaluations for p in with p leaving a are ordered as suggested by the of in the lemmas from section then p is evaluated for each p in not in and hence in exit for a part of a finally we evaluate leaving a as in theorem the time for the jth p evaluation with p in has t except perhaps for the last one with p leaving a in this one case lemma yields si s old ­ where old is the old bound from the proof of theorem on the time required to evaluate and hence ya for m leaving a and is the number of leave statements within a there are also i i ­ choices of j with p not in and in lemma and lemma the term ta from theorem is therefore old where has node set and arc set such that i i i i i i i i ­ now is at most the sum of all the numbers i exit i ­ for in for k i i i and i i are k by and old is linear in kt so ta is l exit ­ l ­ l ­ l but i ng i is still i z i so the total time from theorem is ta i i which is the desired bound s assertions about the importance of exit control structures are in the literature and of structured programming one precise meaning for such assertions might be that syntax and semantics are so simply related as to permit syntax directed data flow analysis at a cost linear in some reasonable measure of program size when the size of a program is the number of statements in it by t theorem shows that cal structured programming is sufficient for such rem shows that the escapes needed for practical structured pro gramming do not this provided they ar used in the number should be fairly small compared to the size of the program because in solve uses a given flow cover for each statements induced flow scheme the time bound in theorem would be of little interest if finding flow covers required more time than using them did lemma shows that we can pass from the expected flow cover to the actual one very quickly provided we know the various path bits fi m n finding all these bits can be done in time i by the obvious adaptation of the rules for computing whether a variable can be preserved along some path through a statement and the use of p in can be as including discovery of p without changing the time bound this a strategy for dealing with programs that are not known to be for let us assume that the only kind of statement besides those already considered is the simple jump goto with we have nc entering leaving as in def l but a entering entering a instead of def as with the expected induced graph eg is irrelevant designated and exits are still as in and entering is also in exit for any with a except in the special case where a happens to be but now we may have as well in we find that entering a is also in for any with s and a there is a new real arc in gy for the smallest y that includes both and a unlike the arcs added by escape statements these new arcs may add many new paths to gy cycles there is no formula like def our for with jumps is based on a version of law whatever can go wrong will but not often a compilers lexical analysis phase can easily detect the presence of jumps in a program well written pro grams in well designed languages will often be free of jumps and hence most of the statements in a program that does have jumps will still be though they may have exits due to goto as well as to leave lemma actually holds for any classical complex statement even when the program as a whole is not the flow cover discovery in can be written as an easy test for whether a is followed by use of lemma or lemma in the common case where this is true when a is complex and not the flow cover discovery routine will need to construct the induced graph ga the time re by the flow cover algorithm from the omitted lemma is in the size of ga but it is only the size of ga which occurs no practical of the size ratio between this graph and the graph of the entire program are available if jumps are used in such a way that few statements fail to be and those that do have small induced graphs then even handling of these statements by need not change the running time of solve on large programs as with escape statements mod is the key in an optimizing compiler along the lines of kn l we must also with introduced jumps the original input to the compiler has few if any jumps but subsequent processing can add them fortunately there is no need to treat added jumps like original jumps precisely because jumps are added according to well understood and well mechanical operations kn p there is no need to apply lemma to the new induced graphs part of understanding and the operations is figures like fig for while loops by boolean expressions with side effects as with the loops of classical structured programming we can find the flow cover long before the compiler a specific while statement in a specific program in writing the compiler we need only write the use of this known flow cover into the handling of while statements by of finite height definition let h be a positive integer the semilattice l has height h iff there is a strictly chain x xl such that all other strictly chains are at most thk long if l has height h then m is but a bound on t in def can be obtained if we consider the effective height instead of the actual height to define effective heights we recall the general cartesian product construction given two sets s and s the usual cartesian product s x s is well known the general construction is similar but it begins with an arbitrary family of sets for each q in some set q we are given a set sq given such a family a q is any map x with domain q such that the value xq of x at q is a mem of sq we xq rather than our usual xq to the ty between general and the more familiar xl xn which correspond to the special case q n the cartesian product of the family sq i q c q is the set of all it is denoted x sq if each sq happens to be a semilattice then the product is also a semilattice with the obvious ordering x y in xq s iff x in sq for all q definition a of l and m is any family lq mq q q of pairs where each lq is a complete meet semilattice and mq is a closed monoid of maps on l such that there are semilattice isomorphisms that make the following diagram commute l xq the left downward arrow is application of a map to an argument while the right downward arrow is application of the map to the argument for example let q be the set of all expressions in a program for each expression q we can indicate whether the expression is available or not by using the semilattice lq o with the usual ordering and the monoid mq generated by the maps describing how a block of text or generates the expression gw p u p the height of lq is instead of solving a separate global flow problem for each expression q it is usual to use a bit vector with one position for each expression the parallelism in and and or operations on arrays i q long of bits is exploited to get the net effect of solving i q i problems in by solving one problem in xq q lq the above definition says why this works in a way that is completely independent of the details of and and or operations the choice of each lq and so on can exist and be useful in many other contexts besides that of bit vectors in the bit vector example q q lq has height i q but it acts as if it had height the common height of all the lq for all purposes of data flow analysis after one more we can prove a lemma that establishes thk kind of behavior in the general context definition the semilattice l has effective height h relative to the monoid m iff there is a lq mq i q c q of l and m such that some lq has height h and all lq have height at most h lemma suppose l has effective height h given g and f in m the loop products and defined by can be computed in at most t l to h steps for q corollary if l has effective with t s h if m is also distributive r height h then m is then and t for any of the traditional bit vector problems the effective height is effective heights can be high in the constant pro monoid cp p or in analyzing the ranges of values of variables in these more forms of data flow analysis the monoids are also not distributive so that is to compute but f yields information the pro could be to specify which loop product is to be used when invoking an optimizing compiler after run time measurements prior to optimization have which parts of the program are critical the compiler could even be to use in a few critical places and elsewhere with high level analysis the programmer and the compiler both control flow in relation to the syntactic structure so detailed communication is possible without an elaborate interface because of technical in comparing solve with any low level method we will only present one simple but general comparison theorem assume l has effective height h obvious of the method of graham and wegman as presented in gw allow it to be applied to any global flow problem for a classical as in def program for example the assumption en l is written into the definitions gw pp but is never used thk is the assumption fails in the application to analysis gw p for a classical program the method finds the same solution that solve does when is defined in a way that is a little better than the same comparison holds for any program but a complete proof would be much too tedious to appear here for each f in m let f if f is fast then f a else f so that m is with g f g f which may have g gw f g f when f is fast theorem let p g f entering r e be a flow problem derived from a classical program rr let be the good solution found by the method of graham and wegman and let be the good solution found by solve with gw then ls proof to each node v in g the method of gw assigns a map v in m such that where no entering t a variable graph g t and a variable assignment f i of members of m to arcs in g are initialized to g and f in the statement of algorithm a gw p g is denoted g and the computations on f are implicit details are in the lemmas from g w sec algorithm a eventually reduces g to a graph with no as the only node if this graph has no arcs then v for v no otherwise it has one arc and v f t v v for v no for v no v is determined when v is removed from g by the t or t transformation in both cases v has a unique u v just before its removal from g and f u v o u of course u is not known at this time but associating the pair u f u v with v will allow v to be computed later in a pass over the nodes of g that the order in which they were removed from g the algorithm is such that u v can only appear in this way in g if u dominates v in g and then f u v at the time v is removed from g will summarize what is known about paths from u to v that do not return to u before reaching v thus the intuition behind f t u v is much like that behind fa c in but the pairs u v that arise in gw are by properties of the entire control flow graph only in classical structured programming do these properties relate to source level syntax in the simple way described below if a is one of the three kinds of loop statement let head a be entering a if a is a while statement and testing a if a is a iteration statement otherwise a has the form until do and head a is entering the set t initially computed by algorithm a is precisely the set of all head a for a a loop statement in r the initial choice of h from t is the head of an innermost loop and subsequent computations of t just remove the previous h value and choose the next one to be innermost among the remaining members of t for h head a and the corresponding set s in algorithm a includes h a if a is an until or iteration statement and all nodes of n that are still in g there are no other nodes in s the call on h removes all nodes in s ­ h except those with whose targets are not in s if a is an until statement then testing a after h but otherwise all nodes in s ­ h have been removed at this point the next loop to be processed is chosen when all loops have been processed in this way g will be acyclic except for arcs from nodes to themselves the above relation between algorithm a and the syntax of n the proof that v vv for all nodes v where kv is defined by moving topdown through if v is in non then tv v otherwise v is in no for unique in for unique a and vu for u entering a is already available let v fb entering b v o entering o that v t can be proved by annotating algorithm a with appropriate inductive assertions this is tedious but not difficult each call on in solve has entry y so follows from the programming in section finally corollary as in theorem found by solve with f has the good solution for f f in m with f f we have g gw fl g f and f w g f g this implies that interpreting a flow expression with gw for yields a value the value obtained with for therefore s and so s s conclusions and open problems monoids that arise in data flow analysis can be classified by the presence or absence of three properties idempotence distributivity and solutions to global flow problems that are greater than any fixpoint are when m is not distributive as when con propagation uses compile time arithmetic p or when elaborate analyses of the ranges of values of variables are exploited for the very compilers in ca kn l one wants solutions better than fixpoints but cannot hope for optimal solutions like gw ta our algorithm solve finds a good solution one at least as large as any fixpoint in analyzing structured programs that avoid escapes and jumps solve does as well as gw and some times better by choosing the loop product in various ways the can time for of information roughly speaking gw corresponds to choices that emphasize speed similarly for ta for programs that fall within classical struc programming or that use escapes but not jumps the time bounds for solve and gw are similar because we did not assume i e in section and because the basic hierarchy in section is symmetric regarding and exits it is an easy exercise to adapt solve to problems like the detection of dead variables information flows backwards along arcs and is given initially for exit nodes at the time of exit like most methods that of gw can also be so adapted the lack of symmetry makes the task more difficult g w pp than with solve contrary to what might be expected from the in program ming complexity observed in going from to gw to ta solve is a simple algorithm this combination of power and simplicity is obtained by using a hierarchical tion of control flow instead of the usual large graph representing the entire program after translation to a relatively low level intermediate text low level methods solve a problem regardless of where it from high level methods like solve remember and exploit the structure of the program that gives rise to a problem we have used structure expressed by the parse tree but the general formulation of solve and its theorem in section are applicable to other hierarchies as such as the one derived from a low level representation by interval analysis ac u compilers need to update the results of data flow analysis to reflect program changes so that the opportunities that one optimization creates for another are the advantages of high level analysis for thk purpose are sketched in sec and are implicit in the first two of the proof of theorem here high level analysis also leads directly to concise but informative data flow at source level sec in addition to the compiling applications we have high level representations of control flow are useful in denotational semantics sec and program proving la p r sec the problem of efficiently finding a flow cover for any global flow scheme not covered by the lemmas in sections and has been left open here such schemes will be extremely in structured programming even with the escapes and jumps of practical structured programming the problem is still and it could become in an application where the hierarchy does not come from the parse tree the techniques in ta may be useful here another open lacks a mathematical formulation but is quite important many small examples are known where gw or solve finds a good solution better than any fixpoint but real compilers deal with programs often very large ones that are written to compute something rather than to illustrate the and cons of data flow analysis methods in compilers with tive data flow monoids are these solutions significantly better in that they permit more extensive optimization the tional data flow monoids are idempotent as well as distributive data flow problems can be solved very quickly but the answers are less informative than with monoids but are they significantly less informative for optimization in the real world such open tions will not be simply and will require a of theoretical and experimental investigation references ac allen f e and j a program data flow analysis procedure comm acm ca jl a case study of a new code generating technique for compilers comm cm to appear cc cousot p and cousot r abstract interpretation a unified lattice model for static analysis of programs by construction or approximation of proc th acm symp on principles of programming languages january o j dijkstra e w and hoare car structured programming academic press london and new york gw graham algorithm s l and wegman m for global flow analysis a fast and usually linear j acm wh a new strategy for code generation general purpose optimizing compiler proc rh acm on principles of programming languages january the symp variables wh ieee compiler analysis of the value ranges for trans on software engineering j b and unman jd monotone data flow analysis frameworks acts informatica j b and unman jd global data flow analysis and iterative algorithms acm ki ga a unified approach to global program optimization proc acm symp on principles of programming languages october kn knuth de structured programming computing with goto statements la lamport l grams ieee proving the correctness of trans on software engineering pro lm structures h f and m a comm acm of control l db program improvement by source to source transformation j acm r rosen bk correctness of parallel programs the church approach theoretical computer science rosen bk applications of high level control th acm symp on principles of programming january flow proc languages rosen bk high level data flow comm acm tk k and t an on algorithm for com the set of available expressions of acts ta tarjan re solving path problems on directed graphs computer sci dept stanford u no ta tarjan re iterative march algorithms for global flow analysis computer sci dept stanford u u unman jd fast algorithms for the elimination of common subexpressions acts we b property extraction in well property sets ieee trans on software engineering wu w a et al the design of an optimizing new york compiler za ct a control programming statement for natural topdown notes in computer sci m v and optimization of structured programs software practice and experience figure the node entering a appears as an upper half marked a the node leaving a appears as a lower half marked a conditional while and sequential compound statements are illustrated bl ql t e na bm q b qz b q q q figure the node testing a appears as a marked a one part conditional case until and iteration statements are illustrated bl q qk a i a 