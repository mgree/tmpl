a portable compiler theory and practice s c johnson bell hill new extended abstract a compiler for the c language has recently been constructed which is now compiling c for about half a machines the compiler was in various ways by recent theoretical developments this paper gives an overview of the compiler structure and algorithms those areas where theory was helpful and discussing the approaches taken where theory was introduction the c is the principal programming language for the operating system running on computers not only is the system itself almost entirely written in c together with most of the compilers and other support software but most of the applications which use unix are written in c as well there are now several of the unix system this together with the general acceptance of c has led to interest in providing c capability on hardware other than the a year after c was first introduced had constructed a portable compiler and made c available on the compiler was very slow and contained some significant implementation difficulties moreover the c language was and has to grow this made it difficult to keep different implementations of c compatible accordingly we interested in providing a c compiler that would be easily moving to a new piece of hardware should take only a few work moreover improvements in the c language should be easily transmitted to all the compilers is a bell provide reasonable code quality initially for example code was out however the structure should allow for later to production quality output be self the number of people who both understood the target machines instruction set and the inner of the compiler was expected to be small therefore the compiler should to produce code rather than produce wrong code use state of the art tools in particular it was of interest to determine to what extent the compiler could take advantage of recent results in code generation theory be compatible on some systems notably ibm and we were constrained to be compatible with existing support software and libraries for this reason we had less say over the procedure calling sequence than we would have and also had to write the compiler to produce output to the local conventions it was recognized early that the key to achieving all of these goals was flexibility previous with moving compilers had shown that the only thing to expect was the issues like in the ibm character handling on the or support of and on the could easily end up the design this would be a since other similar issues were certain to arise on other machines and would have to be dealt with as they it is some of the things which were not goals we were not interested in the island problem on a island with a machine to provide a tape which allows the of a language or program onto the machine we are far too of the working environment in the unix system and find it too useful in the production of software to be to give it up moreover we were aided by extensive if somewhat communication facilities which made as a method of the compiler onto the target machine compiler speed was not a primary goal although we were not going to let get out of hand we active consideration of additional optimizing phases we were more interested in than optimization and we should do research in one thing at a time with these goals well out we investigated the relevant theory theory and algorithms there on the face of it to be several areas where theory might be applicable to the construction of portable compilers there is potential for theoretical insight into such parts of the compiler as lexical analysis parsing symbol table management and code generation it should be made clear what role theory to play in a practical compiler it would be naive to expect to go to a and find a paper an algorithm for compiling c on the xyz the theory does not need to solve the whole problem or even a very large part of it what the theory should do is provide insight into the basic problems at least for simplified machines and this insight should lead to a mechanism for structuring what is necessarily a complicated program to the purpose of theory is insight not theorems since the theory is not expected to do everything care must be taken when the theory in tools or applying it in a compiler to permit escapes to handle issues which must be dealt with in the language but happen not to be addressed by the theory examples might be the handling of comments in a context free parser or the handling of condition codes in a code generator the first area where theory could be applied to the compiler was in lexical and syntax analysis fortunately the programs and were available on the unix system and provided a useful of some relevant theory allows the user to provide a contextfree grammar together with some code fragments to be executed as each rule is recognized an parser is constructed the code fragments and this serves as a parser moreover through controlled use of ambiguous grammars the many operators and precedence levels of c can be easily dealt with does a similar job with lexical analysis the theory applied is that of regular expressions both programs produce fast practical modules from their respective specifications our use of these tools has greatly aided the initial design construction and of the compiler the situation is not totally however certain areas of the c language notably declaration processing and initialization require the simulation of top down or coroutine in order to naturally process the semantics the bottom up nature of the parser generated by made these actions harder to write than they might have been under other nevertheless the overall benefits of structuring the parser with these tools far such problems symbol table management is an area regarded as well understood by algorithms for hashing and list processing are well known widely and extensively analyzed in theory nevertheless the application of these standard techniques to c was difficult the symbol table manager was rewritten about a times before efficient and bug free it then had to be totally again when block structure was added to c there is an interaction of issues here which is characteristic of many practical applications of theoretical work for example measurements showed that the speed of symbol table access was an important component of the compiler speed thus simple but inefficient methods like linear search would not do hash tables were of course on the other hand the limited address space of the meant that the linking and methods of so in theory had to be at with a block structure meant that one had to remove symbols from the hash table upon block exit however some entries made within an inner block for example as a result of function calls had to remain throughout the rest of the function in order that the target could be given the necessary information at the appropriate time and place the algorithm finally chosen was a hash table algorithm in which are resolved by linear search through successive entries yes that has very bad behavior when the table gets full it permits us however to do the necessary management of the symbol table upon block exit with a very small amount of this example allows us to make another point about the interaction of theory with practical applications the hashing algorithm has no interesting or significant theoretical properties it does have an important practical property however it is fast enough more precisely the amount of time spent doing symbol table management is now a small enough fraction of the total compiler time that there is little to be in the algorithm further in fact the symbol table further could lead to a net loss in clarity and in the compiler a practical program will contain perhaps of algorithms it is neither necessary nor desirable that all of them be optimal especially since the criterion of optimality in practice is usually a of space and time considerations local optimization is often it usually is desirable and often necessary that a few selected algorithms be very often the decision as to which algorithms to carefully can and should be delayed until after the preliminary program is working and measured the most interesting situation with respect to the interaction of theory and practice is in the code generator here is a case where there has not been a lot of theory but that which has been developed was able to give us a great deal of insight the machine models and models of computation studied fall far short of the of real machines and real languages as an example there has been very little theoretical activity dealing with the treatment of condition codes register classes register pairs generalized assignment operators conversion operators register variables bit fields etc most of these issues must be dealt with in a c compiler for current machines thus the most we can expect from the theory is insight but that is enough optimal code generation in its full generality is a hard problem and sethi using a very simple machine model with only a single register showed that optimal code generation is npcomplete when common subexpressions are to be identified and eliminated aho johnson and ullman proved further that even with a machine with an infinite number of registers so there is no register allocation problem and with common subexpression optimization limited to a very simple form it is still npcomplete simply to decide where to begin the computation thus we decided early to the consideration of common subexpressions for the moment this decision is somewhat easier to live with in c than in some other languages since the language itself contains many operators such as the increment and operators which permit simple constructions which would lead to common subexpressions in other languages in fact experience with the c compiler showed that a compiler which does not do common subexpression elimination was still practical for most applications accordingly the compiler was designed to compile code for one expression at a time and common subexpressions were not identified having out common subexpressions we at the literature on code generation for trees and more generally sethi and ullman showed that it was possible to generate optimal code for very simple expressions on very simple machines in time linear in the size of the expression aho and extended this work showing that it is possible to compute optimal code for trees in linear time provided that the machine register structure is sufficiently uniform in order to do this however it is necessary in some cases to generate code for subexpressions in an order that does not follow a simple of the expression tree the key idea is that if a subexpression is to be computed and stored into a temporary it is best to do so before beginning the main expression all of the registers would be free and thus the subexpression could be computed with all resources available clearly it is necessary to identify those subexpressions which must be stored accordingly the code generator was divided into two pieces one piece identifies subtrees to be computed and the other computes them in order to be able to clearly fix the blame for an incorrect decision the part of the code generator which computes the subtrees was unable to ask for temporary storage locations thus the assertion made by the first part is that the subtrees passed to the second part must be computable without the use of any temporaries if the second part runs out of registers it aborts the compilation pointing an at the first part of course this assertion serves as a in the compiler logical errors are often and the blame easily attached to one or the other part of the compiler the duplication of logic in both parts for example in the detection of special cases is more than made up for by the absence of the good im out of registers what should i do now module in many compilers when it clear that this of containment was off well a similar was adopted towards register allocation if a subtree is to be computed into a register of a given type or a particular register for example because of a function return or call this must be known before the subtree computation is the actual requirement is even stronger the only transfers that are generated by the compiler except as a response to specific operators in the source code come from one in the compiler and in particular must result from a transfer of a computed value into the register which was its intended destination from the if the compiler attempts to move a computed value into a register the computation is aborted this implies that the register allocation phase must operate before the actual generation of code as with the storing of results the difficulties that this causes are largely duplication of logic in two places in the compiler the advantages are again a clear of blame for bad code and the elimination of another module in many compilers the put the result here no matter what and then clean up the module the key to the and techniques was roughly the same attempt to estimate in a bottomup manner the resources needed to compute the subtrees this could then be used to select subtrees to be stored and to order the computation by identifying the to first the dynamic programming approach of aho and johnson to be very difficult to apply to the very rich set of c operators it would be expensive in space and time and its very complexity would seem to lead to errors also one of the major assumptions of this theory that the registers were is one of the most frequently violated in practical machines and there to be little to be done about repair of the algorithm under these conditions however the insight provided by the theory suggests that it is to attempt to estimate or bound the resources and in particular the registers which were needed by subtrees accordingly the compiler attempts this the numbers generated somewhat referred to as sethi unman numbers are useful not only in deciding which subtrees are to be stored but also in generating the final code sequences there are a number of other things which this permits which are useful but somewhat different from the usual notion of register allocation for example many machines have some operations which must have the right operand in memory this is achieved by giving such operands very high numbers forcing them to be stored similarly the ability to do arithmetic operations on data of various types is usually limited for example few machines can multiply or divide a register by a stored in a single byte such have to be loaded into a register before they can be dealt with those operations which are immediately are by giving the righthand operand a number of o this signals to the compiler that this effect has been recognized and can be assumed in later computations to say most of the computation is machine dependent in the specific details but the general notion that of and control of resources has a very broad degree of generality there are distinct limitations in this process unfortunately the notion of of resources breaks down to a greater or extent in almost all of the practical machines examined this does not mean that the notion is useless or must be discarded merely that it moves in certain situations from the ground of algorithm to the of heuristic one common difficulty is that on many machines operations such as multiplication and division require the use of registers often adjacent or register pairs some of the theoretical implications of this are discussed by aho johnson and unman this situation is in theory and a major in practice it is difficult to estimate the number of registers required to do such computations even if the is done correctly it is still an effort to decide how to allocate the registers to avoid or minimize moves with for example four registers it is possible to have two free registers but still be unable to do a multiply operation without prior moves if this is allowed a good deal of the effect of the register transfers is lost this situation is particularly since it seems to be largely unnecessary while it is true that multiplication for example does take two n bit and produce a n bit result in practice nearly all programming languages including c throw away the most significant n bits of this product in some cases with an overflow if appropriate thus while the full length operation is useful it would also seem desirable to have single length multiply divide and remainder operations which were similar to the add and operations this would allow direct application of the and results with a resulting simplification of the compilers at one point there were hardware considerations which argued for special treatment of such register pairs but that no longer seems relevant with hardware a related issue is that of having different register classes for example index registers general purpose registers floating point registers etc here the notion that a single number characterizes the computation is the situation is even more complex when certain computations can be carried out in various different classes of registers thus a computation might require two index registers and one general purpose register or one index register and two general purpose registers a single number is the dynamic programming approach of aho and johnson could probably be extended to deal with this situation but would be expensive and rather for the other reasons mentioned above the practical response to these difficulties is to them in various ways another difficulty with the mechanism arises with long or double length integer in general these require two registers and often must be in register pairs as well the additional difficulty after the register pair issue is dealt with is merely keeping track of the sizes of the operands this is tedious but not conceptually difficult similarly the presence of complicated assignment operators in c such as the operator leads to complicated a b means a a b except that any sideeffects that may result from computing a are done only once at a lower level the generation of the specific code sequences is also nontrivial the language c has a potentially infinite number of types and several operators the machine architecture gives rise to many other cases as well operands in registers in memory on the stack and constant operands to mention just a few there are possibly of code sequences to be dealt with some method of abstraction is crucial to permit the compiler to generate correct code without an impossible data management problem providing this abstraction power bugs which arise from incorrect abstracting of the machine design manual nevertheless without some means of abstracting and the compiler becomes as an example on many machines the add and or and exclusive or operations are very similar in addressing mode and condition code treatment an abstraction facility allows the compiler to share logic and decision facilities for these operators and yet still keep track of the correct similar abstraction is possible with address modes for example names and constants are often treated in similar ways and with the types of the operands all pointers are often treated the same and often pointers are treated the same as integers one of the more interesting results of this abstraction process from the realization that machine frequently return more than one value for example a store instruction may yield useful values in the register being stored in the memory location which is the target of the store and in the condition code registers the exact result which is for later use depends on the goal of the computation at that time and some heuristics this abstraction issue is avoided in most and on compiler design the c compilers have from quite a number of bugs in this area however the use of the abstraction mechanisms provided in the c compiler often the addition of some addressing or features to the compiler which do not actually exist in the machine we may have identified this problem but we have not really solved it as a final attempt at correctness we adopted a discipline which has been extremely successful modeling the compiler views the input expression tree as a model of a computation which is to be done and the compiler keeps a model of the state of the machine for example which registers are free the of instructions for example a load instruction is viewed as a transformation on the input for example converting a name node into a reg node and also on the machine state for example marking a register there is a part of the basic code generator simply concerned with making these transformations correctly at the center of the low level code generation is a table containing the basic tree match templates input rewriting rules resource requests and code to be each table entry has its own validity and is independent of all the other entries code generation consists of selecting transformations which one by one reduce the input producing code if the model of the registers is correct and the table entries correctly match the semantics of the input operators with the rewriting rules and register requests then if the input is reduced to null usually a single node the output code must be correct once again the code generation prob lem has been divided the table entries supply a rich set of correct transformations the rest of the compiler is concerned with selecting subtrees and applicable table entries which reduce the input tree as a side effect of this reduction output code is pro this notion is to the distinction made by pratt the table entries represent the they are correct facts which link the input the machine model and the output the performance aspect is the col of algorithms and heuristics which select subtrees and applicable table entries and work at reducing the input problem remaining to be solved to nothing of course the table may be there may not be enough transformations specified to reduce all legal input trees to the desired state moreover the heuristics may fail to select a proper ordering which would permit a reduction to take place in this case the compiler will simply give up its to handle the input however if the compiler heuristics do successfully reduce a given input one has very high that the resulting code is correct since the process by which the code is produced is the concatenation of a number of small steps in fact the compilers produced using this model have had an there have been very few compiler bugs that have not been detected by the compiler itself to summarize the code generation experience the existing theory was not directly applicable but did suggest a number of principles for the code generator the data management problem of dealing with many possible code sequences seems rather difficult nevertheless by that the transformations of the problem to be solved remain conceptually separate from the logic which the method of attack the compiler becomes quite practical experience the compiler sketched above has as the base of about a half compilers and a number of others are compilers are being maintained on series ibm data general machines and others some statistics on the amount of achieved are of interest there are roughly lines of source code in the c compiler the first pass which does syntax and lexical analysis symbol table management builds expression trees and generates a bit of machine dependent code such as subroutine and consists of lines of code of which are machine dependent in the second pass which does the of the code generation out of lines are machine dependent thus out of a total of lines or ° are machine dependent the other v are shared with the the ibm and other compilers as the compiler is these figures should rise a bit for the ibm the fraction which is machine dependent is ° for the o these figures both and the actual difficulty of moving the compiler the figures actually represent the number of lines on those input files which contain machine dependent code the actual number of lines which must be modified for a new machine is a half or a third of this number on the other hand the part of moving the compiler is understanding the code generation issues the c language and the target machine well enough to be able to make the modifications since we do not have a version of this compiler for the we have no good figures of comparison with the original unix compiler the new compiler is considerably however and would probably be two or three times slower for example the portable compiler does a great deal of bit arithmetic internally to facilitate the compiler does much of this to bit accuracy which is far more efficient the problem of of c is an important one c is more parameterized than other languages if characters are naturally bits on a given machine c does not on simulating thus a number of differences in machine architecture can been seen in the language this paper machine dependence seems to make little difference to the actual of programs we have moved substantial programs including compiler compilers software and file from unix to the ibm and systems and have moved the unix operating system itself and a major part of the unix software to the thus the compiler has demonstrated a practical degree of compatibility with the version and the c language itself has proved to be a fine for in particular our c compilers on the ibm and unix systems are far more compatible than the corresponding fortran compilers the future the theory which the construction of parsers is enough that it can be into a tool that can be used by who knows nothing about the theory the theory of regular expressions is similarly developed in the case of symbol tables the theory is there but it seems to being perhaps this will come in time code generation is still a very area for theoretical and practical work a practical tool seems far off the greatest future need however seems to lie in the area of semantics and semantics are clearly closely related on the face of it is precisely the practical of the notion of semantics the practical separation of meaning from representation having moved a program one needs to know what it is one has moved unfortunately this view is not widely held among rather than study the complicated interaction between meaning and representation the in semantics are towards separation of meaning totally from representation our greatest need is for a language similar in spirit to which can be used to about semantic problems and make semantic and contracts we have not found that any of the current approaches are references d m c reference manual unix document d m and k the unix system comm acm pp july a a portable compiler for the language c thesis m i t cambridge mass s c johnson yet another comp sci tech rep no bell hill new july m e a lexical a generator comp sci tech rep no bell hill new october j and r sethi code generation for a machine pp july a v aho s c johnson and j d unman code generation for expressions with common sions j acm pp jan also in acm symp on principles of programming languages pp a p on programming of arithmetic operations a n pp translation in comm acm pp r sethi and j d unman the generation of optimal code for arithmetic expressions j acm pp oct a v aho and s c johnson optimal code generation for expression trees j acm pp also in proc acm symp on theory of computing pp a v aho s c johnson and j d unman code generation for machines with opera tions proceedings of th sigplan symposium on of programming languages pp january v r pratt the in programming proceedings of the fourth acm symposium on principles of programming languages p 