a calculus for relaxed memory karl crary michael j carnegie mellon university popl consist complete well easy to abstract we propose a new approach to programming architectures in imperative portable programming languages our memory model is based on explicit requirements for order of execution and the visibility of writes the compiler then those requirements in the most efficient manner it can this is in contrast to existing memory models they allow programmer control over synchronization at based on inferring the execution and visibility consequences of synchronization operations or annotations in the code we formalize our memory model in a core calculus called rmc outside of the programmers specified requirements rmc is designed to be strictly more relaxed than existing architectures it an nondeterministic semantics for expressions in which actions can be executed in nearly any order and a store semantics that generalizes sarkar et als and et als models of the power architecture we establish several results for rmc including sequential consistency for two programming disciplines and an appropriate notion of type safety all our results are formalized in coq categories and subject descriptors d language constructs and features concurrent programming structures keywords relaxed memory concurrency introduction modern computer architectures employ relaxed memory models interfaces to memory that are considerably weaker than the conventional model of sequential consistency in a sequentially consistent setting an execution is consistent with some interleaving of the individual memory operations thus the programmer may assume that a all threads have a common view of memory that on the order in which memory operations are performed and b that order respects in regard to operations by the same thread relaxed memory architectures provide no such common view of memory although most do enforce a globally order on writes to individual locations the order does not extend to multiple locations indeed on some architectures notably power and arm it is more useful to start from a view of memory permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than acm must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january Г copyright c acm as simply a pool of available writes and then impose structure than it is to start from memory as a mapping of locations to values and then try to weaken it moreover modern architectures also execute instructions out of order while most do execution in a fashion that is to singlethreaded programs on some important architectures eg power and arm again it is possible for multithreaded programs to expose execution as illustrated by and and others these are two distinct relaxed memory behaviors cannot always be reduced to just one or the other this point is often not clearly understood by programmers or even by authors of documentation which sometimes give rules when reads and writes might be without specifying the order in question relaxed memory architectures can be challenging to program singlethreaded code and also concurrent code free of data races in which accesses to shared variables are protected by usually require no special effort but implementing lockfree data structures or implementing the themselves can be very indeed in the past the problem was made particularly challenging by the lack of clear specifications of the memory models recent work has addressed this difficulty for some important architectures Г but for portable imperative programming languages the memory models are either expressive or quite complex in java and cc the memory model is divided into two parts a simple mechanism for programming and a less simple one for lockfree data structures and for lowlevel implementation of synchronization like much of the work on architecture models our primary concern in this paper is the second aspect our approach to programs discussed in section is conventional variables and ccs atomic variables need not be used within a mutex and thus are suitable for lockfree data structures and synchronization implementation are guaranteed to be sequentially consistent this provides a convenient programming model but it also imposes a cost which can be in particularly code ccs atomics are more flexible the programmer each operation on an atomic with a memory order that indirectly the semantics of the operation which may be as strong as sequentially consistent or may be weaker some of the rules cc memory orders are quite complicated consider the example below for purposes only the details will not be important in the sequel which has the standard message passing idiom at its core wx co wx wx co wx co wy release wx co sw hb wy co vo co wx wy rf ry acquire rx the edges are the program order the dashed red edges are witness relations derived directly from the trace ry reads from rf wy the coherence order co the global ordering among writes to the same location on x is wx wx wx and on y is wy wy from these edges the rules of cc define a of derived edges shown in dotted blue since wy is marked with release order it is considered a release action and forms the head of a release sequence not shown to avoid the diagram that also includes wy since ry is marked with acquire order and reads from release sequence wy ry wy does not with ry because wy has relaxed order and thus is not a release action consequently wx happensbefore rx thus wx is a visible side effect to rx because there is no other write to x that by happensbefore and its visible sequence of side effects not shown consists of itself and wx an atomic read must come from its visible sequence of side effects so rx must read from either wx or wx not wx on the other hand if wy had release order then wy not wy would ry which would mean that wx happensbefore rx so wx not wx would be a visible side effect to rx the visible sequence of side effects would contain only wx so rx could read only from wx this is just a simple example the required reasoning can be much more complex indeed the key cc notion of happensbefore is not transitive to enable certain optimizations on architectures like power and arm when using the consume memory order the rmc memory model we propose a different model for lowlevel programming on relaxed memory architectures rather than have the programmer specify annotations that indirectly determine the relations which writes can satisfy which reads we allow the programmer to specify the key relations directly it is then the compilers job to generate code to realize them in the terminology of et al cc refers to program order as before and to the coherence order as modification order we use our terms to maintain consistent terminology throughout the paper wy rf ry xo rx as before the black edges are given by the program and the dashed red edges are witness relations derived from the trace the edges are again program order in addition the program specifies vo and xo edges in this example to bring about the same outcome as the cc example the programmer indicates that wx is before wy and that ry is before rx the visibility specification means that any thread that can see wy must also see wx and the execution specification means that rx must occur after ry thus rx can see wy since rx takes place after its thread reads from wy so it must also see wx and consequently it cannot read from the wx a programmer who to wx as well would indicate that wx is before wy in this paper we present rmc for relaxed memory calculus a core calculus for imperative computing with relaxed memory that this memory model rmc is intended to admit all the to a sequentially consistent executions permitted by relaxed memory architectures thus an rmc program can be implemented on such architectures with no synchronization overhead beyond what is explicitly specified by the programmer importantly we do not attempt to capture the precise behavior of any architecture on the contrary rmc is specifically intended to be strictly more relaxed in other words even more than any existing architecture this is for two reasons first because we find it more elegant to do so second because we hope that by doing so we can rmc against further by computer thus rmc makes only five assumptions regarding the architecture singlethreaded computations meaning singlethreaded programs and also the critical sections of properly synchronized programs respect sequential consistency there exists a strict partial order called coherence order on all writes to the same location and it is by reads the message passing idiom illustrated above and discussed in more detail in section works there exists a push mechanism this corresponds to sync and xs there exists a mechanism for atomic readwrite operations also known as atomic some architectures might not satisfy or but the importance of those assumptions is now and we expect that future architectures will an architecture might also provide some atomic operations rmc accounts for such but does not require them et al another formalism proposes axioms similar in spirit to these assumptions we compare them to ours in section we take as our reference points the similar power and arm architectures and the x architecture because they rigorous usable specifications we focus on the former because in all cases relevant to this paper the of power and arm those of x like sarkar et als model for power and sewell et als model for x our calculus is based on an operational semantics not an axiomatic semantics nevertheless one aspect of our system does give it some axiomatic our rules for carrying out storage actions require that the coherence order remain acyclic assumption above but to maximum flexibility we do not impose any particular protocol to ensure this thus when reasoning about code this requirement functions much like an axiom et als intermediate machine has a somewhat similar the paper makes several contributions и we show how to program architectures using visibility and edges и we give an elegant calculus capturing and speculative execution и we give an relaxed model of memory it accounts for the behavior of power and arm and a write behavior on arm in addition to other possible not yet observed or even implemented и we prove three main theorems a type safety result including a novel adaptation of progress to nondeterministic execution a theorem showing that sequential consistency can be obtained by interleaving instructions with memory barriers this theorem generalizes a theorem of et al for the more permissive semantics of rmc and also the proof a second theorem showing that programs sequential consistency this theorem a standard result to rmc all the results of the paper are formalized in coq the formalization may be found at a version of this paper with a full appendix containing all the inference rules may be found at tagging main tool for with relaxed memory is edges indicating visibility order who the latter action can see the former and execution order the former action must be executed before the latter it is easy enough to indicate required orderings between operations by edges on an execution trace but of course execution traces are not programs how are specified orders in source code we do this by using tags to identify operations and including a declaration form to express required edges between tags this is illustrated in the following piece of code which implements the middle thread of the example from section in an extension of c using the rmc memory model after x y x y here the write x ie wx is tagged before and the write y ie wy is tagged after the declaration after indicates that a visibility edge exists between operations tagged before and after if we to make wx also visibility ordered before wy we could declare additional tags and edges or just add the before tag to x visibility order implies execution order because it makes little sense to say that i should be visible to who can see i if i can be seen before i even takes place we can get execution order alone using the keyword tagging creates edges only between operations in program order thus we cannot require an operation to execute before an earlier operation this is particularly important when tags appear within loops after for i i i x i y i here the visibility edge reaches from each to the in the next iteration of the loop wx wy wx wy vo we do not generate edges from wy to wx or from wy to wx also observe that tags generate edges between all operations with appropriate tags not only the ones for example the tags in the previous code to obtain after for i i i x i y i generates wx wy wx wy vo vo vo note the long edge from wx to wy this long edge is desired as we will see in an example in section it is also sometimes useful to impose visibility and execution order more than can be done with individual edges for example the code to enter a mutex needs to be before everything in the following critical section and the code to exit a mutex needs to be after everything in the preceding critical section individual edges between the operations of the mutex and the critical section would be tedious and would break the mutex abstraction instead rmc allows the programmer to draw edges between an operation and all its predecessors or successors using the special pre and post visibility the model five kinds of actions reads writes pushes see below and although the primary interest of the visibility order is writes it can also be useful to impose visibility among other actions one reason is that visibility order is transitive so edges can create new paths even when in isolation for instance if i vo i vo i the transitive edge i vo i might be important even if i vo i were in its own right this is the only use for more substantially consider the following trace in this and subsequent traces all locations are initialized to zero these writes are omitted from the diagrams to reduce the wx rf rx edge induces visibility order between wx and rx thus wx is before wy by transitivity consequently rx can see wx so it cannot read from wx wx rf rx ry vo rf xo wy rx pushes perform a global synchronization like sync on power on arm or on x once a push is executed it is visible to all threads consider the following trace adapted from boehm and causality example wx rf rx wy xo xo ry rx rx can read because nothing forces wx to be visible to rx changing the xo edges to vo has no effect because visibility order edges to reads have no more effect than execution order ex possibly to create other paths by transitivity which would not happen here however introducing pushes does change wx rf rx wy vo vo push push xo xo ry rx now rx must read pushes are totally ordered a consequence of being globally visible so either push is visible to push or vice versa if the former then wx is visible to rx if the latter then the trace is impossible because wy is visible to ry this reasoning is generalized in the sequential consistency theorem section for another example of reasoning using pushes consider wx ry rz vo xo rf xo push rf wz rx xo wy observe that wx is before the push but has no specified relationship to wz so we do not know immediately that wx is visible to rx however wy takes place after the push and ry wz rz and rx happen later still therefore since pushes are globally visible rx must see the push and consequently must see wx and speculative execution an rmc expression consists of a monadic sequence of actions in a conventional monadic language an actions goes through two phases first resolve the actions arguments purely then execute the action generating effects in rmc we add an intermediate phase once an actions arguments are resolved we the action which replaces the action by an action identifier later an action can be executed at which time the identifier is replaced by the actions result once an action is execution may proceed out of order the semantics may nondeterministically choose to execute the action immediately or it may delay it and process a later action first for example consider an expression that reads from a then writes to b one possible execution executes the write before the read x ra in in ret x x i in in ret x x i in i in ret x x i in ret in ret x with write effect x i in ret x x ret in ret x with read effect ret the purpose of the phase is to require the execution to commit to the arguments of earlier actions before executing later ones this is important because earlier actions often affect the semantics of later ones even when the later actions are actually executed first execution makes it possible for execution to proceed out of program order but not out of dependency order for example suppose the write to b depends on the value read from a x ra in x in ret x x i in x in ret x the write to b cannot because its arguments are not known however rmc permits execution to on the value of unknown for example we may that x will eventually become producing the execution x i in wy x in ret x i in wy in ret nb i in i in ret иии i in ret ret in ret with read effect ret here the syntax v e in e is a distinct syntactic form from the usual monadic bind x e in e unlike the usual form it binds no variables and instead indicates the value that the bound expression is required to return allows us to step from x e in e to v e in eventually once the expression has been executed we can the from v ret v in e to e the combination of and speculative execution allows us to execute the program in nearly any order subject only to the constraints imposed by the programmer and those inherent in the actions themselves in fact it is actually a derived form built from a more primitive form in section rmc uses nondeterminism in a variety of ways but is the most unlike our other uses eg interleaving of threads execution the choice of which read satisfies a write can take execution down a in the above example we i would return but suppose it did not then we find ourselves in a state like ret in ret from which we can make no further progress we do not refer to states like this as stuck since stuck usually an unsafe state instead we call such states since they arise in executions that cannot actually happen when reasoning about rmc programs one may ignore any executions that become although the use of above might seem at the level it could very easily arise in an optimizing compiler which might be able to determine statically that ra would or at least could return a more common use of by the hardware is to execute past a branch when the value is not yet known for example x ra in x then e else e x i in x then e else e i in then e else e i in e i in e иии an example as a realistic example of code using the rmc memory model consider the code in figure this from the linux kernel implements a a common data structure that implements an imperative queue with a fixed maximum size the maintains front and back pointers into an array and the current contents of the queue are those that lie between the back and front pointers around if necessary elements are inserted by the back pointer and removed by the front pointer the permits at most one writer at a time and at most one reader at a time but allows concurrent access by a writer and a reader note that the buffer is considered empty not full when the front and back coincide thus the buffer is exactly one empty cell remains this minor implementation detail the analysis of the code in an interesting way we wish the to two important properties the elements are the same elements that we that is threads do not read from an array cell without the write having propagated to that thread and no an element that is still current the key lines of code are those tagged write and in and read and in it is not necessary to use disjoint tag variables in different functions we do so only to simplify the for property consider an pair we have write vo rf xo read it follows that write is visible to read property is a bit more complicated since a full has an empty cell it requires two consecutive to a cell the canonical trace we wish to prevent appears in figure in it read reads from write a later write that finds here an expression forming a thunk and force forces a bool buf char ch write unsigned back unsigned front int false if front ch true return int buf read unsigned front unsigned back int ch if front back ch return ch figure a room in the buffer only because of a operation subsequent to read hence a current entry is this problematic trace is impossible since read xo rf xo write rf read alternatively read xo rf xo write rf read in rmc if i rf i then i must be executed earlier than i you cannot read from a write that yet executed so it follows that read must execute strictly before itself which is a contradiction note that this argument relies on xo write or read xo merely having say xo write would be since nothing in the code as written gives us write xo write with cc to implement the in cc one can mark the and operations as release and the and operations as acquire for property in an pair we get since reads from this implies that write happensbefore read for property consider the trace in figure in this trace alternatively so read happensbefore write thus read cannot read from write the latter inference is a bit subtle since happensbefore is not transitive it includes program order on the left but includes it on the right only after xo write vo xo rf rf xo read xo xo write vo xo rf xo read xo figure impossible trace acquire write rf sw hb acquire read consume consume write rf read hb release rf release rf release release acquire acquire write rf sw read release release consume consume write rf read release release figure impossible traces in cc these correctness arguments for rmc and cc are similar in part this is because the cc code is fairly simple without any nontrivial release sequences or visible sequences of sideeffects and in part this is because the cc implementation uses synchronization instead of which may be on certain architectures such as power and arm to obtain potentially better performance in cc one may instead mark and as consume and introduce spurious data dependencies from to write and to read this makes the correctness argument more for property in an pair we get using the former data dependency that is read which using the definition of happensbefore once again ensures that write happensbefore read types unit nat ref numbers n и и и tags t и и и locations и и и identifiers i и и и threads p и и и terms m x n m then m else m e fun xx m m m values v x n e fun xx m attributes b vis labels bb t t e ret m x e in e v v in e rm m m xm push nop i force m new tb te e execution states p e tag sig sig sig contexts t i p x figure syntax the argument for property is a bit more subtle because happensbefore is not transitive in the argument for rmc and for cc with acquire we can construct the illegal path using either rf or rf in contrast with consume we can only use the latter we can observe that is write and that is write however since happensbefore respects program on the left but not in general on the right only the latter suffices to show read happensbefore write in rmc the programmer need not concern or with acquire vs consume he or she simply specifies what is needed such as execution order between and the compiler the most efficient code it can to implement the specification conversely rmc also allows the programmer to make in cc both and must be marked release this entails the same overhead for each to a visibility order specification in rmc but as we have seen visibility order is necessary only in requires merely the execution order rmc the syntax of rmc given in figure tags t and identifiers i range over actual dynamically generated tags and identifiers they do not appear in user programs we do not discuss the allocation primitives in this paper so memory locations can appear in rmc programs here but not in real programs there are two sorts of variables x ranges over values and t ranges over tags we make a standard syntactic distinction between pure terms m and effectful expressions e the terms are standard and most of the expression forms we have discussed already the form v v in e indicates that the values v and v are to be identical while evaluating e the monadic form from section is actually a derived form v e in e def x e in x v in e m ret m i p i e x e e init x e in e e x e x e in e figure static semantics the tag allocation form new tb t e generates two new tags bound to variables t and t in the scope e those tags express either a visibility or execution edge as indicated by the attribute b the labelling form e a label to an expression which is bb either a tag or a and representing pre and post finally an execution state is an association list pairing thread identifiers p with the current expression on that thread static semantics the static semantics of rmc expressions is largely standard expressions are relative to three ambient signatures and a context the ambient signatures specify the valid tags locations and identifiers giving the types for location and identifiers and the threads for identifiers terms are not passed an identifier signature because identifiers cannot appear within wellformed terms execution states are not passed a context because wellformed execution states must be closed the only interesting aspect is in the treatment of the identifier signature which we use to ensure that each identifier appears exactly once and that the stores view of program order with the code thus on a given thread the identifier signature is used like a linear ordered context as shown in figure there are two rules for bind since actions must be in order identifiers may appear in the second expression of a bind only when the first is fully ie all of its actions have been converted to identifiers at the top level not shown for space reasons the identifier signature lists identifiers in the same order they appear in the store and the typing rules for execution states filter out the identifiers for other threads dynamic semantics threads and the store communicate by on a transaction the empty transaction which most thread transitions there is no store effect the transaction n i indicates that the action is being with labels n and is assigned identifier i the execution transaction i v indicates that i has been executed and result v the edge transaction t b t indicates that t and t are fresh tags expressing a b edge the key rules of the dynamic semantics of expressions and execution states appear in figure the dynamic semantics depends on typing because we want the rule to only welltyped values consequently the dynamic semantics for expressions and execution states depends on a tag and location signature and for expressions on a context the evaluation step judgement for expressions indicates the transaction on which that step depends and the judgement for execution states indicates both the transaction and the thread on which the step took place thus in rmc each tag has exactly one from this primitive the compiler builds up a more flexible mechanism that allows tags to be used in multiple edges as in the example of section actions r w v rw v push nop transactions i i v t b t events p label i t t i i histories h h figure dynamic semantics syntax in the interest of brevity we the ambient signatures and context in most of the rules where they are just ignored or passed to premises we also leave out the rules that just evaluate subterms except when they are interesting and we leave out the dynamic semantics of terms which is standard the rules for execution third row are straightforward fourth row allows execution to step from to v v in v xe provided v and v have the same type note that variables are considered values so an important instance is e x v in the of the system is the bottom row once the subterms of an action are evaluated and thus it matches the grammar of actions given in figure the action on a transaction i and is replaced by i as the transaction up it collects any labels that the action within once all the actions within a label have been the label can be eliminated later the action executes and is replaced by a return of its value the semantics deals with expressions by them into readwrite expressions conceptually the expression xm reads location m a location the read value for x in m and writes m all in one atomic operation to implement this the dynamic semantics first evaluates the location this is necessary for type safety to obtain xm then the semantics that the read will return some value v which it for x to obtain a readwrite expression that readwrite expression is then wrapped with to ensure that the readwrite does in fact return v v xm x rw in x v in ret v thus while are actions seen by the store are handled exclusively by the thread semantics note that while rmc admits expressions in great generality we do not assume that languages using the rmc memory model provide such full generality on the contrary we expect that they will provide a small set of such operations eg or supported by the architecture the store store is modelled in spirit after the storage subsystem of sarkar et als model for power with made for generality as in sarkar et al the store is represented not by a mapping of locations to values but by a history of all the events that have taken place the syntax of events appears in figure three events to the of actions p records that i was on thread p records that i rep if the location diverges then there might be no actual location to which values of type have been written the type might be empty so no transition would be possible p e e p e p p e p p e p p e e e e init ret m init i init e init e init x e in e init e init v v in e init e e new tb t e t t t t e e e x e in e x e in e e init x e e x fv x e in e x e in e x ret v in e v v v v in v xe e e v v in e v v in e v v in e e i i e e e e e init e e i iv ret v i if i otherwise figure dynamic semantics threads action and label i records that label is attached to i these three events always occur together but it is technically convenient to treat them as distinct events the event t t records the allocation of two tags and an edge between them two events to executed actions records that i is executed and i records both that i is executed and i read from i the final form i adds a edge from i to i this is not used in the operational semantics but it is useful in some proofs to have a way to add coherence edges store transitions take the form h p h in which the is a transaction that is shared with a transition in thread p in the store transition rules we write h to mean the event appears in h where may contain indicating parts of the event that dont matter as usual we write and for the transitive and the reflexive transitive closures of a relation we write the composition of two relations as x y we say that is acyclic if гx x x we can give three store transitions immediately an empty transaction generates no new events an edge transaction simply records the new edge provided both tags are distinct and fresh we define t to mean t t an transaction records the thread the action and any labels that apply provided the identifier is fresh none h p h t t t t edge h t h t t init h ip h p label i the remaining rules require several auxiliary definitions и identifiers are in program order if they are in order and on the same thread i po h i def h h h p h h p h p h the operational semantics ensures that this notion of program order with the actual program since recall no expression can be executed until any preceding actions are и an is marked as executed by either an exec or an rf event i def i и trace order is the order in which identifiers were actually executed i to h i def h h h h h i i note that this definition makes executed identifiers than identifiers и specified order which the tagging discipline is defined by the rules i po h i i t t i i b h i i po h i b i i b h i i po h i b i i b h i и the key notion of execution order is defined in terms of specified order i xo h i def b i b h i и an identifier is executable if it has not been executed and all its predecessors have been i def i i i xo h i i и a read action is either a read or a readwrite i r rw и similarly a write action is either a write or a readwrite i v w v rw v и i rf h i def i и identifiers i and i are if i is a push and is than i i oh i def push i to h i since pushes are globally visible as soon as they execute this means that i should be visible to i и the key notion of visibility order is defined as the union of specified visibility order and push order i vo h i def i vis h i i rf h i i oh i finally there is the central notion of coherence order written i co h i a strict partial order on actions that write to the same location the coherence order is inferred ex post from the events that in the store in a manner that we discuss in detail in section for now the important property is coherence order must always be acyclic note that our ex post view of coherence order is in contrast to the ex view taken by sarkar et al in our ex post view coherence order is inferred from events that have already occurred in the ex view coherence edges must already exist introduced nondeterministically and in order for events to occur with these definitions in hand we give the remaining rules p r rw v i iw v iw h h i read p w v push nop i h ip h observe that read actions reads and are treated the same and actions writes pushes and are treated the same however their presence in the history can have very different effects on other actions also observe that read has no explicit premise ensuring that the read returns the right value beyond ensuring that iw writes to the same location i reads from the validity of the read is enforced implicitly by the premise if iw is an write for i to read example if iw is than some other write visible to adding i to the history will create a cycle in coherence order the top level the toplevel dynamic semantics appears in figure toplevel state consists of the three ambient signatures a history and an execution state signature state s h an auxiliary judgement over ambient signatures updates them according to the transaction the state can make a transition when all three components agree on a transaction coherence order the question to be resolved is which writes are permitted to satisfy a given read in a sequentially consistent setting the only write permitted to satisfy a given read is the unique write to the reads location in relaxed memory setting the write is no longer unique nevertheless we assume that there exists an order on writes that is by all reads following sarkar et al we call this the coherence order p p label ip i p i p v t t t t tt t ss p h p h p h h figure dynamic semantics toplevel as in sarkar et al the coherence order is a strict partial order that relates only writes to the same location in an effort to our calculus we place only the minimum constraints on coherence order necessary to three goals first singlethreaded computations should exhibit the expected behavior second the message passing via visibility and execution work third readwrite operations should be atomic in an appropriate sense these three aims result in three rules defining coherence order which we explore below the first coherence rule states that a read always reads from the most recent of all writes to its location that it has seen i ir i rf h ir i co h i ii here we write i ir i is to ir to say that the read ir has already seen the write i other writes may also have been seen the prior writes are the ones we know for certain have been seen since ir reads from i instead of i we infer that i is than i we know that ir has seen iw in one of two ways first iw is than ir thus ensuring that singlethreaded computations work properly second iw is to some action that is to ir thus ensuring that the idiom works properly i po h i i v i i i i vo h xo h i i v i i i the second coherence rule says that a write is always more recent than all other writes to its location it has seen i i i co h i here we write i i to say that the write i has already seen the write i this has two rules similar to write read priority i po h i i v i i i v i vo h xo h i i v i v i i another rule says that i has seen i if i has seen some read ir that has seen i i ir ir i i i used here has only a rule although a visibility rule would also be sound it is easy to show that it would not add any new coherence edges so we omit it for simplicity i po h i i i i i v the third coherence rule says that when an atomic readwrite action i reads from a write iw no other write i can come between them in coherence order iw rf h i rw iw co h i i co h i ii the fourth and final coherence rule says that extra coherence edges can be given in the history this is a technical device for corollary the operational semantics never introduces any such events i i co h i discussion the definition of coherence order works in with the require ment for coherence order to be acyclic to prevent illegal resolution of reads for example suppose iw co iw and ir has seen iw then ir cannot read from iw if it did we would infer that iw co iw which would introduce a cycle in coherence order conflicts an reader may have observed that rmc enforces execution order only for edges execution order does not respect program order even for conflicting memory accesses this is an design and it a little discussion suppose iw and iw are writes to the same location and suppose iw po iw it does not follow from any principle identified above that iw must execute before iw all that is required is that iw be than iw so that any read that both must choose the latter in fact existing architectures do this both power and arm employ write forwarding a write that is not yet to be sent to the storage system can nevertheless satisfy subsequent reads on the same processor in view such a write was executed as soon as it to be read from which might well be than some write readwrite conflicts suppose ir reads from the same location iw writes to and suppose ir po iw again it does not follow that iw must execute after ir all that is required is that ir ultimately read from a write than iw in fact existing architectures do this also some arm processors will sometimes carry out a write even when preceding reads are not yet complete and rely on the storage system to ensure that those preceding reads are satisfied by earlier writes the lefthand trace in figure can be observed on the processor figure for this trace to occur the processor must execute rx before the push and hence it must execute wy before ry we call this a write hence executed is not the same thing as sarkar et als committed wx ry rx vo push rf wy to wy rf ry xo wy rf ry xo rx co wy rf rf ry vo wx figure writes left observed right allowed by sarkar et al revised model the write also illustrates why eg must be defined using execution order and not merely trace order as might otherwise seem if trace order to were used rmc would not admit the righthand trace in figure because then wy vo rx to ry would give us wy ry so wy co wy by which would wy co wy which we have by this trace although not yet observed on any processor is by the of the sarkar et al model to account for writes thus our definition of the idiom that we to respect is based only on ordering using execution order and not ordering observed by trace order conflicts suppose iw writes to the same location ir reads from and suppose iw po ir again it does not follow that ir must execute after iw instead it is possible that the architecture could complete the read and then ensure that iw is that the write that satisfied ir this behavior has not been observed on any architecture and we certainly do not it but in light of the existence of writes this sort of read does not seem note however that this behavior can only occur in the presence of a data race if iw is than all other writes to the relevant location as would be the case in properly synchronized code then ir cannot read from any other write in that case ir must read from iw and thus it cannot execute until iw does semantic deadlock one consequence of the very semantics of stores is that execution can find its way into a dead end in which no more progress is possible we call this semantic deadlock to be clear this is a distinct from ordinary deadlock in which a program is unable to make progress often due to a locking protocol in semantic deadlock a correct program is nevertheless unable to make progress due to inconsistent choices made by the nondeterministic semantics semantic deadlock arises when any progress would create a cycle in coherence order this can happen because of reads or pushes an example of the former is shown in the lefthand trace in figure in this trace rx cannot read from wx because if wx rf rx then wy vo rx so wy wy which would create a cycle in coherence order however no other write to x is available so rx cannot execute rz vo wx rx xo wy co wy rf ry vo wz ry vo rf push xo wx co wx rf rf rx vo push xo wy co wy wx ry rz vo rf xo rf xo wy wz rx figure test wx vo wy wy co co vo wx figure w test figure semantic deadlock an example of the latter is shown in the righthand trace of figure in this trace neither push nor wx nor wy can execute suppose push executes before wy then push o wy so wy vo wy and so wy wy which creates a cycle in coherence order thus wy must execute before push similarly wx must execute before push but push xo wx and push xo wy so no progress can be made obviously semantic deadlock cannot occur in a real execution therefore we restrict our attention to consistent histories which are histories that can be extended so that every action is executed without any new actions the former example by adding a new write for rx to read from would be the storage system must function on its own it cannot rely on a thread to break its deadlock this leads to a in type safety theorem similar to our treatment of states resulting from implementation implementing the rmc memory model on x architectures is easy the semantics provides that all instructions are executed in program order or more precisely cannot be observed to be executed out of program order each write is visible either globally or only to its own thread and writes become globally visible in program order consequently edges compile away to nothing and pushes can be implemented by power and arm are more interesting pushes are again implemented by a sync on power on arm necessary visibility can be eliminated realized by a lightweight between the two actions on power the lightweight is arm does not have one so a full is required power and arm do not provide an analogous but execution order can be enforced using a number of standard devices including data or address dependencies possibly spurious control dependencies to an power or arm instruction and control dependencies to writes note that we do not give a direct mapping of rmc constructs onto the instruction set architecture such as the mappings for cc considered in et al in cc the synchronization code is determined by the of an action so such a mapping is meaningful in contrast in rmc the synchronization code is determined by the edges connecting two actions and the resulting code might appear anywhere between the two thus a direct mapping is not meaningful for example a cc is implemented on power by st optimization might improve this but not much in rmc a visibility edge i vo i can be realized by a sync or appearing anywhere between i and i indeed multiple visibility edges might well be realized by the same instruction the cost of as always one can sometimes obtain better performance with assembly language than with compiled code consider the classic test in figure this trace is possible in rmc the edge ry xo wz breaks up the chain of visibility order between wx and rz so we cannot show that wx is prior to rx however when compiled into power using an for vo and a dependency for xo this trace is forbidden power guarantees which says that accepting a memory barrier from another thread has the same effect as generating it on your own thread for ry to happen on power the second thread must accept the barrier and the wx before it thus placing a barrier between wx and wz in our terminology we would say results in wx vo wz and causes wx to be prior to rx to prevent this the programmer would need to insert a push between wx and wy or the ry wz edge to visibility order either way some additional overhead might be as another example consider the w test in figure nothing in rmc prevents this trace from taking place however when compiled into power using an for vo this trace is forbidden power guarantees the of a relation that in rmc we would write co vo which is violated by the trace rmc makes no such guarantee to prevent this the programmer would need to insert pushes between both pairs of writes again some additional overhead might be both traces are also possible when translated into cc so cc shares these potential overheads in rmc we could avoid them by our design on stronger assumptions but we prefer to make the weakest possible assumptions thus some possible additional overhead in cases such as these can be seen as the cost of our minimal assumptions theorems type safety the first property we establish for rmc is type safety typing judgement for toplevel states is h ok with the one rule h ok h ok the auxiliary judgement h states that h is trace coherent which basically means it could be generated by dynamic semantics and hs contents match the details appear in the appendix now we can state the preservation theorem theorem preservation if s s and s ok then s ok the progress theorem is the standard formulation of states either are final or take a for rmc because of nondeterminism the goal of type safety is to show that bad states are not reachable from wellformed programs and the standard formulation identifies bad states with stuck states this is very convenient provided we can design our operational semantics so that good states have at least one transition and bad ones have no transitions in a deterministic setting this is usually feasible but for rmc it is both too weak and too strong suppose our state has two threads running e and e and suppose further that e is in a bad state even if e is stuck the state as a whole may not be because we might still be able to take a step on e in other words fails to capture when we can nondeterministically choose which thread to execute indeed for rmc the problem is even more because no expression can ever be stuck it is always possible to take a step using the rule conversely is also too strong a condition or would be if the rule did not make it trivial we call a state if it contains an v v in e where v and v are closed but not equal or if it is semantically a state might well be stuck except for if it had no work left to do other than or actions but it is not a bad state it is simply a state that from nondeterministically exploring a instead we characterize the good states directly with a judgement s right the details can be found in the appendix then instead of we prove theorem if s ok then s right this is similar to wright and original technique except they characterized the states instead of the good ones and for them was a technical device not the notion of primary interest by itself this is a little because we might have made a in the definition of to this we back to the operational semantics theorem progress if s right then either for every thread p e in s e is final or there exists s such that s s without using the rule or s is sequential consistency we also prove two results that establish sequential consistency for different programming disciplines our first shows that the programmer can achieve sequential consistency by interleaving actions with pushes the proof follows the general lines of et als sequential consistency proof but it is generalized to account for the of rmc eg writes and our main lemma is simpler we begin with three definitions specified sequential consistency indicates that the two actions are separated by a push a edge between i and i means that i reads from a write that is than i sequentially consistent order is the union of co rf and fr i h i i fr h i i sc h i def ip push i vo h ip xo h i def iw iw rf h i iw co h i def i h i i co h i i rf h i i fr h i we also define communication order as sequentially consistent order without i i def i co h i i rf h i i fr h i an important property of communication order is that it relates only accesses to the same location and it with coherence order on writes lemma main lemma suppose h is trace coherent complete that is every identifier in h is executed and coherence acyclic that is h if i xo h i i vo h i where i and i are pushes then i to h i proof we may assume without loss of generality that i is a write if i is a push or then i i because com relates only reads and writes and the result follows immediately if i is a read then it reads from some write i and we can to obtain i xo h i i vo h i since i vo h i since h is complete i and i are executed so either i to h i or i i or i to h i in the first case we are done in both of the remaining cases we have i vo h i either trivially or using push order therefore i vo h i from this we draw a contradiction suppose i is a write then i co h i since i i however we also have i i so i co h i which is a contradiction on the other hand suppose i is a read let iw rf h i then iw i so iw co h i however i i so i co h iw which is a contradiction corollary suppose h is trace coherent complete and coherence acyclic if i xo h i sc h i vo h i where i and i are pushes then i to h i proof by induction on the number of steps in i sc h i theorem suppose h is trace coherent complete and coherence acyclic then sc h is acyclic in practice the edge from ip to i is provided by labelling the push with proof suppose i sc h i we show there must be at least one edge in the cycle suppose the contrary then i i if i is a write then i co h i which is a contradiction if i is a read then i iw rf h i so iw co h iw which is a contradiction thus suppose that i i sc h i expanding we obtain i vo h ip xo h i sc h i for a push ip by corollary ip to h ip which is a contradiction definition a history h is consistent if there exists h containing no is events such that h h is trace coherent complete and coherence acyclic corollary sequential consistency suppose h is suppose further that s is a set of identifiers such that for all i i s i po h i implies i i then there exists a consistent ordering for s in the sense of lamport proof sketch let h extend h so that h is complete and co h totally orders writes to the same location by theorem sc h is acyclic so there exists a total order containing it by Д such an order is a sequentially consistent order for the actions in s observe that the sequentially consistent order includes all of hs writes not only those belonging to s thus writes not belonging to s are adopted into the sequentially consistent order this means that the members of s have a consistent view of the order in which all writes took place not just the writes belonging to s however for that order might not agree with program order the order contains reads too but for purposes of reasoning about s we can ignore them only the writes might satisfy a read in s our second is a standard result establishing sequential for programs we define as follows i sb h i def is i vo h is po h i is in essence i sb i says that i is visible to some synchronization operation is that is before i that is must be labelled so that i will also be synchronized before any successors of i note that this definition is to the mechanism of synchronization it might use an atomic readwrite the algorithm or any other we say that a program is if any two conflicting actions on different threads are ordered by boehm and refer to this sort of data race as a type data race we may then show that programs are sequentially consistent theorem suppose h is trace coherent coherence acyclic and let be the union of po h co h rf h and fr h then is acyclic proof sketch let sq h be the union of po h and sb h we show that sq h is acyclic and contains corollary sequential consistency suppose h is consistent and then there exists a sequentially consistent ordering for all identifiers in h these two sequential consistency results can also be combined into a single theorem for programs that contain some code and also use specified sequential consistency details appear in the coq formalization conclusion the problem as given above rmc makes no effort to rule out reads in which a write of a value itself thereby allowing a value to appear which has no cause to exist out of so to this could be done by the parallel trace mechanism in et al as follows first we break the execute transaction i v into two forms write execution i v for writes and execution still i v for everything else the important difference is that write execution is not passed up by expressions e e not of the form i v v v in e v v in e by itself this prevents values from being written to the store since values appear only in actions that arise within expressions we do not want to go so far as to rule out writes entirely instead we allow a execution in the thread which might be to rendezvous with a write execution provided there exists an alternative trace in which that write could have taken place without h h h halt halt halt h h however this is not a complete solution to the problem it prevents reads but at the cost of preventing some desirable compiler optimizations as and sewell illustrate it is very difficult to distinguish between traces and some desirable traces we are not by the fact that real architectures do not exhibit traces as rmc is designed to be than real architectures traces do not type safety our construct produces only welltyped values et al observe that traces can create difficulties for modular reasoning but they also observe that the problem can be resolved by typechecking however one negative consequence of reads that is still relevant to rmc is a failure of abstraction they allow a program to produce a member of an abstract type by means other than calling the module that defines it which makes abstraction fail we see this as the central problem for traces yet the problem also suggests a possible path forward as yet there is no rigorous theory of abstraction that even comes close to supporting rmc such a theory would offer some structure more than intuition for identifying problematic traces and for them out this is an important for future work related work store is inspired by the storage subsystem of sarkar et al however the rmc store is more general as it is not intended to capture the behavior of a specific architecture and the thread semantics is entirely different from sarkar et als thread subsystem another similar formalism is the axiomatic framework of et al which builds on like rmc is a generic system not specific to any architecture rmc is an operational framework and is axiomatic but this difference is less than it might seem since as noted in section requirement gives it an axiomatic moreover also defines a operational system in a similar fashion transitions may take place provided they violate no axiom and show it equivalent to the axiomatic system the greatest differences between rmc and from their aims first is not intended to serve as a programming language second while rmc aims to be weaker than all real architectures aims to model them thus includes a variety of parameterized machinery that can be instantiated to obtain the behavior of a specific architecture machinery that rmc includes four axioms that relate to five assumptions their sc per location axiom corresponds to our first two assumptions sequential consistency for singlethreaded computations and an acyclic coherence order except that unlike rmc chooses to nevertheless could easily be to permit them and rmc could them with an additional coherence rule their observation axiom includes our third assumption works but also includes recall section which rmc does not their propagation axiom includes our fourth assumption pushes exist and defines a propagation order similar to our visibility order however the propagation axiom goes further requiring in rmc terminology that co vo be acyclic recall section which we do not we omit no axiom while our fifth assumption atomic exist although includes much of the machinery necessary to support them mechanism is inspired by the one suggested in et al like rmc they allow a value to be out of whole provided that is subsequently however the details are quite different in their system the value is written immediately into the store this makes an effectful operation while in rmc it is pure moreover the means of is quite different in rmc is to a value and is when it becomes equal to the value in et al is to a location in the store occurs when the value is written to that location but that condition would be trivial if employed since such a write already at the moment of instead they maintain a parallel trace without the write and the is when the write occurs in the parallel trace putting in the thread semantics as we do rather than the store semantics makes for a formalism since it orthogonal concerns and since it does not require a mechanism on the other hand their approach automatically rules out reads which ours does not as discussed above doing so in our setting requires the addition of a mechanism of formalization all the results of this paper are formalized in coq the formalization consists of lines of code including comments and and takes seconds to check on a intel core i references s and k shared memory consistency models a ieee computer j a shared memory phd thesis paris vii j a formal hierarchy of weak memory models formal methods in system design Г j l s sarkar and p sewell in weak memory models in international conference on computer aided verification j l and m modelling simulation testing and for weak memory acm transactions on programming languages and systems to appear m and p sewell the problem working note available at html m s s sarkar p sewell and t c concurrency the model technical report n aug available at m s s sarkar p sewell and t c concurrency in acm symposium on principles of programming languages austin texas jan m k s s sarkar and p sewell and compiling cc concurrency from c to power in acm symposium on principles of programming languages pennsylvania jan m m and a library abstraction for cc concurrency in acm symposium on principles of programming languages italy jan boehm and s v foundations of the c concurrency memory model in sigplan conference on programming language design and implementation june k crary and m j a calculus for relaxed memory technical report carnegie mellon university school of computer science r arm barrier tests and d and p e circular buffers ibm power version b r c and j generative operational semantics for relaxed memory models in european symposium on programming l lamport how to make a multiprocessor computer that correctly executes programs ieee transactions on computers c sept j w and s v the java memory model in acm symposium on principles of programming languages long california jan j ordered linear logic and applications phd thesis carnegie mellon university school of computer science pittsburgh pennsylvania aug j c reynolds types abstraction and parametric polymorphism in information processing pages Г northholland proceedings of the ifip th world computer s sarkar p sewell j and l executable model arm version unpublished code s sarkar p sewell j l and d understanding power in sigplan conference on programming language design and implementation san california june p sewell s sarkar s f z and m o a rigorous and usable programmers model for x communications of the acm july a k wright and m felleisen a syntactic approach to type soundness information and computation Г 