(lp1
VVirtually every compiler performs transformations on the program it is compiling in an attempt to improve efficiency. Despite their importance, however, there have been few systematic attempts to categorise such transformations and measure their impact.In this paper we describe a particular group of transformations   the "letfloating" transformations   and give detailed measurements of their effect in an optimizing compiler for the nonstrict functional language Haskell. Letfloating has not received much explicit attention in the past, but our measurements show that it is an important group of transformations (at least for lazy languages), offering a reduction of more than 30% in heap allocation and 15% in execution time.
p2
aVWe investigate the computational efficiency of the sharing graphs of Lamping [Lam90], Gonthier, Abadi, and L&eacute;vy [GAL92], and Asperti [Asp94], designed to effect socalled optimal evaluation, with the goal of reconciling optimality, efficiency, and the clarification of reasonable cost models for the &lambda;calculus. Do these graphs suggest reasonable cost models for the &lambda;calculus? If they are optimal, are they efficient?We present a brief survey of these optimal evaluators, identifying their common characteristics, as well as their shared failures. We give a lower bound on the efficiency of sharing graphs by identifying a class of &lambda;terms that are normalizable in &Theta;(n) time, and require &Theta;(n) "fan interactions," but require &Omega;(2n) bookkeeping steps. For [GAL92], we analyze this anomaly in terms of the dynamic maintenance of deBruijn indices for intermediate terms. We give another lower bound showing that sharing graphs can do &Omega;(2n) work (via fan interactions) on graphs that have no &beta;redexes. Finally, we criticize a proposed cost model for &lambda;calculus given by Frandsen and Sturtivant [FS91], showing by example that the model does not take account of the size of intermediate forms. Our example is a term requiring &Theta;(2n) steps while having proposed cost &Theta;(n). We propose some cost models that both reflect this parameter, and simultaneously reconcile key concepts from optimal reduction.
p3
aVAn extension of the simplytyped lambda calculus is presented which contains both wellstructured inductive and coinductive types, and which also identifies a class of types for which general recursion is possible. The motivations for this work are certain natural constructions in category theory, in particular the notion of an algebraically bounded functor, due to Freyd. We propose that this is a particularly elegant core language in which to work with recursive objects, since the potential for general recursion is contained in a single operator which interacts well with the facilities for bounded iteration and coiteration.
p4
aVIn this paper we present a construction smoothly integrating pattern matching with abstract data types. We review some previous proposals [19, 23, 20, 6, 1] and their drawbacks, and show how our proposal can solve them. In particular we pay attention to equational reasoning about programs containing this new facility. We also give its formal syntax and semantics, as well as some guidelines in order to compile the construction efficiently.
p5
aVThis paper studies type inference for a functional, MLstyle language with subtyping, and focuses on the issue of simplifying inferred constraint sets. We propose a powerful notion of entailment between constraint sets, as well as an algorithm to check it, which we prove to be sound. The algorithm, although very powerful in practice, is not complete. We also introduce two new typing rules which allow simplifying constraint sets. These rules give very good practical results.
p6
aVSystem kernel Fun is an abstract version of the system Fun defined by Cardelli's and Wegner's seminal paper [CW85], and is strictly related to system F&le;. Extensions of these two systems are currently the basis of most proposals for strong type systems for objectoriented languages.We study here the problem of subtype checking for system kernel Fun, presenting the following results. We show that the standard kernel Fun subtype checking algorithm has an exponential complexity, and generates an exponential number of different subproblems. We then present a new subtype checking algorithm which has a polynomial complexity. In the process we study how variable names can be managed by a kernel Fun subtype checker which is not based on the De Bruijn encoding, and we show how to perform kernel Fun subtype checking with a "constraint generating" technique.The algorithm we give is described by a set of type rules, which we prove to be equivalent to the standard one. This new presentation of kernel Fun type system is characterized by a "multiplicative" behaviour, and it may open the way to new presentations for system F&le; as well.
p7
aVWe present the formal semantics of future in a Schemelike language which has both sideeffects and firstclass continuations. Correctness is established by proving that programs annotated by future have the same observable behaviour as their nonannotated counterparts, even though evaluation may be parallel.
p8
aVOur purpose is to promote a secondclass mechanism   the synchronization barrier   to a firstclass value. We introduce the synchron, a novel synchronization mechanism that enables the coordination of a dynamically varying set of concurrent threads that share access to a firstclass synchronization token. We demonstrate how synchrons can be used to modularly manage resources in cases where existing techniques are either inapplicable or nonmodular. In particular, synchronized lazy aggregates enable the first spaceefficient aggregate data decomposition of a wide range of algorithms. We also introduce explicitdemand graph reduction, a new semantic framework that we have developed to describe concurrency and explain the meaning of a synchron rendezvous.
p9
aVThis paper describes the distributed memory implementation of a shared memory parallel functional language. The language is Id, an implicitly parallel, mostly functional language that is currently evolving into a dialect of Haskell. The target is a distributed memory machine, because we expect these to be the most widely available parallel platforms in the future. The difficult problem is to bridge the gap between the shared memory language model and the distributed memory machine model. The language model assumes that all data is uniformly accessible, whereas the machine has a severe memory hierarchy: a processor's access to remote memory (using explicit communication) is orders of magnitude slower than its access to local memory. Thus, avoiding communication is crucial for good performance. The Id language, and its general dataflowinspierd compilation to multithreaded code are described elsewhere. In this paper, we focus on our new parallel runtime system and its features for avoiding communication and for tolerating its latency when necessary: multithreading, scheduling and load balancing; the distributed heap model and distributed coherent cacheing, and parallel garbage collection. We have completed the first implementation, and we present some preliminary performance mearsurements.
p10
aVWe have designed and implemented a programgenerator generator (PGG) for an untyped higherorder functional programming language. The program generators perform continuationbased multilevel offline specialization and thus combine the most powerful and general offline partial evaluation techniques. The correctness of the PGG is ensured by deriving it from a multilevel specialize. Our PGG is extremely simple to implement due to the use of multilevel techniques and higherorder abstract syntax.
p11
aVThe designs and implementations of efficient aggregate data structures have been important issues in functional programming. It is not clear how to select a good representation for an aggregate when access patterns to the aggregate are highly variant, or even unpredictable. Previous approaches rely on compile time analyses or programmer annotations. These methods can be unreliable because they try to predict a program's behavior before it is executed.We propose a probabilistic approach, which is based on Markov processes, for automatic selection of data representations. The selection is modeled as a random process moving in a graph with weighted edges. The proposed approach employs coin tossing at run time to help choosing a suitable data representation. The transition probability function used by the coin tossing is constructed in a simple and common way from a measured cost function. We show that, under this setting, random selections of data representations can be quite effective. The probabilistic approach is used to implement bag aggregates, and the performance results are compared to those of deterministic selection strategies.
p12
aVA number of compilers exploit the following strategy: translate a term to continuationpassing style (CPS) and optimize the resulting term using a sequence of reductions. Recent work suggests that an alternative strategy is superior: optimize directly in an extended source calculus. We suggest that the appropriate relation between the source and target calculi may be captured by a special case of a Galois connection known as a reflection. Previous work has focused on the weaker notion of an equational correspondence, which is based on equality rather than reduction. We show that Moggi's monad translation and Plotkin's CPS translation can both be regarded as reflections, and thereby strengthen a number of results in the literature.
p13
aVConcurrent ML (CML) is an extension of Standard ML of New Jersey with concurrent features similar to those of process algebra. In this papes we build upon John Reppy's reduction semantics for CML by constructing a compositional operational semantics for a fragment of CML, based on higherorder process algebra. We use this to build a semantic theory for CML, based on weak bisimulation equivalence. We give some small examples of proofs about CML expressions, and show that our semantics corresponds to Reppy's up to weak firstorder bisimulation.
p14
aVIn this paper we prove time and space bounds for the implementation of the programming language NESL on various parallel machine models. NESL is a sugared typed &lambda;calculus with a set of array primitives and an explicit parallel map over arrays. Our results extend previous work on provable implementation bounds for functional languages by considering space and by including arrays. For modeling the cost of NESL we augment a standard callbyvalue operational semantics to return two cost measures: a DAG representing the sequential dependence in the computation, and a measure of the space taken by a sequential implementation. We show that a NESL program with w work (nodes in the DAG), d depth (levels in the DAG), and s sequential space can be implemented on a p processor butterfly network, hypercube, or CRCW PRAM using O(w/p + d log p) time and O(s + dp log p) reachable space.1 For programs with sufficient parallelism these bounds are optimal in that they give linear speedup and use space within a constant factor of the sequential space.
p15
aVSynchronous dataflow is a programming paradigm which has been successfully applied in reactive systems. In this context, it can be characterized as some class of static bounded memory dataflow networks. In particular, these networks are not recursively defined, and obey some kind of "synchronous" constraints (clock calculus). Based on Kahn's relationship between dataflow and stream functions, the synchronous constraints can be related to Wadler's listlessness, and can be seen as sufficient conditions ensuring listless evaluation. As a byproduct, those networks enjoy efficient compiling techniques. In this paper, we show that it is possible to extend the class of static synchronous dataflow to higher order and dynamical networks, thus giving sense to a larger class of synchronous dataflow networks.This is done by extending both the synchronous operational semantics, the clock calculus and the compiling technique of static dataflow networks, to these more general networks.
p16
aVA context in the &lambda;calculus is a term with some holes. Hole filling differs from &beta;substitution in that name capture is intended. This seemingly simple feature transcends static scope and lies at the heart of modular and objectoriented programming. Still, the name capture feature of hole filling is at odds with hygienic &beta;substitution. In this paper we conservatively extend the &lambda;calculus to incorporate the notion of contexts without jeopardizing the &beta;rule. We perceive contexts as source code and &lambda;terms as target code. Context filling is encoded as compilation operations and the enriched calculus is a theory of separate compilation and incremental program construction. Linking of separatelydeveloped programs is done by coherent renaming of free variables.We apply our contextenriching schema to the &lambda;calculus extended with definitions and devise a calculus of firstclass modules. We show that module linking can be modeled solely by the renaming of import and export variables. We add relinkable variable references to model virtual method references essential to object systems.The inclusion of contexts introduces parameters whose linking is based on names (symbols, identifiers, or keywords). We simulate in the contextenriched calculus other extensions of the &lambda;calculus with namebased programming notions such as Dami's &lambda;calculus with names, A&iuml;tKaci and Garrigue's labelselective &lambda;calculus, Lamping's transparent data parameters, and our quasistatic procedures.
p17
aVNowadays the Net is one of the most obvious driving forces. Yet, to consider it as one global store through which values and code may be shared is still immature. This paper suggests firstclass environments as a means to achieve that goal in a multiuser Scheme framework. We propose two new special forms with a simple semantics. Our model allows precise control over environments (including extensible environments) and does not require (but does not prevent) reflective operations.
p18
aVWe define a weak &lambda;calculus, &lambda;&sigma;w, as a subsystem of the full &lambda;calculus with explicit substitutions &lambda;&sigma;&uArr;. We claim that &lambda;&sigma;w could be the archetypal output language of functional compilers, just as the &lambda;calculus is their universal input language. Furthermore, &lambda;&sigma;&uArr; could be the adequate theory to establish the correctness of simplified functional compilers. Here, we illustrate these claims by proving the correctness of two simplified compilers and runtime systems modeled as abstract machines. We first present the Krivine machine. Then, we give the first formal proofs of Cardelli's FAM and of its compiler.
p19
aVThe context for this paper is functional computation by graph reduction. Our overall aim is more efficient use of memory. The specific topic is the detection of dormant cells in the live graph   those retained in heap memory though not actually playing a useful role in computation. We describe a profiler that can identify heap consumption by such 'useless' cells. Unlike heap profilers based on traversals of the live heap, this profiler works by examining cells postmortem. The new profiler has revealed a surprisingly large proportion of 'useless' cells, even in some programs that previously seemed spaceefficient such as the bootstrapping Haskell compiler nhc.
p20
aVIdentifying some pointers as invisible threads, for the purposes of storage management, is a generalization from several widely used programming conventions, like threaded trees. The necessary invariant is that nodes that are accessible (without threads) emit threads only to other accessible nodes. Dynamic tagging or static typing of threads ameliorates storage recycling both in functional and imperative languages.We have seen the distinction between threads and links sharpen both hardware and softwaresupported storage management in SCHEME, and also in C. Certainly, therefore, implementations of languages that already have abstract management and concrete typing, should detect and use this as a new static type.
p21
aVIn this paper we present a new program analysis method which we call Storage Use Analysis. This analysis deduces how objects are used by the program and allows the optimization of their allocation. This analysis can be applied to both statically typed languages (e.g. ML) and latently typed languages (e.g. Scheme). It handles sideeffects, higher order functions, separate compilation and does not require CPS transformation. We show the application of our analysis to two important optimizations: stack allocation and unboxing. The first optimization replaces some heap allocations by stack allocations for user and system data storage (e.g. lists, vectors, procedures). The second optimization avoids boxing some objects. This analysis and associated optimitations have been implemented in the Bigloo Scheme/ML compiler. Experimental results show that for many allocation intensive programs we get a significant speedup. In particular, numerically intensive programs are almost 20 times faster because floating point numbers are unboxed and no longer heap allocated.
p22
aVTraditional techniques for designing and analyzing amortized data structures in an imperative setting are of limited use in a functional setting because they apply only to singlethreaded data structures, yet functional data structures can be nonsinglethreaded. In earlier work, we showed how lazy evaluation supports functional amortized data structures and described a technique (the banker's method) for analyzing such data structures. In this paper, we present a new analysis technique (the physicist's method) and show how one can sometimes derive a worstcase data structure from an amortized data structure by appropriately scheduling the premature execution of delayed components. We use these techniques to develop new implementations of FIFO queues and binomial queues.
p23
aVIn functional programming, small programs are often glued together to construct a complex program. Program fusion is an optimizing process whereby these small programs are fused into a single one and intermediate data structures are removed. Recent work has made it clear that this process is especially successful if the recursive definitions are expressed in terms of hylomorphisms. In this paper, we propose an algorithm which can automatically turn all practical recursive definitions into structural hylomorphisms making program fusion be easily applied.
p24
aVWe address the problem of dependency analysis and caching in the context of the &lambda;calculus. The dependencies of a &lambda;term are (roughly) the parts of the &lambda;term that contribute to the result of evaluating it. We introduce a mechanism for keeping track of dependencies, and discuss how to use these dependencies in caching.
p25
aVWe observe that the principal typing property of a type system is the enabling technology for modularity and separate compilation [10]. We use this technology to formulate a modular and polyvariant closure analysis, based on the rank 2 intersection types annotated with controlflow information.Modularity manifests itself in a syntaxdirected, annotatedtype inference algorithm that can analyse program fragments containing free variables: a principal typing property is used to formalise it. Polyvariance manifests itself in the separation of different behaviours of the same function at its different uses: this is formalised via the rank 2 intersection types. As the rank 2 intersection type discipline types at least all (core) ML programs, our analysis can be used in the separate compilation of such programs.
p26
aVWe describe an algorithm for automatic inline expansion across module boundaries that works in the presence of higherorder functions and free variables; it rearranges bindings and scopes as necessary to move nonexpansive code from one module to another. We describe and implement the algorithm as transformations on &lambda;calculus. Our inliner interacts well with separate compilation and is efficient, robust, and practical enough for everyday use in the SML/NJ compiler. Inlining improves performance by 4 8% on existing code, and makes it possible to use much more data abstraction by consistently eliminating penalties for modularity.
p27
aVWe show how to implement a calculus with higherorder subtyping and subkinding by replacing uses of implicit subsumption with explicit coercions. To ensure this can be done, a polymorphic function is adjusted to take, as an additional argument, a proof that its type constructor argument has the desired kind. Such a proof is extracted from the derivation of a kinding judgement and may in turn require proof coercions, which are extracted from subkinding judgements. This technique is formalized as a typedirected translation from a calculus of higherorder subtyping to a subtypingfree calculus. This translation generalizes an existing result for secondorder subtyping calculi (such as F &le;).We also discuss two interpretations of subtyping, one that views it as type inclusion and another that views it is the existence of a wellbehaved coercion, and we show, by a typetheoretic construction, that our translation is the minimum consequence of shifting from the inclusion interpretation to the coercionexistence interpretation. This construction shows that the translation is the natural one, and it also provides a framework for extending the translation to richer type systems. Finally, we show how the two interpretations can be reconciled in a common semantics. It is then easy to show the coherence of the translation relative to that semantics.
p28
aVWe present a type system for the programming language Erlang. The type system supports subtyping and declarationfree recursive types, using subtyping constraints. Our system is similar to one explored by Aiken and Wimmers, though it sacrifices expressive power in favour of simplicity. We cover our techniques for type inference, type simplification, and checking when an inferred type conforms to a usersupplied type signature, and report on early experience with our prototype.
p29
aVWe define a general notion of setbased analysis   any language whose operational semantics is defined by environment evaluation has a well defined setbased abstraction. This general definition covers both Aiken and Wimmers' type system and Heintze' setbased analysis. Aiken and Wimmers give a nondeterministic exponential time algorithm for their analysis. Heintze gives an O(n3) procedure. We show that this discrepancy is due to the complexity of the case statements analyzed. For polymorphic programs with deep patterns in case statements (as in the AikenWimmers language) we show that the problem of determining typesafety under setbased abstraction is complete for deterministic exponential time. The problem remains complete for deterministic exponential time for programs without let (monovariant programs). However, for monovariant programs in which all patterns in case statements are shallow, as in the Heintze language, typesafety under setbased abstraction is decidable in cubic time. We give five additional theorems which (nearly exhaustively) characterize the time complexity of setbased analysis as a function of the complexity of case statements.
p30
aVTupling is a wellknown transformation tactic to obtain new efficient recursive functions by grouping some recursive functions into a tuple. It may be applied to eliminate multiple traversals over the common data structure. The major difficulty in tupling transformation is to find what functions are to be tupled and how to transform the tupled function into an efficient one. Previous approaches to tupling transformation are essentially based on fold/unfold transformation. Though general, they suffer from the high cost of keeping track of function calls to avoid infinite unfolding, which prevents them from being used in a compiler.To remedy this situation, we propose a new method to expose recursive structures in recursive definitions and show how this structural information can be explored for calculating out efficient programs by means of tupling. Our new tupling calculation algorithm can eliminate most of multiple data traversals and is easy to be implemented.
p31
aVThe dynamicsized tabulation method can be used to eliminate redundant calls for certain classes of recursive programs. An innovative aspect of the method is the use of lambda abstractions that may subsequently be converted to bounded vectors, in order to share redundant calls via vector lookup.To facilitate this conversion to vector form, we propose a new inference method to conservatively determine the bounds for arithmetic parameters of recursive functions. Suitable techniques for inferring the safe bounds of these parameters are introduced, together with supporting transformations. The resulting method can obtain efficient vectorbased programs without the need for runtime bounds checking.
p32
aVOptimal graph reduction technology for the &lambda;calculus, as developed by Lamping, with modifications by Asperti, Gonthier, Abadl, and L&eacute;vy, has a wellunderstood local dynamics based on a standard menagerie of reduction rules, as well as a global context semantics based on Girard's geometry of interaction. However, the global dynamics of graph reduction has not been subject to careful investigation. In particular, graphs lose their structural resemblance to &lambda;terms after only a few graph reduction steps, and little is known about graph reduction strategies that maintain efficiency or structure. While the context semantics provides global information about the computation, its use as part of a reduction strategy seems computationally infeasible. We propose a tractable graph reduction strategy that preserves computationally relevant global structure, and allows us to efficiently bound the computational resources needed to implement optimal reduction.A simple canonical representation for graphs is introduced that we call fannormal form. This normal form allows us to reduce graphs based on efficient identification of &beta;redexes, rather than being guided by lowerlevel search for interacting nodes. In addition and perhaps more important, the fannormal form facilitates a complexity analysis of graph reductions, showing that the number of fan interactions is essentially bounded by a polynomial in the number of unique L&eacute;vy labels generated during a labelled reduction. This global analysis of the finitary dynamics of optimal reduction is the first demonstration that a reasonable implementationindependent cost model for the &lambda;calculus is in fact realized by Lamping's abstract algorithm. It remains to be seen whether similar claims can be made about socalled bookkeeping for fan interactions.
p33
aVThis paper describes the development of the programming language Erlang during the period 1985 1997.Erlang is a concurrent programming language designed for programming largescale distributed soft realtime control applications.The design of Erlang was heavily influenced by ideas from the logic and functional programming communities. Other sources of inspiration came from languages such as Chill and Ada which are used in industry for programming control systems.
p34
aVWe extend type specialisation to a computational lambda calculus with firstclass references. The resulting specialiser has been used to specialise a selfinterpreter for this typed computational lambda calculus optimally. Furthermore, this specialiser can perform operations on references at specialisation time, when possible.
p35
aVWe introduce the notion of compositional references into the framework of monadic functional programming and propose a set of new primitives based on this notion. They enable us to use a wide range of mutable data structures. There, references may be passed around explicitly, or mutable data structures such as arrays and tuples may be passed implicitly as hidden state. The former style is called the explicit style and is usually more expressive, while the latter is called the implicit style and has simpler semantics. We investigate the relation between the two styles and discus implementation issues.
p36
aVWe present a new framework for transforming data representations in a strongly typed intermediate language. Our method allows both value producers (sources) and value consumers (sinks) to support multiple representations, automatically inserting any required code. Specialized representations can be easily chosen for particular source/sink pairs.The framework is based on these techniques:1. Flow annotated types encode the "flowsfrom" (source) and "flowsto" (sink) information of a flow graph.2. Intersection and union types support (a) encoding precise flow information, (b) separating flow information so that transformations can be well typed, (c) automatically reorganizing flow paths to enable multiple representations.As an instance of our framework, we provide a function representation transformation that encompasses both closure conversion and inlining. Our framework is adaptable to data other than functions.
p37
aVType safety of imperative programs is an area fraught with difficulty and requiring great care. The SML solution to the problem, originally involving imperative type variables, has been recently simplified to the syntacticvalue restriction. In Haskell, the problem is addressed in a rather different way using explicit monadic state. We present an operational semantics for state in Haskell and the first full proof of type safety. We demonstrate that the semantic notion of value provided by the explicit monadic types is able to avoid any problems with generalization.
p38
aVGeneral mediaprocessing programs are easily expressed with bitaddressing and variablesized bitfields. But the natural implementation of bitaddressing relies on dynamic shift offsets and repeated loads, resulting in slow execution. If the code is specialized to the alignment of the data against word boundaries, the offsets become static and many repeated loads can be removed. We show how introducing modular arithmetic into an automatic compiler generator enables the transformation of a program that uses bitaddressing into a synthesizes of fast specialized programs.In partiaievaluation jargon we say: modular arithmetic is supported by extending the binding time lattice used by the static analysis in a polyvariant compiler generator. The new binding time Cyclic functions like a partially static integer.A software cache combined with a fast, optimistic sharing analysis built into the compilers eliminates repeated loads and stores. The utility of the transformation is demonstrated with a collection of examples and benchmark data. The examples include vector arithmetic, audio synthesis, image processing, and a base64 codec.
p39
aVIn this paper we describe the implementation of several graphical programming paradigms (Model View Controller, Fudgets, and Functional Animations) using the GUI library TkGofer. This library relies on a combination of monads and multipleparameter type classes to provide an abstract, type safe interface to Tcl/Tk. We show how choosing the right abstractions makes the given implementations surprisingly concise and easy to understand.
p40
aVFran (Functional Reactive Animation) is a collection of data types and functions for composing richly interactive, multimedia animations. The key ideas in Fran are its notions of behaviors and events. Behaviors are timevarying, reactive values, while events are sets of arbitrarily complex conditions, carrying possibly rich information. Most traditional values can be treated as behaviors, and when images are thus treated, they become animations. Although these notions are captured as data types rather than a programming language, we provide them with a denotational semantics, including a proper treatment of real time, to guide reasoning and implementation. A method to effectively and efficiently perform event detection using interval analysis is also described, which relies on the partial information structure on the domain of event times. Fran has been implemented in Hugs, yielding surprisingly good performance for an interpreterbased system. Several examples are given, including the ability to describe physical phenomena involving gravity, springs, velocity, acceleration, etc. using ordinary differential equations.
p41
aVOne of the attractive features of functional programming languages is that they provide automatic management of the store, in the form of garbage collection. However, the benefits of automatic resource management can be applied to other resources as well. Scsh, a systemsprogramming dialect of Scheme, provides automatic resource management for operatingsystems structures, notably processes, I/O channels, and signal events. This kind of automatic management extends the benefits of garbage collection modularity, robustness, simplicity, and clarity to new sets of objects in programming, and also gives us hints as to how operating systems should be structured from the perspective of functional programming languages.
p42
aVWe develop am abstract model of memory management in distributed systems. The model is lowlevel enough so that we can express communication, allocation and garbage collection, but otherwise hides many of the lowerlevel details of an actual implementation.Recently, such formal models have been developed for memory management in a functional, sequential setting [10]. The models are rewriting systems whose terms are programs. Programs have both the "code" (control string) and the "store" syntactically apparent. Evaluation is expressed as conditional rewriting and includes store operations. Garbage collection becomes a rewriting relation that removes part of the store without affecting the behavior of the program.Distribution adds another dimension to an already complex problem. By using techniques developed for communicating and concurrent systems [9], we extend their work to a distributed environment. Sending and receiving messages is also made apparent at the syntactic level. A very general garbage collection rule based on reachability is introduced and proved correct. Now proving correct a specific collection strategy is reduced to showing that the relation between programs defined by the strategy is a subrelation of the general relation. Any actual implementation which is capable of providing the transitions (including their atomicity constraints) specified by the strategy is therefore correct.This model allows us to specify and prove correct in a compact manner two garbage collectors; the first one does a simple garbage collection local to a node. The second garbage collector uses migration of data in order to be able to reclaim internode cyclic garbage.
p43
aVWe examine the costs and benefits of a variety of copying garbage collection (GC) mechanisms across multiple architectures and programming languages. Our study covers both lowlevel object representation and copying issues as well as the mechanisms needed to support more advanced techniques such as generational collection, large object spaces, and type segregated areas.Our experiments are made possible by a novel performance analysis tool, Oscar. Oscar allows us to capture snapshots of programming language heaps that may then be used to replay garbage collections. The replay program is selfcontained and written in C, which makes it easy to port to other architectures and to analyze with standard performance analysis tools. Furthermore, it is possible to study additional programming languages simply by instrumenting existing implementations to capture heap snapshots.In general, we found that careful implementation of GC mechanisms can have a significant benefit. For a simple collector, we measured improvements of as much as 95%. We then found that while the addition of advanced features can have a sizeable overhead (up to 15%), the net benefit is quite positive, resulting in additional gains of up to 42%. We also found that results varied depending upon the platform and language. Machine characteristics such as cache arrangements, instruction set (RISC/CISC), and register pool were important. For different languages, average object size seemed to be most important.The results of our experiments demonstrate the usefulness of a tool like Oscar for studying GC performance. Without much overhead, we can easily identify areas where programming language implementors could collaborate with GC implementors to improve GC performance.
p44
aVIn 1972, Reynolds outlined a general method for eliminating functional arguments known as defunctionalization. The idea underlying defunctionalization is encoding a functional value as firstorder data, and then realizing the applications of the encoded function via an apply function. Although this process is simple enough, problems arise when defunctionalization is used in a polymorphic language. In such a language, a functional argument of a higherorder function can take different type instances in different applications. As a consequence, its associated apply function can be untypable in the source language. In the paper we present a defunctionalization transformation which preserves typability. Moreover, the transformation imposes no restriction on functional arguments of recursive functions, and it handles functions as results as well as functions encapsulated in constructors. The key to this success is the use of type information in the defunctionalization transformation. Runtime characteristics are preserved by defunctionalization; hence, there is no performance improvement coming from the transformation itself. However closures need not be implemented to compile the transformed program.
p45
aVWe present a methodology for the systematic realisation of control flow analyses and illustrate it for Concurrent ML. We start with an abstract specification of the analysis that is next proved semantically sound with respect to a traditional smallstep operational semantics; this result holds for terminating w well as nonterminating programs. The analysis is defined coinductively and it is shown that all programs have a least analysis result (that is indeed the best one). To realise the analysis we massage the specification in three stages: (i) to explicitly record reachability of subexpressions, (ii) to be defined in a syntaxdirected manner, and (iii) to generate a set of constraints that subsequently can be solved by standard techniques. We prove equivalence results between the different versions of the analysis; in particular it follows that the least solution to the constraints generated will be the least analysis result also to the initial specification.
p46
aVGraph algorithms expressed in functional languages often suffer from their inherited imperative, statebased style. In particular, this impedes formal program manipulation. We show how to model persistent graphs in functional languages by graph constructors. This provides a decompositional view of graphs which is very close to that of data types and leads to a "more fictional" formulation of graph algorithms. Graph constructors enable the definition of general fold operations for graphs. We present a promotion theorem for one of these folds that allows program fusion and the elimination of intermediate results. Fusion is not restricted to the elimination of treelike structures, and we prove another theorem that facilitates the elimination of intermediate graphs. We describe an MLimplementation of persistent graphs which efficiently supports the presented fold operators. For example, depthfirstsearch expressed by a fold over a functional graph has the same complexity as the corresponding imperative algorithm.
p47
aVCatenable doubleended queues are doubleended queues (deques) that support catenation (i.e., append) efficiently without sacrificing the efficiency of other operations. We present a purely functional implementation of catenable deques for which every operation, including catenation, takes O(1) amortized time. Kaplan and Tarjan have independently developed a much more complicated implementation of catenable deques that achieves similar worstcase bounds. The two designs are superficially similar, but differ in the underlying mechanism used to achieve efficiency in a persistent setting (i.e., when used in a nonsinglethreaded fashion). Their implementation uses a technique called recursive slowdown, while ours relies on the simpler mechanism of lazy evaluation.Besides lazy evaluation, our implementation also exemplifies the use of two additional language features: polymorphic recursion and views. Neither is indispensable, but both significantly simplify the presentation.
p48
aVPattern abstractions increase the expressiveness of pattern matthing, enabling the programmer to describe a broader class of regular forests with patterns. Furthermore, pattern abstractions support code reuse and code factoring, features that facilitate maintenance and evolution of code. Past research on pattern abstractions has generally ignored the aspect of compiletime checks for exhaustiveness and redundancy. In this paper we propose a class of expressive patterns that admits these compiletime checks.
p49
aVStatically typed languages with HindleyMilner polymorphism have long been compiled using inefficient and fully boxed data representations. Recently, several new compilation methods have been proposed to support more efficient and unboxed multiword representations. Unfortunately, none of these techniques is fully satisfactory. For example, Leroy's coercionbased approach does not handle recursive data types and mutable types well. The typepassing approach (proposed by Harper and Morrisett) handles all data objects, but it involves extensive runtime type analysis and code manipulations.This paper presents a new flexible representation analysis technique that combines the best of both approaches. Our new scheme supports unboxed representations for recursive and mutable types, yet it only requires little runtime type analysis. In fact, we show that there is a continuum of possibilities between the coercionbased approach and the typepassing approach. By varying the amount of boxing and the type information passed at runtime, a compiler can freely explore any point in the continuum choosing from a wide range of representation strategies based on practical concerns. Finally, our new scheme also easily extends to handle type abstractions across MLlike higherorder modules.
p50
aVAn interprocedural flow analysis can justify inlining in higherorder languages. In principle, more inlining can be performed as analysis accuracy improves. This paper compares four flow analyses to determine how effectively they justify inlining in practice. The paper makes two contributions. First, the relative merits of the flow analyses are measured with all other variables held constant. The four analyses include two monovariant and two polyvariant analyses that cover a wide range of the accuracy/cost spectrum. Our measurements show that the effectiveness of the inliner improves slightly as analysis accuracy improves, but the improvement is offset by the compiletime cost of the accurate analyses. The second contribution is an improvement to the previously reported inlining algorithm used in our experiments. The improvement causes flow information provided by a polyvariant analysis to be selectively merged. By merging flow information depending on the inlining context, the algorithm is able to expose additional opportunities for inlining. This merging technique can be used in any program transformer justified by a polyvariant flow analysis. The revised algorithm is fully implemented in a production Scheme compiler.
p51
aVAvoiding boxing when representing native objects is essential for the efficient compilation of any programming language For polymorphic languages this task is difficult, but several schemes have been proposed that remove boxing on the basis of type information. Leroy's typedirected unboxing transformation is one of them. One of its nicest properties is that it relies only on visible types, which makes it compatible with separate compilation. However it has been noticed that it is not safe both in terms of time and space complexity  i.e. transforming a program may raise its complexity. We propose a refinement of this transformation, still relying only on visible types, and prove that it satisfies the safety condition for time complexity. The proof is an extension of the usual logical relation method, in which correctness and safety are proved simultaneously.
p52
aVWhat is a good method to specify and derive imperative programs? This paper argues that a new form of functional programming fits the bill: where variable functions can be updated at specified points in their domain.Traditional algebraic specification and functional programming are a powerful pair of tools for specifying and implementing domains of discourse and operations on them. Recent work on evolving algebras has introduced the function update in algebraic specifications, and has applied it with good success in the modelling of reactive systems. We show that similar concepts allow one to derive efficient programs in a systematic way from functional specifications. The final outcome of such a derivation can be made as efficient as a traditional imperative program with pointers, but can still be reasoned about at a high level.Variable functions can also play an important role in the structuring of large systems. They can subsume objectoriented programming languages, without incurring the latter's problems with pointer aliasing and modularity.
p53
aVInteraction nets provide a graphical paradigm of computation based on net rewriting. They have proved most successful in understanding the dynamics of reduction in the &lambda;calculus, where the prime example is the implementation of optimal reduction for the &lambda;calculus (Lamping's algorithm), given by Gonthier, Abadi and L&eacute;vy. However, efficient implementations of optimal reduction have had to break away from the interaction net paradigm. In this paper we give a new efficient interaction net encoding of the &lambda;calculus which is not optimal, but overcomes the inefficiencies caused by the bookkeeping operations in the implementations of optimal reduction. We believe that this implementation of the &lambda;calculus could provide the basis for highly efficient implementations of functional languages.
p54
aVMLJ compiles SML'97 into verifiercompliant Java bytecodes. Its features include typechecked interlanguage working extensions which allow ML and Java code to call each other, automatic recompilation management, compact compiled code and runtime performance which, using a 'just in time' compiling Java virtual machine, usually exceeds that of existing specialised bytecode interpreters for ML. Notable features of the compiler itself include wholeprogram optimisation based on rewriting, compilation of polymorphism by specialisation, a novel monadic intermediate language which expresses effect information in the type system and some interesting data representation choices.
p55
aVHigherorder modules are very effective in structuring large programs and defining generic, reusable software components. Unfortunately, many compilation techniques for the core languages do not work across the module boundaries. As a result, few optimizing compilers support these module facilities well.This paper exploits the semantic property of MLstyle modules to support efficient crossmodule compilation. More specifically, we present a typedirected translation of the MacQueenTofte higherorder modules into a predicative variant of the polymorphic &lambda;calculus F&omega;. Because modules can be compiled in the same way as ordinary polymorphic functions, standard typebased optimizations such as representation analysis immediately carry over to the module languages.We further show that the fulltransparency property of the MacQueenTofte system yields a near optimal crossmodule compilation framework. By propagating various static information through the module boundaries, many static program analyses for the core languages can be extended to work across higherorder modules.
p56
aVH/Direct is a foreignlanguage interface for the purely functional language Haskell. Rather than rely on hostlanguage type signatures, H/Direct compiles Interface Definition Language (IDL) to Haskell stub code that marshals data across the interface. This approach allows Haskell to call both C and COM, and allows a Haskell component to be wrapped in a C or COM interface. IDL is a complex language and language mappings for IDL are usually described informally. In contrast, we provide a relatively formal and precise definition of the mapping between Haskell and IDL.
p57
aVThe application of natural semantic specifications of lazy evaluation to areas such as usage analysis, formal profiling and abstract machine construction has shown it to be a useful formalism. This paper introduces several variants and extensions of this specification.The first variant is derived from observations of the Spineless Tagless Gmachine (STG), used in the Glasgow Haskell compiler. We present a modified natural semantic specification which can be formally manipulated to derive an STGlike machine.The second variant is the development of a natural semantic specification which allows functions to be applied to more than one argument at once. The STG and TIM abstract machines both allow this kind of behaviour, and we illustrate a use of this semantics by again modifying this semantics following observations of the STG machine. The resulting semantics can be used to formally derive the STG machine. This effectively proves the STG machine correct with respect to Launchbury's semantics.En route, we also show that update markers in the STG machine are necessary for termination, and show how wellknown abstract machine instructions, such as the squeeze operation, appear quite naturally as optimisations of the derived abstract machine.
p58
aVLava is a tool to assist circuit designers in specifying, designing, verifying and implementing hardware. It is a collection of Haskell modules. The system design exploits functional programming language features, such as monads and type classes, to provide multiple interpretations of circuit descriptions. These interpretations implement standard circuit analyses such as simulation, formal verification and the generation of code for the production of real circuits.Lava also uses polymorphism and higher order functions to provide more abstract and general descriptions than are possible in traditional hardware description languages. Two Fast Fourier Transform circuit examples illustrate this.
p59
aVWe provide a semantical framework for exact real arithmetic using linear fractional transformations on the extended real line. We present an extension of PCF with a real type which introduces an eventually breadthfirst strategy for lazy evaluation of exact real numbers. In this language, we present the constant redundant if, rif, for defining functions by cases which, in contrast to parallel if (pif), overcomes the problem of undecidability of comparison of real numbers in finite time. We use the upper space of the onepoint compactification of the real line to develop a denotational semantics for the lazy evaluation of real programs. Finally two adequacy results are proved, one for programs containing rif and one for those not containing it. Our adequacy results in particular provide the proof of correctness of algorithms for computation of singlevalued elementary functions.
p60
aVWe present two purely functional implementations of the computational differentiation tools   the well known numeric (not symbolic!) techniques which permit to compute pointwise derivatives of functions defined by computer programs economically and exactly. We show how the corecursive (lazy) algorithm formulation permits to construct in a transparent and elegant manner the entire infinite tower of derivatives of higher order for any expressions present in the program, and we present a purely functional variant of the reverse (or adjoint) mode of computational differentiation, using a chain of delayed evaluations represented by closures. Some concrete applications are also discussed.
p61
aVWe present a new distributed garbage collection algorithm that is able to reorganise diffusion trees and to support mobile objects. It has a modular design comprising three components: a reliable transport mechanism, a referencecounting based distributed garbage collector for nonmobile objects, and an extra layer that provides mobility. The algorithm is formalised by an abstract machine and is proved to be correct. The safety property ensures that an object may not be reclaimed as long as it is referred to locally or remotely. The liveness property guarantees that unreachable objects will eventually be reclaimed. The mobility property certifies that messages are always forwarded towards more recent mobile object positions.
p62
aVWe describe a language for defining term rewriting strategies, and its application to the production of program optimizers. Valid transformations on program terms can be described by a set of rewrite rules; rewriting strategies are used to describe when and how the various rules should be applied in order to obtain the desired optimization effects. Separating rules from strategies in this fashion makes it easier to reason about the behavior of the optimizer as a whole, compared to traditional monolithic optimizer implementations. We illustrate the expressiveness of our language by using it to describe a simple optimizer for an MLlike intermediate representation.The basic strategy language uses operators such as sequential composition, choice, and recursion to build transformers from a set of labeled unconditional rewrite rules. We also define an extended language in which the sideconditions and contextual rules that arise in realistic optimizer specifications can themselves be expressed as strategydriven rewrites. We show that the features of the basic and extended languages can be expressed by breaking down the rewrite rules into their primitive building blocks, namely matching and building terms in variable binding environments. This gives us a lowlevel core language which has a clear semantics, can be implemented straightforwardly and can itself be optimized. The current implementation generates C code from a strategy specification.
p63
aVWe present a subtyping extension to the Hindley/Milner type system that is based on name inequivalence. This approach allows the subtype relation to be defined by incremental construction of polymorphic records and datatypes, in a way that subsumes the basic type systems of both languages like ML and Java. As the main contribution of the paper, we describe a partial type inference algorithm for the extended system which favours succinctness over generality, in the sense that it never infers types with subtype constraints. The algorithm is based on an efficient approximating constraint solver, and is able to type a wide range of programs that utilize subtyping and polymorphism in a nontrivial way. Since constrained types are not inferred, the algorithm cannot be complete; however, we provide a completeness result w.r.t. the Hindley/Milner type system as a form of characterizing lower bound.
p64
aVCayenne is a Haskelllike language. The main difference between Haskell and Cayenne is that Cayenne has dependent types, i.e., the result type of a function may depend on the argument value, and types of record components (which can be types or values) may depend on other components. Cayenne also combines the syntactic categories for value expressions and type expressions; thus reducing the number of language concepts.Having dependent types and combined type and value expressions makes the language very powerful. It is powerful enough that a special module concept is unnecessary; ordinary records suffice. It is also powerful enough to encode predicate logic at the type level, allowing types to be used as specifications of programs. However, this power comes at a cost: type checking of Cayenne is undecidable. While this may appear to be a steep price to pay, it seems to work well in practice.
p65
aVIf the continuations in functional datastructuregenerating programs are made explicit and represented as records, they can be "recycled." Once they have served their purpose as temporary, intermediate structures for managing program control, the space they occupy can be reused for the structures that the programs produce as their output. To effect this immediate memory reclamation, we use a sequence of correctnesspreserving program transformations, demonstrated through a series of simple examples. We then apply the transformations to general anamorphism operators, with the important consequence that all finiteoutput anamorphisms can now be run without any stack or continuationspace overhead.
p66
aVThe paper presents a generalization of Haskell's IO monad suitable for synchronous concurrent programming. The new monad integrates the deterministic concurrency paradigm of synchronous programming with the powerful abstraction features of functional languages and with full support for imperative programming. For eventdriven applications, it offers an alternative to the use of existing, threadbased concurrency extensions of functional languages. The concepts presented have been applied in practice in a framework for programming interactive graphics.
p67
aVFolds are appreciated by functional programmers. Their dual, unfolds, are not new, but they are not nearly as well appreciated. We believe they deserve better. To illustrate, we present (indeed, we calculate) a number of algorithms for computing the breadthfirst traversal of a tree. We specify breadthfirst traversal in terms of levelorder traversal, which we characterize first as a fold. The presentation as a fold is simple, but it is inefficient, and removing the inefficiency makes it no longer a fold. We calculate a characterization as an unfold from the characterization as a fold; this unfold is equally clear, but more efficient. We also calculate a characterization of breadthfirst traversal directly as an unfold; this turns out to be the 'standard' queuebased algorithm.
p68
aVIn this paper we explain how recursion operators can be used to structure and reason about program semantics within a functional language. In particular, we show how the recursion operator fold can be used to structure denotational semantics, how the dual recursion operator unfold can be used to structure operational semantics, and how algebraic properties of these operators can be used to reason about program semantics. The techniques are explained with the aid of two main examples, the first concerning arithmetic expressions, and the second concerning Milner's concurrent language CCS. The aim of the paper is to give functional programmers new insights into recursion operators, program semantics, and the relationships between them.
p69
aVA HindleyMilner type system such as ML's seems to prohibit typeindexed values, i.e., functions that map a family of types to a family of values. Such functions generally perform case analysis on the input types and return values of possibly different types. The goal of our work is to demonstrate how to program with typeindexed values within a HindleyMilner type system.Our first approach is to interpret an input type as its corresponding value, recursively. This solution is typesafe, in the sense that the ML type system statically prevents any mismatch between the input type and function arguments that depend on this type.Such specific type interpretations, however, prevent us from combining different typeindexed values that share the same type. To meet this objection, we focus on finding a valueindependent type encoding that can be shared by different functions. We propose and compare two solutions. One requires firstclass and higherorder polymorphism, and, thus, is not implementable in the core language of ML, but it can be programmed using higherorder functors in Standard ML of New Jersey. Its usage, however, is clumsy. The other approach uses embedding/projection functions. It appears to be more practical.We demonstrate the usefulness of typeindexed values through examples including typedirected partial evaluation, C printflike formatting, and subtype coercions. Finally, we discuss the tradeoffs between our approach and some other solutions based on more expressive typing disciplines.
p70
aVIntensional polymorphism, the ability to dispatch to different routines based on types at run time, enables a variety of advanced implementation techniques for polymorphic languages, including tagfree garbage collection, unboxed function arguments, polymorphic marshalling, and flattened data structures. To date, languages that support intensional polymorphism have required a typepassing (as opposed to typeerasure) interpretation where types are constructed and passed to polymorphic functions at run time. Unfortunately, typepassing suffers from a number of drawbacks: it requires duplication of constructs at the term and type levels, it prevents abstraction, and it severely complicates polymorphic closure conversion.We present a typetheoretic framework that supports intensional polymorphism, but avoids many of the disadvantages of type passing. In our approach, runtime type information is represented by ordinary terms. This avoids the duplication problem, allows us to recover abstraction, and avoids complications with closure conversion. In addition, our type system provides another improvement in expressiveness; it allows unknown types to be refined in place thereby avoiding certain betaexpansions required by other frameworks.
p71
aVRecent advances in compiler technology have demonstrated the benefits of using strongly typed intermediate languages to compile richly typed source languages (e.g., ML). A typepreserving compiler can use types to guide advanced optimizations and to help generate provably secure mobile code. Types, unfortunately, are very hard to represent and manipulate efficiently; a naive implementation can easily add exponential overhead to the compilation and execution of a program. This paper describes our experience with implementing the FLINT typed intermediate language in the SML/NJ production compiler. We observe that a typepreserving compiler will not scale to handle large types unless all of its typepreserving stages preserve the asymptotic time and space usage in representing and manipulating types. We present a series of novel techniques for achieving this property and give empirical evidence of their effectiveness.
p72
aVArity raising, also known as variable splitting or flattening, is the program optimization which transforms a function of one argument into a function of several arguments by decomposing the structure of the original one argument into individual components in that structure. This optimization eliminates the need for the structuring of the components and also allows more arguments to be passed in registers during a function call. We present a formal specification of arity raising for a higherorder functional language. This specification supports the general arity raising of functions, even for functions which are passed as arguments or returned as values. We define a practical algorithm, based on algorithm W, which implements arity raising, and we prove this algorithm sound with respect to the deductive system. These results provide a declarative framework for reasoning about arity raising and support a richer form of the transformation than is currently found in compilers for functional languages.
p73
aVIn this paper we present a nondeterministic callbyneed (untyped) lambda calculus &lambda;nd with a constant choice and a letsyntax that models sharing. Our main result is that &lambda;nd has the nice operational properties of the standard lambda calculus: confluence on sets of expressions, and normal order reduction is sufficient to reach head normal form. Using a strong contextual equivalence we show correctness of several program transformations. In particular of lambdalifting using deterministic maximal free expressions. These results show that &lambda;nd is a new and also natural combination of nondeterminism and lambdacalculus, which has a lot of opportunities for parallel evaluation.An intended application of &lambda;nd is as a foundation for compiling lazy functional programming languages with I/O based on direct calls. The set of correct program transformations can be rigorously distinguished from noncorrect ones. All program transformations are permitted with the slight exception that for transformations like common subexpression elimination and lambdalifting with maximal free expressions the involved subexpressions have to be deterministic ones.
p74
aVSharing of evaluation is crucial for the efficiency of lazy functional languages, but unfortunately the machinery to implement it carries an inherent overhead. In abstract machines this overhead shows up as the cost of performing updates, many of them actually unnecessary, and also in the cost of the associated bookkeeping, that is keeping track of when and where to update. In spineless abstract machines, such as the STGmachine and the TIM, this bookkeeping consists of pushing, checking for and popping update markers. Checking for update markers is a very frequent operation and indeed the implementation of the STGmachine has been optimised for fast update marker checks at the expense of making the pushing and popping of update markers more costly.In this paper we present a type based sharing analysis that can determine when updates can be safely omitted and marker checks bypassed. The type system is proved sound with respect to the lazy Krivine machine. We have implemented the analysis and the preliminary benchmarks seem very promising. Most notably, virtually all update marker checks can be avoided. This may make the tradeoffs of current implementations obsolete and calls for new abstract machine designs.
p75
aVThe familiar HindleyMilner type system of the ML language family is extended with monad annotations to account for possible side effects of expression evaluation. This also allows effects to be effectively encapsulated by lexical scopes &Gamma; with enforcement provided by type checking. A typeandeffects analysis supports type inference. Type soundness and completeness theorems establish the coherence of monadic type inference with the reference semantics of a small MLstyle language.
p76
aVGifford and others proposed an effect typing discipline to delimit the scope of computational effects within a program, while Moggi and others proposed monads for much the same purpose. Here we marry effects to monads, uniting two previously separate lines of research. In particular, we show that the type, region, and effect system of Talpin and Jouvelot carries over directly to an analogous system for monads, including a type and effect reconstruction algorithm. The same technique should allow one to transpose any effect systems into a corresponding monad system.
p77
aVThe Fudgets system is a toolkit for developing graphical applications in the lazy functional programming language Haskell. In this paper we develop an operational semantics for a subset of this system, inspired by ideas from concurrency theory. A semantic theory based on bisimulation is defined and shown to be a congruence. We consider two applications of this theory: firstly, some equational rules useful for reasoning about Fudget programs are verified; secondly, we show how the operational semantics can be used to check the correctness of implementations of the Fudgets system.
p78
aVPLAN (Packet Language for Active Networks) is a new language for programs that form the packets of a programmable network. These programs replace the packet headers (which can be viewed as very rudimentary programs) used in current networks. As such, PLAN programs are lightweight and of restricted functionality. These limitations are mitigated by allowing PLAN code to call noderesident service routines written in other, more powerful languages. This twolevel architecture, in which PLAN serves as a scripting or 'glue' language for more general services, is the primary contribution of this paper. We have successfully applied the PLAN programming environment to implement an IPfree internetwork.PLAN is based on the simply typed lambda calculus and provides a restricted set of primitives and datatypes. PLAN defines a special construct called a chunk used to describe the remote execution of PLAN programs on other nodes. Primitive operations on chunks are used to provide basic data transport in the network and to support layering of protocols. Remote execution can make debugging difficult, so PLAN provides strong static guarantees to the programmer, such as type safety. A more novel property aimed at protecting network availability is a guarantee that PLAN programs use a bounded amount of network resources.
p79
aVModule and class systems have evolved to meet the demand for reuseable software components. Considerable effort has been invested in developing new module and class systems, and in demonstrating how each promotes code reuse. However, relatively little has been said about the interaction of these constructs, and how using modules and classes together can improve programs. In this paper, we demonstrate the synergy of a particular form of modules and classes called units and mixins, respectively for solving complex reuse problems in a natural manner.
p80
aVIn an impure functional language, there are programs whose behaviour is completely functional (in that they behave extensionally on inputs), but the functions they compute cannot be written in the purely functional fragment of the language. That is, the class of programs with functional behaviour is more expressive than the usual class of pure functional programs. In this paper we introduce this extended class of "functional" programs by means of examples in Standard ML, and explore what they might have to offer to programmers and language implementors.After reviewing some theoretical background, we present some examples of functions of the above kind, and discuss how they may be implemented. We then consider two possible programming applications for these functions: the implementation of a search algorithm, and an algorithm for exact realnumber integration. We discuss the advantages and limitations of this style of programming relative to other approaches. We also consider the increased scope for compiler optimizations that these functions would offer.
p81
aVBased on our experience with modelling and verifying microarchitectural designs within Haskell, this paper examines our use of Haskell as host for an embedded language. In particular, we highlight our use of Haskell's lazy lists, type classes, lazy state monad, and unsafe Perform I0, and point to several areas where Haskell could be improved in the future. We end with an example of a benefit gained by bringing the functional perspective to microarchitectural modelling.
p82
aVWe present a functional language with a type system such that well typed programs run within stated spacebounds. The language is a strict, firstorder variant of ML with constructs for explicit storage management. The type system is a variant of Tofte and Talpin's region inference system to which the notion of sized types, of Hughes, Pareto and Sabry, has been added.
p83
aVMany properties of parametric, polymorphic functions can be determined simply by inspection of their types. Such results are usually proven using Reynolds's parametricity theorem. However, Reynolds's theorem can be difficult to show in some settings, particularly ones involving computational effects. I present an alternative technique for proving some parametricity results. This technique is considerably simpler and easily generalizes to effectful settings. It works by instantiating polymorphic functions with singleton types that fully specify the behavior of the functions. Using this technique, I show that callers' stacks are protected from corruption during function calls in Typed Assembly Language programs.
p84
aVWe investigate finiterank intersection type systems, analyzing the complexity of their type inference problems and their relation to the problem of recognizing semantically equivalent terms. Intersection types allow something of type &tau;1 &Lambda; &tau;2 to be used in some places at type &tau;1 and in other places at type &tau;2. A finiterank intersection type system bounds how deeply the &Lambda; can appear in type expressions. Such type systems enjoy strong normalization, subject reduction, and computable type inference, and they support a pragmatics for implementing parametric polymorphism. As a consequence, they provide a conceptually simple and tractable alternative to the impredicative polymorphism of System F and its extensions, while typing many more programs than the HindleyMilner type system found in ML and Haskell.While type inference is computable at every rank, we show that its complexity grows exponentially as rank increases. Let K(0, n) = n and K(t + 1, n) = 2K(t,n); we prove that recognizing the pure &lambda;terms of size n that are typable at rank k is complete for DTIME[K(k&minus;1, n)]. We then consider the problem of deciding whether two &lambda;terms typable at rank k have the same normal form, generalizing a wellknown result of Statman from simple types to finiterank intersection types. We show that the equivalence problem is DTIME[K(K(k &minus; 1, n), 2)]complete. This relationship between the complexity of typability and expressiveness is identical in wellknown decidable type systems such as simple types and HindleyMilner types, but seems to fail for System F and its generalizations. The correspondence gives rise to a conjecture that if &Tau; is a predicative type system where typability has complexity t(n) and expressiveness has complexity e(n), then t(n) = &Omega;(log* e(n)).
p85
aVPositive recursive (fixpoint) types can be added to the polymorphic (Churchstyle) lambda calculus &lambda;2 (System F) in several different ways, depending on the choice of the elimination operator. We compare several such definitions and we show that they fall into two equivalence classes with respect to mutual interpretability by means of betaeta reductions. Elimination operators for fixpoint types are thus classified as either "iterators" or "recursors". This classification has an interpretation in terms of the CurryHoward correspondence: types of iterators and recursors can be seen as images of induction axioms under different dependencyerasing maps. Systems with recursors are betaeta equivalent to a calculus &lambda;2U of recursive types with the operators Fold: &sigma;[&mu;&alpha;.&sigma;/&alpha;]&larr;&mu;&alpha;.&sigma; and Unfold: &mu;&alpha;.&sigma;&larr;&sigma;[&mu;&alpha;.&sigma;/&alpha;], where the composition Unfold or Fold reduces to identity.It is known that systems with iterators can be defined within &lambda;2, by means of beta reductions. We conjecture that systems with recursors can not. In this paper we show that the system &lambda;2U does not have such a property. For this we study the notion of polymorphic type embeddability (via (beta) leftinvertible terms) and we show that if a type &sigma; is embedded into another type &tau; then &tau; must be of depth at least equal to the depth of &sigma;.
p86
aVThe increasing popularity of componentbased programming tools offer a big opportunity to designers of advanced programming languages, such as Haskell. If we can package our programs as software components, then it is easy to integrate them into applications written in other languages.In earlier work we described a preliminary integration of Haskell with Microsoft's Component Object Model (COM), focusing on how Haskell can create and invoke COM objects. This paper develops that work, concentrating on the mechanisms that support externallycallable Haskell functions, and the encapsulation of Haskell programs as COM objects.
p87
aVA good foreignlanguage interface is crucial for the success of any modern programming language implementation. Although all serious compilers for functional languages have some facility for interlanguage working, these are often limited and awkward to use.This article describes the features for bidirectional interlanguage working with Java that are built into the latest version of the MLj compiler. Because the MLj foreign interface is to another highlevel typed language which shares a garbage collector with compiled ML code, and because we are willing to extend the ML language, we are able to provide unusually powerful, safe and easy to use interlanguage working features. Indeed, rather then being a traditional foreign interface, our language extensions are more a partial integration of Java features into SML.We describe this integration of Standard ML and Java, first informally with example program fragments, and then formally in the notation used by The Definition of Standard ML.
p88
aVThe MrEd virtual machine serves both as the implementation platform for the DrScheme programming environment, and as the underlying Scheme engine for executing expressions and programs entered into DrScheme's readevalprint loop. We describe the key elements of the MrEd virtual machine for building a programming environment, and we step through the implementation of a miniature version of DrScheme in MrEd. More generally, we show how MrEd defines a highlevel operating system for graphical programs.
p89
aVWe present two complementary approaches to writing XML documentprocessing applications in a functional language.In the first approach, the generic tree structure of XML documents is used as the basis for the design of a library of combinators for generic processing: selection, generation, and transformation of XML trees.The second approach is to use a typetranslation framework for treating XML document type definitions (DTDs) as declarations of algebraic data types, and a derivation of the corresponding functions for reading and writing documents as typed values in Haskell.
p90
aVThis paper defines an extended polymorphic type system for an MLstyle programming language, and develops a sound and complete type inference algorithm. Different frdm the conventional ML type discipline, the proposed type system allows full rank 1 polymorphism, where polymorphic types can appear in other types such as product types, disjoint union types and range types of function types. Because of this feature, the proposed type system significantly reduces the valueonly restriction of polymorphism, which is currently adopted in most of MLstyle impure languages. It also serves as a basis for efficient implementation of typedirected compilation of polymorphism. The extended type system achieves more efficient type inference algorithm, and it also contributes to develop more efficient typepassing implementation of polymorphism. We show that the conventional ML polymorphism sometimes introduces exponential overhead both at compiletime elaboration and runtime typepassing execution, and that these problems can be eliminated by our type inference system. Compared with a more powerful rank 2 type inference systems based on semiunification, the proposed type inference algorithm infers a most general type for any typable expression by using the conventional firstorder unification, and it is therefore easily adopted in existing implementation of ML family of languages.
p91
aVType dispatch constructs are an important feature of many programming languages. Scheme has predicates for testing the runtime type of a value. Java has a class cast expression and a try statement for switching on an exception's class. Crucial to these mechanisms, in typed languages, is type refinement: The static type system will use type dispatch to refine types in successful branches. Considerable previous work has addressed type case constructs for structural type systems without subtyping, but these do not extend to named type systems with subtyping, as is common in object oriented languages. Previous work on type dispatch in named type systems with subtyping has not addressed its implementation formally.This paper describes a number of type dispatch constructs that share a common theme: class cast and class case constructs in object oriented languages, ML style exceptions, hierarchical extensible sums, and multimethods. I describe a unifying mechanism, tagging, that abstracts the operation of these constructs, and I formalise a small tagging language. After discussing how to implement the tagging language, I present a typed language without type dispatch primitives, and a give a formal translation from the tagging language.
p92
aVWe propose a conservative extension of the polymorphic lambda calculus (F&omega;) as an intermediate language for compiling languages with namebased class and interface hierarchies. Our extension enriches standard F&omega; with recursive types, existential types, and row polymorphism, but only ordered records with no subtyping. Basing our language on F&omega; makes it also a suitable target for translation from other higherorder languages; this enables the safe interoperation between classbased and higherorder languages and the reuse of common typedirected optimization techniques, compiler back ends, and runtime support.We present the formal semantics of our intermediate language and illustrate its features by providing a formal translation from a subset of Java, including classes, interfaces, and private instance variables. The translation preserves the namebased hierarchical relation between Java classes and interfaces, and allows access to private instance variables of parameters of the same class as the one defining the method. It also exposes the details of method invocation and instance variable access and allows many standard optimizations to be performed on the objectoriented code.
p93
aVPrograms are often structured around the idea that different pieces of code comprise distinct principals, each with a view of its environment. Typical examples include the modules of a large program, a host and its clients, or a collection of interactive agents.In this paper, we formalize this notion of principal in the programming language itself. The result is a language in which intuitive statements such as, "the client must call open to obtain a file handle," can be phrased and proven formally.We add principals to variants of the simplytyped &lambda;calculus and show how we can track the code corresponding to each principal throughout evaluation. This multiagent calculus yields syntactic proofs of some type abstraction properties that traditionally require semantic arguments.
p94
aVThis paper presents a technique for compiling Standard ML Modules into typed intermediate language fragments, which may be compiled separately and linked using traditional linking technology to form executable code. The technique is called static interpretation and allows compiletime implementation details to propagate across module boundaries. Static interpretation eliminates all modulelevel code at compile time.The technique scales to full Standard ML and is used in the ML Kit with Regions compiler. A framework for smart recompilation makes the technique useful for compiling large programs.
p95
aVMLstyle modules are valuable in the development and maintenance of large software systems, unfortunately, none of the existing languages support them in a fully satisfactory manner. The Official SML'97 Definition does not allow higherorder functors, so a module that refers to externally defined functors cannot accurately describe its import interface. MacQueen and Tofte [26] extended SML'97 with fully transparent higherorder functors, but their system does not have a typetheoretic semantics thus fails to support fully syntactic signatures. The systems of manifest types [19, 20] and translucent sums [12] support fully syntactic signatures but they may propagate fewer type equalities than fully transparent functors. This paper presents a module calculus that supports both fully transparent higherorder functors and fully syntactic signatures (and thus true separate compilation). We give a simple typetheoretic semantics to our calculus and show how to compile it into an F&omega;like &lambda;calculus extended with existential types.
p96
aVRuntime type dispatch enables a variety of advanced optimization techniques for polymorphic languages, including tagfree garbage collection, unboxed function arguments, and flattened data structures. However, modern typepreserving compilers transform types between stages of compilation, making type dispatch prohibitively complex at low levels of typed compilation. It is crucial therefore for type analysis at these low levels to refer to the types of previous stages. Unfortunately, no current intermediate language supports this facility.To fill this gap, we present the language LX, which provides a rich language of type constructors supporting type analysis (possibly of previousstage types) as a programming idiom. This language is quite flexible, supporting a variety of other applications such as analysis of quantified types, analysis with incomplete type information, and type classes. We also show that LX is compatible with a typeerasure semantics.
p97
aVDeforestation optimises a functional program by transforming it into another one that does not create certain intermediate data structures. Short cut deforestation is a deforestation method which is based on a single, local transformation rule. In return, short cut deforestation expects both producer and consumer of the intermediate structure in a certain form. Warm fusion wan proposed to automatically transform functions into this form. Unfortunately, it is costly and hard to implement. Starting from the fact that short cut deforestation is based on a parametricity theorem of the secondorder typed &lambda;calculus, we show how the required form of a list producer can be derived by the use of type inference. Typability for the secondorder typed &lambda;calculus is undecidable. However, we present a lineartime algorithm that solves a partial type inference problem and that, together with controlled inlining and polymorphic type instantiation, suffices for deforestation. The resulting new short cut deforestation algorithm is efficient and removes more intermediate lists than the original.
p98
aVWe present an approach for the verification of Erlang programs using abstract interpretation and model checking. In general model checking for temporal logics like LTL and Erlang programs is undecidable. Therefore we define a framework for abstract interpretations for a core fragment of Erlang. In this framework it is guaranteed, that the abstract operational semantics preserves all paths of the standard operational semantics. We consider properties that have to hold on all paths of a system, like properties in LTL. If these properties can be proved for the abstract operational semantics, they also hold for the Erlang program. They can be proved with model checking if the abstract operational semantics is a finite transition system. Therefore we introduce a example abstract interpretation, which has this property. We have implemented this approach as a prototype and were able to prove properties like mutual exclusion or the absence of deadlocks and lifelocks for some Erlang programs.
p99
aVIn a programming language with procedures and assignments, it is often important to isolate uses of state to particular program fragments. The frameworks of type, region, and effect inference, and monadic state are technologies that have been used to state and enforce the property that an expression has no visible sideeffects. This property has been exploited to justify the deallocation of memory regions despite the presence of dangling pointers.Starting from an idea developed in the context of monadic state in Haskell, we develop an MLlike language with full assignments and an operator that enforces the encapsulation of effects. Using this language, we formalize and prove the folklore connection between effect masking and monadic encapsulation. Then, by employing a novel set of reductions to deal with dangling pointers, we establish the soundness of the typebased encapsulation with a proof based on a standard subject reduction argument.
p100
aVA trampolined program is organized as a single loop in which computations are scheduled and their execution allowed to proceed in discrete steps. Writing programs in trampolined style supports primitives for multithreading without language support for continuations. Various forms of trampolining allow for different degrees of interaction between threads. We present two architectures based on an only mildly intrusive trampolined style. Concurrency can be supported at multiple levels of granularity by performing the trampolining transformation multiple times.
p101
aVSquare matrices serve as an interesting case study in functional programming. Common representations, such as lists of lists, are both inefficient at least for access to individual elements and errorprone, because the compiler cannot enforce "squareness". Switching to a typical balancedtree representation solves the first problem, but not the second. We develop a representation that solves both problems: it offers logarithmic access to each individual element and it captures the shape invariants in the type, where they can be checked by the compiler. One interesting feature of our solution is that it translates the wellknown fast exponentiation algorithm to the level of types. Our implementation also provides a stress test for today's advanced type systems it uses nested types, polymorphic recursion, higherorder kinds, and rank2 polymorphism.
p102
aVThe advantage of lazy functional languages is that programs may be written declaratively without specifying the exact evaluation order. The ensuing order of evaluation can however be quite involved which makes it difficult to debug such programs using traditional, operational techniques. A solution is to trace the computation in a way which focuses on the declarative aspects and hides irrelevant operational details. The main problem with this approach is the immense cost in time and space of tracing large computations. Dealing with these performance issues is thus the key to practical, general purpose debuggers for lazy functional languages. In this paper we show that computing partial traces on demand by reexecuting the traced program is a viable way to overcome these difficulties. This allows any program to be traced using only a fixed amount of extra storage. Since it takes a lot of time to build a complete trace, most of which is wasted since only a fraction of a typical trace is investigated during debugging, partial tracing and repeated reexecution is also attractive from a time perspective. Performance figures are presented to substantiate our claims.
p103
aVLanguages such as Java, ML, Scheme, and Haskell provide automatic storage management, that is, garbage collection. The two fundamental operations performed on a garbagecollected heap are "allocate" and "collect." Because the heap is in an inconsistent state during these operations, they must be performed atomically. Otherwise, a heap client might access the heap during a time when its fundamental invariants do not hold, corrupting the heap.Standard techniques for providing this atomicity guarantee have large latencies and other performance problems that impede their application in highperformance, interruptladen, threadbased systems applications. In particular, the standard techniques prevent thread schedulers from switching threads on VM page faults.We cast the space of possible implementations into a general taxonomy, and describe a new technique that provides a simple, lowoverhead, lowlatency interlock. We have implemented this technique in a version of SML/NJ, and, because of its applicability to threadbased systems, are currently implementing it in the scheduler of our rawhardware SMLbased kernel, ML/OS. Our technique can be extended to provide other atomic sequences besides storage allocation.
p104
aVKleisli is a modern data integration system that has made a significant impact on bioinformatics data integration. The primary query language provided by Kleisli is called CPL, which is a functional query language whose surface syntax is based on the comprehension syntax. Kleisli is itself implemented using the functional language SML. This paper describes the influence of functional programming research that benefits the Kleisli system, especially the less obvious ones at the implementation level.
p105
aVFranTk is a new high level library for programming Graphical User Interfaces (GUIs) in Haskell. It is based on Fran (Functional Reactive Animation), and uses the notions of Behaviors and Events to structure code. Behaviors are timevarying, reactive values. They can be used to represent the state of an application. Events are streams of values that occur at discrete points in time. They can be used, for instance, to represent user input. FranTk allows user interfaces to be structured in a more declarative manner than has been possible with previous functional GUI libraries. We demonstrate, through a series of examples, how this is achieved, and why it is important. These examples are elements of a prototype, Air Traffic Control simulator. FranTk uses a binding to the popular Tcl/Tk toolkit to provide a powerful set of platform independent set of widgets. It has been released as a Haskell library that runs under Hugs and GHC.
p106
aVThis paper describes FC++: a rich library supporting functional programming in C++. Prior approaches to encoding higher order functions in C++ have suffered with respect to polymorphic functions from either lack of expressiveness or high complexity. In contrast, FC++ offers full and concise support for higherorder polymorphic functions through a novel use of C++ type inference.Another new element in FC++ is that it implements a subtype polymorphism policy for functions, in addition to the more common parametric polymorphism facilities. Subtype polymorphism is common in object oriented languages and ensures that functions in FC++ fit well within the C++ object model.Apart from these conceptual differences, FC++ is also an improvement in technical terms over previous efforts in the literature. Our function objects are referencecounted and can be aliased without needing to be copied, resulting in an efficient implementation. The referencecounting mechanism is also exported to the user as a generalpurpose replacement of native C++ pointers. Finally, we supply a number of useful functional operators (a large part of the Haskell Standard Prelude) to facilitate programming with FC++. The end result is a library that is usable and efficient, while requiring no extensions to the base C++ language.
p107
aVThe past three decades have seen a plethora of language features for largescale software composition. Some of these are fairly simple, others quite sophisticated. Each embodies an implicit claim that its particular combination of features is both necessary and sufficient for some problem domain. But there have been few attempts to compare module systems, or to explain the practical and theoretical issues that motivate the choices they embody. This can make it difficult to evaluate which features are really needed for a given setting, and which may be overkill.
p108
aVEvery programmer has blind spots. Breadthfirst numbering is an interesting toy problem that exposes a blind spot common to many perhaps most functional programmers.
p109
aVIn this paper we propose a new method for deriving a practical lineartime algorithm from the specification of a maximumweightsum problem: From the elements of a data structure x, find a subset which satisfies a certain property p and whose weightsum is maximum. Previously proposed methods for automatically generating lineartime algorithms are theoretically appealing, but the algorithms generated are hardly useful in practice due to a huge constant factor for space and time. The key points of our approach are to express the property p by a recursive boolean function over the structure x rather than a usual logical predicate and to apply program transformation techniques to reduce the constant factor. We present an optimization theorem, give a calculational strategy for applying the theorem, and demonstrate the effectiveness of our approach through several nontrivial examples which would be difficult to deal with when using the methods previously available.
p110
aVCheap eagerness is an optimization where cheap and safe expressions are evaluated before it is known that their values are needed. Many compilers for lazy functional languages implement this optimization, but they are limited by a lack of information about the global flow of control and about which variables are already evaluated. Without this information, even a variable reference is a potentially unsafe expression!In this paper we show that significant speedups are achievable by cheap eagerness. Our cheapness analysis uses the results of a programwide data and control flow analysis to find out which variables may be unevaluated and which variables may be bound to functions which are dangerous to call.
p111
aVWe present an operational semantics for parallel lazy evaluation that accurately models the parallel behaviour of the nonstrict parallel functional language GpH. Parallelism is modelled synchronously, that is, single reductions are carried out separately then combined before proceeding to the next set of reductions. Consequently the semantics has two levels, with transition rules for individual threads at one level and combining rules at the other. Each parallel thread is modelled by a binding labelled with an indication of its activity status. To the best of our knowledge this is the first semantics that models such thread states. A set of labelled bindings corresponds to a heap and is used to model sharing.The semantics is set at a higher level of abstraction than an abstract machine and is therefore more manageable for proofs about programs rather than implementations. At the same time, it is sufficiently low level to allow us to reason about programs in terms of parallelism (i.e. the number of processors used) as well as work and runtime with different numbers of processors.The framework used by the semantics is sufficiently flexible and general that it can easily be adapted to express other evaluation models such as sequential callbyneed, speculative evaluation, nondeterministic choice and others.
p112
aVMonads have become a popular tool for dealing with computational effects in Haskell for two significant reasons: equational reasoning is retained even in the presence of effects; and program modularity is enhanced by hiding "plumbing" issues inside the monadic infrastructure. Unfortunately, not all the facilities provided by the underlying language are readily available for monadic computations. In particular, while recursive monadic computations can be defined directly using Haskell's builtin recursion capabilities, there is no natural way to express recursion over the values of monadic actions. Using examples, we illustrate why this is a problem, and we propose an extension to Haskell's donotation to remedy the situation. It turns out that the structure of monadic valuerecursion depends on the structure of the underlying monad. We propose an axiomatization of the recursion operation and provide a catalogue of definitions that satisfy our criteria.
p113
aVIn a paper about pretty printing J. Hughes introduced two fundamental techniques for deriving programs from their specification, where a specification consists of a signature and properties that the operations of the signature are required to satisfy. Briefly, the first technique, the term implementation, represents the operations by terms and works by defining a mapping from operations to observations   this mapping can be seen as defining a simple interpreter. The second, the contextpassing implementation, represents operations as functions from their calling context to observations. We apply both techniques to derive a backtracking monad transformer that adds backtracking to an arbitrary monad. In addition to the usual backtracking operations   failure and nondeterministic choice   the prolog cut and an operation for delimiting the effect of a cut are supported.
p114
aVWe show that standard formulations of intersection type systems are unsound in the presence of computational effects, and propose a solution similar to the value restriction for polymorphism adopted in the revised definition of Standard ML. It differs in that it is not tied to letexpressions and requires an additional weakening of the usual subtyping rules. We also present a bidirectional typechecking algorithm for the resulting language that does not require an excessive amount of type annotations and illustrate it through some examples. We further show that the type assignment system can be extended to incorporate parametric polymorphism. Taken together, we see our system and associated typechecking algorithm as a significant step towards the introduction of intersection types into realistic programming languages. The added expressive power would allow many more properties of programs to be stated by the programmer and statically verified by a compiler.
p115
aVWe propose regular expression types as a foundation for XML processing languages. Regular expression types are a natural generalization of Document Type Definitions (DTDs), describing structures in XML documents using regular expression operators (i.e., *, ?, |, etc.) and supporting a simple but powerful notion of subtyping.The decision problem for the subtype relation is EXPTIMEhard, but it can be checked quite efficiently in many cases of practical interest. The subtyping algorithm developed here is a variant of Aiken and Murphy's setinclusion constraint solver, to which are added several optimizations and two new properties: (1) our algorithm is provably complete, and (2) it allows a useful "subtagging" relation between nodes with different labels in XML trees.
p116
aVWe show that a nonduplicating CPS transformation has no effect on controlflow analysis and that it has a positive effect on bindingtime analysis: a monovariant controlflow analysis yields equivalent results on a directstyle program and on its CPS counterpart, and a monovariant bindingtime analysis yields more precise results on a CPS program than on its directstyle counterpart. Our proof technique amounts to constructing the continuationpassing style (CPS) counterpart of flow information and of binding times.Our results confirm a folklore theorem about bindingtime analysis, namely that CPS has a positive effect on binding times. What may be more surprising is that this benefit holds even if contexts or continuations are not duplicated.The present study is symptomatic of an unsettling property of program analyses: their quality is unpredictably vulnerable to syntactic accidents in source programs, i.e., to the way these programs are written. More reliable program analyses require a better understanding of the effect of syntactic change.
p117
aVAlgorithms for checking subtyping between recursive types lie at the core of many programming language implementations. But the fundamental theory of these algorithms and how they relate to simpler declarative specifications is not widely understood, due in part to the difficulty of the available introductions to the area. This tutorial paper offers an "endtoend" introduction to recursive types and subtyping algorithms, from basic theory to efficient implementation, set in the unifying mathematical framework of coinduction.
p118
aVWe present the &mu; calculus, a syntax for &lambda;calculus + control operators exhibiting symmetries such as program/context and callbyname/callbyvalue. This calculus is derived from implicational Gentzen's sequent calculus LK, a key classical logical system in proof theory. Under the CurryHoward correspondence between proofs and programs, we can see LK, or more precisely a formulation called LK&mu; , as a syntaxdirected system of simple types for &mu; calculus. For &mu; calculus, choosing a callbyname or callbyvalue discipline for reduction amounts to choosing one of the two possible symmetric orientations of a critical pair. Our analysis leads us to revisit the question of what is a natural syntax for callbyvalue functional computation. We define a translation of &lambda;&mu;calculus into &mu; calculus and two dual translations back to &lambda;calculus, and we recover known CPS translations by composing these translations.
p119
aVMemory is the performance bottleneck of modern architectures. Keeping memory consumption as low as possible enables fast and unobtrusive applications. But it is not easy to estimate the memory use of programs implemented in functional languages, due to both the complex translations of some high level constructs, and the use of automatic memory managers.To help understand memory allocation behavior of Scheme programs, we have designed two complementary tools. The first one reports on frequency of allocation, heap configurations and on memory reclamation. The second tracks down memory leaks1. We have applied these tools to our Scheme compiler, the largest Scheme program we have been developing. This has allowed us to drastically reduce the amount of memory consumed during its bootstrap process, without requiring much development time.Development tools will be neglected unless they are both conveniently accessible and easy to use. In order to avoid this pitfall, we have carefully designed the user interface of these two tools. Their integration into a real programming environment for Scheme is detailed in the paper.
p120
aVWe describe an efficient technique for incorporating Baker's incremental garbage collection algorithm into the Spineless Tagless Gmachine on stock hardware. This algorithm eliminates the stop/go execution associated with bulk copying collection algorithms, allowing the system to place an upper bound on the pauses due to garbage collection. The technique exploits the fact that objects are always accessed by jumping to code rather than being explicitly dereferenced. It works by modifying the entry codepointer when an object is in the transient state of being evacuated but not scavenged. An attempt to enter it from the mutator causes the object to "selfscavenge" transparently before resetting its entry code pointer. We describe an implementation of the scheme in v4.01 of the Glasgow Haskell Compiler and report performance results obtained by executing a range of applications. These experiments show that the read barrier can be implemented in dynamic dispatching systems such as the STGmachine with very short mutator pause times and with negligible overhead on execution time.
p121
aVQuick Check is a tool which aids the Haskell programmer in formulating and testing properties of programs. Properties are described as Haskell functions, and can be automatically tested on random input, but it is also possible to define custom test data generators. We present a number of case studies, in which the tool was successfully used, and also point out some pitfalls to avoid. Random testing is especially suitable for functional programs because properties can be stated at a fine grain. When a function is built from separately tested components, then random testing suffices to obtain good coverage of the definition under test.
p122
aVWhile developing the software of a browseroperated educational CDROM, we had to face a number of problems. This paper presents these problems and the solutions we found. Amusingly, most of our solutions rely on continuations. Are browsers and multimedia the future of continuations?Through their "Back" button or "Clone window" menu item, browsers have powerful abilities that force servers to take care of multiply and simultaneously answered questions. A comprehensive tool to apprehend these problems as well as to solve them is to view these abilities as operators acting on the continuations of the computation performed by servers.Thematical trails are provided to walk through the CDROM but do not prevent students to wander elsewhere. A trail may contain choices or quizzes so the rest of the trail should adapt to the walked part. We consider the trail as a computation and the position of the student as a continuation within that computation.Moreover this paper advocates a computationcentric view of servers (in opposition to the usual pagecentric view) where interactions with users suspend the computation into continuations that may be later resumed. This approach is superior because the continuation reifies, automatically and without errors, the whole state of the computation.
p123
aVA number of security systems for programming languages have recently appeared, including systems for enforcing some form of access control. The Java JDK 1.2 security architecture is one such system that is widely studied and used. While the architecture has many appealing features, access control checks are all implemented via dynamic method calls. This is a highly nondeclarative form of specification which is hard to read, and which leads to additional runtime overhead. In this paper, we present a novel security type system that enforces the same security guarantees as Java Stack Inspection, but via a static type system with no additional runtime checks. The system allows security properties of programs to be clearly expressed within the types themselves. We also define and prove correct an inference algorithm for security types, meaning that the system has the potential to be layered on top of the existing Java architecture, without requiring new syntax.
p124
aVThis paper shows how to systematically extend an arbitrary type system with dependency information, and how soundness and noninterference proofs for the new system may rely upon, rather than duplicate, the soundness proof of the original system. This allows enriching virtually any of the type systems known today with information flow analysis, while requiring only a minimal proof effort.Our approach is based on an untyped operational semantics for a labelled calculus akin to core ML. Thus, it is simple, and should be applicable to other computing paradigms, such as object or process calculi.The paper also discusses access control, and shows it may be viewed as entirely independent of information flow control. Letting the two mechanisms coexist, without interacting, yields a simple and expressive type system, which allows, in particular, "selective" declassification.
p125
aVIn a language with nonparametric or adhoc polymorphism, it is possible to determine the identity of a type variable at runtime. With this facility, we can write a function to convert a term from one abstract type to another, if the two hidden types are identical. However, the naive implementation of this function requires that the term be destructed and rebuilt. In this paper, we show how to eliminate this overhead using higherorder type abstraction. We demonstrate this solution in two frameworks for adhoc polymorphism: intensional type analysis and type classes.
p126
aVI present a typepreserving translation that eliminates subtyping and bounded quantification without introducing any runtime costs. This translation is based on Mitchell and Pierce's encoding of bounded quantification using intersection types. I show that, previous negative observations notwithstanding, the encoding is adequate given a sufficiently rich target type theory. The necessary target type theory is made easily typecheckable by including a collection of explicit coercion combinators, which are already desired for eliminating subtyping. However, no form of coercion abstraction is necessary (even to support bounded quantification), leading to a simple target language.
p127
aVCompilers for polymorphic languages can use runtime type inspection to support advanced implementation techniques such as tagless garbage collection, polymorphic marshalling, and flattened data structures. Intensional type analysis is a typetheoretic framework for expressing and certifying such typeanalyzing computations. Unfortunately, existing approaches to intensional analysis do not work well on types with universal, existential, or fixpoint quantifiers. This makes it impossible to code applications such as garbage collection, persistence, or marshalling which must be able to examine the type of any runtime value.We present a typed intermediate language that supports fully reflexive intensional type analysis. By fully reflexive, we mean that typeanalyzing operations are applicable to the type of any runtime value in the language. In particular, we provide both typelevel and termlevel constructs for analyzing quantified types. Our system supports structural induction on quantified types yet type checking remains decidable. We show how to use reflexive type analysis to support typesafe marshalling and how to generate certified typeanalyzing object code.
p128
aVThis paper generalises the flattening transformation a technique for the efficient implementation of nested data parallelism and reconciles it with main stream functional programming. Nested data parallelism is significantly more expressive and convenient to use than the flat data parallelism typically used in conventional parallel languages like High Performance Fortran and C*. The flattening transformation of Blelloch and Sabot is a key technique for the efficient implementation of nested parallelism via flat parallelism, but originally it was severely restricted, as it did not permit general sum types, recursive types, higherorder functions, and separate compilation. Subsequent work, including some of our own, generalised the transformation and allowed higherorder functions and recursive types. In this paper, we take the final step of generalising flattening to cover the full range of types available in modern languages like Haskell and ML; furthermore, we enable the use of separate compilation. In addition, we present a completely new formulation of the transformation, which is based on the standard lambda calculus notation, and replace a previously adhoc transformation step by a systematic generic programming technique. First experiments demonstrate the efficiency of our approach.
p129
aVTell category theorists about the concept of abstract syntax for a language and they may say "that's just the initial algebra for a sumofproducts functor on the category of sets". Despite what you might think, they are trying to be helpful since the initiality property is the common denominator of both definitions by structural recursion and proofs by structural induction [5, Sect. 4.4]. In recent years we have learned how to extend this initial algebra view of abstract syntax to encompass languages with statically scoped binders. In the presence of such binders one wants to abstract away from the specific names of bound variables, either by quotienting parse trees by a suitable notion of alphaequivalence, or by replacing conventional trees with ones containing de Bruijn indices [1]. By changing from the category of sets to other wellknown, but still 'setlike' categories of sheaves or presheaves, one can regain an initial algebra view of this even more than normally abstract syntax the payoff being new and automatically generated forms of structural recursion and induction that respect alphaequivalence [2, 3]. One good test of these new ideas is to see if they give rise to new forms of functional programming. In fact they do. The paper [6] sketches a functional programming language for representing and manipulating syntactical structure involving binders, based on the mathematical model of variablebinding in [3, 4]. In this MLlike language there are new forms of type for names and namebinding that come along with facilities for declaring fresh names, for binding names in abstractions and for pulling apart such nameabstractions via patternmatching. The key idea is that properly abstract uses of names, i.e. ones that do not descend below the level of alphaconversion, can be imposed on the user by a static type system that deduces information about the freshness of names. Even though we appear to be giving users a 'gensym' facility, the type system restricts the way it can be used to the extent that we keep within effectfree functional programming, in the sense that the usual laws of pure functional programming remain valid (augmented with new laws for names and nameabstractions). In this talk I will introduce this new approach to representing languages static binders in functional programming and discuss some of the difficulties we have had verifying its semantic properties.
p130
aVWe demonstrate a natural mapping from XML element types to ML module expressions. The mapping is inductive and definitions of common XML operations can be derived as the module expressions are composed. We show how to derive, in a generic way, the validation function, which checks an XML document for conformance to its DTD (Document Type Definition). One can view validation as assigning ML types to XML elements and the validation procedure a prerequisite for typeful XML programming in ML. Our mapping uses the parametric module facility of ML in some contrived way. For example, in validating WML (WAP Markup Language) documents, we need to use 36ary type constructors, as well as higherorder modules that take in as many as 17 modules as input. That one can systematically model XML DTDs at the module level suggests MLlike languages are suitable for typesafe prototyping of DTDaware XML applications.
p131
aVThis paper presents a new implementation technique for priority search queues. This abstract data type is an amazing blend of finite maps and priority queues. Our implementation supports logarithmic access to a binding with a given key and constant access to a binding with the minimum value. Priority search queues can be used, for instance, to give a simple, purely functional implementation of Dijkstra's singlesource shortestpaths algorithm. A nontechnical concern of the paper is to foster abstract data types and views. Priority search queues have been largely ignored by the functional programming community and we believe that they deserve to be known better. Views prove their worth both in defining a convenient interface to the abstract data type and in providing a readable implementation.
p132
aVLula is a system for computerassisted stage lighting design and control. Whereas other systems for the same purpose are usually the results of long chains of incremental improvements of historic concepts, Lula represents a complete redesign. Whereas other systems focus on control aspects of lighting, Lula focuses on design and generates control information from it. This approach gives significantly more flexibility to the lighting designer and shortens the design process itself. Lula's design and implementation draw from a number of disciplines in advanced programming. It is written in Scheme and runs atop PLT Scheme, and benefits from its highlevel GUI library. Lula uses an algebraic model for lighting looks based on just three combinators. It employs Functional Reactive Programming for all dynamic aspects of lighting, and is programmable via a functional reactive domainspecific language. Lula is an actual product and has users who have neither interest in nor knowledge of functional programming.
p133
aVIt is an established notion among financial analysts that price moves in patterns and these patterns can be used to forecast future price. As the definitions of these patterns are often subjective, every analyst has a need to define and search meaningful patterns from historical time series quickly and efficiently. However, such discovery process can be extremely laborious and technically challenging in the absence of a highlevel pattern definition language. In this paper, we propose a chartpattern language (CPL for short) to facilitate pattern discovery process. Our language enables financial analysts to (1) define patterns with subjective criteria, through introduction of fuzzy constraints, and (2) incrementally compose complex patterns from simpler patterns. We demonstrate through an array of examples how real life patterns can be expressed in CPL. In short, CPL provides a highlevel platform upon which analysts can define and search patterns easily and without any programming expertise. CPL is a domainspecific language embedded in Haskell. We show how various features of a functional language, such as pattern matching, higherorder functions, lazy evaluation, facilitate pattern definitions and implementation. Furthermore Haskell's type system frees the Programmers from annotating the programs with types.
p134
aVFunctional reactive programming (FRP) is a declarative programming paradigm where the basic notions are continuous, timevarying behaviors and discrete, eventbased reactivity. FRP has been used successfully in many reactive programming domains such as animation, robotics, and graphical user interfaces. The success of FRP in these domains encourages us to consider its use in realtime applications, where it is crucial that the cost of running a program be bounded and known before runtime. But previous work on the semantics and implementation of FRP was not explicitly concerned about the issues of cost. In fact, the resource consumption of FRP programs in the current implementation is often hard to predict. As a first step towards addressing these concerns, this paper presents realtime FRP (RTFRP), a staticallytyped language where the time and space cost of each execution step for a given program is statically bounded. To take advantage of existing work on languages with bounded resources, we split RTFRP into two parts: a reactive part that captures the essential ingredients of FRP programs, and a base language part that can be instantiated to any generic programming language that has been shown to be terminating and resourcebounded. This allows us to focus on the issues specific to RTFRP, namely, two forms of recursion. After presenting the operational explanation of what can go wrong due to the presence of recursion, we show how the typed version of the language is terminating and resourcebounded. Most of our FRP programs are expressible directly in RT. The rest are expressible via a simple mechanism that integrates RTFRP with the base language.
p135
aVWe describe a new and simpler implementation in Haskell of CML's\u000aevents, which encode reactions by a thread to combinations of\u000amessages from other threads. We add a new type of Guarded Events,\u000aby which recipients can filter messages with conditions on their\u000avalue known as Guards. We implement guarded channels. The guard\u000atype and the indexing algorithm are not part of the channel\u000adefinition, so that the user can trade off what guards are required\u000aagainst the cost of indexing. As an example we sketch the\u000aencapsulation of a graphical user interface toolkit. This can be\u000adone concisely not only because of guarded events, but also because\u000awe construct events monadically. Monadic events are especially\u000ahelpful for representing concurrent processes which transform\u000athemselves in reaction to external communications.
p136
aVWe present a dependently typed assembly language (DTAL) in which the type system supports the use of a restricted form of dependent types, reaping some benefits of dependent types at the assembly level. DTAL improves upon TAL , enabling certain important compiler optimizations such as runtime array bound check elimination and tag check elimination. Also, DTAL formally addresses the issue of representing sum types at assembly level, making it suitable for handling not only datatypes in ML but also dependent datatypes in Dependent ML (DML).
p137
aVWe explore how two different mechanisms for reasoning about state,\u000alinear typing and the type, region and effect discipline,\u000acomplement one another in the design of a strongly typed functional\u000aprogramming language. The basis for our language is a simple lambda\u000acalculus containing firstclass memory regions, which are\u000aexplicitly passed as arguments to functions, returned as results\u000aand stored in userdefined data structures. In order to ensure\u000aappropriate memory safety properties, we draw upon the literature\u000aon linear type systems to help control access to and deallocation\u000aof regions. In fact, we use two different interpretations of linear\u000atypes, one in which multipleuse values are freely copied and\u000adiscarded and one in which multipleuse values are explicitly\u000areferencecounted, and show that both interpretations give rise to\u000ainteresting invariants for manipulating regions. We also explore\u000anew programming paradigms that arise by mixing firstclass regions\u000aand conventional linear data structures.
p138
aVThe type systems of most typed functional programming languages are\u000abased on the HindleyMilner type system. A practical problem with\u000athese type systems is that it is often hard to understand why a\u000aprogram is not type correct or a function does not have the\u000aintended type. We suggest that at the core of this problem is the\u000adifficulty of explaining why a given expression has a certain type.\u000aThe type system is not defined compositionally. We propose to\u000aexplain types using a variant of the HindleyMilner type system\u000athat defines a compositional type explanation graph of principal\u000atyping. We describe how the programmer understands types by\u000ainteractive navigation through the explanation graph. Furthermore,\u000athe explanation graph can be the foundation for algorithmic\u000adebugging of type errors, that is, semiautomatic localisation of\u000athe source of a type error without even having to understand the\u000atype inference steps. We implemented a prototype of a tool to\u000aexplore the usefulness of the proposed methods.
p139
aVThis paper introduces a new approach to optimizing array algorithms in functional languages. We are specifically aiming at an efficient implementation of irregular array algorithms that are hard to implement in conventional array languages such as Fortran. We optimize the storage layout of arrays containing complex data structures and reduce the running time of functions operating on these arrays by means of equational program transformations. In particular, this paper discusses a novel form of combinator loop fusion, which by removing intermediate structures optimizes the use of the memory hierarchy. We identify a combinator named loop P that provides a general scheme for iterating over an array and that in conjunction with an array constructor replicate P is sufficient to express a wide range of array algorithms. On this basis, we define equational transformation rules that combine traversals of loop P and replicate P as well as sequences of applications of loop P into a single loop P traversal. Our approach naturally generalizes to a parallel implementation and includes facilities for optimizing load balancing and communication. A prototype implementation based on the rewrite rule pragma of the Glasgow Haskell Compiler is significantly faster than standard Haskell arrays and approaches the speed of hand coded C for simple examples.
p140
aVContification is a compiler optimization that turns a function that always returns to the same place into a continuation. Compilers for functional languages use contification to expose the controlflow information that is required by many optimizations, including traditional loop optimizations. This paper gives a formal presentation of contification in MLton, a wholeprogram optimizing Standard ML compiler. We present two existing algorithms for contification in our framework, as well as a new algorithm based on the dominator tree of a program's call graph. We prove that the dominator algorithm is optimal. We present benchmark results on realistic SML programs demonstrating that contification has minimal overhead on compile time and significantly improves run time.
p141
aVAlgorithms in Computational Geometry and Computer Aided Design are often developed for the Real RAM model of computation, which assumes exactness of all the input arguments and operations. In practice, however, the exactness imposes tremendous limitations on the algorithms   even the basic operations become uncomputable, or prohibitively slow. When the computations of interest are limited to determining the sign of polynomial expressions over floating point numbers, faster approaches are available. One can evaluate the polynomial in floatingpoint first, together with some estimate of the rounding error, and fall back to exact arithmetic only if this error is too big to determine the sign reliably. A particularly efficient variation on this approach has been used by Shewchuk in his robust implementations of Orient and InSphere geometric predicates. We extend Shewchuk's method to arbitrary polynomial expressions. The expressions are given as programs in a suitable source language featuring basic arithmetic operations of addition, subtraction, multiplication and squaring, which are to be perceived by the programmer as exact. The source language also allows for anonymous functions, and thus enables the common functional programming technique of staging. The method is presented formally through several judgments that govern the compilation of the source expression into target code, which is then easily transformed into SML or, in case of singlestage expressions, into C.
p142
aVThe categorical notion of monad, used by Moggi to structure denotational descriptions, has proved to be a powerful tool for structuring combinator libraries. Moreover, the monadic programming style provides a convenient syntax for many kinds of computation, so that each library defines a new sublanguage. Recently, several workers have proposed a generalization of monads, called variously "arrows" or Freydcategories. The extra generality promises to increase the power, expressiveness and efficiency of the embedded approach, but does not mesh as well with the native abstraction and application. Definitions are typically given in a pointfree style, which is useful for proving general properties, but can be awkward for programming specific instances. In this paper we define a simple extension to the functional language Haskell that makes these new notions of computation more convenient to use. Our language is similar to the monadic style, and has similar reasoning properties. Moreover, it is extensible, in the sense that new combining forms can be defined as expressions in the host language.
p143
aVA major problem for writing extensible software arises when recursively defined datatypes and operations on these types have to be extended simultaneously without modifying existing code. This paper introduces Extensible Algebraic Datatypes with defaults, which promote a simple programming pattern to solve this wellknown problem. We show that it is possible to encode extensible algebraic datatypes in an objectoriented language, using a new design pattern for extensible visitors. Extensible algebraic datatypes have been successfully applied in the implementation of an extensible Java compiler. Our technique allows for the reuse of existing components in compiler extensions without the need for any adaptations.
p144
aVA cost recurrence describes an upper bound for the running time of a program in terms of the size of its input. Finding cost recurrences is a frequent intermediate step in complexity analysis, and this step requires an abstraction from data to data size. In this article, we use information contained in dependent types to achieve such an abstraction: Dependent ML (DML), a conservative extension of ML, provides dependent types that can be used to associate data with size information, thus describing a possible abstraction. We automatically extract cost recurrences from firstorder DML programs, guiding the abstraction from data to data size with information contained in DML type derivations.
p145
aVThe CIL compiler for core Standard ML compiles whole ML programs using a novel typed intermediate language that supports the generation of typesafe customized data representations. In this paper, we present empirical data comparing the relative efficacy of several different flowbased customization strategies for function representations. We develop a cost model to interpret dynamic counts of operations required for each strategy. In this cost model, customizing the representation of closed functions gives a 1217% improvement on average over uniform closure representations, depending on the layout of the closure. We also present data on the relative effectiveness of various strategies for reducing representation pollution, i.e., situations where flow constraints require the representation of a value to be less efficient than it would be in ideal circumstances. For the benchmarks tested and the types of representation pollution detected by our compiler, the pollution removal strategies we consider often cost more in overhead than they gain via enabled customizations. Notable exceptions are selective defunctionalization, a function representation strategy that often achieves significant customization benefits via aggressive pollution removal, and a simple form of flowdirected inlining, in which pollution removal allows multiple functions to be inlined at the same call site.
p146
aVWe present improvements to the backtracking technique of patternmatching compilation. Several optimizations are introduced, such as commutation of patterns, use of exhaustiveness information, and control flow optimization through the use of labeled static exceptions and context information. These optimizations have been integrated in the ObjectiveCaml compiler. They have shown good results in increasing the speed of patternmatching intensive programs, without increasing final code size.
p147
aVIt is possible to translate code written in Emacs Lisp or another Lisp dialect which uses dynamic scoping to a more modern programming language with lexical scoping while largely preserving structure and readability of the code. The biggest obstacle to such an idiomatic translation from Emacs Lisp is the translation of dynamic binding into suitable instances of lexical binding: Many binding constructs in real programs in fact exhibit identical behavior under both dynamic and lexical binding. An idiomatic translation needs to detect as many of these binding constructs as possible and convert them into lexical binding constructs in the target language to achieve readability and efficiency of the target code. The basic prerequisite for such an idiomatic translation is thus a dynamic scope analysis which associates variable occurrences with binding constructs. We present such an analysis. It is an application of the Nielson/Nielson framework for flow analysis to a semantics for dynamic binding akin to Moreau's. Its implementation handles a substantial portion of Emacs Lisp, has been applied to realistic Emacs Lisp code, and is highly accurate and reasonably efficient in practice.
p148
aVStandard ML is a statically typed programming language that is suited for the construction of both small and large programs. "Programming in the small" is captured by Standard ML's Core language. "Programming in the large" is captured by Standard ML's Modules language that provides constructs for organizing related Core language definitions into selfcontained modules with descriptive interfaces. While the Core is used to express details of algorithms and data structures, Modules is used to express the overall architecture of a software system. In Standard ML, modular programs must have a strictly hierarchical structure: the dependency between modules can never be cyclic. In particular, definitions of mutually recursive Core types and values, that arise frequently in practice, can never span module boundaries. This limitation compromises modular programming, forcing the programmer to merge conceptually (i.e. architecturally) distinct modules. We propose a practical and simple extension of the Modules language that caters for cyclic dependencies between both types and terms defined in separate modules. Our design leverages existing features of the language, supports separate compilation of mutually recursive modules and is easy to implement.
p149
aVWhile dynamic linking has become an integral part of the runtime execution of modem programming languages, there is increasing recognition of the need for support for hot swapping of running modules, particularly in longlived server applications. The interesting challenge for such a facility is to allow the new module to change the types exported by the original module, while preserving type safety. This paper describes a typebased approach to hot swapping running modules. The approach is based on a reflective mechanism for dynamically adding type sharing constraints to the type system, realized by programmerdefined version adapters in the runtime.
p150
aVWith few exceptions, macros have traditionally been viewed as operations on syntax trees or even on plain strings. This view makes macros seem ad hoc, and is at odds with two desirable features of contemporary typed functional languages: static typing and static scoping. At a deeper level, there is a need for a simple, usable semantics for macros. This paper argues that these problems can be addressed by formally viewing macros as multistage computations. This view eliminates the need for freshness conditions and tests on variable names, and provides a compositional interpretation that can serve as a basis for designing a sound type system for languages supporting macros, or even for compilation. To illustrate our approach, we develop and present MacroML, an extension of ML that supports inlining, recursive macros, and the definition of new binding constructs. The latter is subtle, and is the most novel addition in a statically typed setting. The semantics of a core subset of MacroML is given by an interpretation into MetaML, a staticallytyped multistage programming language. It is then easy to show that MacroML is stage and typesafe: macro expansion does not depend on runtime evaluation, and both stages do not "go wrong.
p151
aVAs a functional pearl, we describe an efficient, modularized\u000aimplementation of unification using the state of mutable reference\u000acells to encode substitutions. We abstract our algorithms along two\u000adimensions, first abstracting away from the structure of the terms\u000ato be unified, and second over the monad in which the mutable state\u000ais encapsulated. We choose this example to illustrate two important\u000atechniques that we believe many functional programmers would find\u000auseful. The first of these is the definition of recursive data\u000atypes using two levels: a structure defining level, and a recursive\u000aknottying level. The second is the use of rank2 polymorphism\u000ainside Haskell's record types to implement a form of type\u000aparameterized modules.
p152
aVProgramming languages are the way for a person to express a mental plan in a way that the computer can understand. Therefore, it is appropriate to consider properties of people when designing new programming languages. In our research, we are investigating how people think about algorithms, and how programming languages can be made easier to learn and more effective for people to use. By taking humanproductivity aspects of programming languages seriously, designers can more effectively match programming language features with human capabilities and problem solving methods. Human factors methods can be used to measure the effects, so unsubstantiated claims can be avoided.This talk will present a quick summary of new and old results in what is known about people and programming, from areas that are sometimes called "empirical studies of programmers" and "psychology of programming." Much is known about what people find difficult, and what syntax and language features are especially tricky and bugprone. Our new research has discovered how people naturally think about algorithms and data structures, which can help with making programming languages more closely match people's problem solving techniques.
p153
aVIn this paper we present the first exception analysis for a nonstrict language. We augment a simplytyped functional language with exceptions, and show that we can define a typebased inference system to detect uncaught exceptions. We have implemented this exception analysis in the GHC compiler for Haskell, which has been recently extended with exceptions. We give empirical evidence that the analysis is practical.
p154
aVOne promising approach for adding objectoriented (OO) facilities to functional languages like ML is to generalize the existing datatype and function constructs to be hierarchical and extensible, so that datatype variants simulate classes and function cases simulate methods. This approach allows existing datatypes to be easily extended with both new operations and new variants, resolving a longstanding conflict between the functional and OO styles. However, previous designs based on this approach have been forced to give up modular typechecking, requiring wholeprogram checks to ensure type safety. We describe Extensible ML (eml), an MLlike language that supports hierarchical, extensible datatypes and functions while preserving purely modular typechecking. To achieve this result, eml's type system imposes a few requirements on datatype and function extensibility, but eml is still able to express both traditional functional and OO idioms. We have formalized a core version of eml and proven the associated type system sound, and we have developed a prototype interpreter for the language.
p155
aVSome functional programming languages are also mathematical logics. One can reason formally, traditionally, and directly about programs in such languages. This is driving a new application area for functional programming: modeling microarchitectures, hardware design languages, and imperative programming languages. Such models serve the dual purposes of simulation and formal analysis.ACL2, "<u>A</u> <u>C</u>omputational <u>L</u>ogic for <u>A</u>pplicative <u>C</u>ommon <u>L</u>isp," is a functional programming language that is also a firstorder mathematical logic supported by a BoyerMoore style mechanical theorem prover [5]. It is being used to model and verify artifacts of commercial and industrial interest. The registertransfer level circuit descriptions for the elementary floatingpoint arithmetic on the AMD Athlon microprocessor were modeled in ACL2. These models were tested on millions of floatingpoint test vectors as part of the Athlon validation. In addition, the models were mechanically proved to satisfy the IEEE floatingpoint specifications. Bugs were found before fabrication. The Athlon that you buy has verified floatingpoint circuitry in it [7]. Avionics microprocessors produced by Rockwell Collins have been modeled in ACL2. Those models have been used as prefabrication simulation test benches. In addition, theorems relating various microprocessor models have been proved mechanically [3]. An executable pipelinelevel model of the Motorola CAP digital signal processor was proved to implement a sequential microcode engine and microcoded DSP programs were verified [2]. An executable model of the Java Virtual Machine has been used to prove functional correctness of some simple Java classes, including a safety property for a multithreaded class [6]..Other examples are reported in [4].Execution efficiency for industrialscale simulators, in combination with adherence to an axiomatic semantics, has forced some novel implementation features [1]. In addition, the ACL2 theorem prover is coded in ACL2 and so represents a significant application of functional programming.
p156
aVWe present an alternative approach to shortcut fusion based on the function unfoldr,. Despite its simplicity the technique can remove intermediate lists in examples which are known to be difficult. We show that it can remove all lists from definitions involving ziplike functions and functions using accumulating parameters.
p157
aVMonads are a useful abstraction of computation, as they model diverse computational effects such as stateful computations, exceptions and I/O in a uniform manner. Their potential to provide both a modular semantics and a modular programming style was soon recognised. However, in general, monads proved difficult to compose and so research focused on special mechanisms for their composition such as distributive monads and monad transformers.We present a new approach to this problem which is general in that nearly all monads compose, mathematically elegant in using the standard categorical tools underpinning monads and computationally expressive in supporting a canonical recursion operator. In a nutshell, we propose that two monads should be composed by taking their coproduct. Although abstractly this is a simple idea, the actual construction of the coproduct of two monads is nontrivial. We outline this construction, show how to implement the coproduct within Haskell and demonstrate its usage with a few examples. We also discuss its relationship with other ways of combining monads, in particular distributive laws for monads and monad transformers.
p158
aVAn interactive graphical environment for supporting the development and use of Haskell applications programs is described. The environment, named Vital, is particularly intended for supporting the openended, incremental development style often preferred by nonspecialist users in which successive steps of program development are motivated and informed by results so far obtained.Significant features of Vital include: the graphical display of data structures in a format defined by a datatypeindexed stylesheet, the way that evaluation of (possibly infinite) values is demanddriven by the action of the user scrolling around an unbounded workspace, and support for copyandpaste graphical editing of data structures. This latter allows, for example, the user to modify a complex data structure by pointandclick operations, or to create (by functional evaluation) a regular data structure and then edit values or expressions into it. The effect of each editing operation is immediately reflected in the Haskell program source code.
p159
aVEven when programming in a statically typed language we every now and then encounter statically untypable values; such values result from interpreting values or from communicating with the outside world. To cope with this problem most languages include some form of dynamic types. It may be that the core language has been explicitly extended with such a type, or that one is allowed to live dangerously by using functions like unsafeCoerce. We show how, by a careful use of existentially and universally quantified types, one may achievem the same effect, without extending the language with new or unsafe features. The techniques explained are universally applicable, provided the core language is expressive enough; this is the case for the common implementations of Haskell. The techniques are used in the description of a type checking compiler that, starting from an expression term, constructs a typed function representing the semantics of that expression. In this function the overhead associated with the type checking is only once being paid for; in this sense we have thus achieved static type checking.
p160
aVWe present a minimal extension of the Hindley/Milner system to allow for overloading of identifiers. Our approach relies on a combination of the HM(X) type system framework with Constraint Handling Rules (CHRs). CHRs are a declarative language for writing incremental constraint solvers. CHRs allow us to precisely describe the relationships among overloaded identifiers. Under some sufficient conditions on the CHRs we achieve decidable type inference and the semantic meaning of programs is unambiguous. Our approach allows us to combine open and closed world overloading. We also show how to deal with overlapping definitions.
p161
aVWe propose an extension of Haskell's type class system with lambda abstractions in the type language. Type inference for our extension relies on a novel constrained unification procedure called guided higherorder unification. This unification procedure is more general than Haskell's kindpreserving unification but less powerful than full higherorder unification.The main technical result is the soundness and completeness of the unification rules for the fragment of lambda calculus that we admit on the type level.
p162
aVWe present the type theory LTT, intended to form a basis for typed target languages, providing an internal notion of logical proposition and proof. The inclusion of explicit proofs allows the type system to guarantee properties that would otherwise be incompatible with decidable type checking. LTT also provides linear facilities for tracking ephemeral properties that hold only for certain program states.Our type theory allows for reuse of typechecking software by casting a variety of type systems within a single language. We illustrate our methodology of representation by means of two examples, one functional and one stateful, and describe the associated operational semantics and proofs of type safety.
p163
aVThe abstract data type onesided flexible array, also called randomaccess list, supports lookup and update of elements and can grow and shrink at one end. We describe a purely functional implementation based on weightbalanced multiway trees that is both simple and versatile. A novel feature of the representation is that the running time of the operations can be tailored to one's needs even dynamically at arraycreation time. In particular, one can trade the running time of lookup operations for the running time of update operations. For instance, if the multiway trees have a fixed degree, the operations take \u03b8(log n) time, where n is the size of the flexible array. If the degree doubles levelwise, lookup speeds up to \u03b8(sqrtlog n) while update slows down to \u03b8(2sqrt log n). We show that different tree shapes can be conveniently modelled after mixedradix number systems.
p164
aVMetaprogramming languages provide infrastructure to generate and execute object programs at runtime. In a typed setting, they contain a modal type constructor which classifies object code. These code types generally come in two flavors: closed and open. Closed code expressions can be invoked at runtime, but the computations over them are more rigid, and typically produce less efficient residual object programs. Open code provides better inlining and partial evaluation of object programs, but once constructed, expressions of this type cannot in general be evaluated.Recent work in this area has focused on combining the two notions into a sound system. We present a novel way to achieve this. It is based on adding the notion of names from the work on Nominal Logic and FreshML to the \u03bb calculus of proof terms for the necessity fragment of modal logic S4. The resulting language provides a more finegrained control over free variables of object programs when compared to the existing languages for metaprogramming. In addition, this approach lends itself well to addition of intensional code analysis, i.e. ability of meta programs to inspect and destruct object programs at runtime in a typesafe manner, which we also undertake.
p165
aVMultistage programming languages provide a convenient notation for explicitly staging programs. Staging a definitional interpreter for a domain specific language is one way of deriving an implementation that is both readable and efficient. In an untyped setting, staging an interpreter "removes a complete layer of interpretive overhead", just like partial evaluation. In a typed setting however, HindleyMilner type systems do not allow us to exploit typing information in the language being interpreted. In practice, this can mean a slowdown cost by a factor of three or mor.Previously, both type specialization and tag elimination were applied to this problem. In this paper we propose an alternative approach, namely, expressing the definitional interpreter in a dependently typed programming language. We report on our experience with the issues that arise in writing such an interpreter and in designing such a language. .To demonstrate the soundness of combining staging and dependent types in a general sense, we formalize our language (called MetaD) and prove its type safety. To formalize MetaD, we extend Shao, Saha, Trifonov and Papaspyrou's \u03bbH language to a multilevel setting. Building on \u03bbH allows us to demonstrate type safety in a setting where the type language contains all the calculus of inductive constructions, but without having to repeat the work needed for establishing the soundness of that system.
p166
aVWe present a programming pattern where a recursive function traverses a data structure typically a list at return time. The idea is that the recursive calls get us there (typically to a base case) and the returns get us back again while traversing the data structure. We name this programming pattern of traversing a data structure at return time "There And Back Again" (TABA).The TABA pattern directly applies to computing a symbolic convolution. It also synergizes well with other programming patterns, e.g., dynamic programming and traversing a list at double speed. We illustrate TABA and dynamic programming with Catalan numbers. We illustrate TABA and traversing a list at double speed with palindromes and we obtain a novel solution to this traditional exercise.A TABAbased function written in direct style makes full use of an Algollike control stack and needs no heap allocation. Conversely, in a TABAbased function written in continuationpassing style, the continuation acts as a list iterator. In general, the TABA pattern saves one from constructing intermediate lists in reverse order.
p167
aVMotivated by applications to proof assistants based on dependent types, we develop and prove correct a strong reducer and equivalence checker for the \u03bbcalculus with products, sums, and guarded fixpoints. Our approach is based on compilation to the bytecode of an abstract machine performing weak reductions on nonclosed terms, derived with minimal modifications from the ZAM machine used in the Objective Caml bytecode interpreter, and complemented by a recursive "read back" procedure. An implementation in the Coq proof assistant demonstrates important speedups compared with the original interpreterbased implementation of strong reduction in Coq.
p168
aVGenerational collection has improved the efficiency of garbage collection in fastallocating programs by focusing on collecting young garbage, but has done little to reduce the cost of collecting a heap containing large amounts of older data. A new generational technique, olderfirst collection, shows promise in its ability to manage older data.This paper reports on an implementation study that compared two olderfirst collectors to traditional (youngerfirst) generational collectors. One of the olderfirst collectors performed well and was often effective at reducing the firstorder cost of collection relative to youngerfirst collectors. Olderfirst collectors perform especially well when objects have queuelike or random lifetimes.
p169
aVWe have added a Java virtual machine (henceforth JVM) bytecode generator to the optimizing SchemetoC compiler Bigloo. We named this new compiler BiglooJVM. We have used this new compiler to evaluate how suitable the JVM bytecode is as a target for compiling strict functional languages such as Scheme. In this paper, we focus on the performance issue. We have measured the execution time of many Scheme programs when compiled to C and when compiled to JVM. We found that for each benchmark, at least one of our hardware platforms ran the BiglooJVM version in less than twice the time taken by the Bigloo version. In order to deliver fast programs the generated JVM bytecode must be carefully crafted in order to benefit from the speedup of justintime compilers.
p170
aVWe present a direct implementation of the shift and reset control operators in the SFE system. The new implementation improves upon the traditional technique of simulating shift and reset via callcc. Typical applications of these operators exhibit space savings and a significant overall performance gain. Our technique is based upon the popular incremental stack/heap strategy for representing continuations. We present implementation details as well as some benchmark measurements for typical applications.
p171
aVWe introduce a new transformation method to eliminate intermediate data structures occurring in functional programs due to repeated list concatenations and other data manipulations (additionally exemplified with list reversal and mapping of functions over lists).The general idea is to uniformly abstract from data constructors and manipulating operations by means of rank2 polymorphic combinators that exploit algebraic properties of these operations to provide an optimized implementation. The correctness of transformations is proved by using the free theorems derivable from parametric polymorphic types.
p172
aVThis paper presents a monadic approach to incremental computation, suitable for purely functional languages such as Haskell. A program that uses incremental computation is able to perform an incremental amount of computation to accommodate for changes in input data. Recently, Acar, Blelloch and Harper presented a small Standard ML library that supports efficient, highlevel incremental computations [1]. Here, we present a monadic variant of that library, written in Haskell extended with firstclass references. By using monads, not only are we able to provide a purely functional interface to the library, the types also enforce "correct usage" without having to resort to any typesystem extension. We also find optimization opportunities based on standard monadic combinators.This is an exercise in putting to work monad transformers with environments, references, and continuations.
p173
aVPackrat parsing is a novel technique for implementing parsers in a lazy functional programming language. A packrat parser provides the power and flexibility of topdown parsing with backtracking and unlimited lookahead, but nevertheless guarantees linear parse time. Any language defined by an LL(k) or LR(k) grammar can be recognized by a packrat parser, in addition to many languages that conventional lineartime algorithms do not support. This additional power simplifies the handling of common syntactic idioms such as the widespread but troublesome longestmatch rule, enables the use of sophisticated disambiguation strategies such as syntactic and semantic predicates, provides better grammar composition properties, and allows lexical analysis to be integrated seamlessly into parsing. Yet despite its power, packrat parsing shares the same simplicity and elegance as recursive descent parsing; in fact converting a backtracking recursive descent parser into a lineartime packrat parser often involves only a fairly straightforward structural change. This paper describes packrat parsing informally with emphasis on its use in practical applications, and explores its advantages and disadvantages with respect to the more conventional alternatives.
p174
aVAssertions play an important role in the construction of robust software. Their use in programming languages dates back to the 1970s. Eiffel, an objectoriented programming language, wholeheartedly adopted assertions and developed the "Design by Contract" philosophy. Indeed, the entire objectoriented community recognizes the value of assertionbased contracts on methods.In contrast, languages with higherorder functions do not support assertionbased contracts. Because predicates on functions are, in general, undecidable, specifying such predicates appears to be meaningless. Instead, the functional languages community developed type systems that statically approximate interesting predicates.In this paper, we show how to support higherorder function contracts in a theoretically wellfounded and practically viable manner. Specifically, we introduce \u03bbcon, a typed lambda calculus with assertions for higherorder functions. The calculus models the assertion monitoring system that we employ in DrScheme. We establish basic properties of the model (type soundness, etc.) and illustrate the usefulness of contract checking with examples from DrScheme's code base.We believe that the development of an assertion system for higherorder functions serves two purposes. On one hand, the system has strong practical potential because existing type systems simply cannot express many assertions that programmers would like to state. On the other hand, an inspection of a large base of invariants may provide inspiration for the direction of practical future type system research.
p175
aVBy extending an MLstyle type system with record polymorphism, recursive type definition, and an ordering relation induced by field inclusion, it is possible to achieve seamless and type safe interoperability with an objectoriented language. Based on this observation, we define a polymorphic language that can directly access external objects and methods, and develop a type inference algorithm. This calculus enjoys the features of both higherorder programming with ML polymorphism and classbased objectoriented programming with dynamic method dispatch. To establish type safety, we define a sample objectoriented language with multiple inheritance as the target for interoperability, define an operational semantics of the calculus, and show that the type system is sound with respect to the operational semantics. These results have been implemented in our prototype interpretable language, which can access Java class files and other external resources.
p176
aVMany macro systems, especially for Lisp and Scheme, allow macro transformers to perform general computation. Moreover, the language for implementing compiletime macro transformers is usually the same as the language for implementing runtime functions. As a side effect of this sharing, implementations tend to allow the mingling of compiletime values and runtime values, as well as values from separate compilations. Such mingling breaks programming tools that must parse code without executing it. Macro implementors avoid harmful mingling by obeying certain macrodefinition protocols and by inserting phasedistinguishing annotations into the code. However, the annotations are fragile, the protocols are not enforced, and programmers can only reason about the result in terms of the compiler's implementation. MzScheme the language of the PLT Scheme tool suite addresses the problem through a macro system that separates compilation without sacrificing the expressiveness of macros.
p177
aVCompilers for dynamically and statically typed languages ensure safe execution by verifying that all operations are performed on appropriate values. An operation as simple as car in Scheme and hd in SML will include a run time check unless the compiler can prove that the argument is always a nonempty list using some type analysis. We present a demanddriven type analysis that can adapt the precision of the analysis to various parts of the program being compiled. This approach has the advantage that the analysis effort can be spent where it is justified by the possibility of removing a run time check, and where added precision is needed to accurately analyze complex parts of the program. Like the kcfa our approach is based on abstract interpretation but it can analyze some important programs more accurately than the kcfa for any value of k. We have built a prototype of our type analysis and tested it on various programs with higher order functions. It can remove all run time type checks in some nontrivial programs which use map and the Y combinator.
p178
aVMicroscopic physics is entirely reversible, yet almost all computation is done using irreversible processes. Adding two numbers together, for example, destroys information, unless one of the numbers is retained. Over the past few years, some advantages of taking reversible computation seriously have emerged   first in the context of saving energy, through the development of dissipationless logic, and then in the promise of quantum computation, which is necessarily also reversible and dissipationless.The necessary conservation of information in these systems raises the question of how we can best capture this strong constraint from an abstract point of view. Already ideas such as linear logic can be applied, but the full implementation of conservative logic ideas will require language and architectural innovation and a rethink of our computational models.
p179
aVMost programming languages adopt static binding, but for distributed programming an exclusive reliance on static binding is too restrictive: dynamic binding is required in various guises, for example when a marshalled value is received from the network, containing identifiers that must be rebound to local resources. Typically it is provided only by adhoc mechanisms that lack clean semantics.In this paper we adopt a foundational approach, developing core dynamic rebinding mechanisms as extensions to simplytyped callbyvalue ? calculus. To do so we must first explore refinements of the callbyvalue reduction strategy that delay instantiation, to ensure computations make use of the most recent versions of rebound definitions. We introduce redextime and destructtime strategies. The latter forms the basis for a ?marsh calculus that supports dynamic rebinding of marshalled values, while remaining as far as possible staticallytyped. We sketch an extension of ? marsh with concurrency and communication, giving examples showing how wrappers for encapsulating untrusted code can be expressed. Finally, we show that a highlevel semantics for dynamic updating can also be based on the destructtime strategy, defining a ?marsh calculus with simple primitives to provide typesafe updating of running code. We thereby establish primitives and a common semantic foundation for a variety of realworld dynamic rebinding requirements.
p180
aVProgram analysis is the heart of modern compilers. Most control flow analyses are reduced to the problem of finding a fixed point in a certain transition system, and such fixed point is commonly computed through an iterative procedure that repeats tracing until convergence.This paper proposes a new method to analyze programs through recursive graph traversals instead of iterative procedures, based on the fact that most programs (without spaghetti GOTO) have wellstructured control flow graphs, graphs with bounded tree width. Our main techniques are; an algebraic construction of a control flow graph, called SP Term, which enables control flow analysis to be defined in a natural recursive form, and the Optimization Theorem, which enables us to compute optimal solution by dynamic programming.We illustrate our method with two examples; dead code detection and register allocation. Different from the traditional standard iterative solution, our dead code detection is described as a simple combination of bottomup and topdown traversals on SP Term. Register allocation is more interesting, as it further requires optimality of the result. We show how the Optimization Theorem on SP Terms works to find an optimal register allocation as a certain dynamic programming.
p181
aVWe give a tutorial and firstprinciples description of the context semantics of Gonthier, Abadi, and Lvy [5, 4], a computerscience analogue of Girard's geometry of interaction [3]. In the spirit of the invited presentation of Tom Knight (see this Proceedings [7]), the semantics is reversible, and supports pseudoquantum computation via a superposed sharing of terms and evaluation contexts [2].Context semantics provides a mechanism for modelling ?calculus, and more generally multiplicativeexponential linear logic (MELL); we explain the the callbyname (CBN) coding of the ?calculus, and sketch a proof of the correctness of readback, where the normal form of a ?term is recovered from its semantics. This analysis yields the algorithmic correctness of Lamping's optimal reduction algorithm [8]. We relate the context semantics to linear logic types and to ideas from game semantics, used to prove full abstraction theorems for PCF and other ?calculus variants [1, 6, 10]. Readback is essentially a game played by an environment (the Opponent) who wants to discover the Bhm tree (normal form) of a term known by a Player. A type plays the role using the games jargon of an arena of possible moves, and a term of that type provides a winning strategy for the Player, permitting the Player to respond correctly to moves made by the Opponent. The interaction between Opponent and Player describes a perfect flow analysis which answers questions like, "can call site a ever call procedure p?" The context semantics provides a lowlevel coding mechanism for describing such flows, the positions of subexpressions and head variables in Bhm trees, as well as moves in the above described twoplayer games.
p182
aVThis paper define the semantics of MinAML, an idealized aspectoriented programming language, by giving a typedirected translation from its userfriendly external language to its compact, welldefined core language. We argue that our framework is an effective way to give semantics to aspectoriented programming languages in general because the translation eliminates shallow syntactic differences between related constructs and permits definition of a clean, easytounderstand, and easytoreasonabout core language.The core language extends the simplytyped lambda calculus with two central new abstractions: explicitly labeled program points and firstclass advice. The labels serve both to trigger advice and to mark continuations that the advice may return to. These constructs are defined orthogonally to the other features of the language and we show that our abstractions can be used in both functional and objectoriented contexts. The labels are wellscoped and the language as a whole is welltyped. Consequently, programmers can use lexical scoping in the standard way to prevent aspects from interfering with local program invariants.
p183
aVGeneric Haskell is an extension of Haskell that supports the construction of generic programs. During the development of several applications, such as an XML editor and compressor, we encountered a number of limitations with the existing (Classic) Generic Haskell language, as implemented by the current Generic Haskell compiler. Specifically, generic definitions become disproportionately more difficult to write as their complexity increases, such as when one generic function uses another, because recursion is implicit in generic definitions. In the current implementation, writing such functions suffers the burden of a large administrative overhead and is at times counterintuitive. Furthermore, the absence of type checking in the current implementation can make Generic Haskell hard to use.In this paper we develop the foundations of Dependencystyle Generic Haskell which addresses the above problems, shifting the burden from the programmer to the compiler. These foundations consist of a full type system for Dependencystyle Generic Haskell's core language and appropriate reduction rules. The type system enables the programmer to write generic functions in a more natural style, taking care of dependency details which were previously the programmer's responsibility.
p184
aVFunctional Reactive Programming (FRP) is a framework for reactive programming in a functional setting. FRP has been applied to a number of domains, such as graphical animation, graphical user interfaces, robotics, and computer vision. Recently, we have been interested in applying FRPlike principles to hybrid modeling and simulation of physical systems. As a step in that direction, we have extended an existing FRP implementation, Yampa, in two new ways that make it possible to express certain models in a very natural way, and reduces the amount of work needed to put modeling equations into a suitable form for simulation. First, we have added Dirac impulses that allow certain types of discontinuities to be handled in an easy yet rigorous manner. Second, we have adapted automatic differentiation to the setting of Yampa, and generalized it to work correctly with Dirac impulses. This allows derivatives of piecewise continuous signals to be welldefined at all points. This paper reviews the basic ideas behind automatic differentiation, in particular Jerzy Karczmarczuk's elegant version for a lazy functional language with overloading, and then considers the integration with Yampa and the addition of Dirac impulses.
p185
aVWe describe extensions to the Excel spreadsheet that integrate userdefined functions into the spreadsheet grid, rather than treating them as a "bolton". Our first objective was to bring the benefits of additional programming language features to a system that is often not recognised as a programming language. Second, in a project involving the evolution of a wellestablished language, compatibility with previous versions is a major issue, and maintaining this compatibility was our second objective. Third and most important, the commercial success of spreadsheets is largely due to the fact that many people find them more usable than programming languages for programminglike tasks. Thus, our third objective (with resulting constraints) was to maintain this usability advantage.Simply making Excel more like a conventional programming language would not meet these objectives and constraints. We have therefore taken an approach to our design work that emphasises the cognitive requirements of the user as a primary design criterion. The analytic approach that we demonstrate in this project is based on recent developments in the study of programming usability, including the Cognitive Dimensions of Notations and the Attention Investment model of abstraction use. We believe that this approach is also applicable to the design and extension of other programming languages and environments.
p186
aVThe shift and reset operators, proposed by Danvy and Filinski, are powerful control primitives for capturing delimited continuations. Delimited continuation is a similar concept as the standard (unlimited) continuation, but it represents part of the rest of the computation, rather than the whole rest of computation. In the literature, the semantics of shift and reset has been given by a CPStranslation only. This paper gives a direct axiomatization of calculus with shift and reset, namely, we introduce a set of equations, and prove that it is sound and complete with respect to the CPStranslation. We also introduce a calculus with control operators which is as expressive as the calculus with shift and reset, has a sound and complete axiomatization, and is conservative over Sabry and Felleisen's theory for firstclass continuations.
p187
aVThe rules of classical logic may be formulated in pairs corresponding to De Morgan duals: rules about & are dual to rules about V. A line of work, including that of Filinski (1989), Griffin (1990), Parigot (1992), Danos, Joinet, and Schellinx (1995), Selinger (1998,2001), and Curien and Herbelin (2000), has led to the startling conclusion that callbyvalue is the de Morgan dual of callbyname.This paper presents a dual calculus that corresponds to the classical sequent calculus of Gentzen (1935) in the same way that the lambda calculus of Church (1932,1940) corresponds to the intuitionistic natural deduction of Gentzen (1935). The paper includes crisp formulations of callbyvalue and callbyname that are obviously dual; no similar formulations appear in the literature. The paper gives a CPS translation and its inverse, and shows that the translation is both sound and complete, strengthening a result in Curien and Herbelin (2000).
p188
aVAll classical ?terms typable with disjunctive normal forms are shown to share a common computational behavior: they implement a local exception handling mechanism whose exact workings depend on the tautology. Equivalent and more efficient control combinators are described through a specialized sequent calculus and shown to be correct.
p189
aVTo improve the quality of type error messages in functional programming languages,we propose four techniques which influence the behaviour of constraintbased type inference processes. These techniques take the form of externally supplied type inference directives, precluding the need to make any changes to the compiler. A second advantage is that the directives are automatically checked for soundness with respect to the underlying type system. We show how the techniques can be used to improve the type error messages reported for a combinator library. More specifically, how they can help to generate error messages which are conceptually closer to the domain for which the library was developed. The techniques have all been incorporated in the Helium compiler, which implements a large subset of Haskell.
p190
aVWe develop an explicit two level system that allows programmers to reason about the behavior of effectful programs. The first level is an ordinary MLstyle type system, which confers standard properties on program behavior. The second level is a conservative extension of the first that uses a logic of type refinements to check more precise properties of program behavior. Our logic is a fragment of intuitionistic linear logic, which gives programmers the ability to reason locally about changes of program state. We provide a generic resource semantics for our logic as well as a sound, decidable, syntactic refinementchecking system. We also prove that refinements give rise to an optimization principle for programs. Finally, we illustrate the power of our system through a number of examples.
p191
aVThis paper presents a static type system for JAVA Virtual Machine (JVM) code that enforces an access control mechanism similar to the one found, for example, in a JAVA implementation. In addition to verifying type consistency of a given JVM code, the type system statically verifies that the code accesses only those resources that are granted by the prescribed access policy. The type system is proved to be sound with respect to an operational semantics that enforces access control dynamically, similarly to JAVA stack inspection. This result ensures that "well typed code cannot violate access policy." The paper then develops a type inference algorithm and shows that it is sound with respect to the type system and that it always infers a minimal set of access privileges. These results allows us to develop a static system for JVM access control without resorting to costly runtime stack inspection.
p192
aVWe present the derivation of a space efficient parser combinator library: the constructed parsers do not keep unnecessary references to the input, produce online results and efficiently handle ambiguous grammars. The underlying techniques can be applied in many contexts where traditionally backtracking is used.We present two data types, one for keeping track of the progress of the search process, and one for representing the final result in a linear way. Once these data types are combined into a single type, we can perform a breadthfirst search, while returning parts of the result as early as possible.
p193
aVHigherorder abstract syntax is a simple technique for implementing languages with functional programming. Object variables and binders are implemented by variables and binders in the host language. By using this technique, one can avoid implementing common and tricky routines dealing with variables, such as captureavoiding substitution. However, despite the advantages this technique provides, it is not commonly used because it is difficult to write sound elimination forms (such as folds or catamorphisms) for higherorder abstract syntax. To fold over such a datatype, one must either simultaneously define an inverse operation (which may not exist) or show that all functions embedded in the datatype are parametri.In this paper, we show how firstclass polymorphism can be used to guarantee the parametricity of functions embedded in higherorder abstract syntax. With this restriction, we implement a library of iteration operators over datastructures containing functionals. From this implementation, we derive "fusion laws" that functional programmers may use to reason about the iteration operator. Finally, we show how this use of parametric polymorphism corresponds to the Schrmann, Despeyroux and Pfenning method of enforcing parametricity through modal types. We do so by using this library to give a sound and complete encoding of their calculus into System F?. This encoding can serve as a starting point for reasoning about higherorder structures in polymorphic languages.
p194
aVFreshML extends ML with elegant and practical constructs for declaring and manipulating syntactical data involving statically scoped binding operations. Userdeclared FreshML datatypes involving binders are concrete, in the sense that values of these types can be deconstructed by matching against patterns naming bound variables explicitly. This may have the computational effect of swapping bound names with freshly generated ones; previous work on FreshML used a complicated static type system inferring information about the 'freshness' of names for expressions in order to tame this effect. The main contribution of this paper is to show (perhaps surprisingly) that a standard type system without freshness inference, coupled with a conventional treatment of fresh name generation, suffices for FreshML's crucial correctness property that values of datatypes involving binders are operationally equivalent if and only if they represent aequivalent pieces of objectlevel syntax. This is established via a novel denotational semantics. FreshML without static freshness inference is no more impure than ML and experience with it shows that it supports a programming style pleasingly close to informal practice when it comes to dealing with objectlevel syntax modulo aequivalence.
p195
aVBy allowing the programmer to write code that can generate code at runtime, metaprogramming offers a powerful approach to program construction. For instance, metaprogramming can often be employed to enhance program efficiency and facilitate the construction of generic programs. However, metaprogramming, especially in an untyped setting, is notoriously errorprone. In this paper, we aim at making metaprogramming less errorprone by providing a type system to facilitate the construction of correct metaprograms. We first introduce some code constructors for constructing typeful code representation in which program variables are replaced with deBruijn indices, and then formally demonstrate how such typeful code representation can be used to support metaprogramming. The main contribution of the paper lies in recognition and then formalization of a novel approach to typed metaprogramming that is practical, general and flexible.
p196
aVLazy programs are beautiful, but they are slow because they build many thunks. Simple measurements show that most of these thunks are unnecessary: they are in fact always evaluated, or are always cheap. In this paper we describe Optimistic Evaluation   an evaluation strategy that exploits this observation. Optimistic Evaluation complements compiletime analyses with runtime experiments: it evaluates a thunk speculatively, but has an abortion mechanism to back out if it makes a bad choice. A runtime adaption mechanism records expressions found to be unsuitable for speculative evaluation, and arranges for them to be evaluated more lazily in the future.We have implemented optimistic evaluation in the Glasgow Haskell Compiler. The results are encouraging: many programs speed up significantly (525%), some improve dramatically, and none go more than 15% slower.
p197
aVWe propose a type system for locating the source of type errors in an applied lambda calculus with MLstyle polymorphism. The system is based on discriminative sum types known from work on soft typing with annotation subtyping and recursive types. This way, type clashes can be registered in the type for later reporting. The annotations track the potential producers and consumers for each value so that clashes can be traced to their cause.Every term is typeable in our system and type inference is decidable. A type derivation in our system describes all type errors present in the program, so that a principal derivation yields a principal description of all type errors present. Error messages are derived from completed type derivations. Thus, error messages are independent of the particular algorithm used for type inference, provided it constructs such a derivation.
p198
aVWe propose a type system MLF that generalizes ML with firstclass polymorphism as in System F. Expressions may contain secondorder type annotations. Every typable expression admits a principal type, which however depends on type annotations. Principal types capture all other types that can be obtained by implicit type instantiation and they can be inferred.All expressions of ML are welltyped without any annotations. All expressions of System F can be mechanically encoded into MLF by dropping all type abstractions and type applications, and injecting types of lambdaabstractions into MLF types. Moreover, only parameters of lambdaabstractions that are used polymorphically need to remain annotated.
p199
aVWe propose a conservative extension of HM(X), a generic constraintbased type inference framework, with bounded existential (a.k.a. abstract) and universal (a.k.a. polymorphic) datatypes. In the first part of the article, which remains abstract of the type and constraint language (i.e. the logic X), we introduce the type system, prove its safety and define a type inference algorithm which computes principal typing judgments. In the second part, we propose a realistic constraint solving algorithm for the case of structural subtyping, which handles the nonstandard construct of the constraint language generated by type inference: a form of bounded universal quantification.
p200
aVWe present the functional language CDuce, discuss some design issues, and show its adequacy for working with XML documents. Distinctive features of CDuce are a powerful pattern matching, first class functions, overloaded functions, a very rich type system (arrows, sequences, pairs, records, intersections, unions, differences), precise type inference for patterns and error localization, and a natural interpretation of types as sets of values. We also outline some important implementation issues; in particular, a dispatch algorithm that demonstrates how static type information can be used to obtain very efficient compilation schemas..
p201
aVPattern matching mechanisms based on regular expressions feature in a number of recent languages for processing treestructured data such as XML. A compiler for such a language must address not only the familiar problems of pattern optimization for MLstyle algebraic datatypes and pattern matching, but also some new ones, arising principally from the use of recursion in patterns. We identify several factors playing a significant role in the quality of the generated code, propose two pattern compilers one generating backtracking target programs, the other nonbacktracking and sketch proofs of their correctness.
p202
aVA threeyear study collected information bearing on the question of whether studying mathematics improves programming skills. An analysis of the data revealed significant differences in the programming effectiveness of two populations of students: (1) those who studied discrete mathematics through examples focused on reasoning about software and (2) those who studied the same mathematical topics illustrated with more traditional examples. Functional programming played a central role in the study because it provides a straightforward framework for the presentation of concepts such as predicate logic and proof by induction. Such topics can be covered in depth, staying almost entirely within the context of reasoning about software. The intricate complexities in logic that mutable variables carry with them need not arise, early on, to confuse novices struggling to understand new ideas. In addition, because functional languages provide useful and compact ways to express mathematical concepts, and because the choice of notation in mathematics courses is often at the discretion of the instructor (in contrast to the notational restrictions often fiercely guarded by the faculty in programming courses), discrete mathematics courses, as they are found in most computer science programs, provide an easy opportunity to enhance the education of students by exposing them to functional programming concepts.
p203
aVType abstraction is a key feature of MLlike languages for writing large programs. Marshalling is necessary for writing distributed programs, exchanging values via network bytestreams or persistent stores. In this paper we combine the two, developing compiletime and runtime semantics for marshalling, that guarantee abstractionsafety between separatelybuilt programs. We obtain a namespace for abstract types that is global, i.e. meaningful between programs, by hashing module declarations. We examine the scenarios in which values of abstract types are communicated from one program to another, and ensure, by constructing hashes appropriately, that the dynamic and static notions of type equality mirror each other. We use singleton kinds to express abstraction in the static semantics; abstraction is tracked in the dynamic semantics by coloured brackets. These allow us to prove preservation, erasure, and coincidence results. We argue that our proposal is a good basis for extensions to existing MLlike languages, pragmatically straightforward for language users and for implementors.
p204
aVIt is possible to extend the basic notion of "function call" to allow functions to have multiple return points. This turns out to be a surprisingly useful mechanism. This paper conducts a fairly wideranging tour of such a feature: a formal semantics for a minimal \u03bb calculus capturing the mechanism; a motivating example; a static type system; useful transformations; implementation concerns and experience with an implementation; and comparison to related mechanisms, such as exceptions, sumtypes and explicit continuations. We conclude that multiplereturn function call is not only a useful and expressive mechanism, both at the sourcecode and intermediaterepresentation level, but is also quite inexpensive to implement.
p205
aVRecent functional logic languages such as Curry and Toy combine lazy functional programming with logic programming features including logic variables, nondeterminism, unification, narrowing, fair search, concurrency, and residuation. In this paper, we show how to extend a conventional interpreter for a lazy functional language to handle these features by adding support for reference cells, processlike and threadlike concurrency mechanisms, and a novel form of multiversioned store. Our interpretation scheme is practical, and can be easily extended to perform compilation. The language specified by our interpreter is designed so that programs are deterministic in a novel and useful sense.
p206
aVRegionbased type systems provide programmer control over memory management without sacrificing typesafety. However, the type systems for regionbased languages, such as the MLKit or Cyclone, are relatively complicated, so proving their soundness is nontrivial. This paper shows that the complication is in principle unnecessary. In particular, we show that plain old parametric polymorphism, as found in Haskell, is all that is needed. We substantiate this claim by giving a type and meaningpreserving translation from a regionbased language based on core Cyclone to a monadic variant of System F with region primitives whose types and operations are inspired by (and generalize) the ST monad.
p207
aVAbadi et al. introduced the dependency core calculus (DCC) as a unifying framework to study many important program analyses such as binding time, information flow, slicing, and function call tracking. DCC uses a lattice of monads and a nonstandard typing rule for their associated bind operations to describe the dependency of computations in a program. Abadi et al. proved a noninterference theorem that establishes the correctness of DCC's type system and thus the correctness of the type systems for the analyses above.In this paper, we study the relationship between DCC and the GirardReynolds polymorphic lambda calculus (System F). We encode the recursionfree fragment of DCC into F via a typedirected translation. Our main theoretical result is that, following from the correctness of the translation, the parametricity theorem for F implies the noninterference theorem for DCC. In addition, the translation provides insights into DCC's type system and suggests implementation strategies of dependency calculi in polymorphic languages.
p208
aVIf a subexpression in a query will never contribute data to the query answer, this should be regarded as an error. This principle has been recently accepted into mainstream XML query languages, but was still waiting for a complete treatment. We provide here a precise definition for this class of errors, and define a type system that is sound and complete, in its search for such errors, for a core language, under mild restrictions on the use of recursion in type definitions. In the process, we describe a dichotomy among existential and universal type systems, which is useful to understand some unusual features of our type system.
p209
aVUseful type inference must be faster than normalization. Otherwise, you could check safety conditions by running the program. We analyze the relationship between bounds on normalization and type inference. We show how the success of type inference is fundamentally related to the amnesia of the type system: the nonlinearity by which all instances of a variable are constrained to have the same type.Recent work on intersection types has advocated their usefulness for static analysis and modular compilation. We analyze SystemI (and some instances of its descendant, System E), an intersection type system with a type inference algorithm. Because SystemI lacks idempotency, each occurrence of a variable requires a distinct type. Consequently, type inference is equivalent to normalization in every single case, and time bounds on type inference and normalization are identical. Similar relationships hold for other intersection type systems without idempotency.The analysis is founded on an investigation of the relationship between linear logic and intersection types. We show a lockstep correspondence between normalization and type inference. The latter shows the promise of intersection types to facilitate static analyses of varied granularity, but also belies an immense challenge: to add amnesia to such analysis without losing all of its benefits.
p210
aVWe study a type system equipped with universal types and equirecursive types, which we refer to as F\u0358. We show that type equality may be decided in time O(nlog n), an improvement over the previous known bound of O(n2 ). In fact, we show that two more general problems, namely entailment of type equations and type unification, may be decided in time O(nlog n), a new result. To achieve this bound, we associate, with every F\u0358 type, a firstorder canonical form, which may be computed in time O(nlogn). By exploiting this notion, we reduce all three problems to equality and unification of firstorder recursive terms, for which efficient algorithms are known.
p211
aVIt is now 20 years since the Ericsson Computer Science Laboratory (CSLab) was formed, and 20 years since CSLab started performing the programming language experiments that eventually led to the development of the Erlang programming language. For 15 years, Ericsson has conducted experiments with Erlang in an industrial setting, and for 12 years, actual commercial products have been developed.This talk summarizes some of the lessons learned from this work. It is well known that it is difficult to introduce a new programming technology. Even so, engineers often fail to realize that the reasons for resisting change are more often social/political than technical, even though the objections may seem technical in nature. This talk will highlight a few of the common objections to Erlang, and measure them against practical experience. In addition, it will suggest some topics for research that could result in significant cost savings in the near future, as well as inspire closer cooperation between industry and academia.
p212
aVWe present a process logic for the p calculus with the linear/affine type discipline [6, 7, 31, 32, 33, 59, 60]. Built on the preceding studies on logics for programs and processes, simple systems of assertions are developed, capturing the classes of behaviours ranging from purely functional interactions to those with destructive update, local state and genericity. A central feature of the logic is representation of the behaviour of an environment as the dual of that of a process in an assertion, which is crucial for obtaining compositional proof systems. From the process logic we can derive compositional program logics for various higherorder programming languages, whose soundness is proved via their embeddings into the process logic. In this paper, the key technical framework of the process logic and its applications is presented focussing on pure functional behaviour and a prototypical callbyvalue functional language, leaving the full technical development to [27, 26].
p213
aVConcurrency, as a useful feature of many modern programming languages and systems, is generally hard to reason about. Although existing work has explored the verification of concurrent programs using highlevel languages and calculi, the verification of concurrent assembly code remains an open problem, largely due to the lack of abstraction at a lowlevel. Nevertheless, it is sometimes necessary to reason about assembly code or machine executables so as to achieve higher assurance.In this paper, we propose a logicbased "type" system for the static verification of concurrent assembly programs, applying the "invariance proof" technique for verifying general safety properties and the "assumeguarantee" paradigm for decomposition. In particular, we introduce a notion of "local guarantee" for the threadmodular verification in a nonpreemptive setting.Our system is fully mechanized. Its soundness has been verified using the Coq proof assistant. A safety proof of a program is semiautomatically constructed with help of Coq, allowing the verification of even undecidable safety properties. We demonstrate the usage of our system using three examples, addressing mutual exclusion, deadlock freedom, and partial correctness respectively.
p214
aVAs a company, Galois began its life with the mission simply of supplying functional programming services, building tools and products for clients, leveraging the productivity of functional languages and the abilities of our engineers. This went well, except that our business lacked focus. Every new job had to be found and sold from scratch. We realized that we had to focus on a specific market if we wanted to achieve stability and growth. We chose High Assurance Software as it was a natural fit both for our technology and our expertise, further narrowing our attention to information assurance (IA).
p215
aVEven in statically typed languages it is useful to have certain invariants checked dynamically. Findler and Felleisen gave an algorithm for dynamically checking expressive higherorder types called contracts. If we postulate soundness (in the sense that whenever a term is accused of violating its contract it really does fail to satisfy it), then their algorithm implies a semantics for contracts. Unfortunately, the implicit nature of the resulting model makes it rather unwieldy.In this paper we demonstrate that a direct approach yields essentially the same semantics without having to refer to contractchecking in its definition. The sodefined model largely coincides with intuition, but it does expose some peculiarities in its interpretation of predicate contracts where a notion of safety (which we define in the paper) "leaks" into the semantics of Findler and Felleisen's original unrestricted predicate contracts.This counterintuitive aspect of the semantics can be avoided by changing the language, replacing unrestricted predicate contracts with a restricted version. The corresponding loss in expressive power can be recovered by also providing a way of explicitly expressing safety as a contracteither in adhoc fashion or, e.g., by including general recursive contracts.
p216
aVCompilers structured as a small number of monolithic passes are difficult to understand and difficult to maintain. Adding new optimizations often requires major restructuring of existing passes that cannot be understood in isolation. The steep learning curve is daunting, and even experienced developers find it hard to modify existing passes without introducing subtle and tenacious bugs. These problems are especially frustrating when the developer is a student in a compiler class.An attractive alternative is to structure a compiler as a collection of many small passes, each of which performs a single task. This "micropass" structure aligns the actual implementation of a compiler with its logical organization, simplifying development, testing, and debugging. Unfortunately, writing many small passes duplicates code for traversing and rewriting abstract syntax trees and can obscure the meaningful transformations performed by individual passes.To address these problems, we have developed a methodology and associated tools that simplify the task of building compilers composed of many finegrained passes. We describe these compilers as "nanopass" compilers to indicate both the intended granularity of the passes and the amount of source code required to implement each pass. This paper describes the methodology and tools comprising the nanopass framework.
p217
aVThis paper presents a methodology for implementing natural language morphology in the functional language Haskell. The main idea behind is simple: instead of working with untyped regular expressions, which is the state of the art of morphology in computational linguistics, we use finite functions over hereditarily finite algebraic datatypes. The definitions of these datatypes and functions are the languagedependent part of the morphology. The languageindependent part consists of an untyped dictionary format which is used for synthesis of word forms, and a decorated trie, which is used for analysis.Functional Morphology builds on ideas introduced by Huet in his computational linguistics toolkit Zen, which he has used to implement the morphology of Sanskrit. The goal has been to make it easy for linguists, who are not trained as functional programmers, to apply the ideas to new languages. As a proof of the productivity of the method, morphologies for Swedish, Italian, Russian, Spanish, and Latin have already been implemented using the library. The Latin morphology is used as a running example in this article.
p218
aVAmong slidepresentation systems, the dominant application offers essentially no abstraction capability. Slideshow, an extension of PLT Scheme, represents our effort over the last several years to build an abstractionfriendly slide system. We show how functional programming is well suited to the task of slide creation, we report on the programming abstractions that we have developed for slides, and we describe our solutions to practical problems in rendering slides. We also describe a prototype extension to DrScheme that supports a mixture of programmatic and WYSIWYG slide creation.
p219
aVA generic function is a function that can be instantiated on many data types to obtain data type specific functionality. Examples of generic functions are the functions that can be derived in Haskell, such as show, read, and '=='. The recent years have seen a number of proposals that support the definition of generic functions. Some of the proposals define new languages, some define extensions to existing languages. As a common characteristic none of the proposals can be made to work within Haskell 98: they all require something extra, either a more sophisticated type system or an additional language construct. The purpose of this pearl is to show that one can, in fact, program generically within Haskell 98 obviating to some extent the need for fancy type systems or separate tools. Haskell's type classes are at the heart of this approach: they ensure that generic functions can be defined succinctly and, in particular, that they can be used painlessly.
p220
aVHigherorder languages that encourage currying are implemented using one of two basic evaluation models: push/enter or eval/apply. Implementors use their intuition and qualitative judgements to choose one model or the other.Our goal in this paper is to provide, for the first time, a more substantial basis for this choice, based on our qualitative and quantitative experience of implementing both models in a stateoftheart compiler for Haskell.Our conclusion is simple, and contradicts our initial intuition: compiled implementations should use eval/apply.
p221
aVPrograms in embedded languages contain invariants that are not automatically detected or enforced by their host language. We show how to use macros to easily implement partial evaluation of embedded interpreters in order to capture invariants encoded in embedded programs and render them explicit in the terms of their host language. We demonstrate the effectiveness of this technique in improving the results of a value flow analysis.
p222
aVThis paper presents an approach to searching for deadlocks in Concurrent Haskell programs. The search is based on a redefinition of the IO monad which allows the reversal of Concurrent Haskells concurrency primitives. Hence, it is possible to implement this search by a backtracking algorithm checking all possible schedules of the system. It is integrated in the Concurrent Haskell Debugger (CHD), and automatically searches for deadlocks in the background while debugging. The tool is easy to use and the small modifications of the source program are done by a preprocessor. In the tool we use iterative deepening as search strategy which quickly detects deadlocks close to the actual system configuration and utilizes idle time during debugging at the best.
p223
aVThere is a correspondence between classical logic and programming language calculi with firstclass continuations. With the addition of control delimiters (prompts), the continuations become composable and the calculi are believed to become more expressive. We formalise that the addition of prompts corresponds to the addition of a single dynamicallyscoped variable modelling the special toplevel continuation. From a type perspective, the dynamicallyscoped variable requires effect annotations. From a logic perspective, the effect annotations can be understood in a standard logic extended with the dual of implication, namely subtraction.
p224
aVPast attempts to relate two wellknown models of backtracking computation have met with only limited success. We relate these two models using logical relations. We accommodate higherorder values and infinite computations. We also provide an operational semantics, and we prove it adequate for both models.
p225
aVSometimes it's worth doing things badly in order to do them fast. When is this the right plan? Are some kinds of mistakes worse than others? Could the right infrastructure make trial and error programming more effective? How will it play out in the future?
p226
aVWe extend Haskell with regular expression patterns. Regular expression patterns provide means for matching and extracting data which goes well beyond ordinary pattern matching as found in Haskell. It has proven useful for string manipulation and for processing structured data such as XML. Regular expression patterns can be used with arbitrary lists, and work seamlessly together with ordinary pattern matching in Haskell. Our extension is lightweight, it is little more than syntactic sugar. We present a semantics and a type system, and show how to implement it as a preprocessor to Haskell.
p227
aVHindley and Milner's type system, in its purest form, allows what one might call full type inference. Even when a program contains no explicit type information, its very structure gives rise to a conjunction of type equations (or, more generally, to a constraint) whose resolution allows reconstructing the type of every variable and of every subexpression. Actual programming languages based on this type system, such as Standard ML, Objective Caml, and Haskell, do allow users to provide explicit type information via optional type annotations. Yet, this extra information does not make type inference any easier: it simply gives rise to extra equations, leading to a more specific constraint.There are extensions of Hindley and Milner's type system where full type inference becomes undecidable or intractable. Then, it is common to require explicit type information, via mandatory type annotations. Polymorphic recursion is a simple and wellknown instance of this phenomenon. This talk presents two more elaborate instances, commonly known as arbitraryrank polymorphism and generalized algebraic data types.In both cases, type inference is made tractable thanks to mandatory type annotations. There is a twist, though: these required annotations are often numerous and redundant. So, it seems desirable to make them optional again, and to set up a distinct mechanism to guess some of the elided annotations. This mechanism, referred to as elaboration, can be heuristic. It must be predictable. In the two cases considered in this talk, it is a form of local type inference.This approach leads to a socalled stratified type inference system, where type information is propagated first in a local, ad hoc manner during elaboration, then in a possibly nonlocal, but wellspecified manner during constraint solving. This appears to be one of the less displeasing ways of describing type inference systems that are able to exploit optional type annotations to infer better (or different) types.
p228
aVWe present a new approach to the old problem of adding side effects to purely functional languages. Our idea is to extend the language with "witnesses," which is based on an arguably more pragmatic motivation than past approaches. We give a semantic condition for correctness and prove it is sufficient. We also give a static checking algorithm that makes use of a network flow property equivalent to the semantic condition.
p229
aVWe describe a monadic interface to lowlevel hardware features that is a suitable basis for building operating systems in Haskell. The interface includes primitives for controlling memory management hardware, usermode process execution, and lowlevel device I/O. The interface enforces memory safety in nearly all circumstances. Its behavior is specified in part by formal assertions written in a programming logic called PLogic. The interface has been implemented on bare IA32 hardware using the Glasgow Haskell Compiler (GHC) runtime system. We show how a variety of simple O/S kernels can be constructed on top of the interface, including a simple separation kernel and a demonstration system in which the kernel, window system, and all device drivers are written in Haskell.
p230
aVThis talk presents the tumultuous history of JavaScript, from its first appearance in Netscape 2 beta releases in the fall of 1995 through the present, with emphasis on the unvarnished, realworld side of designing, implementing, shipping, and standardizing a functional programming language used by millions of people. JavaScript was conceived of as an "objectbased scripting language", but its inspiration came originally from Scheme, with an admixture of Self. Designing a language to fit the constraints of the target audience of HTML authors, the embedding browser application, and the market conditions of that time was challenging. We discuss what worked and what did not, and how the language evolved due to competition and de jure standardization. The talk ranges widely over the history of the web, including the recent renaissance of JavaScript and its development in the ECMA standards group. We close by presenting new work likely to appear in the next version of the language.
p231
aVWe explore partial typeinference for System F based on typecontainment. We consider both cases of a purely functional semantics and a callbyvalue stateful semantics. To enable typeinference, we require higherrank polymorphism to be userspecified via type annotations on source terms. We allow implicit predicative typecontainment and explicit impredicative typeinstantiation. We obtain a core language that is both as expressive as System F and conservative over ML. Its type system has a simple logical specification and a partial typereconstruction algorithm that are both very close to the ones for ML. We then propose a surface language where some annotations may be omitted and rebuilt by some algorithmically defined but logically incomplete elaboration mechanism.
p232
aVMLF is a type system that extends a functional language with impredicative rankn polymorphism. Type inference remains possible and only in some clearly defined situations, a local type annotation is required. Qualified types are a general concept that can accommodate a wide range of type systems extension, for example, type classes in Haskell. We show how the theory of qualified types can be used seamlessly with the higherranked impredicative polymorphism of MLF, and give a solution to the nontrivial problem of evidence translation in the presence of impredicative datatypes.
p233
aVA mixin module is a programming abstraction that simultaneously generalizes \u03bbabstractions, records, and mutually recursive definitions. Although various mixin module type systems have been developed, no one has investigated principal typings or developed type inference for firstclass mixin modules, nor has anyone added Milner's letpolymorphism to such a system.This paper proves that typability is NPcomplete for the naive approach followed by previous mixin module type systems. Because a \u03bbcalculus extended with record concatenation is a simple restriction of our mixin module calculus, we also prove the folk belief that typability is NPcomplete for the naive early type systems for record concatenation.To allow feasible type inference, we present Martini, a new system of simple types for mixin modules with principal typings. Martini is conceptually simple, with no subtyping and a clean and balanced separation between unificationbased type inference with type and row variables and constraint solving for safety of linking and field extraction. We have implemented a type inference algorithm and we prove its complexity to be O(n2), or O(n) given a fixed bound on the number of field labels. To prove the complexity, we need to present an algorithm for row unification that may have been implemented by others, but which we could not find written down anywhere. Because Martini has principal typings, we successfully extend it with Milner's letpolymorphism.
p234
aVThis paper explains how the highlevel treatment of datatypes in functional languages\u2014using features like constructor functions and pattern matching\u2014can be made to coexist with bitdata. We use this term to describe the bit level representations of data that are required in the construction of many different applications, including operating systems, device drivers, and assemblers. We explain our approach as a combination of two language extensions, each of which could potentially be adapted to any modern functional language. The first adds simple and elegant constructs for manipulating raw bitfield values, while the second provides a viewlike mechanism for defining distinct new bitdata types with finecontrol over the underlying representation. Our design leverages polymorphic type inference, as well as techniques for improvement of qualified types, to track both the type and the width of bitdata structures. We have implemented our extensions in a small functional language interpreter, and used it to show that our approach can handle a wide range of practical bitdata types.
p235
aVRecent research has shown how boilerplate code, or repetitive code for traversing datatypes, can be eliminated using generic programming techniques already available within some implementations of Haskell. One particularly intractable kind of boilerplate is nameplate, or code having to do with names, namebinding, and fresh name generation. One reason for the difficulty is that operations on data structures involving names, as usually implemented, are not regular instances of standard map, fold, or zip operations. However, in nominal abstract syntax, an alternative treatment of names and binding based on swapping, operations such as \u03b1equivalence, captureavoiding substitution, and free variable set functions are much betterbehaved.In this paper, we show how nominal abstract syntax techniques similar to those of FreshML can be provided as a Haskell library called FreshLib. In addition, we show how existing genericmprogramming techniques can be used to reduce the amount of nameplate code that needs to be written for new datatypes involving names and binding to almost nothing\u2014in short, how to scrap your nameplate.
p236
aVWe design and implement a library for adding backtracking computations to any Haskell monad. Inspired by logic programming, our library provides, in addition to the operations required by the MonadPlus interface, constructs for fair disjunctions, fair conjunctions, conditionals, pruning, and an expressive toplevel interface. Implementing these additional constructs is easy in models of backtracking based on streams, but not known to be possible in continuationbased models. We show that all these additional constructs can be generically and monadically realized using a single primitive msplit. We present two implementations of the library: one using success and failure continuations; and the other using control operators for manipulating delimited continuations.
p237
aVThe 'Scrap your boilerplate' approach to generic programming allows the programmer to write generic functions that can traverse arbitrary data structures, and yet have typespecific cases. However, the original approach required all the typespecific cases to be supplied at once, when the recursive knot of generic function definition is tied. Hence, generic functions were closed. In contrast, Haskell's type classes support open, or extensible, functions that can be extended with new typespecific cases as new data types are defined. In this paper, we extend the 'Scrap your boilerplate' approach to support this open style. On the way, we demonstrate the desirability of abstraction over type classes, and the usefulness of recursive dictionarie.
p238
aVWriting loops with tailrecursive function calls is the equivalent of writing them with goto's. Given that loop packages for Lispfamily languages have been around for over 20 years, it is striking that none have had much success in the Scheme world. I suggest the reason is that Scheme forces us to be precise about the scoping of the various variables introduced by our loop forms, something previous attempts to design ambitious loop forms have not managed to do.I present the design of a loop package for Scheme with a welldefined and natural scoping rule, based on a notion of control dominance that generalizes the standard lexicalscope rule of the \u03bbcalculus. The new construct is powerful, clear, modular and extensible.The loop language is defined in terms of an underlying language for expressing controlflow graphs. This language itself has interesting properties as an intermediate representation.
p239
aVImplementing firstclass continuations can pose a challenge if the target machine makes no provisions for accessing and reinstalling the runtime stack. In this paper, we present a novel translation that overcomes this problem. In the first half of the paper, we introduce a theoretical model that shows how to eliminate the capture and the use of firstclass continuations in the presence of a generalized stack inspection mechanism. The second half of the paper explains how to translate this model into practice in two different contexts. First, we reformulate the servlet interaction language in the PLT Web server, which heavily relies on firstclass continuations. Using our technique, servlet programs can be run directly under the control of noncooperative web servers such as Apache. Second, we show how to use our new technique to copy and reconstitute the stack on MSIL.Net using exception handlers. This establishes that Scheme's firstclass continuations can exist on noncooperative virtual machines.
p240
aVNarrowingdriven partial evaluation is a powerful technique for the specialization of (firstorder) functional and functional logic programs. However, although it gives good results on small programs, it does not scale up well to realistic problems (e.g., interpreter specialization). In this work, we introduce a faster partial evaluation scheme by ensuring the termination of the process offline. For this purpose, we first characterize a class of programs which are quasiterminating, i.e., the computations performed with needed narrowing\u2014the symbolic computation mechanism of narrowingdriven partial evaluation\u2014only contain finitely many different terms (and, thus, partial evaluation terminates). Since this class is quite restrictive, we also introduce an annotation algorithm for a broader class of programs so that they behave like quasiterminating programs w.r.t. an extension of needed narrowing. Preliminary experiments are encouraging and demonstrate the usefulness of our approach.
p241
aVWhat does it mean for a programming language to exist? Usually languages are defined by an informal description augmented by a reference compiler whose behavior is regarded as normative. This approach works well so long as the one true implementation suffices, but as soon as we wish to have multiple compilers for the same language, we must agree on what the language is independently of its implementations. Most often this is accomplished through social processes such as standardization committees for building consensus.These processes have served us well, and will continue to be important for language design. But they are not sufficient to support the level of rigor required to prove theorems about languages and programs written in them. For that we need a semantics, which provides an objective foundation for such analyses, typically in the form of a type system and an operational semantics. But merely having such a rigorous definition for a language is not enough \u2014 it must be validated by a body of metatheory that establishes its coherence and its consistency with expectations.But how are we to develop and maintain this body of theory? For fullscale languages the task is so onerous as to inhibit innovation and foster stagnation. The way forward is to take advantage of the recent advances in mechanized reasoning. By representing a language definition within a logical framework we may subject it to formal analysis, much as we use types to express and enforce crucial invariants in our programs. I will describe our use of the Twelf implementation of the LF logical framework, and discuss our successes and difficulties in using it as a tool for mechanizing the metatheory of programming languages.
p242
aVHaskell programmers often use a multiparameter type class in which one or more type parameters are functionally dependent on the first. Although such functional dependencies have proved quite popular in practice, they express the programmer's intent somewhat indirectly. Developing earlier work on associated data types, we propose to add functionally dependent types as type synonyms to typeclass bodies. These associated type synonyms constitute an interesting new alternative to explicit functional dependencies.
p243
aVProofcarrying code (PCC) is a general framework that can, in principle, verify safety properties of arbitrary machinelanguage programs. Existing PCC systems and typed assembly languages, however, can only handle sequential programs. This severely limits their applicability since many realworld systems use some form of concurrency in their core software. Recently Yu and Shao proposed a logicbased "type" system for verifying concurrent assembly programs. Their thread model, however, is rather restrictive in that no threads can be created or terminated dynamically and no sharing of code is allowed between threads. In this paper, we present a new formal framework for verifying general multithreaded assembly code with unbounded dynamic thread creation and termination as well as sharing of code between threads. We adapt and generalize the relyguarantee methodology to the assembly level and show how to specify the semantics of thread "fork" with argument passing. In particular, we allow threads to have different assumptions and guarantees at different stages of their lifetime so they can coexist with the dynamically changing thread environment. Our work provides a foundation for certifying realistic multithreaded programs and makes an important advance toward generating proofcarrying concurrent code.
p244
aVIn this paper a languagebased approach to functionally correct imperative programming is proposed. The approach is based on a programming language called RSP1, which combines dependent types, general recursion, and imperative features in a typesafe way, while preserving decidability of type checking. The methodology used is that of internal verification, where programs manipulate programmersupplied proofs explicitly as data. The fundamental technical idea of RSP1 is to identify problematic operations as impure, and keep them out of dependent types. The resulting language is powerful enough to verify statically nontrivial properties of imperative and functional programs. The paper presents the ideas through the examples of statically verified merge sort, statically verified imperative binary search trees, and statically verified directed acyclic graphs.
p245
aVWe present a compositional program logic for callbyvalue imperative higherorder functions with general forms of aliasing, which can arise from the use of reference names as function parameters, return values, content of references and parts of data structures. The program logic extends our earlier logic for aliasfree imperative higherorder functions with new modal operators which serve as building blocks for clean structural reasoning about programs and data structures in the presence of aliasing. This has been an open issue since the pioneering work by CartwrightOppen and Morris twentyfive years ago. We illustrate usage of the logic for description and reasoning through concrete examples including a higherorder polymorphic Quicksort. The logical status of the new operators is clarified by translating them into (in)equalities of reference names. The logic is observationally complete in the sense that two programs are observationally indistinguishable if they satisfy the same set of assertions.
p246
aVMonads are commonplace programming devices that are used to uniformly structure computations with effects such as state, exceptions, and I/O. This paper further develops the monadic programming paradigm by investigating the extent to which monadic computations can be optimised by using generalisations of short cut fusion to eliminate monadic structures whose sole purpose is to "glue together" monadic program components.We make several contributions. First, we show that every inductive type has an associated build combinator and an associated short cut fusion rule. Second, we introduce the notion of an inductive monad to describe those monads that give rise to inductive types, and we give examples of such monads which are widely used in functional programming. Third, we generalise the standard augment combinators and cata/augment fusion rules for algebraic data types to types induced by inductive monads. This allows us to give the first cata/augment rules for some common data types, such as rose trees. Fourth, we demonstrate the practical applicability of our generalisations by providing Haskell implementations for all concepts and examples in the paper. Finally, we offer deep theoretical insights by showing that the augment combinators are monadic in nature, and thus that our cata/build and cata/augment rules are arguably the best generally applicable fusion rules obtainable.
p247
aVThis paper defines PolyAML, a typed functional, aspectoriented programming language. The main contribution of Poly<SMALL>AML</SMALL> is the seamless integration of polymorphism, runtime type analysis and aspectoriented programming language features. In particular, Poly<SMALL>AML</SMALL> allows programmers to define typesafe polymorphic advice using pointcuts constructed from a collection of polymorphic join points. Poly<SMALL>AML</SMALL> also comes equipped with a type inference algorithm that conservatively extends HindleyMilner type inference. To support firstclass polymorphic pointcut designators, a crucial feature for developing aspectoriented profiling or logging libraries, the algorithm blends the conventional HindleyMilner type inference algorithm with a simple form of local type inference.We give our language operational meaning via a typedirected translation into an expressive typesafe intermediate language. Many complexities of the source language are eliminated in this translation, leading to a modular specification of its semantics. One of the novelties of the intermediate language is the definition of polymorphic labels for marking controlflow points. These labels are organized in a tree structure such that a parent in the tree serves as a representative for all of its children. Type safety requires that the type of each child is less polymorphic than its parent type. Similarly, when a set of labels is assembled as a pointcut, the type of each label is an instance of the type of the pointcut.
p248
aVExisting languages provide good support for typeful programming of standalone programs. In a distributed system, however, there may be interaction between multiple instances of many distinct programs, sharing some (but not necessarily all) of their module structure, and with some instances rebuilt with new versions of certain modules as time goes on. In this paper we discuss programminglanguage support for such systems, focussing on their typing and naming issues.We describe an experimental language, Acute, which extends an ML core to support distributed development, deployment, and execution, allowing typesafe interaction between separatelybuilt programs. The main features are: (1) typesafe marshalling of arbitrary values; (2) type names that are generated (freshly and by hashing) to ensure that type equality tests suffice to protect the invariants of abstract types, across the entire distributed system; (3) expressionlevel names generated to ensure that name equality tests suffice for typesafety of associated values, e.g. values carried on named channels; (4) controlled dynamic rebinding of marshalled values to local resources; and (5) thunkification of threads and mutexes to support computation mobility.These features are a large part of what is needed for typeful distributed programming. They are a relatively lightweight extension of ML, should be efficiently implementable, and are expressive enough to enable a wide variety of distributed infrastructure layers to be written as simple library code above the bytestring network and persistent store APIs. This disentangles the language runtime from communication intricacies. This paper highlights the main design choices in Acute. It is supported by a full language definition (of typing, compilation, and operational semantics), by a prototype implementation, and by example distribution libraries.
p249
aVCurrent languages allow a programmer to describe an interface only by enumerating its parts, possibly including other interfaces wholesale. Such languages cannot express relationships between interfaces, yet when independently developed software components are combined into a larger system, significant relationships arise.To address this shortcoming, we define, as a conservative extension of ML, a language for manipulating interfaces. Our language includes operations for adding, renaming, and removing components; for changing the type associated with a value; for making manifest types abstract and vice versa; and for combining interfaces. These operations can express useful relationships among interfaces. We have defined a formal semantics in which an interface denotes a group of four sets; we show how these sets determine a subtyping relation, and we sketch the elaboration of an interface into its denotation.
p250
aVExistential types provide a simple and elegant foundation for understanding generative abstract data types, of the kind supported by the Standard ML module system. However, in attempting to extend ML with support for recursive modules, we have found that the traditional existential account of type generativity does not work well in the presence of mutually recursive module definitions. The key problem is that, in recursive modules, one may wish to define an abstract type in a context where a name for the type already exists, but the existential type mechanism does not allow one to do so.We propose a novel account of recursive type generativity that resolves this problem. The basic idea is to separate the act of generating a name for an abstract type from the act of defining its underlying representation. To define several abstract types recursively, one may first "forwarddeclare" them by generating their names, and then define each one secretly within its own defining expression. Intuitively, this can be viewed as a kind of backpatching semantics for recursion at the level of types. Care must be taken to ensure that a type name is not defined more than once, and that cycles do not arise among "transparent" type definitions.In contrast to the usual continuationpassing interpretation of existential types in terms of universal types, our account of type generativity suggests a destinationpassing interpretation. Briefly, instead of viewing a value of existential type as something that creates a new abstract type every time it is unpacked, we view it as a function that takes as input a preexisting undefined abstract type and defines it. By leaving the creation of the abstract type name up to the client of the existential, our approach makes it significantly easier to link abstract data types together recursively.
p251
aVA limited form of dependent types, called Generalized Algebraic Data Types (GADTs), has recently been added to the list of Haskell extensions supported by the Glasgow Haskell Compiler. Despite not being fullfledged dependent types, GADTs still offer considerably enlarged scope for enforcing important code and data invariants statically. Moreover, GADTs offer the tantalizing possibility of writing more efficient programs since capturing invariants statically through the type system sometimes obviates entire layers of dynamic tests and associated data markup. This paper is a case study on the applications of GADTs in the context of Yampa, a domainspecific language for Functional Reactive Programming in the form of a selfoptimizing, arrowbased Haskell combinator library. The paper has two aims. Firstly, to explore what kind of optimizations GADTs make possible in this context. Much of that should also be relevant for other domainspecific embedded language implementations, in particular arrowbased ones. Secondly, as the actual performance impact of the GADTbased optimizations is not obvious, to quantify this impact, both on tailored micro benchmarks, to establish the effectiveness of individual optimizations, and on two fairly large, realistic applications, to gauge the overall impact. The performance gains for the micro benchmarks are substantial. This implies that the Yampa API could be simplified as a number of "precomposed" primitives that were there mainly for performance reasons are no longer needed. As to the applications, a worthwhile performance gain was obtained in one case whereas the performance was more or less unchanged in the other.
p252
aVApplied Type System (ATS) is recently proposed as a framework for designing and formalizing (advanced) type systems in support of practical programming. In ATS, the definition of type equality involves a constraint relation, which may or may not be algorithmically decidable. To support practical programming, we adopted a design in the past that imposes certain restrictions on the syntactic form of constraints so that some effective means can be found for solving constraints automatically. Evidently, this is a rather em ad hoc design in its nature. In this design, which we claim to be both novel and practical. Instead of imposing syntactical restrictions on constraints, we provide a means for the programmer to construct proofs that attest to the validity of constraints. In particular, we are to accommodate a programming paradigm that enables the programmer to combine programming with theorem proving. Also we present some concrete examples in support of the practicality of this design.
p253
aVThe concept of a "unique" object arises in many emerging programming languages such as Clean, CQual, Cyclone, TAL, and Vault. In each of these systems, unique objects make it possible to perform operations that would otherwise be prohibited (e.g., deallocating an object) or to ensure that some obligation will be met (e.g., an opened file will be closed). However, different languages provide different interpretations of "uniqueness" and have different rules regarding how unique objects interact with the rest of the language.Our goal is to establish a common model that supports each of these languages, by allowing us to encode and study the interactions of the different forms of uniqueness. The model we provide is based on a substructural variant of the polymorphic \u03bbcalculus, augmented with four kinds of mutable references: unrestricted, relevant, affine, and linear. The language has a natural operational semantics that supports deallocation of references, strong (typevarying) updates, and storage of unique objects in shared references. We establish the strong soundness of the type system by constructing a novel, semantic interpretation of the types.
p254
aVWe have designed, implemented, and evaluated AtomCaml, an extension to Objective Caml that provides a synchronization primitive for atomic (transactional) execution of code. A firstclass primitive function of type (unit>'a)>'a evaluates its argument (which may call other functions, even external C functions) as though no other thread has interleaved execution. Our design ensures fair scheduling and obstructionfreedom. Our implementation extends the Objective Caml bytecode compiler and runtime system to support atomicity. A loggingandrollback approach lets us undo uncompleted atomic blocks upon thread preemption, and retry them when the thread is rescheduled. The mostly functional nature of the Caml language and the Objective Caml implementation's commitment to a uniprocessor execution model (i.e., threads are interleaved, not executed simultaneously) allow particularly efficient logging. We have evaluated the efficiency and easeofuse of AtomCaml by writing libraries and microbenchmarks, writing a small application, and replacing all locking with atomicity in an existing, large multithreaded application. Our experience indicates the performance overhead is negligible, atomic helps avoid synchronization mistakes, and idioms such as condition variables adapt reasonably to the atomic approach.
p255
aVChez Scheme is now over 20 years old, the first version having been released in 1985. This paper takes a brief look back on the history of Chez Scheme's development to explore how and why it became the system it is today.
p256
aVWe describe an sexpression based syntaxextension framework much like Scheme macros, with a key additional facility: the ability to define static semantics, such as type systems or program analysis, for the new, userdefined forms or embedded languages, thus allowing us to construct "towers" of language levels. In addition, the static semantics of the languages at two adjacent levels in the tower can be connected, allowing improved reasoning power at a higher (and perhaps more restricted) level to be reflected down to the static semantics of the language level below. We demonstrate our system by designing macros for an assembly language, together with some example static analyses (termination analysis, type inference and controlflow analysis).
p257
aVConcurrent programs require highlevel abstractions in order to manage complexity and enable compositional reasoning. In this paper, we introduce a novel concurrency abstraction, dubbed transactional events, which combines firstclass synchronous messagepassing events with allornothing transactions. This combination enables simple solutions to interesting problems in concurrent programming. For example, guarded synchronous receive can be implemented as an abstract transactional event, whereas in other languages it requires a nonabstract, nonmodular protocol. Likewise, threeway rendezvous can also be implemented as an abstract transactional event, which is impossible using firstclass events alone. Both solutions are easy to code and easy to reason about.The expressive power of transactional events arises from a sequencing combinator whose semantics enforces an allornothing transactional property  either both of the constituent events synchronize in sequence or neither of them synchronizes. This sequencing combinator, along with a nondeterministic choice combinator, gives transactional events the compositional structure of a monadwithplus. We provide a formal semantics for and a preliminary implementation of transactional events.
p258
aVTransient faults that arise in largescale software systems can often be repaired by reexecuting the code in which they occur. Ascribing a meaningful semantics for safe reexecution in multithreaded code is not obvious, however. For a thread to correctly rexecute a region of code, it must ensure that all other threads which have witnessed its unwanted effects within that region are also reverted to a meaningful earlier state. If not done properly, data inconsistencies and other undesirable behavior may result. however, automatically determining what constitutes a consistent global checkpoint is not straightforward since thread interactions are a dynamic property of the program.In this paper, we present a safe and efficient checkpointing mechanism for Concurrent ML (CML) that can be used to recover from transient faults. We introduce a new linguistic abstraction called stabilizers that permits the specification of perthread monitors and the restoration of globally consistent checkpoints. Safe global states are computed through lightweight monitoring of communication events among threads (e.g. messagepassing operations or updates to shared variables).Our experimental results on several realistic, multithreaded, serverstyle CML applications, including a web server and a windowing toolkit, show that the overheads to use stabilizers are small, and lead us to conclude that they are a viable mechanism for defining safe checkpoints in concurrent functional programs.
p259
aVThe problem of expressing I/O and side effects in functional languages is a wellestablished one. This paper addresses this problem from a general semantic viewpoint by giving a unified framework for describing shared state, I/O and deterministic concurrency. We develop a modified state transformer which lets us mathematically model the API, then investigate and machine verify some broad conditions under which confluence holds. This semantics is used as the basis for a small deterministic Haskell language extension called CURIO, which enforces determinism using runtime checks.Our confluence condition is first shown to hold for a variety of small components, such as individual shared variables, 1to1 communication channels, and Istructures. We then show how models of substantial APIs (like a modification of Haskell's file I/O API which permits interprocess communication) may be constructed from these smaller components using "combinators" in such a way that determinism is always preserved. We describe combinators for product, nameindexing and dynamic allocation, the last of which requires some small extensions to cater for process locality.
p260
aVI report on an experience using the Coq proof assistant to develop a program verification tool with a machinecheckable proof of full correctness. The verifier is able to prove memory safety of x86 machine code programs compiled from code that uses algebraic datatypes. The tool's soundness theorem is expressed in terms of the bitlevel semantics of x86 programs, so its correctness depends on very few assumptions. I take advantage of Coq's support for programming with dependent types and modules in the structure of my development. The approach is based on developing a library of reusable functors for transforming a verifier at one level of abstraction into a verifier at a lower level. Using this library, it's possible to prototype a verifier based on a new type system with a minimal amount of work, while obtaining a very strong soundness theorem about the final product.
p261
aVWe investigate the development of a generalpurpose framework for mechanized reasoning about the metatheory of programming languages. In order to provide a standard, uniform account of a programming language, we propose to define it as a logic in a logical framework, using the same mechanisms for definition, reasoning, and automation that are available to other logics. Then, in order to reason about the language's metatheory, we use reflection to inject the programming language into (usually richer and more expressive) metatheory.One of the key features of our approach is that structure of the language is preserved when it is reflected, including variables, metavariables, and binding structure. This allows the structure of proofs to be preserved as well, and there is a onetoone map from proof steps in the original programming logic to proof steps in the reflected logic. The act of reflecting a language is automated; all definitions, theorems, and proofs are preserved by the transformation and all the key lemmas (such as proof and structural induction) are automatically derived.The principal representation used by the reflected logic is higherorder abstract syntax (HOAS). However, reasoning about terms in HOAS can be awkward in some cases, especially for variables. For this reason, we define a computationally equivalent variablefree de Bruijn representation that is interchangeable with the HOAS in all contexts. The de Bruijn representation inherits the properties of substitution and alphaequality from the logical framework, and it is not complicated by administrative issues like variable renumbering.We further develop the concepts and principles of proofs, provability, and structural and proof induction. This work is fully implemented in the MetaPRL theorem prover. We illustrate with an application to F<: as defined in the POPLmark challenge.
p262
aVThis paper gives a precise characterization for the complexity of the problem of proving equal two streams defined with a finite number of equations: \u03a00 over 2. Since the \u03a0 0 over 2 class includes properly both the reursively enumerable and the corecursively enumerable classes, this result implies that neither the set of pairs of equal streams nor the set of pairs of unequal streams is recursively enumerable. Consequently, one can find no algorithm for determining equality of streams, as well as no algorithm for determining inequality of streams. In particular, there is no complete proof system for equality of streams and no complete system for inequality of streams.
p263
aVThis paper presents the core type system and type inference algorithm of OCamlDuce, a merger between OCaml and XDuce. The challenge was to combine two type checkers of very different natures while preserving the best properties of both (principality and automatic type reconstruction on one side; very precise types and implicit subtyping on the other side). Type inference can be described by two successive passes: the first one is an MLlike unificationbased algorithm which also extracts data flow constraints about XML values; the second one is an XDucelike algorithm which computes XML types in a direct way. An optional preprocessing pass, called strengthening, can be added to allow more implicit use of XML subtyping. This pass is also very similar to an ML type checker.
p264
aVOften, independent organizations define and advocate different XML formats for a similar purpose and, as a result, application programs need to mutually convert between such formats. Existng XML transformation languages, such as XSLT and XDuce, are unsatisfactory for this purpose since we would have to write, e.g., two programs for the forward and the backward transformations in case of two formats, incur high developing and maintenance costs.This paper proposes the bidirectional XML transformation language biXid, allowing us to write only one program for both directions of conversion. Our language adopts a common paradigm programmingbyrelation, where a program defines a relation over documents and transforms a document to another in a way satisfying this relation. Our contributions here are specific language features for facilitating realistic conversions whose target formats are loosely in parallel but have many discrepancies in details. Concretely, we (1) adopt XDucestyle regular expression patterns for describing and analyzing XML structures, (2) fully permit ambiguity for treating formats that do not have equivalent expressivenesses, and (3) allow  nonlinear pattern variables for expressing nontrivial transformations that cannot be written only with linear patterns, such as conversion between unordered and ordered data.We further develop an efficient evaluation algorithm for biXid, consisting of the "parsing" phase that transforms the input document to an intermediate "parse tree" structure and the "unparsing" phase that transforms it to an output document. Both phases use a variant of finite tree automata for performing a onepass scan on the input or the parse tree by using a standard technique that "maintains the set of all transitable states." However, the construction of the "unparsing" phase is challenging since ambiguity causes different ways of consuming the parse tree and thus results in multiple possible outputs that may have different structures.We have implemented a prototype system of biXid and confirmed that it has enough expressiveness and a lineartime performance from experiments with several realistic bidirectional transformations including one between vCardXML and ContactXML.
p265
aVWe present two independent and complementary improvements for flowbased analysis of higherorder languages: (1) abstract garbage collection and (2) abstract counting, collectively titled \u0393CFA.Abstract garbage collection is an analog to its concrete counterpart: we determine when an abstract resource has become unreachable, and then reallocate it as fresh. This prevents flow sets from merging in the abstract, which has two immediate effects: (1) the precision of the analysis is increased, and (2) the running time of the analysis is frequently reduced. In some nontrivial cases, we achieve an order of magnitude improvement in precision and time simultaneously.In abstract counting, we track how many times an abstract resource has been allocated. A count of one implies that the abstract resource momentarily represents only one concrete resource. This, in turn, allows us to perform environment analysis and to expand the kinds (rather than just the degree) of optimizations available to the compiler.
p266
aVIn 1991, when the Journal of Functional Programming was inaugurated, the editors, Simon Peyton Jones and Philip Wadler, asked me to contribute a regular column to be called Functional Pearls. The idea was to emulate the very successful series of essays that Jon Bentley had written under the title Programming Pearls in the Communications of the ACM. A possible alternative model for the column was Martin Rem's Small Programming Exercises that appeared regularly in the Science of Computer Programming in the 1980s. In Rem's articles, various programming tasks were posed in one issue, and solved in the subsequent one. It was felt that similar material could be adapted to a functional style, using equational reasoning rather than the DijkstraHoare framework to derive the final product. After all, one reason that functional programming stimulated the interest of many at that time was that it was good for equational reasoning, a slogan captured in Mark Jone's GOFER system (Good For Equational Reasoning).I agreed to the suggestion, but only under the proviso that other contributors to the column should be sought. Counting to the end of the present year, 2006, about 64 pearls will have appeared in JFP, of which I have written 14. There are also various pearls that have been presented at ICFP and at MPC (Mathematics of Program Construction). The pearls range in content, from (hopefully) instructive exercises in program calculation  my own area of interest, to attractive presentations of new functional data structures  of which Ralf Hinze and Chris Okasaki were the main contributors, as well as interesting algorithms in their own right, such as Runciman's Lazy wheel sieves, and Huet's Zipper.This talk will review a little of the history of Functional Pearls, and tentatively try to suggest what ingredients make a good pearl and how pearls differ from normal research papers. Indeed, my brief from the Program Chair for this talk was expressed as follows: "Well done Functional Pearls are often the highlight of an ICFP conference, but many of the submitted ones somehow miss the mark, by being too trivial, too complicated, or somehow not quite the elegant solution one hopes for. So it would be interesting to hear about your experiences as to what makes a good one and how to go about creating it.Having accepted this daunting commission, and being mindful of Horace's remark that good advice should be short, I am now busily engaged in finding a pearl that is not too trivial, nor too complicated, and is sufficiently elegant to serve as a decent example, both to illustrate the talk and to provide some technical content.
p267
aVThis paper introduces a pattern for almost compositional functions over recursive data types, and over families of mutually recursive data types. Here "almost compositional" means that for a number of the constructors in the type(s), the result of the function depends only on the constructor and the results of calling the function on the constructor's arguments. The pattern consists of a generic part constructed once for each data type or family of data types, and a taskspecific part. The generic part contains the code for the predictable compositional cases, leaving the interesting work to the taskspecific part. Examples of the pattern implemented in dependent type theory with inductive families, in Haskell with generalized algebraic data types and rank2 polymorphism, and in Java using a variant of the Visitor design pattern are given. The relationship to the "Scrap Your Boilerplate" approach to generic programming, and to general tree types in dependent type theory are also investigated.
p268
aVWe present a unifying solution to the problem of fusion of functions, where both the producer function and the consumer function have one accumulating parameter. The key idea in this development is to formulate the producer function as a function which computes over a monoid of data contexts. Upon this formulation, we develop a fusion method called algebraic fusion based on the elementary theory of universal algebra and monoids. The producer function is fused with a monoid homomorphism that is derived from the definition of the consumer function, and is turned into a higherorder function f that computes over the monoid of endofunctions.We then introduce a general concept called improvement, in order to reduce the cost of computing over the monoid of endofunctions (i. e., function closures). An improvement of the function f via a monoid homomorphism h is a function g that satisfies f =h g. This provides a principled way of finding a firstorder function representing a solution to the fusion problem. It also presents a clean and unifying account for varying fusion methods that have been proposed so far. Furthermore, we show that our method extends to support partial and infinite data structures, by means of an appropriate monoid structure.
p269
aVWe present language mechanisms for polymorphic, extensible records and their exact dual, polymorphic sums with extensible firstclass cases. These features make it possible to easily extend existing code with new cases. In fact, such extensions do not require any changes to code that adheres to a particular programming style. Using that style, individual extensions can be written independently and later be composed to form larger components. These language mechanisms provide a solution to the expression problem.We study the proposed mechanisms in the context of an implicitly typed, purely functional language PolyR. We give a type system for the language and provide rules for a 2phase transformation: first into an explicitly typed \u03bbcalculus with record polymorphism, and finally to efficient indexpassing code. The first phase eliminates sums and cases by taking advantage of the duality with records.We implement a version of PolyR extended with imperative features and pattern matching  we call this language MLPolyR. Programs in MLPolyR require no type annotations  the implementation employs a reconstruction algorithm to infer all types. The compiler generates machine code (currently for PowerPC) and optimizes the representation of sums by eliminating closures generated by the dual construction.
p270
aVLanguages with rich type systems are beginning to employ a blend of type inference and type checking, so that the type inference engine is guided by programmersupplied type annotations. In this paper we show, for the first time, how to combine the virtues of two wellestablished ideas: unificationbased inference, and bidirectional propagation of type annotations. The result is a type system that conservatively extends HindleyMilner, and yet supports both higherrank types and impredicativity.
p271
aVThe Dependency Core Calculus (DCC) is an extension of the computational lambda calculus that was designed in order to capture the notion of dependency that arises in informationflow control, partial evaluation, and other programminglanguage settings. We show that, unexpectedly, DCC can also be used as a calculus for access control in distributed systems. Initiating the study of DCC from this perspective, we explore some of its appealing properties.
p272
aVMonads are widely used in Haskell for modeling computational effects, but defining monads remains a daunting challenge. Since every part of a monad's definition depends on its computational effects, programmers cannot leverage the common behavior of all monads easily and thus must build from scratch each monad that models a new computational effect.I propose the Unimo framework which allows programmers to define monads and monad transformers in a modular manner. Unimo contains a heavily parameterized observer function which enforces the monad laws, and programmers define a monad by invoking the observer function with arguments that specify the computational effects of the monad. Since Unimo provides the common behavior of all monads in a reusable form, programmers no longer need to rebuild the semantic boilerplate for each monad and can instead focus on the more interesting and rewarding task of modeling the desired computational effects.
p273
aVDynamic binding and delimited control are useful together in many settings, including Web applications, database cursors, and mobile code. We examine this pair of language features to show that the semantics of their interaction is illdefined yet not expressive enough for these uses.We solve this open and subtle problem. We formalise a typed language DB+DC that combines a calculus DB of dynamic binding and a calculus DC of delimited control. We argue from theoretical and practical points of view that its semantics should be based on delimited dynamic binding: capturing a delimited continuation closes over part of the dynamic environment, rather than all or none of it; reinstating the captured continuation supplements the dynamic environment, rather than replacing or inheriting it. We introduce a type and reductionpreserving translation from DB + DC to DC, which proves that delimited control macroexpresses dynamic binding. We use this translation to implement DB+DC in Scheme, OCaml, and Haskell.We extend DB + DC with mutable dynamic variables and a facility to obtain not only the latest binding of a dynamic variable but also older bindings. This facility provides for stack inspection and (more generally) folding over the execution context as an inductive data structure.
p274
aVA transient hardware fault occurs when an energetic particle strikes a transistor, causing it to change state. These faults do not cause permanent damage, but may result in incorrect program execution by altering signal transfers or stored values. While the likelihood that such transient faults will cause any significant damage may seem remote, over the last several years transient faults have caused costly failures in highend machines at America Online, eBay, and the Los Alamos Neutron Science Center, among others [6, 44, 15]. Because susceptibility to transient faults is proportional to the size and density of transistors, the problem of transient faults will become increasingly important in the coming decades.This paper defines the first formal, typetheoretic framework for studying reliable computation in the presence of transient faults. More specifically, it defines \u03bbzap, a lambda calculus that exhibits intermittent data faults. In order to detect and recover from these faults, \u03bbzap programs replicate intermediate computations and use majority voting, thereby modeling softwarebased fault tolerance techniques studied extensively, but informally [10, 20, 30, 31, 32, 33, 41].To ensure that programs maintain the proper invariants and use \u03bbzap primitives correctly, the paper defines a type system for the language. This type system guarantees that welltyped programs can tolerate any single data fault. To demonstrate that \u03bbzap can serve as an idealized typed intermediate language, we define a typepreserving translation from a standard simplytyped lambda calculus into \u03bbzap.
p275
aVGeneralized algebraic data types (GADTs), sometimes known as "guarded recursive data types" or "firstclass phantom types", are a simple but powerful generalization of the data types of Haskell and ML. Recent works have given compelling examples of the utility of GADTs, although type inference is known to be difficult. Our contribution is to show how to exploit programmersupplied type annotations to make the type inference task almost embarrassingly easy. Our main technical innovation is wobbly types, which express in a declarative way the uncertainty caused by the incremental nature of typical typeinference algorithms.
p276
aVIn previous work, we proposed a Hoare Type Theory (HTT) which combines effectful higherorder functions, dependent types and Hoare Logic specifications into a unified framework. However, the framework did not support polymorphism, and ailed to provide a modular treatment of state in specifications. In this paper, we address these shortcomings by showing that the addition of polymorphism alone is sufficient for capturing modular state specifications in the style of Separation Logic. Furthermore, we argue that polymorphism is an essential ingredient of the extension, as the treatment of higherorder functions requires operations not encodable via the spatial connectives of Separation Logic.
p277
aVTheML module system is useful for building largescale programs. The programmer can factor programs into nested and parameterized modules, and can control abstraction with signatures. Yet ML prohibits recursion between modules. As a result of this constraint, the programmer may have to consolidate conceptually separate components into a single module, intruding on modular programming. Introducing recursive modules is a natural way out of this predicament. Existing proposals, however, vary in expressiveness and verbosity. In this paper, we propose a type system for recursive modules, which can infer their signatures. Opaque signatures can also be given explicitly, to provide type abstraction either inside or outside the recursion. The type system is decidable, and is sound for a callbyvalue semantics. We also present a solution to the expression problem, in support of our design choices.
p278
aVComponent programming techniques encourage abstraction and reuse through external linking. Some parts of a program, however, must use concrete, internally specified references, so a pure component system is not a sufficient mechanism for structuring programs. We present the combination of a static, internallylinked module system and a purely abstractive component system. The latter extends our previous model of typed units to properly account for translucency and sharing. We also show how units and modules can express an SMLstyle system of structures and functors, and we explore the consequences for recursive structures and functors.
p279
aVDespite its powerful module system, ML has not yet evolved for the modern world of dynamic and open modular programming, to which more primitive languages have adapted better so far. We present the design and semantics of a simple yet expressive firstclass component system for ML. It provides dynamic linking in a typesafe and typeflexible manner, and allows selective execution in sandboxes. The system is defined solely by reduction to higherorder modules plus an extension with simple modulelevel dynamics, which we call packages. To represent components outside processes we employ generic pickling. We give a module calculus formalising the semantics of packages and pickling.
p280
aVIt is rare to give a semantic definition of a fullscale programming language, despite the many potential benefits. Partly this is because the available metalanguages for expressing semantics  usually either L<scp>a</scp>TEX for informal mathematics, or the formal mathematics of a proof assistant  make it much harder than necessary to work with large definitions. We present a metalanguage specifically designed for this problem, and a tool, ott, that sanitychecks such definitions and compiles them into proof assistant code for Coq, HOL, Isabelle, and (in progress) Twelf, together with L<scp>a</scp>TEX code for productionquality typesetting, and OCaml boilerplate. The main innovations are:(1) metalanguage design to make definitions concise, and easy to read and edit;(2) an expressive but intuitive metalanguage for specifying binding structures; and (3) compilation to proof assistant code. This has been tested in substantial case studies, including modular specifications of calculi from the TAPL text, a Lightweight Java with Java JSR 277/294 module system proposals, and a large fragment of OCaml (around 306 rules), with machine proofs of various soundness results. Our aim with this work is to enable a phase change: making it feasible to work routinely, without heroic effort, with rigorous semantic definitions of realistic languages.
p281
aVWe present a pair of reasoning principles, definition and proof by rigid induction, which can be seen as proper generalizations of lazydatatype induction to monadic effects other than partiality. We further show how these principles can be integrated into logicalrelations arguments, and obtain as a particular instance a general and principled proof that the successstream and failurecontinuation models of backtracking are equivalent. As another application, we present a monadic model of general search trees, not necessarily traversed depthfirst. The results are applicable to both lazy and eager languages, and we emphasize this by presenting most examples in both Haskell and SML.
p282
aVThe MLF type system by Le Botlan and Rmy is a natural extension of HindleyMilner type inference that supports full firstclass polymorphism, where types can be of higherrank and impredicatively instantiated. Even though MLF is theoretically very attractive, it has not seen widespread adoption. We believe that this partly because it is unclear how the rich language of MLF types relate to standard System F types. In this article we give the first type directed translation of MLF terms to System F terms. Based on insight gained from this translation, we also define "Rigid MLF" (MLF=), a restriction of MLF where all bound values have a System F type. The expressiveness of MLF= is the same as that of boxy types, but MLF= needs fewer annotations and we give a detailed comparison between them.
p283
aVIn this talk, I will consider some possible extensions to existing functional programming languages that would make them more suitable for the important and growing class of artificial intelligence applications. First, I will motivate the need for these language extensions. Then I will give some technical detail about these extensions that provide the logic programming idioms, probabilistic computation, and modal computation. Some examples will be given to illustrate these ideas which have been implemented in the Bach programming language that is an extension of Haskell.
p284
aVWe present a model checker for verifying distributed programs written in the Erlang programming language. Providing a model checker for Erlang is especially rewarding since the language is by now being seen as a very capable platform for developing industrial strength distributed applications with excellent failure tolerance characteristics. In contrast to most other Erlang verification attempts, we provide support for a very substantial part of the language. The model checker has full Erlang data type support, support for general process communication, node semantics (interprocess behave subtly different from intraprocess communication), fault detection and fault tolerance through process linking, and can verify programs written using the OTP Erlang component library (used by most modern Erlang programs). As the model checking tool is itself implemented in Erlang we benefit from the advantages that a (dynamically typed) functional programming language offers: easy prototyping and experimentation with new verification algorithms, rich executable models that use complex data structures directly programmed in Erlang, the ability to treat executable models interchangeably as programs (to be executed directly by the Erlang interpreter) and data, and not least the possibility to cleanly structure and to cleanly combine various verification subtasks. In the paper we discuss the design of the tool and provide early indications on its performance.
p285
aVReactis is a commercially successful testing and validation tool which is implemented almost entirely in Standard ML. Our experience using a functional language to develop a commercial product has led us to the conclusion that while functional languages have some disadvantages, in the case of Reactis the benefits of a functional language substantially outweigh the drawbacks.
p286
aVIn this paper we introduce the iTask system: a set of combinators to specify work flows in a pure functional language at a very high level of abstraction. Work flow systems are automated systems in which tasks are coordinated that have to be executed by humans and computers. The combinators that we propose support work flow patterns commonly found in commercial work flow systems. Compared with most of these commercial systems, the iTask system offers several advantages: tasks are statically typed, tasks can be higher order, the combinators are fully compositional, dynamic and recursive work flows can be specified, and last but not least, the specification is used to generate an executable webbased multiuser work flow application. With the iTask system, useful work flows can be defined which cannot be expressed in other systems: work can be interrupted and subsequently directed to other workers for further processing. The implementation is special as well. It is based on the Clean iData toolkit which makes it possible to create fully dynamic, interactive, thin client web applications. Thanks to the generic programming techniques used in the iData toolkit, the programming effort is reduced significantly: state handling, form rendering, user interaction, and storage management is handled automatically. The iTask system allows a task to be regarded as a special kind of persistent redex being reduced by the application user via task completion. The combinators control the order in which these redexes are made available to the application user. The system rewrites the persistent task redexes in a similar way as functions are rewritten in lazy functional languages.
p287
aVOver the past year Untyped has developed some 40'000 lines of Scheme code for a variety of webbased applications, which receive over 10'000 hits a day. This is, to our knowledge, the largest webbased application deployment of PLT Scheme. Our experiences developing with PLT Scheme show that deficiencies in the existing infrastructure can be easily overcome, and we can exploit advanced language features to improve productivity. We conclude that PLT Scheme makes an excellent platform for developing webbased applications, and is competitive with more mainstream choices.
p288
aVFilinski showed that callcc and a single mutable reference cell are sufficient to express the delimited control operators shift and reset. However, this implementation interacts poorly with dynamic bindings like exception handlers. We present a variation on Filinski's encoding of delimited continuations that behaves appropriately in the presence of exceptions and give an implementation in Standard ML of New Jersey. We prove the encoding correct with respect to the semantics of delimited dynamic binding.
p289
aVOperators for delimiting control and for capturing composable continuations litter the landscape of theoretical programming language research. Numerous papers explain their advantages, how the operators explain each other (or don't), and other aspects of the operators' existence. Production programming languages, however, do not support these operators, partly because their relationship to existing and demonstrably useful constructs  such as exceptions and dynamic binding  remains relatively unexplored. In this paper, we report on our effort of translating the theory of delimited and composable control into a viable implementation for a production system. The report shows how this effort involved a substantial design element, including work with a formal model, as well as significant practical exploration and engineering. The resulting version of PLT Scheme incorporates the expressive combination of delimited and composable control alongside dynamicwind, dynamic binding, and exception handling. None of the additional operators subvert the intended benefits of existing control operators, so that programmers can freely mix and match control operators.
p290
aVWe present a series of CPSbased intermediate languages suitable for functional language compilation, arguing that they have practical benefits over directstyle languages based on Anormal form (ANF) or monads. Inlining of functions demonstrates the benefits most clearly: in ANFbased languages, inlining involves a renormalization step that rearranges let expressions and possibly introduces a new 'join point' function, and in monadic languages, commuting conversions must be applied; in contrast, inlining in our CPS language is a simple substitution of variables for variables. We present a contification transformation implemented by simple rewrites on the intermediate language. Exceptions are modelled using socalled 'doublebarrelled' CPS. Subtyping on exception constructors then gives a very straightforward effect analysis for exceptions. We also show how a graphbased representation of CPS terms can be implemented extremely efficiently, with lineartime term simplification.
p291
aVFinger Trees (Hinze & Paterson, 2006) are a general purpose persistent data structure with good performance. Their genericity permits developing a wealth of structures like ordered sequences or interval trees on top of a single implementation. However, the type systems used by current functional languages do not guarantee the coherent parameterization and specialization of Finger Trees, let alone the correctness of their implementation. We present a certified implementation of Finger Trees solving these problems using the Program extension of Coq. We not only implement the structure but also prove its invariants along the way, which permit building certified structures on top of Finger Trees in an elegant way.
p292
aVAs a means of transmitting not only data but also code encapsulated within functions, higherorder channels provide an advanced form of task parallelism in parallel computations. In the presence of mutable references, however, they pose a safety problem because references may be transmitted to remote threads where they are no longer valid. This paper presents an MLlike parallel language with typesafe higherorder channels. By type safety, we mean that no value written to a channel contains references, or equivalently, that no reference escapes via a channel from the thread where it is created. The type system uses a typing judgment that is capable of deciding whether the value to which a term evaluates contains references or not. The use of such a typing judgment also makes it easy to achieve another desirable feature of channels, channel locality, that associates every channel with a unique thread for serving all values addressed to it. Our type system permits mutable references in sequential computations and also ensures that mutable references never interfere with parallel computations. Thus it provides both flexibility in sequential programming and ease of implementing parallel computations.
p293
aVThe paper investigates the impact of high level distributed programming language constructs on the engineering of realistic software components. Based on reengineering two nontrivial telecoms components, we compare two highlevel distributed functional languages, Erlang and GdH, with conventional distributed technologies C++/CORBA and C++/UDP. We investigate several aspects of highlevel distributed languages including the impact on code size of highlevel constructs. We identify three language constructs that primarily contribute to the reduction in application size and quantify their impact. We provide the first evidence based on analysis of a substantial system to support the widelyheld supposition that highlevel constructs reduce programming effort associated with specifying distributed coordination. We investigate whether a language with sophisticated highlevel fault tolerance can produce suitably robust components, and both measure and analyse the additional programming effort needed to introduce robustness. Finally, we investigate some implications of a range of type systems for engineering distributed software.
p294
aVWe report on our experience using functional programming languages in the development of a commercial GNU/Linux distribution, discussing features of several significant systems: hardware detection and system configuration; OS installer CD creation; package compilation and management. Static typing helps compensate for the lack of a complete testing lab and helps us be effective with a very small team. Most importantly, we believe that going beyond merely using functional languages to using purely functional designs really helps to create simple, effective tools.
p295
aVChurch's system of simple types has proven to be remarkably robust: callbyname, callbyneed, and callbyvalue languages, with or without effects, and even logical frameworks can be based on the same typing rules. When type systems become more expressive, this unity fractures. An early example is the value restriction for parametric polymorphism which is necessary for ML but not Haskell; a later manifestation is the lack of distributivity of function types over intersections in callbyvalue languages with effects. In this talk we reexamine the logical justification for systems of subtyping and intersection types and then explore the consequences in two different settings: logical frameworks and functional programming. In logical frameworks functions are pure and their definitions observable, but complications could arise from the presence of dependent types. We show that this is not the case, and that we can obtain soundness and completeness theorems for a certain axiomatization of subtyping. We also sketch a connection to the typetheoretic notion of proof irrelevance. In functional programming we investigate how the encapsulation of effects in monads interacts with subtyping and intersection types, providing an updated analysis of the value restriction and related phenomena. While at present this study is far from complete, we believe that its origin in purely logical notions will give rise to a uniform theory that can easily be adapted to specific languages and their operational interpretations.
p296
aVThis paper summarizes experiences from an open source project that builds a free Haskell IDE based on Eclipse (an open source IDE platform). Eclipse is extensible and has proved to be a good basis for IDEs for several programming languages. Difficulties arise mainly because it is written in Java and requires extensions to be written in Java. This made it hard to reuse existing development tools implemented in Haskell, and turned out to be a considerable obstacle to finding contributors. Several approaches to resolve these issues are described and their advantages and disadvantages discussed.
p297
aVMashMaker is a webbased tool that makes it easy for a normal user to create web mashups by browsing around, without needing to type, or plan in advance what they want to do. Like a web browser, Mashmaker allows users to create mashups by browsing, rather than writing code, and allows users to bookmark interesting things they find, forming new widgets  reusable mashup fragments. Like a spreadsheet, MashMaker mixes program and data and allows adhoc unstructured editing of programs. MashMaker is also a modern functional programming language with nonside effecting expressions, higher order functions, and lazy evaluation. MashMaker programs can be manipulated either textually, or through an interactive tree representation, in which a program is presented together with the values it produces. In order to cope with this unusual domain, MashMaker contains a number of deviations from normal function languages. The most notable of these is that, in order to allow the programmer to write programs directly on their data, all data is stored in a single tree, and evaluation of an expression always takes place at a specific point in this tree, which also functions as its scope.
p298
aVSharing analysis and uniqueness typing are static analyses that aim at determining which of a program's objects are to be used at most once. There are many commonalities between these two forms of usage analysis. We make their connection precise by developing an expressive generic analysis that can be instantiated to both sharing analysis and uniqueness typing. The resulting system, which combines parametric polymorphism with effect subsumption, is specified within the general framework of qualified types, so that readily available tools and techniques can be used for the development of implementations and metatheory.
p299
aVIn this paper we present our use of functional programming (FP), specifically Haskell, to provide an operational semantics for a domainspecific language, CellML, that describes mathematical models of biological processes. We analyse the benefits and shortcomings of this approach, in comparison with other semantic definitions for CellML. It is our claim that using FP for our semantics results in a more concise and useful artifact for describing what such a model means. The use of lazy evaluation removes the need to explicitly determine an evaluation order for the model, resulting in a more elegant interpreter. Crucially, using FP enables us to prove the correctness of optimisation techniques for such models. This gives us more confidence in scientific deductions from simulation results. We compare the Python implementation of these optimisation techniques with our use of Haskell in proving their correctness.
p300
aVIn this paper we present an automated way of using spare CPU resources within a shared memory multiprocessor or multicore machine. Our approach is (i) to profile the execution of a program, (ii) from this to identify pieces of work which are promising sources of parallelism, (iii) recompile the program with this work being performed speculatively via a workstealing system and then (iv) to detect at runtime any attempt to perform operations that would reveal the presence of speculation. We assess the practicality of the approach through an implementation based on GHC 6.6 along with a limit study based on the execution profiles we gathered. We support the full Concurrent Haskell language compiled with traditional optimizations and including I/O operations and synchronization as well as pure computation. We use 20 of the larger programs from the 'nofib' benchmark suite. The limit study shows that programs vary a lot in the parallelism we can identify: some have none, 16 have a potential 2x speedup, 4 have 32x. In practice, on a 4core processor, we get 1080% speedups on 7 programs. This is mainly achieved at the addition of a second core rather than beyond this. This approach is therefore not a replacement for manual parallelization, but rather a way of squeezing extra performance out of the threads of an alreadyparallel program or out of a program that has not yet been parallelized.
p301
aVDesigning debugging tools for lazy functional programming languages is a complex task which is often solved by expensive tracing of lazy computations. We present a new approach in which the information collected as a trace is reduced considerably (kilobytes instead of megabytes). The idea is to collect a kind of step information for a callbyvalue interpreter, which can then efficiently reconstruct the computation for debugging/viewing tools, like declarative debugging. We show the correctness of the approach, discuss a proofofconcept implementation with a declarative debugger as back end and present some benchmarks comparing our new approach with the Haskell debugger Hat.
p302
aVCRules is a business rules management system developed by Constraint Technologies International1 (CTI), designed for use in transportation problems. Users define rules describing various aspects of a problem, such as solution costs and legality, which are then queried from a host application, typically an optimising solver. At its core, CRules provides a functional expression language which affords users both power and flexibility when formulating rules. In this paper we will describe our experiences of using functional programming both at the enduser level, as well as at the implementation level. We highlight some of the benefits we, and the product's users, have enjoyed from the decision to base our rule system on features such as: higherorder functions, referential transparency, and static, polymorphic typing. We also outline some of our experiences in using Haskell to build an efficient compiler for the core language.
p303
aVIn the light of evidence that Haskell programs compiled by GHC exhibit large numbers of mispredicted branches on modern processors, we reexamine the "tagless" aspect of the STGmachine that GHC uses as its evaluation model. We propose two tagging strategies: a simple strategy called semitagging that seeks to avoid one common source of unpredictable indirect jumps, and a more complex strategy called dynamic pointertagging that uses the spare low bits in a pointer to encode information about the pointedto object. Both of these strategies have been implemented and exhaustively measured in the context of a production compiler, GHC, and the paper contains detailed descriptions of the implementations. Our measurements demonstrate significant performance improvements (14% for dynamic pointertagging with only a 2% increase in code size), and we further demonstrate that much of the improvement can be attributed to the elimination of mispredicted branch instructions. As part of our investigations we also discovered that one optimisation in the STGmachine, vectoredreturns, is no longer worthwhile and we explain why.
p304
aVThere has been much work in recent years on extending ML with recursive modules. One of the most difficult problems in the development of such an extension is the double vision problem, which concerns the interaction of recursion and data abstraction. In previous work, I defined a type system called RTG, which solves the double vision problem at the level of a SystemFstyle core calculus. In this paper, I scale the ideas and techniques of RTG to the level of a recursive MLstyle module calculus called RMC, thus establishing that no tradeoff between data abstraction and recursive modules is necessary. First, I describe RMC's typing rules for recursive modules informally and discuss some of the design questions that arose in developing them. Then, I present the formal semantics of RMC, which is interesting in its own right. The formalization synthesizes aspects of both the Definition and the HarperStone interpretation of Standard ML, and includes a novel twopass algorithm for recursive module typechecking in which the coherence of the two passes is emphasized by their representation in terms of the same set of inference rules.
p305
aVThe forthcoming Revised6 Report on Scheme differs from previous reports in that the language it describes is structured as a set of libraries. It also provides a syntax for defining new portable libraries. The same library may export both procedure and hygienic macro definitions, which allows procedures and syntax to be freely intermixed, hidden, and exported. This paper describes the design and implementation of a portable version of R6RS libraries that expands libraries into a core language compatible with existing R5RS implementations. Our implementation is characterized by its use of inference to determine when the bindings of an imported library are needed, e.g., run time or compile time, relieving programmers of the burden of declaring usage requirements explicitly.
p306
aVThis paper presents an automatic deforestation system, stream fusion, based on equational transformations, that fuses a wider range of functions than existing shortcut fusion systems. In particular, stream fusion is able to fuse zips, left folds and functions over nested lists, including list comprehensions. A distinguishing feature of the framework is its simplicity: by transforming list functions to expose their structure, intermediate values are eliminated by general purpose compiler optimisations. We have reimplemented the Haskell standard List library on top of our framework, providing stream fusion for Haskell lists. By allowing a wider range of functions to fuse, we see an increase in the number of occurrences of fusion in typical Haskell programs. We present benchmarks documenting time and space improvements.
p307
aVPattern matching of algebraic data types (ADTs) is a standard feature in typed functional programming languages, but it is well known that it interacts poorly with abstraction. While several partial solutions to this problem have been proposed, few have been implemented or used. This paper describes an extension to the .NET language F# called active patterns, which supports pattern matching over abstract representations of generic heterogeneous data such as XML and term structures, including where these are represented via object models in other .NET languages. Our design is the first to incorporate both ad hoc pattern matching functions for partial decompositions and "views" for total decompositions, and yet remains a simple and lightweight extension. We give a description of the language extension along with numerous motivating examples. Finally we describe how this feature would interact with other reasonable and related language extensions: existential types quantified at data discrimination tags, GADTs, and monadic generalizations of pattern matching.
p308
aVOver forty years ago, David Barron and Christopher Strachey published a startlingly elegant program for the Cartesian product of a list of lists, expressing it with a three nested occurrences of the function we now call foldr. This program is remarkable for its time because of its masterful display of higherorder functions and lexical scope, and we put it forward as possibly the first ever functional pearl. We first characterize it as the result of a sequence of program transformations, and then apply similar transformations to a program for the classical power set example. We also show that using a higherorder representation of lists allows a definition of the Cartesian product function where foldr is nested only twice.
p309
aVBidirectional transformation is a pair of transformations: a view function and a backward transformation. A view function maps one data structure called source onto another called view. The corresponding backward transformation reflects changes in the view to the source. Its practically useful applications include replicated data synchronization, presentationoriented editor development, tracing software development, and view updating in the database community. However, developing a bidirectional transformation is hard, because one has to give two mappings that satisfy the bidirectional properties for system consistency. In this paper, we propose a new framework for bidirectionalization that can automatically generate a useful backward transformation from a view function while guaranteeing that the two transformations satisfy the bidirectional properties. Our framework is based on two known approaches to bidirectionalization, namely the constant complement approach from the database community and the combinator approach from the programming language community, but it has three new features: (1) unlike the constant complement approach, it can deal with transformations between algebraic data structures rather than just tables; (2) unlike the combinator approach, in which primitive bidirectional transformations have to be explicitly given, it can derive them automatically; (3) it generates a view update checker to validate updates on views, which has not been well addressed so far. The new framework has been implemented and the experimental results show that our framework has promise.
p310
aVWe present a userfriendly approach to unifying program creation and execution, based on a notion of "tangible values" (TVs), which are visual and interactive manifestations of pure values, including functions. Programming happens by gestural composition of TVs. Our goal is to give endusers the ability to create parameterized, composable content without imposing the usual abstract and linguistic working style of programmers. We hope that such a system will put the essence of programming into the hands of many more people, and in particular people with artistic/visual creative style. In realizing this vision, we develop algebras for visual presentation and for "deep" function application, where function and argument may both be nested within a structure of tuples, functions, etc. Composition gestures are translated into chains of combinators that act simultaneously on statically typed values and their visualizations.
p311
aVThe analysis and verification of higherorder programs raises the issue of controlflow analysis for higherorder languages. The problem of constructing an accurate call graph for a higherorder program has been the topic of extensive research, and numerous methods for flow analysis, varying in complexity and precision, have been suggested. While termination analysis of higherorder programs has been studied, there has been little examination of the impact of call graph construction on the precision of termination checking. We examine the effect of various controlflow analysis techniques on a termination analysis for higherorder functional programs. We present a termination checking framework and instantiate this with three call graph constructions varying in precision and complexity, and illustrate by example the impact of the choice of call graph construction. Our second aim is to use the resulting analyses to shed light on the relationship between controlflow analyses. We prove precise inclusions between the classes of programs recognised as terminating by the same termination criterion over different call graph analyses, giving one of the first characterisations of expressive power of flow analyses for higherorder programs.
p312
aVWe analyze the computational complexity of kCFA, a hierarchy of control flow analyses that determine which functions may be applied at a given callsite. This hierarchy specifies related decision problems, quite apart from any algorithms that may implement their solutions. We identify a simple decision problem answered by this analysis and prove that in the 0CFA case, the problem is complete for polynomial time. The proof is based on a nonstandard, symmetric implementation of Boolean logic within multiplicative linear logic (MLL). We also identify a simpler version of 0CFA related to \u03b7expansion, and prove that it is complete for logarithmic space, using arguments based on computing paths and permutations. For any fixed k>0, it is known that kCFA (and the analogous decision problem) can be computed in time exponential in the program size. For k=1, we show that the decision problem is NPhard, and sketch why this remains true for larger fixed values of k. The proof technique depends on using the approximation of CFA as an essentially nondeterministic computing mechanism, as distinct from the exactness of normalization. When k=n, so that the "depth" of the control flow analysis grows linearly in the program length, we show that the decision problem is complete for exponential time. In addition, we sketch how the analysis presented here may be extended naturally to languages with control operators. All of the insights presented give clear examples of how straightforward observations about linearity, and linear logic, may in turn be used to give a greater understanding of functional programming and program analysis.
p313
aVThe distinction between lazy and eager (or strict) evaluation has been studied in programming languages since Algol 60's call by name, as a way to avoid unnecessary work and to deal gracefully with infinite structures such as streams. It is deeply integrated in some languages, notably Haskell, and can be simulated in many languages by wrapping a lazy expression in a lambda. Less well studied is the role of laziness, and its opposite, speculation, in computer systems, both hardware and software. A wide range of techniques can be understood as applications of these two ideas. Laziness is the idea behind: Redo logging for maintaining persistent state and replicated state machines: the log represents the current state, but it is evaluated only after a failure or to bring a replica online. Copyonwrite schemes for maintaining multiple versions of a large, slowly changing state, usually in a database or file system. Write buffers and writeback caches in memory and file systems, which are lazy about updating the main store. Relaxed memory models and eventual consistency replication schemes (which require weakening the spec). Clipping regions and expose events in graphics and window systems. Carrysave adders, which defer propagating carries until a clean result is needed. "Infinity" and "Not a number" results of floating point operations. Futures (in programming) and out of order execution (in CPUs), which launch a computation but are lazy about consuming the result. Dataflow is a generalization. "Formatting operators" in text editors, which apply properties such as "italic" to large regions of text by attaching a sequence of functions that compute the properties; the functions are not evaluated until the text needs to be displayed. Stream processing in database queries, Unix pipes, etc., which conceptually applies operators to unbounded sequences of data, but rearranges the computation when possible to apply a sequence of operators to each data item in turn. Speculation is the idea behind: Optimistic concurrency control in databases, and more recently in transactional memory Prefetching in memory and file systems. Branch prediction, and speculative execution in general in modern CPUs. Data speculation, which works especially well when the data is cached but might be updated by a concurrent process. This is a form of optimistic concurrency control. Exponential backoff schemes for scheduling a resource, most notably in LANs such as WiFi or classical Ethernet. All forms of caching, which speculate that it's worth filling up some memory with data in the hope that it will be used again. In both cases it is usual to insist that the laziness or speculation is strictly a matter of scheduling that doesn't affect the result of a computation but only improves the performance. Sometimes, however, the spec is weakened, for example in eventual consistency. I will discuss many of these examples in detail and examine what they have in common, how they differ, and what factors govern the effectiveness of laziness and speculation in computer systems.
p314
aVXML database query languages have been studied extensively, but XML database updates have received relatively little attention, and pose many challenges to language design. We are developing an XML update language called FLUX, which stands for FunctionaL Updates for XML, drawing upon ideas from functional programming languages. In prior work, we have introduced a core language for FLUX with a clear operational semantics and a sound, decidable static type system based on regular expression types. Our initial proposal had several limitations. First, it lacked support for recursive types or update procedures. Second, although a highlevel source language can easily be translated to the core language, it is difficult to propagate meaningful type errors from the core language back to the source. Third, certain updates are wellformed yet contain path errors, or "dead" subexpressions which never do any useful work. It would be useful to detect path errors, since they often represent errors or optimization opportunities. In this paper, we address all three limitations. Specifically, we present an improved, sound type system that handles recursion. We also formalize a source update language and give a translation to the core language that preserves and reflects typability. We also develop a patherror analysis (a form of deadcode analysis) for updates.
p315
aVXML transformations are very sensitive to types: XML types describe the tags and attributes of XML elements as well as the number, kind, and order of their subelements. Therefore, operations, even simple ones, that modify these features may affect the types of documents. Operations on XML documents are performed by iterators that, to be useful, need to be typed by a kind of polymorphism that goes beyond what currently exists. For this reason these iterators are not programmed but, rather, hardcoded in the languages. However, this approach soon reaches its limits, as the hardcoded iterators cannot cover fairly standard usage scenarios. As a solution to this problem we propose a generic language to define iterators for XML data. This language can either be used as a compilation target (e.g., for XPATH) or it can be grafted on any statically typed host programming language (as long as this has product types) to endow it with XML processing capabilities. We show that our language mostly offers the required degree of polymorphism, study its formal properties, and show its expressiveness and practical impact by providing several usage examples and encodings.
p316
aVThis paper presents AURA, a programming language for access control that treats ordinary programming constructs (e.g., integers and recursive functions) and authorization logic constructs (e.g., principals and access control policies) in a uniform way. AURA is based on polymorphic DCC and uses dependent types to permit assertions that refer directly to AURA values while keeping computation out of the assertion level to ensure tractability. The main technical results of this paper include fully mechanically verified proofs of the decidability and soundness for AURA's type system, and a prototype typechecker and interpreter.
p317
aVThis paper exhibits the power of programming with dependent types by dint of embedding three domainspecific languages: Cryptol, a language for cryptographic protocols; a small data description language; and relational algebra. Each example demonstrates particular design patterns inherent to dependentlytyped programming. Documenting these techniques paves the way for further research in domainspecific embedded type systems.
p318
aVWe report on an extension of Haskell with open typelevel functions and equality constraints that unifies earlier work on GADTs, functional dependencies, and associated types. The contribution of the paper is that we identify and characterise the key technical challenge of entailment checking; and we give a novel, decidable, sound, and complete algorithm to solve it, together with some practicallyimportant variants. Our system is implemented in GHC, and is already in active use.
p319
aVMLF is a type system that seamlessly merges MLstyle type inference with SystemF polymorphism. We propose a system of graphic (type) constraints that can be used to perform type inference in both ML or MLF. We show that this constraint system is a small extension of the formalism of graphic types, originally introduced to represent MLF types. We give a few semantic preserving transformations on constraints and propose a strategy for applying them to solve constraints. We show that the resulting algorithm has optimal complexity for MLF type inference, and argue that, as for ML, this complexity is linear under reasonable assumptions.
p320
aVThere has been a lot of interest of late for programming languages that incorporate features from dependent type systems and proof assistants, in order to capture important invariants of the program in the types. This allows typebased program verification and is a promising compromise between plain old types and full blown Hoare logic proofs. The introduction of GADTs in GHC (and more recently type families) made such dependent typing available in an industryquality implementation, making it possible to consider its use in large scale programs. We have undertaken the construction of a complete compiler for System F, whose main property is that the GHC type checker verifies mechanically that each phase of the compiler properly preserves types. Our particular focus is on "types rather than proofs": reasonably few annotations that do not overwhelm the actual code. We believe it should be possible to write such a typepreserving compiler with an amount of extra code comparable to what is necessary for typical typed intermediate languages, but with the advantage of static checking. We will show in this paper the remaining hurdles to reach this goal.
p321
aVThis paper describes our experience using a functional language, Haskell, to build an embedded, domainspecific language (DSL) for component configuration in largescale, realtime, embedded systems. Prior to the introduction of the DSL, engineers would describe the steps needed to configure a particular system in a handwritten XML document. In this paper, we outline the application domain, give a brief overview of the DSL that we developed, and provide concrete data to demonstrate its effectiveness. In particular, we show that the DSL has several significant benefits over the original, XMLbased approach including reduced code size, increased modularity and scalability, and detection and prevention of common defects. For example, using the DSL, we were able to produce clear and intuitive descriptions of component configurations that were sometimes less than 1/30 of the size of the original XML.
p322
aVWe introduce the notion of discrimination as a generalization of both sorting and partitioning and show that worstcase lineartime discrimination functions (discriminators) can be defined generically, by (co)induction on an expressive language of order denotations. The generic definition yields discriminators that generalize both distributive sorting and multiset discrimination. The generic discriminator can be coded compactly using list comprehensions, with order denotations specified using Generalized Algebraic Data Types (GADTs). A GADTfree combinator formulation of discriminators is also given. We give some examples of the uses of discriminators, including a new mostsignificantdigit lexicographic sorting algorithm. Discriminators generalize binary comparison functions: They operate on n arguments at a time, but do not expose more information than the underlying equivalence, respectively ordering relation on the arguments. We argue that primitive types with equality (such as references in ML) and ordered types (such as the machine integer type), should expose their equality, respectively standard ordering relation, as discriminators: Having only a binary equality test on a type requires \u0398(n2) time to find all the occurrences of an element in a list of length n, for each element in the list, even if the equality test takes only constant time. A discriminator accomplishes this in linear time. Likewise, having only a (constanttime) comparison function requires \u0398(n log n) time to sort a list of n elements. A discriminator can do this in linear time.
p323
aVTransactional events (TE) are an approach to concurrent programming that enriches the firstclass synchronous messagepassing of Concurrent ML (CML) with a combinator that allows multiple messages to be passed as part of one allornothing synchronization. Donnelly and Fluet (2006) designed and implemented TE as a Haskell library and demonstrated that it enables elegant solutions to programming patterns that are awkward or impossible in CML. However, both the definition and the implementation of TE relied fundamentally on the code in a synchronization not using mutable memory, an unreasonable assumption for mostly functional languages like ML where functional interfaces may have impure implementations. We present a definition and implementation of TE that supports MLstyle references and nested synchronizations, both of which were previously unnecessary due to Haskell's more restrictive type system. As in prior work, we have a highlevel semantics that makes nondeterministic choices such that synchronizations succeed whenever possible and a lowlevel semantics that uses search to implement the highlevel semantics soundly and completely. The key design tradeoff in the semantics is to allow updates to mutable memory without requiring the implementation to consider all possible thread interleavings. Our solution uses firstclass heaps and allows interleavings only when a message is sent or received. We have used Coq to prove the high and lowlevel semantics equivalent. We have implemented our approach by modifying the Objective Caml runtime system. By modifying the runtime system, rather than relying solely on a library, we can eliminate the potential for nonterminating computations within unsuccessful synchronizations to run forever.
p324
aVWe investigated the relative merits of C++ and Erlang in the implementation of a parallel acoustic ray tracing algorithm for the U.S. Navy. We found a much smaller learning curve and better debugging environment for parallel Erlang than for pthreadsbased C++ programming. Our C++ implementation outperformed the Erlang program by at least 12x. Attempts to use Erlang on the IBM Cell BE microprocessor were frustrated by Erlang's memory footprint.
p325
aVThe increasing availability of commodity multicore processors is making parallel computing available to the masses. Traditional parallel languages are largely intended for largescale scientific computing and tend not to be wellsuited to programming the applications one typically finds on a desktop system. Thus we need new parallellanguage designs that address a broader spectrum of applications. In this paper, we present Manticore, a language for building parallel applications on commodity multicore hardware including a diverse collection of parallel constructs for different granularities of work. We focus on the implicitlythreaded parallel constructs in our highlevel functional language. We concentrate on those elements that distinguish our design from related ones, namely, a novel parallel binding form, a nondeterministic parallel case form, and exceptions in the presence of data parallelism. These features differentiate the present work from related work on functional data parallel language designs, which has focused largely on parallel problems with regular structure and the compiler transformations   most notably, flattening   that make such designs feasible. We describe our implementation strategies and present some detailed examples utilizing various mechanisms of our language.
p326
aVThis document illustrates how functional implementations of formal semantics (structural operational semantics, reduction semantics, smallstep and bigstep abstract machines, natural semantics, and denotational semantics) can be transformed into each other. These transformations were foreshadowed by Reynolds in "Definitional Interpreters for HigherOrder Programming Languages" for functional implementations of denotational semantics, natural semantics, and bigstep abstract machines using closure conversion, CPS transformation, and defunctionalization. Over the last few years, the author and his students have further observed that functional implementations of smallstep and of bigstep abstract machines are related using fusion by fixedpoint promotion and that functional implementations of reduction semantics and of smallstep abstract machines are related using refocusing and transition compression. It furthermore appears that functional implementations of structural operational semantics and of reduction semantics are related as well, also using CPS transformation and defunctionalization. This further relation provides an element of answer to Felleisen's conjecture that any structural operational semantics can be expressed as a reduction semantics: for deterministic languages, a reduction semantics is a structural operational semantics in continuation style, where the reduction context is a defunctionalized continuation. As the defunctionalized counterpart of the continuation of a onestep reduction function, a reduction context represents the rest of the reduction, just as an evaluation context represents the rest of the evaluation since it is the defunctionalized counterpart of the continuation of an evaluation function.
p327
aVWe present parametric higherorder abstract syntax (PHOAS), a new approach to formalizing the syntax of programming languages in computer proof assistants based on type theory. Like higherorder abstract syntax (HOAS), PHOAS uses the meta language's binding constructs to represent the object language's binding constructs. Unlike HOAS, PHOAS types are definable in generalpurpose type theories that support traditional functional programming, like Coq's Calculus of Inductive Constructions. We walk through how Coq can be used to develop certified, executable program transformations over several staticallytyped functional programming languages formalized with PHOAS; that is, each transformation has a machinechecked proof of type preservation and semantic preservation. Our examples include CPS translation and closure conversion for simplytyped lambda calculus, CPS translation for System F, and translation from a language with MLstyle pattern matching to a simpler language with no variablearity binding constructs. By avoiding the syntactic hassle associated with firstorder representation techniques, we achieve a very high degree of proof automation.
p328
aVLanguagebased security relies on the assumption that all potential attacks are bound by the rules of the language in question. When programs are compiled into a different language, this is true only if the translation process preserves observational equivalence. We investigate the problem of fully abstract compilation, i.e., compilation that both preserves and reflects observational equivalence. In particular, we prove that typed closure conversion for the polymorphic calculus with existential and recursive types is fully abstract. Our proof uses operational techniques in the form of a stepindexed logical relation and construction of certain wrapper terms that "backtranslate" from target values to source values. Although typed closure conversion has been assumed to be fully abstract, we are not aware of any previous result that actually proves this.
p329
aVOptimal path queries are queries to obtain an optimal path specified by a given criterion of optimality. There have been many studies to give efficient algorithms for classes of optimal path problem. In this paper, we propose a generic framework for optimal path queries. We offer a domainspecific language to describe optimal path queries, together with an algorithm to find an optimal path specified in our language. One of the most distinct features of our framework is the use of recursive functions to specify queries. Recursive functions reinforce expressiveness of our language so that we can describe many problems including known ones; thus, we need not learn existing results. Moreover, we can derive an efficient querying algorithm from the description of a query written in recursive functions. Our algorithm is a generalization of existing algorithms, and answers a query in O(n log n) time on a graph of O(n) size. We also explain our implementation of an optimal path querying system, and report some experimental results.
p330
aVThe Revised6 Report on Scheme requires its generic equivalence predicate, equal?, to terminate even on cyclic inputs. While the terminating equal? can be implemented via a DFAequivalence or unionfind algorithm, these algorithms usually require an additional pointer to be stored in each object, are not suitable for multithreaded code due to their destructive nature, and may be unacceptably slow for the small acyclic values that are the most likely inputs to the predicate. This paper presents a variant of the unionfind algorithm for equal? that addresses these issues. It performs well on large and small, cyclic and acyclic inputs by interleaving a lowoverhead algorithm that terminates only for acyclic inputs with a more general algorithm that handles cyclic inputs. The algorithm terminates for all inputs while never being more than a small factor slower than whichever of the acyclic or unionfind algorithms would have been faster. Several intermediate algorithms are also presented, each of which might be suitable for use in a particular application, though only the final algorithm is suitable for use in a library procedure, like equal?, that must work acceptably well for all inputs.
p331
aVStreams, infinite sequences of elements, live in a coworld: they are given by a coinductive data type, operations on streams are implemented by corecursive programs, and proofs are conducted using coinduction. But there is more to it: suitably restricted, stream equations possess unique solutions, a fact that is not very widely appreciated. We show that this property gives rise to a simple and attractive proof technique essentially bringing equational reasoning to the coworld. In fact, we redevelop the theory of recurrences, finite calculus and generating functions using streams and stream operators building on the cornerstone of unique solutions. The development is constructive: streams and stream operators are implemented in Haskell, usually by oneliners. The resulting calculus or library, if you wish, is elegant and fun to use. Finally, we rephrase the proof of uniqueness using generalised algebraic data types.
p332
aVWe propose a novel notion of dataflow coverage for testing declarative programs. Moreover, we extend an automatic testcase generator such that it can achieve dataflow coverage. The coverage information is obtained by instrumenting a program such that it collects coverage information during its execution. Finally, we show the benefits of dataflow based testing for a couple of example applications.
p333
aVReasoning about imperative programs requires the ability to track aliasing and ownership properties. We present a type system that provides this ability, by using regions, capabilities, and singleton types. It is designed for a highlevel calculus with higherorder functions, algebraic data structures, and references (mutable memory cells). The type system has polymorphism, yet does not require a value restriction, because capabilities act as explicit store typings. We exhibit a typedirected, typepreserving, and meaningpreserving translation of this imperative calculus into a pure calculus. Like the monadic translation, this is a storepassing translation. Here, however, the store is partitioned into multiple fragments, which are threaded through a computation only if they are relevant to it. Furthermore, the decomposition of the store into fragments can evolve dynamically to reflect ownership transfers. The translation offers deep insight about the inner workings and soundness of the type system. If coupled with a semantic model of its target calculus, it leads to a semantic model of its imperative source calculus. Furthermore, it provides a foundation for our longterm objective of designing a system for specifying and certifying imperative programs with dynamic memory allocation.
p334
aVWe have implemented a twostage language, Paradise, for building reusable components which are used to price financial products. Paradise is embedded in Haskell and makes heavy use of typeclass based overloading, allowing the second stage to be compiled into a variety of backend platforms. Paradise has enabled us to begin moving away from implementation directly in monolithic Excel spreadsheets and towards a more modular and retargetable approach.
p335
aVWe describe an axiomatic extension to the Coq proof assistant, that supports writing, reasoning about, and extracting higherorder, dependentlytyped programs with sideeffects. Coq already includes a powerful functional language that supports dependent types, but that language is limited to pure, total functions. The key contribution of our extension, which we call Ynot, is the added support for computations that may have effects such as nontermination, accessing a mutable store, and throwing/catching exceptions. The axioms of Ynot form a small trusted computing base which has been formally justified in our previous work on Hoare Type Theory (HTT). We show how these axioms can be combined with the powerful type and abstraction mechanisms of Coq to build higherlevel reasoning mechanisms which in turn can be used to build realistic, verified software components. To substantiate this claim, we describe here a representative series of modules that implement imperative finite maps, including support for a higherorder (effectful) iterator. The implementations range from simple (e.g., association lists) to complex (e.g., hash tables) but share a common interface which abstracts the implementation details and ensures that the modules properly implement the finite map abstraction.
p336
aVThe trend in microprocessor design toward multicore and manycore processors means that future performance gains in software will largely come from harnessing parallelism. To realize such gains, we need languages and implementations that can enable parallelism at many different levels. For example, an application might use both explicit threads to implement coursegrain parallelism for independent tasks and implicit threads for finegrain dataparallel computation over a large array. An important aspect of this requirement is supporting a wide range of different scheduling mechanisms for parallel computation. In this paper, we describe the scheduling framework that we have designed and implemented for Manticore, a strict parallel functional language. We take a microkernel approach in our design: the compiler and runtime support a small collection of scheduling primitives upon which complex scheduling policies can be implemented. This framework is extremely flexible and can support a wide range of different scheduling policies. It also supports the nesting of schedulers, which is key to both supporting multiple scheduling policies in the same application and to hierarchies of speculative parallel computations. In addition to describing our framework, we also illustrate its expressiveness with several popular scheduling techniques. We present a (mostly) modular approach to extending our schedulers to support cancellation. This mechanism is essential for implementing eager and speculative parallelism. We finally evaluate our framework with a series of benchmarks and an analysis.
p337
aVThis paper presents a semantic space profiler for parallel functional programs. Building on previous work in sequential profiling, our tools help programmers to relate runtime resource use back to program source code. Unlike many profiling tools, our profiler is based on a cost semantics. This provides a means to reason about performance without requiring a detailed understanding of the compiler or runtime system. It also provides a specification for language implementers. This is critical in that it enables us to separate cleanly the performance of the application from that of the language implementation. Some aspects of the implementation can have significant effects on performance. Our cost semantics enables programmers to understand the impact of different scheduling policies while hiding many of the details of their implementations. We show applications where the choice of scheduling policy has asymptotic effects on space use. We explain these use patterns through a demonstration of our tools. We also validate our methodology by observing similar performance in our implementation of a parallel extension of Standard ML.
p338
aVWith features that include lightweight syntax, expressive type systems, and deep semantic foundations, functional languages are now being used to develop an increasingly broad range of complex, realworld applications. In the area of systems software, however, where performance and interaction with lowlevel aspects of hardware are central concerns, many practitioners still eschew the advantages of higherlevel languages for the potentially unsafe but predictable behavior of traditional imperative languages like C. It is ironic that critical applications such as operating systems kernels, device drivers, and VMMs  where a single bug could compromise the reliability or security of a whole system  are among the least likely to benefit from the abstractions and safety guarantees of modern language designs. Over the last few years, our group has been investigating the potential for using Haskell to develop realistic operating systems that can boot and run on bare metal. The House system, developed primarily by Thomas Hallgren and Andrew Tolmach, demonstrates that it is indeed possible to construct systems software in a functional language. But House still relies on a layer of runtime support primitives  some written using unsafe Haskell primitives and others written in C  to provide services ranging from garbage collection to control of the page table structures used by the hardware memory management unit. We would like to replace as much of this layer as possible with code written in a functional language without compromising on type or memory safety. Our experiences with House have led us to believe that a new functional language is required to reflect the needs of the systems domain more directly. Interestingly, however, we have concluded that this does not require fundamental new language design. In this invited talk, I will give an update on the current status of our project and I will describe how we are leveraging familiar components of the Haskell type system  including polymorphism, kinds, qualified types and improvement  to capture more precise details of effect usage, data representation, and termination. I will also discuss the challenges of writing and compiling performancesensitive code written in a functional style. It was once considered radical to use C in place of assembly language to construct systems software. Is it possible that functional languages might one day become as commonplace in this application domain as C is today?
p339
aVIn the context of program verification in an interactive theorem prover, we study the problem of transforming function definitions with MLstyle (possibly overlapping) pattern matching into minimal sets of independent equations. Since independent equations are valid unconditionally, they are better suited for the equational proof style using induction and rewriting, which is often found in proofs in theorem provers or on paper. We relate the problem to the wellknown minimization problem for propositional DNF formulas and show that it is P/2complete. We then develop a concrete algorithm to compute minimal patterns, which naturally generalizes the standard QuineMcCluskey procedure to the domain of term patterns.
p340
aVWe give an exact characterization of the computational complexity of the kCFA hierarchy. For any k > 0, we prove that the control flow decision problem is complete for deterministic exponential time. This theorem validates empirical observations that such control flow analysis is intractable. It also provides more general insight into the complexity of abstract interpretation.
p341
aVHMF is a conservative extension of HindleyMilner type inference with firstclass polymorphism. In contrast to other proposals, HML uses regular System F types and has a simple type inference algorithm that is just a small extension of the usual DamasMilner algorithm W. Given the relative simplicity and expressive power, we feel that HMF can be an attractive type system in practice. There is a reference implementation of the type system available online together with a technical report containing proofs (Leijen 2007a,b).
p342
aVLanguages supporting polymorphism typically have adhoc restrictions on where polymorphic types may occur. Supporting "firstclass" polymorphism, by lifting those restrictions, is obviously desirable, but it is hard to achieve this without sacrificing type inference. We present a new type system for higherrank and impredicative polymorphism that improves on earlier proposals: it is an extension of DamasMilner; it relies only on System F types; it has a simple, declarative specification; it is robust to program transformations; and it enjoys a complete and decidable type inference algorithm.
p343
aVML modules provide hierarchical namespace management, as well as finegrained control over the propagation of type information, but they do not allow modules to be broken up into mutually recursive, separately compilable components. Mixin modules facilitate recursive linking of separately compiled components, but they are not hierarchically composable and typically do not support type abstraction. We synthesize the complementary advantages of these two mechanisms in a novel module system design we call MixML. A MixML module is like an ML structure in which some of the components are specified but not defined. In other words, it unifies the ML structure and signature languages into one. MixML seamlessly integrates hierarchical composition, translucent MLstyle data abstraction, and mixinstyle recursive linking. Moreover, the design of MixML is clean and minimalist; it emphasizes how all the salient, semantically interesting features of the ML module system (as well as several proposed extensions to it) can be understood simply as stylized uses of a small set of orthogonal underlying constructs, with mixin composition playing a central role.
p344
aVSelfadjusting programs respond automatically and efficiently to input changes by tracking the dynamic data dependences of the computation and incrementally updating the output as needed. In order to identify data dependences, previously proposed approaches require the user to make use of a set of monadic primitives. Rewriting an ordinary program into a selfadjusting program with these primitives, however, can be difficult and errorprone due to various monadic and properusage restrictions, some of which cannot be enforced statically. Previous work therefore suggests that selfadjusting computation would benefit from direct language and compiler support. In this paper, we propose a languagebased technique for writing and compiling selfadjusting programs from ordinary programs. To compile selfadjusting programs, we use a continuationpassing style (cps) transformation to automatically infer a conservative approximation of the dynamic data dependences. To prevent the inferred, approximate dependences from degrading the performance of change propagation, we generate memoized versions of cps functions that can reuse previous work even when they are invoked with different continuations. The approach offers a natural programming style that requires minimal changes to existing code, while statically enforcing the invariants required by selfadjusting computation. We validate the feasibility of our proposal by extending Standard ML and by integrating the transformation into MLton, a wholeprogram optimizing compiler for Standard ML. Our experiments indicate that the proposed compilation technique can produce selfadjusting programs whose performance is consistent with the asymptotic bounds and experimental results obtained via manual rewriting (up to a constant factor).
p345
aVSeverely resourceconstrained devices present a confounding challenge to the functional programmer: we are used to having powerful abstraction facilities at our fingertips, but how can we make use of these tools on a device with an 8 or 16bit CPU and at most tens of kilobytes of RAM? Motivated by this challenge, we have developed Flask, a domain specific language embedded in Haskell that brings the power of functional programming to sensor networks, collections of highly resourceconstrained devices. Flask consists of a staging mechanism that cleanly separates nodelevel code from the metalanguage used to generate nodelevel code fragments; syntactic support for embedding standard sensor network code; a restricted subset of Haskell that runs on sensor networks and constrains program space and time consumption; a higherlevel "data stream" combinator library for quickly constructing sensor network programs; and an extensible runtime that provides commonlyused services. We demonstrate Flask through several small code examples as well as a compiler that generates nodelevel code to execute a networkwide query specified in a SQLlike language. We show how using Flask ensures constraints on space and time behavior. Through microbenchmarks and measurements on physical hardware, we demonstrate that Flask produces programs that are efficient in terms of CPU and memory usage and that can run effectively on existing sensor network hardware.
p346
aVBluespec is a hardwaredesign tools startup whose core technology is developed using Haskell. Haskell is an unusual choice for a startup because it adds technical risk to the inherent business risk. In the years since Bluespec's founding, we have discovered that Haskell's purity is an unexpected match for the development needs of a startup. Based on Bluespec's experience, we conclude that pure programming languages can be the source of a competitive advantage for startup software companies.
p347
aVIn efforts to overcome the complexity of the syntax and the lack of formal semantics of conventional hardware description languages, a number of functional hardware description languages have been developed. Like conventional hardware description languages, however, functional hardware description languages eventually convert all source programs into netlists, which describe wire connections in hardware circuits at the lowest level and conceal all highlevel descriptions written into source programs. We develop a variant of the lambda calculus, called l\u03bb (linear lambda), which may serve as a highlevel substitute for netlists. In order to support higherorder functions, l\u03bb uses a linear type system which enforces the linear use of variables of function type. The translation of l\u03bb into structural descriptions of hardware circuits is sound and complete in the sense that it maps expressions only to realizable hardware circuits and that every realizable hardware circuit has a corresponding expression in l\u03bb. To illustrate the use of l\u03bb as a highlevel substitute for netlists, we design a simple hardware description language that extends l\u03bb with polymorphism, and use it to implement a Fast Fourier Transform circuit.
p348
aVExisting package and system configuration management tools suffer from an imperative model, where system administration actions such as upgrading packages or changes to system configuration files are stateful: they destructively update the state of the system. This leads to many problems, such as the inability to roll back changes easily, to run multiple versions of a package sidebyside, to reproduce a configuration deterministically on another machine, or to reliably upgrade a system. In this paper we show that we can overcome these problems by moving to a purely functional system configuration model. This means that all static parts of a system (such as software packages, configuration files and system startup scripts) are built by pure functions and are immutable, stored in a way analogously to a heap in a purely function language. We have implemented this model in NixOS, a nontrivial Linux distribution that uses the Nix package manager to build the entire system configuration from a purely functional specification.
p349
aVScientific visualization is the transformation of data into images. The pipeline model is a widelyused implementation strategy. This term refers not only to linear chains of processing stages, but more generally to demanddriven networks of components. Apparent parallels with functional programming are more than superficial: e.g. some pipelines support streams of data, and a limited form of lazy evaluation. Yet almost all visualization systems are implemented in imperative languages. We challenge this position. Using Haskell, we have reconstructed several fundamental visualization techniques, with encouraging results both in terms of novel insight and performance. In this paper we set the context for our modest rebellion, report some of our results, and reflect on the lessons that we have learned.
p350
aVThere are now a number of BIDIRECTIONAL PROGRAMMING LANGUAGES, where every program can be read both as a forward transformation mapping one data structure to another and as a reverse transformation mapping an edited output back to a correspondingly edited input. Besides parsimony  the two related transformations are described by just one expression  such languages are attractive because they promise strong behavioral laws about how the two transformations fit together  e.g., their composition is the identity function. It has repeatedly been observed, however, that such laws are actually a bit too strong: in practice, we do not want them "on the nose," but only up to some equivalence, allowing inessential details, such as whitespace, to be modified after a round trip. Some bidirectional languages loosen their laws in this way, but only for specific, bakedin equivalences. In this work, we propose a general theory of QUOTIENT LENSES  bidirectional transformations that are well behaved modulo equivalence relations controlled by the programmer. Semantically, quotient lenses are a natural refinement of LENSES, which we have studied in previous work. At the level of syntax, we present a rich set of constructs for programming with CANONIZERS and for quotienting lenses by canonizers. We track equivalences explicitly, with the type of every quotient lens specifying the equivalences it respects. We have implemented quotient lenses as a refinement of the bidirectional string processing language Boomerang. We present a number of useful primitive canonizers for strings, and give a simple extension of Boomerang's regularexpressionbased type system to statically typecheck quotient lenses. The resulting language is an expressive tool for transforming realworld, adhoc data formats. We demonstrate the power of our notation by developing an extended example based on the UniProt genome database format and illustrate the generality of our approach by showing how uses of quotienting in other bidirectional languages can be translated into our notation.
p351
aVArrows are a popular form of abstract computation. Being more general than monads, they are more broadly applicable, and in particular are a good abstraction for signal processing and dataflow computations. Most notably, arrows form the basis for a domain specific language called Yampa, which has been used in a variety of concrete applications, including animation, robotics, sound synthesis, control systems, and graphical user interfaces. Our primary interest is in better understanding the class of abstract computations captured by Yampa. Unfortunately, arrows are not concrete enough to do this with precision. To remedy this situation we introduce the concept of commutative arrows that capture a kind of noninterference property of concurrent computations. We also add an init operator, and identify a crucial law that captures the causal nature of arrow effects. We call the resulting computational model causal commutative arrows. To study this class of computations in more detail, we define an extension to the simply typed lambda calculus called causal commutative arrows (CCA), and study its properties. Our key contribution is the identification of a normal form for CCA called causal commutative normal form (CCNF). By defining a normalization procedure we have developed an optimization strategy that yields dramatic improvements in performance over conventional implementations of arrows. We have implemented this technique in Haskell, and conducted benchmarks that validate the effectiveness of our approach. When combined with stream fusion, the overall methodology can result in speedups of greater than two orders of magnitude.
p352
aVFunctional programming languages ought to play a central role in mathematics education for middle schools (age range: 1014). After all, functional programming is a form of algebra and programming is a creative activity about problem solving. Introducing it into mathematics courses would make prealgebra course come alive. If input and output were invisible, students could implement fun simulations, animations, and even interactive and distributed games all while using nothing more than plain mathematics. We have implemented this vision with a simple framework for purely functional I/O. Using this framework, students design, implement, and test plain mathematical functions over numbers, booleans, string, and images. Then the framework wires them up to devices and performs all the translation from external information to internal data (and vice versa) just like every other operating system. Once middle school students are hooked on this form of programming, our curriculum provides a smooth path for them from prealgebra to freshman courses in college on objectoriented design and theorem proving.
p353
aVThis paper presents our experience using a domainspecific functional language, WaveScript, to build embedded sensing applications used in scientific research. We focus on a recent computervision application for detecting birds in their natural environment. The application was ported from a prototype in C++. In reimplementing the application, we gained a much cleaner factoring of its functionality (through higherorder functions and better interfaces to libraries) and a nearlinear parallel speedup with no additional effort. These benefits are offset by one substantial downside: the lack of familiarity with the language of the original vision researchers, who understandably tried to use the language in the familiar way they use C++ and thus ran into various problems.
p354
aVPurely functional programs should run well on parallel hardware because of the absence of side effects, but it has proved hard to realise this potential in practice. Plenty of papers describe promising ideas, but vastly fewer describe real implementations with good wallclock performance. We describe just such an implementation, and quantitatively explore some of the complex design tradeoffs that make such implementations hard to build. Our measurements are necessarily detailed and specific, but they are reproducible, and we believe that they offer some general insights.
p355
aVWe present a new approach for constructing and verifying higherorder, imperative programs using the Coq proof assistant. We build on the past work on the Ynot system, which is based on Hoare Type Theory. That original system was a proof of concept, where every program verification was accomplished via laborious manual proofs, with much code devoted to uninteresting lowlevel details. In this paper, we present a reimplementation of Ynot which makes it possible to implement fullyverified, higherorder imperative programs with reasonable proof burden. At the same time, our new system is implemented entirely in Coq source files, showcasing the versatility of that proof assistant as a platform for research on language design and verification. Both versions of the system have been evaluated with case studies in the verification of imperative data structures, such as hash tables with higherorder iterators. The verification burden in our new system is reduced by at least an order of magnitude compared to the old system, by replacing manual proof with automation. The core of the automation is a simplification procedure for implications in higherorder separation logic, with hooks that allow programmers to add domainspecific simplification rules. We argue for the effectiveness of our infrastructure by verifying a number of data structures and a packrat parser, and we compare to similar efforts within other projects. Compared to competing approaches to data structure verification, our system includes much less code that must be trusted; namely, about a hundred lines of Coq code defining a program logic. All of our theorems and decision procedures have or build machinecheckable correctness proofs from first principles, removing opportunities for tool bugs to create faulty verifications.
p356
aVWe report on our experience using Haskell as an executable specification language in the formal verification of the seL4 microkernel. The verification connects an abstract operational specification in the theorem prover Isabelle/HOL to a C implementation of the microkernel. We describe how this project differs from other efforts, and examine the effect of using Haskell in a largescale formal verification. The kernel comprises 8,700 lines of C code; the verification more than 150,000 lines of proof script.
p357
aVWe define logical relations between the denotational semantics of a simply typed functional language with recursion and the operational behaviour of lowlevel programs in a variant SECD machine. The relations, which are defined using biorthogonality and stepindexing, capture what it means for a piece of lowlevel code to implement a mathematical, domaintheoretic function and are used to prove correctness of a simple compiler. The results have been formalized in the Coq proof assistant.
p358
aVScribble is a system for writing library documentation, user guides, and tutorials. It builds on PLT Scheme's technology for language extension, and at its heart is a new approach to connecting prose references with library bindings. Besides the base system, we have built Scribble libraries for JavaDocstyle API documentation, literate programming, and conference papers. We have used Scribble to produce thousands of pages of documentation for PLT Scheme; the new documentation is more complete, more accessible, and better organized, thanks in large part to Scribble's flexibility and the ease with which we crossreference information across levels. This paper reports on the use of Scribble and on its design as both an extension and an extensible part of PLT Scheme.
p359
aVAmbitious experiments using proof assistants for programming language research and teaching are all the rage. In this talk, I'll report on one now underway at the University of Pennsylvania and several other sites: a onesemester graduate course in the theory of programming languages presented entirely  every lecture, every homework assignment  in Coq. I'll try to give a sense of what the course is like for both instructors and students, describe some of the most interesting challenges in developing it, and explain why I now believe such machineassisted courses are the way of the future.
p360
aVWe construct a logical framework supporting datatypes that mix binding and computation, implemented as a universe in the dependently typed programming language Agda 2. We represent binding pronominally, using wellscoped de Bruijn indices, so that types can be used to reason about the scoping of variables. We equip our universe with datatypegeneric implementations of weakening, substitution, exchange, contraction, and subordinationbased strengthening, so that programmers need not reimplement these operations for each individual language they define. In our mixed, pronominal setting, weakening and substitution hold only under some conditions on types, but we show that these conditions can be discharged automatically in many cases. Finally, we program a variety of standard difficult test cases from the literature, such as normalizationbyevaluation for the untyped lambdacalculus, demonstrating that we can express detailed invariants about variable usage in a program's type while still writing clean and clear code.
p361
aVType abstraction and intensional type analysis are features seemingly at oddstype abstraction is intended to guarantee parametricity and representation independence, while type analysis is inherently nonparametric. Recently, however, several researchers have proposed and implemented "dynamic type generation" as a way to reconcile these features. The idea is that, when one defines an abstract type, one should also be able to generate at run time a fresh type name, which may be used as a dynamic representative of the abstract type for purposes of type analysis. The question remains: in a language with nonparametric polymorphism, does dynamic type generation provide us with the same kinds of abstraction guarantees that we get from parametric polymorphism? Our goal is to provide a rigorous answer to this question. We define a stepindexed Kripke logical relation for a language with both nonparametric polymorphism (in the form of typesafe cast) and dynamic type generation. Our logical relation enables us to establish parametricity and representation independence results, even in a nonparametric setting, by attaching arbitrary relational interpretations to dynamicallygenerated type names. In addition, we explore how programs that are provably equivalent in a more traditional parametric logical relation may be "wrapped" systematically to produce terms that are related by our nonparametric relation, and vice versa. This leads us to a novel "polarized" form of our logical relation, which enables us to distinguish formally between positive and negative notions of parametricity.
p362
aVWe address the problem of testing and debugging concurrent, distributed Erlang applications. In concurrent programs, race conditions are a common class of bugs and are very hard to find in practice. Traditional unit testing is normally unable to help finding all race conditions, because their occurrence depends so much on timing. Therefore, race conditions are often found during system testing, where due to the vast amount of code under test, it is often hard to diagnose the error resulting from race conditions. We present three tools (QuickCheck, PULSE, and a visualizer) that in combination can be used to test and debug concurrent programs in unit testing with a much better possibility of detecting race conditions. We evaluate our method on an industrial concurrent case study and illustrate how we find and analyze the race conditions.
p363
aVMemoization is a wellknown optimization technique used to eliminate redundant calls for pure functions. If a call to a function f with argument v yields result r, a subsequent call to f with v can be immediately reduced to r without the need to reevaluate f's body. Understanding memoization in the presence of concurrency and communication is significantly more challenging. For example, if f communicates with other threads, it is not sufficient to simply record its input/output behavior; we must also track interthread dependencies induced by these communication actions. Subsequent calls to f can be elided only if we can identify an interleaving of actions from these callsites that lead to states in which these dependencies are satisfied. Similar issues arise if f spawns additional threads. In this paper, we consider the memoization problem for a higherorder concurrent language whose threads may communicate through synchronous messagebased communication. To avoid the need to perform unbounded state space search that may be necessary to determine if all communication dependencies manifest in an earlier call can be satisfied in a later one, we introduce a weaker notion of memoization called partial memoization that gives implementations the freedom to avoid performing some part, if not all, of a previously memoized call. To validate the effectiveness of our ideas, we consider the benefits of memoization for reducing the overhead of recomputation for streaming, serverbased, and transactional applications executed on a multicore machine. We show that on a variety of workloads, memoization can lead to substantial performance improvements without incurring high memory costs.
p364
aVFree theorems are a charm, allowing the derivation of useful statements about programs from their (polymorphic) types alone. We show how to reap such theorems not only from polymorphism over ordinary types, but also from polymorphism over type constructors restricted by class constraints. Our prime application area is that of monads, which form the probably most popular type constructor class of Haskell. To demonstrate the broader scope, we also deal with a transparent way of introducing difference lists into a program, endowed with a neat and general correctness proof.
p365
aVI describe the initial attempt of experienced business software developers with minimal functional programming background to write a nontrivial, businesscritical application entirely in Haskell. Some parts of the application domain are well suited to a mathematicallyoriented language; others are more typically done in languages such as C++. I discuss the advantages and difficulties of Haskell in these circumstances, with a particular focus on issues that commercial developers find important but that may receive less attention from the academic community. I conclude that, while academic implementations of "advanced" programming languages arguably may lag somewhat behind implementations of commercial languages in certain ways important to businesses, this appears relatively easy to fix, and that the other advantages that they offer make them a good, albeit longterm, investment for companies where effective IT implementation can offer a crucial advantage to success.
p366
aVAutomatic differentiation (AD) is a precise, efficient, and convenient method for computing derivatives of functions. Its forwardmode implementation can be quite simple even when extended to compute all of the higherorder derivatives as well. The higherdimensional case has also been tackled, though with extra complexity. This paper develops an implementation of higherdimensional, higherorder, forwardmode AD in the extremely general and elegant setting of calculus on manifolds and derives that implementation from a simple and precise specification. In order to motivate and discover the implementation, the paper poses the question "What does AD mean, independently of implementation?" An answer arises in the form of naturality of sampling a function and its derivative. Automatic differentiation flows out of this naturality condition, together with the chain rule. Graduating from firstorder to higherorder AD corresponds to sampling all derivatives instead of just one. Next, the setting is expanded to arbitrary vector spaces, in which derivative values are linear maps. The specification of AD adapts to this elegant and very general setting, which even simplifies the development.
p367
aVWe describe in this paper our implementation of the Xenstored service which is part of the Xen architecture. Xenstored maintains a hierarchical and transactional database, used for storing and managing configuration values. We demonstrate in this paper that mixing functional datastructures together with reference cell comparison, which is a limited form of pointer comparison, is: (i) safe; and (ii) efficient. This demonstration is based, first, on an axiomatization of operations on the treelike structure we used to represent the Xenstored database. From this axiomatization, we then derive an efficient algorithm for coalescing concurrent transactions modifying that structure. Finally, we experimentally compare the performance of our implementation, that we called OXenstored, and the C implementation of the Xenstored service distributed with the Xen hypervisor sources: the results show that Oxenstored is much more efficient than its C counterpart. As a direct result of this work, OXenstored will be included in future releases of Xenserver, the virtualization product distributed by Citrix Systems, where it will replace the current implementation of the Xenstored service.
p368
aVHighlevel tools have become unavoidable in industrial software development processes. Safetycritical embedded programs don't escape this trend. In the context of safetycritical embedded systems, the development processes follow strict guidelines and requirements. The development quality assurance applies as much to the final embedded code, as to the tools themselves. The French company Esterel Technologies decided in 2006 to base its new SCADE SUITE 6TM certifiable code generator on Objective Caml. This paper outlines how it has been challenging in the context of safety critical software development by the rigorous norms DO178B, IEC 61508, EN 50128 and such.
p369
aVDuring the life cycle of an XML application, both schemas and queries may change from one version to another. Schema evolutions may affect query results and potentially the validity of produced data. Nowadays, a challenge is to assess and accommodate the impact of these changes in evolving XML applications. Such questions arise naturally in XML static analyzers. These analyzers often rely on decision procedures such as inclusion between XML schemas, query containment and satisfiability. However, existing decision procedures cannot be used directly in this context. The reason is that they are unable to distinguish information related to the evolution from information corresponding to bugs. This paper proposes a predicate language within a logical framework that can be used to make this distinction. We present a system for monitoring the effect of schema evolutions on the set of admissible documents and on the results of queries. The system is very powerful in analyzing various scenarios where the result of a query may not be anymore what was expected. Specifically, the system is based on a set of predicates which allow a finegrained analysis for a wide range of forward and backward compatibility issues. Moreover, the system can produce counterexamples and witness documents which are useful for debugging purposes. The current implementation has been tested with realistic use cases, where it allows identifying queries that must be reformulated in order to produce the expected results across successive schema versions.
p370
aVThere is certain diverse class of diagram that is found in a variety of branches of mathematics and which all share this property: there is a common scheme for translating all of these diagrams into useful functional code. These diagrams include Bayesian networks, quantum computer circuits [1], trace diagrams for multilinear algebra [2], Feynman diagrams and even knot diagrams [3]. I will show how a common thread lying behind these diagrams is the presence of a commutative monad and I will show how we can use this fact to translate these diagrams directly into Haskell code making use of donotation for monads. I will also show a number of examples of such translated code at work and use it to solve problems ranging from Bayesian inference to the topological problem of untangling tangled strings. Along the way I hope to give a little insight into the subjects mentioned above and illustrate how a functional programming language can be a valuable tool in mathematical research and experimentation.
p371
aVMany datatypegeneric functions need access to the recursive positions in the structure of the datatype, and therefore adopt a fixed point view on datatypes. Examples include variants of fold that traverse the data following the recursive structure, or the Zipper data structure that enables navigation along the recursive positions. However, HindleyMilnerinspired type systems with algebraic datatypes make it difficult to express fixed points for anything but regular datatypes. Many reallife examples such as abstract syntax trees are in fact systems of mutually recursive datatypes and therefore excluded. Using Haskell's GADTs and type families, we describe a technique that allows a fixedpoint view for systems of mutually recursive datatypes. We demonstrate that our approach is widely applicable by giving several examples of generic functions for this view, most prominently the Zipper.
p372
aVAttribute Grammars (AGs), a generalpurpose formalism for describing recursive computations over data types, avoid the tradeoff which arises when building software incrementally: should it be easy to add new data types and data type alternatives or to add new operations on existing data types? However, AGs are usually implemented as a preprocessor, leaving e.g. type checking to later processing phases and making interactive development, proper error reporting and debugging difficult. Embedding AG into Haskell as a combinator library solves these problems. Previous attempts at embedding AGs as a domainspecific language were based on extensible records and thus exploiting Haskell's type system to check the well formedness of the AG, but fell short in compactness and the possibility to abstract over oft occurring AG patterns. Other attempts used a very generic mapping for which the AG wellformedness could not be statically checked. We present a typed embedding of AG in Haskell satisfying all these requirements. The key lies in using HListlike typed heterogeneous collections (extensible polymorphic records) and expressing AG wellformedness conditions as typelevel predicates (i.e., typeclass constraints). By further typelevel programming we can also express common programming patterns, corresponding to the typical use cases of monads such as Reader, Writer and State. The paper presents a realistic example of typeclassbased typelevel programming in Haskell.
p373
aVConcurrent ML (CML) is a highlevel messagepassing language that supports the construction of firstclass synchronous abstractions called events. This mechanism has proven quite effective over the years and has been incorporated in a number of other languages. While CML provides a concurrent programming model, its implementation has always been limited to uniprocessors. This limitation is exploited in the implementation of the synchronization protocol that underlies the event mechanism, but with the advent of cheap parallel processing on the desktop (and laptop), it is time for Parallel CML. Parallel implementations of CMLlike primitives for Java and Haskell exist, but build on highlevel synchronization constructs that are unlikely to perform well. This paper presents a novel, parallel implementation of CML that exploits a purposebuilt optimistic concurrency protocol designed for both correctness and performance on sharedmemory multiprocessors. This work extends and completes an earlier protocol that supported just a strict subset of CML with synchronization on input, but not output events. Our main contributions are a modelchecked reference implementation of the protocol and two concrete implementations. This paper focuses on Manticore's functional, continuationbased implementation but briefly discusses an independent, threadbased implementation written in C# and running on Microsoft's stock, parallel runtime. Although very different in detail, both derive from the same design. Experimental evaluation of the Manticore implementation reveals good performance, dispite the extra overhead of multiprocessor synchronization.
p374
aVIn Concurrent ML, synchronization abstractions can be defined and passed as values, much like functions in ML. This mechanism admits a powerful, modular style of concurrent programming, called higherorder concurrent programming. Unfortunately, it is not clear whether this style of programming is possible in languages such as Concurrent Haskell, that support only firstorder message passing. Indeed, the implementation of synchronization abstractions in Concurrent ML relies on fairly lowlevel, languagespecific details. In this paper we show, constructively, that synchronization abstractions can be supported in a language that supports only firstorder message passing. Specifically, we implement a library that makes Concurrent MLstyle programming possible in Concurrent Haskell. We begin with a core, formal implementation of synchronization abstractions in the \u03c0calculus. Then, we extend this implementation to encode all of Concurrent ML's concurrency primitives (and more!) in Concurrent Haskell. Our implementation is surprisingly efficient, even without possible optimizations. In several small, informal experiments, our library seems to outperform OCaml's standard library of Concurrent MLstyle primitives. At the heart of our implementation is a new distributed synchronization protocol that we prove correct. Unlike several previous translations of synchronization abstractions in concurrent languages, we remain faithful to the standard semantics for Concurrent ML's concurrency primitives. For example, we retain the symmetry of choose, which can express selective communication. As a corollary, we establish that implementing selective communication on distributed machines is no harder than implementing firstorder message passing on such machines.
p375
aVThis experience report describes the choice of OCaml as the implementation language for FramaC, a framework for the static analysis of C programs. OCaml became the implementation language for FramaC because it is expressive. Most of the reasons listed in the remaining of this article are secondary reasons, features which are not specific to OCaml (modularity, availability of a C parser, control over the use of resources...) but could have prevented the use of OCaml for this project if they had been missing.
p376
aVWe derive a controlflow analysis that approximates the interprocedural controlflow of both function calls and returns in the presence of firstclass functions and tailcall optimization. In addition to an abstract environment, our analysis computes for each expression an abstract control stack, effectively approximating where function calls return across optimized tail calls. The analysis is systematically calculated by abstract interpretation of the stackbased CaEK abstract machine of Flanagan et al. using a series of Galois connections. Abstract interpretation provides a unifying setting in which we 1) prove the analysis equivalent to the composition of a continuationpassing style (CPS) transformation followed by an abstract interpretation of a stackless CPS machine, and 2) extract an equivalent constraintbased formulation, thereby providing a rational reconstruction of a constraintbased controlflow analysis from abstract interpretation principles.
p377
aVContinuationbased Web servers provide distinct advantages over traditional Web application development: expressive power and modularity. This power leads to fewer errors and more interesting applications. Furthermore, these Web servers are more than prototypes; they are used in some real commercial applications. Unfortunately, they pay a heavy price for the additional power in the form of lack of scalability. We fix this key problem with a modular program transformation that produces scalable, continuationbased Web programs based on the REST architecture. Our programs use the same features as nonscalable, continuationbased Web programs, so we do not sacrifice expressive power for performance. In particular, we allow continuation marks in Web programs. Our system uses 10 percent (or less) of the memory required by previous approaches.
p378
aVThe evolution of Web sites towards very dynamic applications makes it necessary to reconsider current Web programming technologies. We believe that Web development would benefit greatly from more abstract paradigms and that a more semantical approach would result in huge gains in expressiveness. In particular, functional programming provides a really elegant solution to some important Web interaction problems, but few frameworks take advantage of it. The Ocsigen project is an attempt to provide global solutions to these needs. We present our experience in designing this general framework for Web programming, written in Objective Caml. It provides a fully featured Web server and a framework for programming Web applications, with the aim of improving expressiveness and safety. This is done by taking advantage of functional programming and static typing as much as possible.
p379
aVWe describe the implementation of firstclass polymorphic delimited continuations in the programming language Scala. We use Scala's pluggable typing architecture to implement a simple type and effect system, which discriminates expressions with control effects from those without and accurately tracks answer type modification incurred by control effects. To tackle the problem of implementing firstclass continuations under the adverse conditions brought upon by the Java VM, we employ a selective CPS transform, which is driven entirely by effectannotated types and leaves pure code in direct style. Benchmarks indicate that this highlevel approach performs competitively.
p380
aVA number of important program rewriting scenarios can be recast as typedirected coercion insertion. These range from more theoretical applications such as coercive subtyping and supporting overloading in type theories, to more practical applications such as integrating static and dynamically typed code using gradual typing, and inlining code to enforce security policies such as access control and provenance tracking. In this paper we give a general theory of typedirected coercion insertion. We specifically explore the inherent tradeoff between expressiveness and ambiguity the more powerful the strategy for generating coercions, the greater the possibility of several, semantically distinct rewritings for a given program. We consider increasingly powerful coercion generation strategies, work out example applications supported by the increased power (including those mentioned above), and identify the inherent ambiguity problems of each setting, along with various techniques to tame the ambiguities.
p381
aVAlan Perlis, inverting OscarWilde's famous quip about cynics, once suggested, decades ago, that a Lisp programmer is one who knows the value of everything and the cost of nothing. Now that the conference on Lisp and Functional Programming has become ICFP, some may think that OCaml and Haskell programmers have inherited this (now undeserved) epigram. I do believe that as multicore processors are becoming prominent, and soon ubiquitous, it behooves all programmers to rethink their programming style, strategies, and tactics, so that their code may have excellent performance. For the last six years I have been part of a team working on a programming language, Fortress, that has borrowed ideas not only from Fortran, not only from Java, not only from Algol and Alphard and CLU, not only from MADCAP and MODCAP and MIRFAC and the KlererMay systembut also from Haskell, and I would like to repay the favor. In this talk I will discuss three ideas (none original with me) that I have found to be especially powerful in organizing Fortress programs so that they may be executed equally effectively either sequentially or in parallel: userdefined associative operators (and, more generally, userdefined monoids); conjugate transforms of data; and monoidcaching trees (as described, for example, by Hinze and Paterson). I will exhibit pleasant little code examples (some original with me) that make use of these ideas.
p382
aVThis pearl aims to demonstrate the ideas of wholemeal and projective programming using the Towers of Hanoi puzzle as a running example. The puzzle has its own beauty, which we hope to expose along the way.
p383
aVFunctional logic programming and probabilistic programming have demonstrated the broad benefits of combining laziness (nonstrict evaluation with sharing of the results) with nondeterminism. Yet these benefits are seldom enjoyed in functional programming, because the existing features for nonstrictness, sharing, and nondeterminism in functional languages are tricky to combine. We present a practical way to write purely functional lazy nondeterministic programs that are efficient and perspicuous. We achieve this goal by embedding the programs into existing languages (such as Haskell, SML, and OCaml) with highquality implementations, by making choices lazily and representing data with nondeterministic components, by working with custom monadic data types and search strategies, and by providing equational laws for the programmer to reason about their code.
p384
aVFunctional Reactive Programming (FRP) is an approach to reactive programming where systems are structured as networks of functions operating on signals. FRP is based on the synchronous dataflow paradigm and supports both continuoustime and discretetime signals (hybrid systems). What sets FRP apart from most other languages for similar applications is its support for systems with dynamic structure and for higherorder reactive constructs. Statically guaranteeing correctness properties of programs is an attractive proposition. This is true in particular for typical application domains for reactive programming such as embedded systems. To that end, many existing reactive languages have type systems or other static checks that guarantee domainspecific properties, such as feedback loops always being wellformed. However, they are limited in their capabilities to support dynamism and higherorder dataflow compared with FRP. Thus, the onus of ensuring such properties of FRP programs has so far been on the programmer as established static techniques do not suffice. In this paper, we show how dependent types allow this concern to be addressed. We present an implementation of FRP embedded in the dependentlytyped language Agda, leveraging the type system of the host language to craft a domainspecific (dependent) type system for FRP. The implementation constitutes a discrete, operational semantics of FRP, and as it passes the Agda type, coverage, and termination checks, we know the operational semantics is total, which means our type system is safe.
p385
aVMy talk will celebrate Robin Milner's contribution to functional programming via a combination of reminiscences about the early days of ML and speculations about its future.
p386
aVNested dataparallelism (NDP) is a declarative style for programming irregular parallel applications. NDP languages provide language features favoring the NDP style, efficient compilation of NDP programs, and various common NDP operations like parallel maps, filters, and sumlike reductions. In this paper, we describe the implementation of NDP in Parallel ML (PML), part of the Manticore project. Managing the parallel decomposition of work is one of the main challenges of implementing NDP. If the decomposition creates too many small chunks of work, performance will be eroded by too much parallel overhead. If, on the other hand, there are too few large chunks of work, there will be too much sequential processing and processors will sit idle. Recently the technique of Lazy Binary Splitting was proposed for dynamic parallel decomposition of work on flat arrays, with promising results. We adapt Lazy Binary Splitting to parallel processing of binary trees, which we use to represent parallel arrays in PML. We call our technique Lazy Tree Splitting (LTS). One of its main advantages is its performance robustness: perprogram tuning is not required to achieve good performance across varying platforms. We describe LTSbased implementations of standard NDP operations, and we present experimental data demonstrating the scalability of LTS across a range of benchmarks.
p387
aVWe study a firstorder functional language with the novel combination of the ideas of refinement type (the subset of a type to satisfy a Boolean expression) and typetest (a Boolean expression testing whether a value belongs to a type). Our core calculus can express a rich variety of typing idioms; for example, intersection, union, negation, singleton, nullable, variant, and algebraic types are all derivable. We formulate a semantics in which expressions denote terms, and types are interpreted as firstorder logic formulas. Subtyping is defined as valid implication between the semantics of types. The formulas are interpreted in a specific model that we axiomatize using standard firstorder theories. On this basis, we present a novel typechecking algorithm able to eliminate many dynamic tests and to detect many errors statically. The key idea is to rely on an SMT solver to compute subtyping efficiently. Moreover, interpreting types as formulas allows us to call the SMT solver at runtime to compute instances of types.
p388
aVProgrammers reason about their programs using a wide variety of formal and informal methods. Programmers in untyped languages such as Scheme or Erlang are able to use any such method to reason about the type behavior of their programs. Our type system for Scheme accommodates common reasoning methods by assigning variable occurrences a subtype of their declared type based on the predicates prior to the occurrence, a discipline dubbed occurrence typing. It thus enables programmers to enrich existing Scheme code with types, while requiring few changes to the code itself. Three years of practical experience has revealed serious shortcomings of our type system. In particular, it relied on a system of adhoc rules to relate combinations of predicates, it could not reason about subcomponents of data structures, and it could not follow sophisticated reasoning about the relationship among predicate tests, all of which are used in existing code. In this paper, we reformulate occurrence typing to eliminate these shortcomings. The new formulation derives propositional logic formulas that hold when an expression evaluates to true or false, respectively. A simple proof system is then used to determine types of variable occurrences from these propositions. Our implementation of this revised occurrence type system thus copes with many more untyped programming idioms than the original system.
p389
aVIn 1995, my team and I decided to create an outreach project that would use our research on functional programming to change the K12 computer science curriculum. We had two different goals in mind. On the one hand, our curriculum should rely on mathematics to teach programming, and it d exploit programming to teach mathematics. All students  not just those who major in computer science  should benefit. On the other hand, our course should demonstrate that introductory programming can focus on program design, not just a specific syntax. We also wished to create a smooth path from a designoriented introductory course all the way to courses on large software projects. My talk presents a checkpoint of our project, starting with our major scientific goal, a comprehensive theory of program design. Our work on this theory progresses through the development of program design courses for all age groups. At this point, we offer curricular materials for middle schools, high schools, three collegelevel freshman courses, and a juniorlevel course on constructing large components. We regularly use these materials to train K12 teachers, afterschool volunteers, and college faculty; thus far, we have reached hundreds of instructors, who in turn have dealt with thousands of students in their classrooms.
p390
aVWe present a technique for higherorder representation of substructural logics such as linear or modal logic. We show that such logics can be encoded in the (ordinary) Logical Framework, without any linear or modal extensions. Using this encoding, metatheoretic proofs about such logics can easily be developed in the Twelf proof assistant.
p391
aVReasoning about program equivalence is one of the oldest problems in semantics. In recent years, useful techniques have been developed, based on bisimulations and logical relations, for reasoning about equivalence in the setting of increasingly realistic languages  languages nearly as complex as ML or Haskell. Much of the recent work in this direction has considered the interesting representation independence principles enabled by the use of local state, but it is also important to understand the principles that powerful features like higherorder state and control effects disable. This latter topic has been broached extensively within the framework of game semantics, resulting in what Abramsky dubbed the "semantic cube": fully abstract gamesemantic characterizations of various axes in the design space of MLlike languages. But when it comes to reasoning about many actual examples, game semantics does not yet supply a useful technique for proving equivalences. In this paper, we marry the aspirations of the semantic cube to the powerful proof method of stepindexed Kripke logical relations. Building on recent work of Ahmed, Dreyer, and Rossberg, we define the first fully abstract logical relation for an MLlike language with recursive types, abstract types, general references and call/cc. We then show how, under orthogonal restrictions to the expressive power our language  namely, the restriction to firstorder state and/or the removal of call/cc  we can enhance the proving power of our possibleworlds model in correspondingly orthogonal ways, and we demonstrate this proving power on a range of interesting examples. Central to our story is the use of state transition systems to model the way in which properties of local state evolve over time.
p392
aVWe want assurances that sensitive information will not be disclosed when aggregate data derived from a database is published. Differential privacy offers a strong statistical guarantee that the effect of the presence of any individual in a database will be negligible, even when an adversary has auxiliary knowledge. Much of the prior work in this area consists of proving algorithms to be differentially private one at a time; we propose to streamline this process with a functional language whose type system automatically guarantees differential privacy, allowing the programmer to write complex privacysafe query programs in a flexible and compositional way. The key novelty is the way our type system captures function sensitivity, a measure of how much a function can magnify the distance between similar inputs: welltyped programs not only can't go wrong, they can't go too far on nearby inputs. Moreover, by introducing a monad for random computations, we can show that the established definition of differential privacy falls out naturally as a special case of this soundness principle. We develop examples including known differentially private algorithms, privacyaware variants of standard functional programming idioms, and compositionality principles for differential privacy.
p393
aVSeveral recent securitytyped programming languages, such as Aura, PCML5, and Fine, allow programmers to express and enforce access control and information flow policies. In this paper, we show that securitytyped programming can be embedded as a library within a generalpurpose dependently typed programming language, Agda. Our library, Aglet, accounts for the major features of existing securitytyped programming languages, such as decentralized access control, typed proofcarrying authorization, ephemeral and dynamic policies, authentication, spatial distribution, and information flow. The implementation of Aglet consists of the following ingredients: First, we represent the syntax and proofs of an authorization logic, Garg and Pfenning's BL0, using dependent types. Second, we implement a proof search procedure, based on a focused sequent calculus, to ease the burden of constructing proofs. Third, we represent computations using a monad indexed by pre and postconditions drawn from the authorization logic, which permits ephemeral policies that change during execution. We describe the implementation of our library and illustrate its use on a number of the benchmark examples considered in the literature.
p394
aVMatsuda et al. [2007, ICFP] and Voigtlnder [2009, POPL] introduced two techniques that given a sourcetoview function provide an update propagation function mapping an original source and an updated view back to an updated source, subject to standard consistency conditions. Being fundamentally different in approach, both techniques have their respective strengths and weaknesses. Here we develop a synthesis of the two techniques to good effect. On the intersection of their applicability domains we achieve more than what a simple union of applying the techniques side by side delivers.
p395
aVBidirectional programming languages are a practical approach to the view update problem. Programs in these languages, called lenses, define both a view and an update policy  i.e., every program can be read as a function mapping sources to views as well as one mapping updated views back to updated sources. One thorny issue that has not received sufficient attention in the design of bidirectional languages is alignment. In general, to correctly propagate an update to a view, a lens needs to match up the pieces of the view with the corresponding pieces of the underlying source, even after data has been inserted, deleted, or reordered. However, existing bidirectional languages either support only simple strategies that fail on many examples of practical interest, or else propose specific strategies that are baked deeply into the underlying theory. We propose a general framework of matching lenses that parameterizes lenses over arbitrary heuristics for calculating alignments. We enrich the types of lenses with "chunks" identifying reorderable pieces of the source and view that should be realigned after an update, and we formulate behavioral laws that capture essential constraints on the handling of chunks. We develop a core language of matching lenses for strings, together with a set of "alignment combinators" that implement a variety of alignment strategies.
p396
aVWe present a closed dependent type theory whose inductive types are given not by a scheme for generative declarations, but by encoding in a universe. Each inductive datatype arises by interpreting its description  a firstclass value in a datatype of descriptions. Moreover, the latter itself has a description. Datatypegeneric programming thus becomes ordinary programming. We show some of the resulting generic operations and deploy them in particular, useful ways on the datatype of datatype descriptions itself. Simulations in existing systems suggest that this apparently selfsupporting setup is achievable without paradox or infinite regress.
p397
aVBidirectional transformations provide a novel mechanism for synchronizing and maintaining the consistency of information between input and output. Despite many promising results on bidirectional transformations, these have been limited to the context of relational or XML (treelike) databases. We challenge the problem of bidirectional transformations within the context of graphs, by proposing a formal definition of a wellbehaved bidirectional semantics for UnCAL, i.e., a graph algebra for the known UnQL graph query language. The key to our successful formalization is full utilization of both the recursive and bulk semantics of structural recursion on graphs. We carefully refine the existing forward evaluation of structural recursion so that it can produce sufficient trace information for later backward evaluation. We use the trace information for backward evaluation to reflect inplace updates and deletions on the view to the source, and adopt the universal resolving algorithm for inverse computation and the narrowing technique to tackle the difficult problem with insertion. We prove our bidirectional evaluation is wellbehaved. Our current implementation is available online and confirms the usefulness of our approach with nontrivial applications.
p398
aVA wide range of computer programs, including compilers and theorem provers, manipulate data structures that involve names and binding. However, the design of programming idioms which allow performing these manipulations in a safe and natural style has, to a large extent, remained elusive. In this paper, we present a novel approach to the problem. Our proposal can be viewed either as a programming language design or as a library: in fact, it is currently implemented within Agda. It provides a safe and expressive means of programming with names and binders. It is abstract enough to support multiple concrete implementations: we present one in nominal style and one in de Bruijn style. We use logical relations to prove that "welltyped programs do not mix names with different scope". We exhibit an adequate encoding of Pittsstyle nominal terms into our system.
p399
aVA student learning how to program learns best when the programming language and programming environment cater to her specific needs. These needs are different from the requirements of a professional programmer. Consequently, the design of teaching languages poses challenges different from the design of professional languages. Using a functional language by itself gives advantages over more popular, professional languages, but fully exploiting these advantages requires careful adaptation to the needs of the students' asis, these languages do not support the students nearly as well as they could. This paper describes our experience adopting the didactic approach of How to Design Programs, focussing on the design process for our own set of teaching languages. We have observed students as they try to program as part of our introductory course, and used these observations to significantly improve the design of these languages. This paper describes the changes we have made, and the journey we took to get there.
p400
aVExisting macro systems force programmers to make a choice between clarity of specification and robustness. If they choose clarity, they must forgo validating significant parts of the specification and thus produce lowquality language extensions. If they choose robustness, they must write in a style that mingles the implementation with the specification and therefore obscures the latter. This paper introduces a new language for writing macros. With the new macro system, programmers naturally write robust language extensions using easytounderstand specifications. The system translates these specifications into validators that detect misuses  including violations of contextsensitive constraints  and automatically synthesize appropriate feedback, eliminating the need for ad hoc validation code.
p401
aVFunctional programming presents several important advantages in the design, analysis and implementation of parallel algorithms: It discourages iteration and encourages decomposition. It supports persistence and hence easy speculation. It encourages higherorder aggregate operations. It is well suited for defining cost models tied to the programming language rather than the machine. Implementations can avoid false sharing. Implementations can use cheaper weak consistency models. And most importantly, it supports safe deterministic parallelism. In fact functional programming supports a level of abstraction in which parallel algorithms are often as easy to design and analyze as sequential algorithms. The recent widespread advent of parallel machines therefore presents a great opportunity for functional programming languages. However, any changes will require significant education at all levels and involvement of the functional programming community. In this talk I will discuss an approach to designing and analyzing parallel algorithms in a strict functional and fully deterministic setting. Key ideas include a cost model defined in term of analyzing work and span, the use of divideandconquer and contraction, the need for arrays (immutable) to achieve asymptotic efficiency, and the power of (deterministic) randomized algorithms. These are all ideas I believe can be taught at any level.
p402
aVSparse matrix formats are typically implemented with lowlevel imperative programs. The optimized nature of these implementations hides the structural organization of the sparse format and complicates its verification. We define a variablefree functional language (LL) in which even advanced formats can be expressed naturally, as a pipelinestyle composition of smaller construction steps. We translate LL programs to Isabelle/HOL and describe a proof system based on parametric predicates for tracking relationship between mathematical vectors and their concrete representations. This proof theory automatically verifies full functional correctness of many formats. We show that it is reusable and extensible to hierarchical sparse formats.
p403
aVWe present a novel approach to regular, multidimensional arrays in Haskell. The main highlights of our approach are that it (1) is purely functional, (2) supports reuse through shape polymorphism, (3) avoids unnecessary intermediate structures rather than relying on subsequent loop fusion, and (4) supports transparent parallelisation. We show how to embed two forms of shape polymorphism into Haskell's type system using type classes and type families. In particular, we discuss the generalisation of regular array transformations to arrays of higher rank, and introduce a typesafe specification of array slices. We discuss the runtime performance of our approach for three standard array algorithms. We achieve absolute performance comparable to handwritten C code. At the same time, our implementation scales well up to 8 processor cores.
p404
aVWe describe the design, implementation, and use of a machinecertified framework for correct compilation and execution of programs in garbagecollected languages. Our framework extends Leroy's Coqcertified Compcert compiler and Cminor intermediate language. We add: (i) a new intermediate language, GCminor, that includes primitives for allocating memory in a garbagecollected heap and for specifying GC roots; (ii) a precise, lowlevel specification for a Cminor library for garbage collection; and (iii) a proven semanticspreserving translation from GCminor to Cminor plus the GC library. GCminor neatly encapsulates the interface between mutator and collector code, while remaining simple and flexible enough to be used with a wide variety of source languages and collector styles. Front ends targeting GCminor can be implemented using any compiler technology and any desired degree of verification, including full semantics preservation, type preservation, or informal trust. As an example application of our framework, we describe a compiler for Haskell that translates the Glasgow Haskell Compiler's Core intermediate language to GCminor. To support a simple but useful memory safety argument for this compiler, the front end uses a novel combination of type preservation and runtime checks, which is of independent interest.
p405
aVA monadic parser combinator library which guarantees termination of parsing, while still allowing many forms of left recursion, is described. The library's interface is similar to those of many other parser combinator libraries, with two important differences: one is that the interface clearly specifies which parts of the constructed parsers may be infinite, and which parts have to be finite, using dependent types and a combination of induction and coinduction; and the other is that the parser type is unusually informative. The library comes with a formal semantics, using which it is proved that the parser combinators are as expressive as possible. The implementation is supported by a machinechecked correctness proof.
p406
aVPartial evaluation aims to improve the efficiency of a program by specialising it with respect to some known inputs. In this paper, we show that partial evaluation can be an effective and, unusually, easy to use technique for the efficient implementation of embedded domainspecific languages. We achieve this by exploiting dependent types and by following some simple rules in the definition of the interpreter for the domainspecific language. We present experimental evidence that partial evaluation of programs in domainspecific languages can yield efficient residual programs whose performance is competitive with their Java and C equivalents and which are also, through the use of dependent types, verifiably resourcesafe. Using our technique, it follows that a verifiably correct and resourcesafe program can also be an efficient program
p407
aVWe show how the binary encoding and decoding of typed data and typed programs can be understood, programmed, and verified with the help of questionanswer games. The encoding of a value is determined by the yes/no answers to a sequence of questions about that value; conversely, decoding is the interpretation of binary data as answers to the same question scheme. We introduce a general framework for writing and verifying gamebased codecs. We present games for structured, recursive, polymorphic, and indexed types, building up to a representation of welltyped terms in the simplytyped \u03bbcalculus. The framework makes novel use of isomorphisms between types in the definition of games. The definition of isomorphisms together with additional simple properties make it easy to prove that codecs derived from games never encode two distinct values using the same code, never decode two codes to the same value, and interpret any bit sequence as a valid code for a value or as a prefix of a valid code.
p408
aVSupercompilation is a program optimisation technique that is particularly effective at eliminating unnecessary overheads. We have designed a new supercompiler, making many novel choices, including different termination criteria and handling of let bindings. The result is a supercompiler that focuses on simplicity, compiles programs quickly and optimises programs well. We have benchmarked our supercompiler, with some programs running more than twice as fast than when compiled with GHC.
p409
aVThis paper describes CFML, the first program verification tool based on characteristic formulae. Given the source code of a pure Caml program, this tool generates a logical formula that implies any valid postcondition for that program. One can then prove that the program satisfies a given specification by reasoning interactively about the characteristic formula using a proof assistant such as Coq. Our characteristic formulae improve over Honda et al's total characteristic assertion pairs in that they are expressible in standard higherorder logic, allowing to exploit them in practice to verify programs using existing proof assistants. Our technique has been applied to formally verify more than half of the content of Okasaki's Purely Functional Data Structures reference book
p410
aVModern proof assistants such as Coq and Isabelle provide high degrees of expressiveness and assurance because they support formal reasoning in higherorder logic and supply explicit machinecheckable proof objects. Unfortunately, large scale proof development in these proof assistants is still an extremely difficult and timeconsuming task. One major weakness of these proof assistants is the lack of a single language where users can develop complex tactics and decision procedures using a rich programming model and in a typeful manner. This limits the scalability of the proof development process, as users avoid developing domainspecific tactics and decision procedures. In this paper, we present VeriML  a novel language design that couples a typesafe effectful computational language with firstclass support for manipulating logical terms such as propositions and proofs. The main idea behind our design is to integrate a rich logical framework  similar to the one supported by Coq  inside a computational language inspired by ML. The language design is such that the added features are orthogonal to the rest of the computational language, and also do not require significant additions to the logic language, so soundness is guaranteed. We have built a prototype implementation of VeriML including both its typechecker and an interpreter. We demonstrate the effectiveness of our design by showing a number of typesafe tactics and decision procedures written in VeriML.
p411
aVReynolds' abstraction theorem shows how a typing judgement in System F can be translated into a relational statement (in second order predicate logic) about inhabitants of the type. We (in second order predicate logic) about inhabitants of the type. We obtain a similar result for a single lambda calculus (a pure type system), in which terms, types and their relations are expressed. Working within a single system dispenses with the need for an interpretation layer, allowing for an unusually simple presentation. While the unification puts some constraints on the type system (which we spell out), the result applies to many interesting cases, including dependentlytyped ones.
p412
aVCody, Hazel, and Theo, two experienced Haskell programmers and an expert in automata theory, develop an elegant Haskell program for matching regular expressions: (i) the program is purely functional; (ii) it is overloaded over arbitrary semirings, which not only allows to solve the ordinary matching problem but also supports other applications like computing leftmost longest matchings or the number of matchings, all with a single algorithm; (iii) it is more powerful than other matchers, as it can be used for parsing every contextfree language by taking advantage of laziness. The developed program is based on an old technique to turn regular expressions into finite automata which makes it efficient both in terms of worstcase time and space bounds and actual performance: despite its simplicity, the Haskell implementation can compete with a recently published professional C++ program for the same problem.
p413
aVIn system administration, the languages of choice for solving automation tasks are scripting languages, owing to their flexibility, extensive library support and quick development cycle. Functional programming is more likely to be found in software development teams and the academic world. This separation means that system administrators cannot use the most effective tool for a given problem; in an ideal world, we should be able to mix and match different languages, based on the problem at hand. This experience report details our initial introduction and use of Haskell in a mature, medium size project implemented in Python. We also analyse the interaction between the two languages, and show how Haskell has excelled at solving a particular type of realworld problems
p414
aVTo fix bugs or to enhance a software system without service disruption, one has to update it dynamically during execution. Most prior dynamic software updating techniques require that the code to be changed is not running at the time of the update. However, this restriction precludes any change to the outermost loops of servers, OS scheduling loops and recursive functions. Permitting a dynamic update to more generally manipulate the program's execution state, including the runtime stack, alleviates this restriction but increases the likelihood of type errors. In this paper we present ReCaml, a language for writing dynamic updates to running programs that views execution state as a delimited continuation. ReCaml includes a novel feature for introspecting continuations called match_cont which is sufficiently powerful to implement a variety of updating policies. We have formalized the core of ReCaml and proved it sound (using the Coq proof assistant), thus ensuring that statemanipulating updates preserve typesafe execution of the updated program. We have implemented ReCaml as an extension to the Caml bytecode interpreter and used it for several examples.
p415
aVWhile many type systems based on the intuitionistic fragment of linear logic have been proposed, applications in programming languages of the full power of linear logic  including doublenegation elimination  have remained elusive. Meanwhile, linearity has been used in many type systems for concurrent programs  e.g., session types  which suggests applicability to the problems of concurrent programming, but the ways in which linearity has interacted with concurrency primitives in lambda calculi have remained somewhat adhoc. In this paper we connect classical linear logic and concurrent functional programming in the language Lolliproc, which provides simple primitives for concurrency that have a direct logical interpretation and that combine to provide the functionality of session types. Lolliproc features a simple process calculus "under the hood" but hides the machinery of processes from programmers. We illustrate Lolliproc by example and prove soundness, strong normalization, and confluence results, which, among other things, guarantees freedom from deadlocks and race conditions.
p416
aVWe describe a derivational approach to abstract interpretation that yields novel and transparently sound static analyses when applied to wellestablished abstract machines. To demonstrate the technique and support our claim, we transform the CEK machine of Felleisen and Friedman, a lazy variant of Krivine's machine, and the stackinspecting CM machine of Clements and Felleisen into abstract interpretations of themselves. The resulting analyses bound temporal ordering of program events; predict returnflow and stackinspection behavior; and approximate the flow and evaluation of byneed parameters. For all of these machines, we find that a series of wellknown concrete machine refactorings, plus a technique we call storeallocated continuations, leads to machines that abstract into static analyses simply by bounding their stores. We demonstrate that the technique scales up uniformly to allow static analysis of realistic language features, including tail calls, conditionals, side effects, exceptions, firstclass continuations, and even garbage collection.
p417
aVWe present a type and effect system for flow analysis that makes essential use of higherranked polymorphism. We show that, for higherorder functions, the expressiveness of higherranked types enables us to improve on the precision of conventional letpolymorphic analyses. Modularity and decidability of the analysis are guaranteed by making the analysis of each program parametric in the analyses of its inputs; in particular, we have that higherorder functions give rise to higherorder operations on effects. As flow typing is archetypical to a whole class of type and effect systems, our approach can be used to boost the precision of a wide range of typebased program analyses for higherorder languages.
p418
aVThe leading implementations of graph reduction all target conventional processors designed for lowlevel imperative execution. In this paper, we present a processor specially designed to perform graphreduction. Our processor   the Reduceron   is implemented using offtheshelf reconfigurable hardware. We highlight the lowlevel parallelism present in sequential graph reduction, and show how parallel memories and dynamic analyses are used in the Reduceron to achieve an average reduction rate of 0.55 function applications per clockcycle.
p419
aVWe present a casestudy of using OCaml within a large product development project, focussing on both the technical and nontechnical issues that arose as a result. We draw comparisons between the OCaml team and the other teams that worked on the project, providing comparative data on hiring patterns and crossteam code contribution.
p420
aVIt has been more than 20 years since monads were proposed as a unifying concept for computational effects, in both formal semantics and functional programs. Over that period, there has been substantial incremental progress on several fronts within the ensuing research area, including denotational, operational, and axiomatic characterizations of effects; principles and frameworks for combining effects; prescriptive vs. descriptive effecttype systems; specification vs. implementation of effects; and realizations of effectrelated theoretical constructions in practical functional languages, both eager and lazy. Yet few would confidently claim that programs with computational effects are by now as well understood, and as thoroughly supported by formal reasoning techniques, as types and terms in purely functional settings. This talk outlines (one view of) the landscape of effectful functional programming, and attempts to assess our collective progress towards the goal of a broad yet coherent theory of monadic effects. We are not quite there yet, but intriguingly, many potential ingredients of such a theory have been repeatedly discovered and developed, with only minor variations, in seemingly unrelated contexts. Some strongerthanexpected ties between the research topics mentioned above also instill hope that there is indeed a natural, comprehensive theory of monadic effects, waiting to be fully explicated.
p421
aVWe define and study parametric polymorphism for a type system with recursive, product, union, intersection, negation, and function types. We first recall why the definition of such a system was considered hard "when not impossible" and then present the main ideas at the basis of our solution. In particular, we introduce the notion of "convexity" on which our solution is built up and discuss its connections with parametricity as defined by Reynolds to whose study our work sheds new light.
p422
aVWe consider a type algebra equipped with recursive, product, function, intersection, union, and complement types together with type variables and implicit universal quantification over them. We consider the subtyping relation recently defined by Castagna and Xu over such type expressions and show how this relation can be decided in EXPTIME, answering an open question. The novelty, originality and strength of our solution reside in introducing a logical modeling for the semantic subtyping framework. We model semantic subtyping in a tree logic and use a satisfiabilitytesting algorithm in order to decide subtyping. We report on practical experiments made with a full implementation of the system. This provides a powerful polymorphic type system aiming at maintaining full static typesafety of functional programs that manipulate trees, even with higherorder functions, which is particularly useful in the context of XML.
p423
aVDivideandconquer is an important technique in parallel programming. However, algebraic data structures do not fit divideandconquer parallelism. For example, the usual pointerbased implementation of lists cannot efficiently be divided at their middle, which prevents us from developing listiterating divideandconquer parallel programs. Treeiterating programs possibly face a similar problem, because trees might be illbalanced and listlike shapes. This paper examines parallel programming based on balanced trees: we consider balancedtree structures and develop recursive functions on them. By virtue of their balancing nature, either bottomup or topdown recursive functions exploit divideandconquer parallelism. Our main contribution is to demonstrate the promise of this approach. We propose a way of systematically developing balanced trees from parallel algorithms, and then, we show that efficient parallel programs on them can be developed by equational reasoning powered by Reynolds' relational parametricity. We consider functions that operate either lists or binary trees, and show that our methods can uniformly deal with both cases. The developed parallel programs are purely functional, correct by construction, and sometimes even simpler than known algorithms.
p424
aVComputational problems that involve dynamic data, such as physics simulations and program development environments, have been an important subject of study in programming languages. Building on this work, recent advances in selfadjusting computation have developed techniques that enable programs to respond automatically and efficiently to dynamic changes in their inputs. Selfadjusting programs have been shown to be efficient for a reasonably broad range of problems but the approach still requires an explicit programming style, where the programmer must use specific monadic types and primitives to identify, create and operate on data that can change over time. We describe techniques for automatically translating purely functional programs into selfadjusting programs. In this implicit approach, the programmer need only annotate the (toplevel) input types of the programs to be translated. Type inference finds all other types, and a typedirected translation rewrites the source program into an explicitly selfadjusting target program. The type system is related to informationflow type systems and enjoys decidable type inference via constraint solving. We prove that the translation outputs welltyped selfadjusting programs and preserves the source program's inputoutput behavior, guaranteeing that translated programs respond correctly to all changes to their data. Using a cost semantics, we also prove that the translation preserves the asymptotic complexity of the source program.
p425
aVAgda is a modern functional programming language equipped with an interactive proof assistant as its developing environment. Its features include dependent types, type universe, inductive and coinductive families of types, pattern matching, records, and nested parameterized modules. Based on the "propositions as types, proofs as programs" correspondence in MartinLf's Type Theory, Agda lets users to construct, verify, and execute a smooth mixture of programs and proofs. Using Agda is similar to using an editor in a modern IDE. Users have more direct control over how programs / proofs are written than in automationoriented systems using commandscripts for proof construction. Agda thus encourages users to express their ideas with more sophisticated dependently typed programming and less logical proofs. Programming techniques for readability and maintainability now translate to techniques for writing verified documents for human communication. Agda has been developed at Chalmers University of Technology by Ulf Norell and others. A growing international community of developers and users applies it in research, education, and industry. At AIST in Japan, we aim to introduce its merits to construction, verification, maintenance, and runtime evaluation of "assurance cases", which are documented bodies of systems assurance arguments used as the hub for assurance and riskcommunication among stakeholders. The talk gives an overview of Agda and presents our current effort on programming assurance cases in Agda.
p426
aVWe present instance arguments: an alternative to type classes and related features in the dependently typed, purely functional programming language/proof assistant Agda. They are a new, general type of function arguments, resolved from callsite scope in a typedirected way. The mechanism is inspired by both Scala's implicits and Agda's existing implicit arguments, but differs from both in important ways. Our mechanism is designed and implemented for Agda, but our design choices can be applied to other programming languages as well. Like Scala's implicits, we do not provide a separate structure for type classes and their instances, but instead rely on Agda's standard dependently typed records, so that standard language mechanisms provide features that are missing or expensive in other proposals. Like Scala, we support the equivalent of local instances. Unlike Scala, functions taking our new arguments are firstclass citizens and can be abstracted over and manipulated in standard ways. Compared to other proposals, we avoid the pitfall of introducing a separate typelevel computational model through the instance search mechanism. All values in scope are automatically candidates for instance resolution. A final novelty of our approach is that existing Agda libraries using records gain the benefits of type classes without any modification. We discuss our implementation in Agda (to be part of Agda 2.2.12) and we use monads as an example to show how it allows existing concepts in the Agda standard library to be used in a similar way as corresponding Haskell code using type classes. We also demonstrate and discuss equivalents and alternatives to some advanced type classrelated patterns from the literature and some new patterns specific to our system.
p427
aVMusic theory has been essential in composing and performing music for centuries. Within Western tonal music, from the early Baroque on to modernday jazz and pop music, the function of chords within a chord sequence can be explained by harmony theory. Although Western tonal harmony theory is a thoroughly studied area, formalising this theory is a hard problem. We present a formalisation of the rules of tonal harmony as a Haskell (generalized) algebraic datatype. Given a sequence of chord labels, the harmonic function of a chord in its tonal context is automatically derived. For this, we use several advanced functional programming techniques, such as typelevel computations, datatypegeneric programming, and errorcorrecting parsers. As a detailed example, we show how our model can be used to improve contentbased retrieval of jazz songs. We explain why Haskell is the perfect match for these tasks, and compare our implementation to an earlier solution in Java. We also point out shortcomings of the language and libraries that limit our work, and discuss future developments which may ameliorate our solution.
p428
aVMost interactive theorem provers provide support for some form of usercustomizable proof automation. In a number of popular systems, such as Coq and Isabelle, this automation is achieved primarily through tactics, which are programmed in a separate language from that of the prover's base logic. While tactics are clearly useful in practice, they can be difficult to maintain and compose because, unlike lemmas, their behavior cannot be specified within the expressive type system of the prover itself. We propose a novel approach to proof automation in Coq that allows the user to specify the behavior of custom automated routines in terms of Coq's own type system. Our approach involves a sophisticated application of Coq's canonical structures, which generalize Haskell type classes and facilitate a flexible style of dependentlytyped logic programming. Specifically, just as Haskell type classes are used to infer the canonical implementation of an overloaded term at a given type, canonical structures can be used to infer the canonical proof of an overloaded lemma for a given instantiation of its parameters. We present a series of design patterns for canonical structure programming that enable one to carefully and predictably coax Coq's type inference engine into triggering the execution of usersupplied algorithms during unification, and we illustrate these patterns through several realistic examples drawn from Hoare Type Theory. We assume no prior knowledge of Coq and describe the relevant aspects of Coq type inference from first principles.
p429
aVBehavioral contracts are embraced by software engineers because they document module interfaces, detect interface violations, and help identify faulty modules (packages, classes, functions, etc). This paper extends prior higherorder contract systems to also express and enforce temporal properties, which are common in software systems with imperative state, but which are mostly left implicit or are at best informally specified. The paper presents both a programmatic contract API as well as a temporal contract language, and reports on experience and performance results from implementing these contracts in Racket. Our development formalizes module behavior as a trace of events such as function calls and returns. Our contract system provides both noninterference (where contracts cannot influence correct executions) and also a notion of completeness (where contracts can enforce any decidable, prefixclosed predicate on event traces).
p430
aVWe present a functional approach to parsing unrestricted contextfree grammars based on Brzozowski's derivative of regular expressions. If we consider contextfree grammars as recursive regular expressions, Brzozowski's equational theory extends without modification to contextfree grammars (and it generalizes to parser combinators). The supporting actors in this story are three concepts familiar to functional programmers  laziness, memoization and fixed points; these allow Brzozowski's original equations to be transliterated into purely functional code in about 30 lines spread over three functions. Yet, this almost impossibly brief implementation has a drawback: its performance is sour  in both theory and practice. The culprit? Each derivative can double the size of a grammar, and with it, the cost of the next derivative. Fortunately, much of the new structure inflicted by the derivative is either dead on arrival, or it dies after the very next derivative. To eliminate it, we once again exploit laziness and memoization to transliterate an equational theory that prunes such debris into working code. Thanks to this compaction, parsing times become reasonable in practice. We equip the functional programmer with two equational theories that, when combined, make for an abbreviated understanding and implementation of a system for parsing contextfree languages.
p431
aVOne of the appeals of pure functional programming is that it is so amenable to equational reasoning. One of the problems of pure functional programming is that it rules out computational effects. Moggi and Wadler showed how to get round this problem by using monads to encapsulate the effects, leading in essence to a phase distinction  a pure functional evaluation yielding an impure imperative computation. Still, it has not been clear how to reconcile that phase distinction with the continuing appeal of functional programming; does the impure imperative part become inaccessible to equational reasoning? We think not; and to back that up, we present a simple axiomatic approach to reasoning about programs with computational effects.
p432
aVMotivated by developing a memory management system that allows functional languages to seamlessly interoperate with C, we propose an efficient nonmoving garbage collection algorithm based on bitmap marking and report its implementation and performance evaluation. In our method, the heap consists of subheaps Hi | c \u2264 i \u2264 B of exponentially increasing allocation sizes (Hi for 2i bytes) and a special subheap for exceptionally large objects. Actual space for each subheap is dynamically allocated and reclaimed from a pool of fixed size allocation segments. In each allocation segment, the algorithm maintains a bitmap representing the set of live objects. Allocation is done by searching for the next free bit in the bitmap. By adding metalevel bitmaps that summarize the contents of bitmaps hierarchically and maintaining the current bit position in the bitmap hierarchy, the next free bit can be found in a small constant time for most cases, and in log32(segmentSize) time in the worst case on a 32bit architecture. The collection is done by clearing the bitmaps and tracing live objects. The algorithm can be extended to generational GC by maintaining multiple bitmaps for the same heap space. The proposed method does not require compaction and objects are not moved at all. This property is significant for a functional language to interoperate with C, and it should also be beneficial in supporting multiple native threads. The proposed method has been implemented in a fullscale Standard ML compiler. Our benchmark tests show that our nonmoving collector performs as efficiently as a generational copying collector designed for functional languages.
p433
aVCreating correct hardware is hard. Though there is much talk of using formal and semiformal methods to develop designs and implementations, in practice most implementations are written without the support of any formal or semiformal methodology. Having such a methodology brings many benefits, including improved likelihood of a correct implementation, lowering the cost of design exploration and lowering the cost of certification. In this paper, we introduce a semi formal methodology for connecting executable specifications written in the functional language Haskell to efficient VHDL implementations. The connection is performed by manual edits, using semiformal equational reasoning facilitated by the worker/wrapper transformation, and directed using commutable functors. We explain our methodology on a fullscale example, an efficient LowDensity Parity Check forward error correcting code, which has been implemented on a Virtex5 FPGA.
p434
aVAbramsky's Geometry of Interaction interpretation (GoI) is a logicaldirected way to reconcile the process and functional views of computation, and can lead to a dataflowstyle semantics of programming languages that is both operational (i.e. effective) and denotational (i.e. inductive on the language syntax). The key idea of Ghica's Geometry of Synthesis (GoS) approach is that for certain programming languages (namely Reynolds's affine Syntactic Control of Interference  SCI) the GoI processeslike interpretation of the language can be given a finitary representation, for both internal state and tokens. A physical realisation of this representation becomes a semanticsdirected compiler for SCI into hardware. In this paper we examine the issue of compiling affine recursive programs into hardware using the GoS method. We give syntax and compilation techniques for unfolding recursive computation in space or in time and we illustrate it with simple benchmarkstyle examples. We examine the performance of the benchmarks against conventional CPUbased execution models.
p435
aVThe Mendler style catamorphism (which corresponds to weak induction) always terminates even for negative inductive datatypes. The Mendler style histomorphism (which corresponds to strong induction) is known to terminate for positive inductive datatypes. To our knowledge, the literature is silent on its termination properties for negative datatypes. In this paper, we prove that histomorphisms do not always termintate by showing a counterexample. We also enrich the Mendler collection of recursion combinators by defining a new form of Mendler style catamorphism (msfcata), which terminates for all inductive datatypes, that is more expressive than the original. We organize the collection of combinators by placing them into a hierarchy of ever increasing generality, and describing the termination properties of each point on the hierarchy. We also provide many examples (including a case study on a negative inductive datatype), which illustrate both the expressive power and beauty of the Mendler style. One lesson we learn from this work is that weak induction applies to negative inductive datatypes but strong induction is problematic. We provide a proof of weak induction by exhibiting an embedding of our new combinator into F\u03c9. We pose the open question: Is there a safe way to apply strong induction to negative inductive datatypes?
p436
aVSelfinterpreters can be roughly divided into two sorts: selfrecognisers that recover the input program from a canonical representation, and selfenactors that execute the input program. Major progress for staticallytyped languages was achieved in 2009 by Rendel, Ostermann, and Hofer who presented the first typed selfrecogniser that allows representations of different terms to have different types. A key feature of their type system is a type:type rule that renders the kind system of their language inconsistent. In this paper we present the first staticallytyped language that not only allows representations of different terms to have different types, and supports a selfrecogniser, but also supports a selfenactor. Our language is a factorisation calculus in the style of Jay and GivenWilson, a combinatory calculus with a factorisation operator that is powerful enough to support the patternmatching functions necessary for a selfinterpreter. This allows us to avoid a type:type rule. Indeed, the types of System F are sufficient. We have implemented our approach and our experiments support the theory.
p437
aVWe report on the design and implementation of a programming tool, DynaMoW, to control interactive and incremental mathematical calculations to be presented on the web. This tool is implemented as a language extension of OCaml using Camlp4. Fragments of mathematical code written for a computeralgebra system as well as fragments of mathematical web documents are embedded directly and naturally inside OCaml code. A DynaMoWbased application is made of independent web services, whose parameter types are checked by the OCaml extension. The approach is illustrated by two implementations of online mathematical encyclopedias on top of DynaMoW.
p438
aVDistributed applications are difficult to program reliably and securely. Dependently typed functional languages promise to prevent broad classes of errors and vulnerabilities, and to enable program verification to proceed sidebyside with development. However, as recursion, effects, and rich libraries are added, using types to reason about programs, specifications, and proofs becomes challenging. We present F*, a fullfledged design and implementation of a new dependently typed language for secure distributed programming. Unlike prior languages, F* provides arbitrary recursion while maintaining a logically consistent core; it enables modular reasoning about state and other effects using affine types; and it supports proofs of refinement properties using a mixture of cryptographic evidence and logical proof terms. The key mechanism is a new kind system that tracks several sublanguages within F* and controls their interaction. F* subsumes two previous languages, F7 and Fine. We prove type soundness (with proofs mechanized in Coq) and logical consistency for F*. We have implemented a compiler that translates F* to .NET bytecode, based on a prototype for Fine. F* provides access to libraries for concurrency, networking, cryptography, and interoperability with C#, F#, and the other .NET languages. The compiler produces verifiable binaries with 60% code size overhead for proofs and types, as much as a 45x improvement over the Fine compiler, while still enabling efficient bytecode verification. To date, we have programmed and verified more than 20,000 lines of F* including (1) new schemes for multiparty sessions; (2) a zeroknowledge privacypreserving payment protocol; (3) a provenanceaware curated database; (4) a suite of 17 webbrowser extensions verified for authorization properties; and (5) a cloudhosted multitier web application with a verified reference monitor.
p439
aVModern networks provide a variety of interrelated services including routing, traffic monitoring, load balancing, and access control. Unfortunately, the languages used to program today's networks lack modern features  they are usually defined at the low level of abstraction supplied by the underlying hardware and they fail to provide even rudimentary support for modular programming. As a result, network programs tend to be complicated, errorprone, and difficult to maintain. This paper presents Frenetic, a highlevel language for programming distributed collections of network switches. Frenetic provides a declarative query language for classifying and aggregating network traffic as well as a functional reactive combinator library for describing highlevel packetforwarding policies. Unlike prior work in this domain, these constructs are  by design  fully compositional, which facilitates modular reasoning and enables code reuse. This important property is enabled by Frenetic's novel runtime system which manages all of the details related to installing, uninstalling, and querying lowlevel packetprocessing rules on physical switches. Overall, this paper makes three main contributions: (1) We analyze the stateofthe art in languages for programming networks and identify the key limitations; (2) We present a language design that addresses these limitations, using a series of examples to motivate and validate our choices; (3) We describe an implementation of the language and evaluate its performance on several benchmarks.
p440
aVA filestore is a structured collection of data files housed in a conventional hierarchical file system. Many applications use filestores as a poorman's database, and the correct execution of these applications requires that the collection of files, directories, and symbolic links stored on disk satisfy a variety of precise invariants. Moreover, all of these structures must have acceptable ownership, permission, and timestamp attributes. Unfortunately, current programming languages do not provide support for documenting assumptions about filestores, detecting errors in them, or safely loading from and storing to them. This paper describes the design, implementation, and semantics of Forest, a new domainspecific language for describing filestores. The language uses a typebased metaphor to specify the expected structure, attributes, and invariants of filestores. Forest generates loading and storing functions that make it easy to connect data on disk to an isomorphic representation in memory that can be manipulated as if it were any other data structure. Forest also generates metadata that describes the degree to which the structures on the disk conform to the specification, making error detection easy. In a nutshell, Forest extends the rigorous discipline of typed programming languages to the untyped world of file systems. We have implemented Forest as an embedded domainspecific language in Haskell. In addition to generating infrastructure for reading, writing and checking file systems, our implementation generates type class instances that make it easy to build generic tools that operate over arbitrary filestores. We illustrate the utility of this infrastructure by building a file system visualizer, a file access checker, a generic query interface, descriptiondirected variants of several standard UNIX shell tools and (circularly) a simple Forest description inference engine. Finally, we formalize a core fragment of Forest in a semantics inspired by classical tree logics and prove roundtripping laws showing that the loading and storing functions behave sensibly.
p441
aVIntegrating a database query language into a programming language is becoming increasingly important in recently emerging highlevel cloud computing and other applications, where efficient and sophisticated data manipulation is required during computation. This paper reports on seamless integration of SQL into SML#  an extension of Standard ML. In the integrated language, the type system always infers a principal type for any type consistent SQL expression. This makes SQL queries firstclass citizens, which can be freely combined with any other language constructs definable in Standard ML. For a program involving SQL queries, the compiler separates SQL queries and delegates their evaluation to a database server, e.g. PostgreSQL or MySQL in the currently implemented version. The type system of our language is largely based on Machiavelli, which demonstrates that ML with record polymorphism can represent type structure of SQL. In order to develop a practical language, however, a number of technical challenges have to be overcome, including static enforcement of server connection consistency, proper treatment of overloaded SQL primitives, query compilation, and runtime connection management. This paper describes the necessary extensions to the type system and compilation, and reports on the details of its implementation.
p442
aVMany useful programming constructions can be expressed as monads. Examples include probabilistic modeling, functional reactive programming, parsing, and information flow tracking, not to mention effectful functionality like state and I/O. In this paper, we present a typebased rewriting algorithm to make programming with arbitrary monads as easy as using ML's builtin support for state and I/O. Developers write programs using monadic values of type m \u03c4 as if they were of type \u03c4, and our algorithm inserts the necessary binds, units, and monadtomonad morphisms so that the program type checks. Our algorithm, based on Jones' qualified types, produces principal types. But principal types are sometimes problematic: the program's semantics could depend on the choice of instantiation when more than one instantiation is valid. In such situations we are able to simplify the types to remove any ambiguity but without adversely affecting typability; thus we can accept strictly more programs. Moreover, we have proved that this simplification is efficient (linear in the number of constraints) and coherent: while our algorithm induces a particular rewriting, all related rewritings will have the same semantics. We have implemented our approach for a core functional language and applied it successfully to simple examples from the domains listed above, which are used as illustrations throughout the paper.
p443
aVDe Bruijn indices are a well known technique for programming with names and binders. They provide a representation that is both simple and canonical. However, programming errors tend to be really easy to make. We propose a safer programming interface implemented as a library. Whereas indexing the types of names and terms by a numerical bound is a famous technique, we index them by worlds, a different notion of index that is both finer and more abstract. While being more finely typed, our approach incurs no loss of expressiveness or efficiency. Via parametricity we obtain properties about functions polymorphic on worlds. For instance, welltyped worldpolymorphic functions over open \u03bbterms commute with any renaming of the free variables. Our whole development is conducted within Agda, from the code of the library, to its soundness proof and the properties of external functions. The soundness of our library is demonstrated via the construction of a logical relations argument.
p444
aVImplementors of compilers, program refactorers, theorem provers, proof checkers, and other systems that manipulate syntax know that dealing with name binding is difficult to do well. Operations such as \u03b1equivalence and captureavoiding substitution seem simple, yet subtle bugs often go undetected. Furthermore, their implementations are tedious, requiring "boilerplate" code that must be updated whenever the object language definition changes. Many researchers have therefore sought to specify binding syntax declaratively, so that tools can correctly handle the details behind the scenes. This idea has been the inspiration for many new systems (such as Beluga, Delphin, FreshML, FreshOCaml, C\u03b1ml, FreshLib, and Ott) but there is still room for improvement in expressivity, simplicity and convenience. In this paper, we present a new domainspecific language, Unbound, for specifying binding structure. Our language is particularly expressive  it supports multiple atom types, pattern binders, type annotations, recursive binders, and nested binding (necessary for telescopes, a feature found in dependentlytyped languages). However, our specification language is also simple, consisting of just five basic combinators. We provide a formal semantics for this language derived from a locally nameless representation and prove that it satisfies a number of desirable properties. We also present an implementation of our binding specification language as a GHC Haskell library implementing an embedded domain specific language (EDSL). By using Haskell type constructors to represent binding combinators, we implement the EDSL succinctly using datatypegeneric programming. Our implementation supports a number of features necessary for practical programming, including flexibility in the treatment of userdefined types, besteffort name preservation (for error messages), and integration with Haskell's monad transformer library.
p445
aVWe characterize the data type of terms with bindings, freshness and substitution, as an initial model in a suitable Horn theory. This characterization yields a convenient recursive definition principle, which we have formalized in Isabelle/HOL and employed in a series of case studies taken from the \u03bbcalculus literature.
p446
aVSay you want to prove something about an infinite datastructure, such as a stream or an infinite tree, but you would rather not subject yourself to coinduction. The unique fixedpoint principle is an easytouse, calculational alternative. The proof technique rests on the fact that certain recursion equations have unique solutions; if two elements of a coinductive type satisfy the same equation of this kind, then they are equal. In this paper we precisely characterize the conditions that guarantee a unique solution. Significantly, we do so not with a syntactic criterion, but with a semantic one that stems from the categorical notion of naturality. Our development is based on distributive laws and bialgebras, and draws heavily on Turi and Plotkin's pioneering work on mathematical operational semantics. Along the way, we break down the design space in two dimensions, leading to a total of nine points. Each gives rise to varying degrees of expressiveness, and we will discuss three in depth. Furthermore, our development is generic in the syntax of equations and in the behaviour they encode  we are not caged in the world of streams.
p447
aVLinearity is a multifaceted and ubiquitous notion in the analysis and the development of programming language concepts. We study linearity in a denotational perspective by picking out programs that correspond to linear functions between coherence spaces. We introduce a language, named SlPCF*, that increases the higherorder expressivity of a linear core of PCF by means of new operators related to exception handling and parallel evaluation. SlPCF* allows us to program all the finite elements of the model and, consequently, it entails a full abstraction result that makes the reasoning on the equivalence between programs simpler. Denotational linearity provides also crucial information for the operational evaluation of programs. We formalize two evaluation machineries for the language. The first one is an abstract and concise operational semantics designed with the aim of explaining the new operators, and is based on an infinitebranching search of the evaluation space. The second one is more concrete and it prunes such a space, by exploiting the linear assumptions. This can also be regarded as a base for an implementation.
p448
aVThe third listhomomorphism theorem says that a function is a list homomorphism if it can be described as an instance of both a foldr and a foldl. We prove a dual theorem for unfolds and generalise both theorems to trees: if a function generating a list can be described both as an unfoldr and an unfoldl, the list can be generated from the middle, and a function that processes or builds a tree both upwards and downwards may independently process/build a subtree and its onehole context. The pointfree, relational formalism helps to reveal the beautiful symmetry hidden in the theorem.
p449
aVA bidirectional transformation is a pair of mappings between source and view data objects, one in each direction. When the view is modified, the source is updated accordingly. The key to handling large data objects that are subject to relatively small modifications is to process the updates incrementally. Incrementality has been explored in the semistructured settings of relational databases and graph transformations; this flexibility in structure makes it relatively easy to divide the data into separate parts that can be transformed and updated independently. The same is not true if the data is to be encoded with more generalpurpose algebraic datatypes, with transformations defined as functions: dividing data into welltyped separate parts is tricky, and recursions typically create interdependencies. In this paper, we study transformations that support incremental updates, and devise a constructive process to achieve this incrementality.
p450
aVMost major OS kernels today run on multiprocessor systems and are preemptive: it is possible for a process running in the kernel mode to get descheduled. Existing modular techniques for verifying concurrent code are not directly applicable in this setting: they rely on scheduling being implemented correctly, and in a preemptive kernel, the correctness of the scheduler is interdependent with the correctness of the code it schedules. This interdependency is even stronger in mainstream kernels, such as Linux, FreeBSD or XNU, where the scheduler and processes interact in complex ways. We propose the first logic that is able to decompose the verification of preemptive multiprocessor kernel code into verifying the scheduler and the rest of the kernel separately, even in the presence of complex interdependencies between the two components. The logic hides the manipulation of control by the scheduler when reasoning about preemptable code and soundly inherits proof rules from concurrent separation logic to verify it threadmodularly. This is achieved by establishing a novel form of refinement between an operational semantics of the real machine and an axiomatic semantics of OS processes, where the latter assumes an abstract machine with each process executing on a separate virtual CPU. The refinement is local in the sense that the logic focuses only on the relevant state of the kernel while verifying the scheduler. We illustrate the power of our logic by verifying an example scheduler, modelled on the one from Linux 2.6.11.
p451
aVIn previous work, we introduced an approach to program verification based on characteristic formulae. The approach consists of generating a higherorder logic formula from the source code of a program. This characteristic formula is constructed in such a way that it gives a sound and complete description of the semantics of that program. The formula can thus be exploited in an interactive proof assistant to formally verify that the program satisfies a particular specification. This previous work was, however, only concerned with purelyfunctional programs. In the present paper, we describe the generalization of characteristic formulae to an imperative programming language. In this setting, characteristic formulae involve specifications expressed in the style of Separation Logic. They also integrate the frame rule, which enables local reasoning. We have implemented a tool based on characteristic formulae. This tool, called CFML, supports the verification of imperative Caml programs using the Coq proof assistant. Using CFML, we have formally verified nontrivial imperative algorithms, as well as CPS functions, higherorder iterators, and programs involving higherorder stores.
p452
aVLanguagebased security relies on the assumption that all potential attacks follow the rules of the language in question. When programs are compiled into a different language, this is true only if the translation process preserves observational equivalence. To prove that a translation preserves equivalence, one must show that if two program fragments cannot be distinguished by any source context, then their translations cannot be distinguished by any target context. Informally, target contexts must be no more powerful than source contexts, i.e., for every target context there exists a source context that "behaves the same." This seems to amount to being able to "backtranslate" arbitrary target terms. However, that is simply not viable for practical compilers where the target language is lowerlevel and, thus, contains expressions that have no source equivalent. In this paper, we give a CPS translation from a less expressive source language (STLC) to a more expressive target language (System F) and prove that the translation preserves observational equivalence. The key to our equivalencepreserving compilation is the choice of the right type translation: a source type \u03c3 mandates a set of behaviors and we must ensure that its translation \u03c3+ mandates semantically equivalent behaviors at the target level. Based on this type translation, we demonstrate how to prove that for every target term of type \u03c3+, there exists an equivalent source term of type \u03c3 even when subterms of the target term are not necessarily "backtranslatable" themselves. A key novelty of our proof, resulting in a pleasant proof structure, is that it leverages a multilanguage semantics where source and target terms may interoperate.
p453
aVThe ecology of Earth's first large organisms is an unsolved problem in palaeontology. This experience report discusses the determination of which ecosystems could have been feasible, by considering the biological feedbacks within them. Haskell was used to model the ecosystems for these first large organisms  the Ediacara biota. For verification of the results, the statistical language R was used. Neither Haskell nor R would have been sufficient for this work  Haskell's libraries for statistics are weak, while R lacks the structure for expressing algorithms in a maintainable manner. This work is the first to quantify all feedback loops in an ecosystem, and has generated considerable interest from both the ecological and palaeontological communities.
p454
aVWe make monadic components more reusable and robust to changes by employing two new techniques for virtualizing the monad stack: the monad zipper and monad views. The monad zipper is a higherorder monad transformer that creates virtual monad stacks by ignoring particular layers in a concrete stack. Monad views provide a general framework for monad stack virtualization: they take the monad zipper one step further and integrate it with a wide range of other virtualizations. For instance, particular views allow restricted access to monads in the stack. Furthermore, monad views provide components with a callbyreferencelike mechanism for accessing particular layers of the monad stack. With our two new mechanisms, the monadic effects required by components no longer need to be literally reflected in the concrete monad stack. This makes these components more reusable and robust to changes.
p455
aVWe give a denotational model for graphical user interface (GUI) programming using the Cartesian closed category of ultrametric spaces. The ultrametric structure enforces causality restrictions on reactive systems and allows wellfounded recursive definitions by a generalization of guardedness. We capture the arbitrariness of user input (e.g., a user gets to decide the stream of clicks she sends to a program) by making use of the fact that the closed subsets of an ultrametric space themselves form an ultrametric space, allowing us to interpret nondeterminism with a "powerspace" monad. Algebras for the powerspace monad yield a model of intuitionistic linear logic, which we exploit in the definition of a mixed linear/nonlinear domainspecific language for writing GUI programs. The nonlinear part of the language is used for writing reactive streamprocessing functions whilst the linear sublanguage naturally captures the generativity and usage constraints on the various linear objects in GUIs, such as the elements of a DOM or scene graph. We have implemented this DSL as an extension to OCaml, and give examples demonstrating that programs in this style can be short and readable.
p456
aVWe present a technique, based on the use of firstclass control operators, enabling programs to maintain and invoke rollback logs for sequences of reversible effects. Our technique is modular, in that it provides complete separation between some library of effectful operations, and a client, "driver" program which invokes and rolls back sequences of these operations. In particular, the checkpoint mechanism, which is entirely encapsulated within the effect library, logs not only the library's effects, but also the client's control state. Thus, logging and rollback can be almost completely transparent to the client code. This separation of concerns manifests itself nicely when we must implement software with sophisticated error handling. We illustrate with two examples that exploit the architecture to disentangle some core parsing task from its error management. The parser code is completely separate from the errorcorrection code, although the two components are deeply intertwined at run time.
p457
aVPushdown models are better than controlflow graphs for higherorder flow analysis. They faithfully model the call/return structure of a program, which results in fewer spurious flows and increased precision. However, pushdown models require that calls and returns in the analyzed program nest properly. As a result, they cannot be used to analyze language constructs that break call/return nesting such as generators, coroutines, call/cc, etc. In this paper, we extend the CFA2 flow analysis to create the first pushdown flow analysis for languages with firstclass control. We modify the abstract semantics of CFA2 to allow continuations to escape to, and be restored from, the heap. We then present a summarization algorithm that handles escaping continuations via a new kind of summary edge. We prove that the algorithm is sound with respect to the abstract semantics.
p458
aVWe present a type system with subtyping for firstclass delimited continuations that generalizes Danvy and Filinski's type system for shift and reset by maintaining explicit information about the types of contexts in the metacontext. We exploit this generalization by considering the control operators known as shift0 and reset0 that can access arbitrary contexts in the metacontext. We use subtyping to control the level of information about the metacontext the expression actually requires and in particular to coerce pure expressions into effectful ones. For this type system we prove strong type soundness and termination of evaluation and we present a provably correct type reconstruction algorithm. We also introduce two CPS translations for shift0 and reset0: one targeting the untyped lambda calculus, and another  typedirected  targeting the simplytyped lambda calculus. The latter translation preserves typability and is selective in that it keeps pure expressions in direct style.
p459
aVI explore programming with the dependently typed functional language, AGDA. I present the progress which AGDA has made, demonstrate its usage in a small development, reflect critically on the state of the art, and speculate about the way ahead. I do not seek to persuade you to adopt AGDA as your primary tool for systems development, but argue that AGDA stimulates new useful ways to think about programming problems and deserves not just curiosity but interest, support and contribution.
p460
aVWe describe the Funlogic system which extends a functional language with existentially quantified declarations. An existential declaration introduces a variable and a set of constraints that its value should meet. Existential variables are bound to conforming values by a decision procedure. Funlogic embeds multiple external decision procedures using a common framework. Design principles for embedding decision procedures are developed and illustrated for three different decision procedures from widely varying domains.
p461
aVProgramming with dependent types is a blessing and a curse. It is a blessing to be able to bake invariants into the definition of datatypes: we can finally write correctbyconstruction software. However, this extreme accuracy is also a curse: a datatype is the combination of a structuring medium together with a special purpose logic. These domainspecific logics hamper any effort of code reuse among similarly structured data. In this paper, we exorcise our datatypes by adapting the notion of ornament to our universe of inductive families. We then show how code reuse can be achieved by ornamenting functions. Using these functional ornaments, we capture the relationship between functions such as the addition of natural numbers and the concatenation of lists. With this knowledge, we demonstrate how the implementation of the former informs the implementation of the latter: the user can ask the definition of addition to be lifted to lists and she will only be asked the details necessary to carry on adding lists rather than numbers. Our presentation is formalised in a type theory with a universe of datatypes and all our constructions have been implemented as generic programs, requiring no extension to the type theory.
p462
aVThe higherorder logic found in proof assistants such as Coq and various HOL systems provides a convenient setting for the development and verification of pure functional programs. However, to efficiently run these programs, they must be converted (or "extracted") to functional programs in a programming language such as ML or Haskell. With current techniques, this step, which must be trusted, relates similar looking objects that have very different semantic definitions, such as the settheoretic model of a logic and the operational semantics of a programming language. In this paper, we show how to increase the trustworthiness of this step with an automated technique. Given a functional program expressed in higherorder logic, our technique provides the corresponding program for a functional language defined with an operational semantics, and it provides a mechanically checked theorem relating the two. This theorem can then be used to transfer verified properties of the logical function to the program. We have implemented our technique in the HOL4 theorem prover, translating functions to a core subset of Standard ML, and have applied it to examples including functional data structures, a parser generator, cryptographic algorithms, and a garbage collector.
p463
aVThe operational semantics of a partial, functional language is often given as a relation rather than as a function. The latter approach is arguably more natural: if the language is functional, why not take advantage of this when defining the semantics? One can immediately see that a functional semantics is deterministic and, in a constructive setting, computable. This paper shows how one can use the coinductive partiality monad to define bigstep or smallstep operational semantics for lambdacalculi and virtual machines as total, computable functions (total definitional interpreters). To demonstrate that the resulting semantics are useful type soundness and compiler correctness results are also proved. The results have been implemented and checked using Agda, a dependently typed programming language and proof assistant.
p464
aVToday, all highperformance computer architectures are parallel and heterogeneous; a combination of multiple CPUs, GPUs and specialized processors. This creates a complex programming problem for application developers. Domainspecific languages (DSLs) are a promising solution to this problem because they provide an avenue for applicationspecific abstractions to be mapped directly to low level architecturespecific programming models providing high programmer productivity and high execution performance. In this talk I will describe our approach to building high performance DSLs, which is based on embedding in Scala, lightwieght modular staging and a DSL infrastructure called Delite. I will describe how we transform impure functional programs into efficient firstorder lowlevel code using domain specific optimization, parallelism optimization, locality optimization, scalar optimization, and architecturespecific code generation. All optimizations and transformations are implemented in an extensible DSL compiler architecture that minimizes the programmer effort required to develop a new DSL.
p465
aVIn this paper, we use types for ensuring that programs involving streams are wellbehaved. We extend pure type systems with a type constructor for streams, a modal operator next and a fixed point operator for expressing corecursion. This extension is called Pure Type Systems with Corecursion (CoPTS). The typed lambda calculus for reactive programs defined by Krishnaswami and Benton can be obtained as a CoPTS. CoPTSs allow us to study a wide range of typed lambda calculi extended with corecursion using only one framework. In particular, we study this extension for the calculus of constructions which is the underlying formal language of Coq. We use the machinery of infinitary rewriting and formalise the idea of wellbehaved programs using the concept of infinitary normalisation. The set of finite and infinite terms is defined as a metric completion. We establish a precise connection between the modal operator (\u2022 A) and the metric at a syntactic level by relating a variable of type (\u2022 A) with the depth of all its occurrences in a term. This syntactic connection between the modal operator and the depth is the key to the proofs of infinitary weak and strong normalisation.
p466
aVWe study the complexity of deciding the equality of infinite objects specified by systems of equations, and of infinite objects specified by \u03bbterms. For equational specifications there are several natural notions of equality: equality in all models, equality of the sets of solutions, and equality of normal forms for productive specifications. For \u03bbterms we investigate Bhmtree equality and various notions of observational equality. We pinpoint the complexity of each of these notions in the arithmetical or analytical hierarchy. We show that the complexity of deciding equality in all models subsumes the entire analytical hierarchy. This holds already for the most simple infinite objects, viz. streams over {0,1}, and stands in sharp contrast to the low arithmetical \u03d602completeness of equality of equationally specified streams derived in [17] employing a different notion of equality.
p467
aVEmbedded domainspecific languages (EDSLs) are an approach for quickly building new languages while maintaining the advantages of a rich metalanguage. We argue in this experience report that the "EDSL approach" can surprisingly ease the task of building a highassurance compiler. We do not strive to build a fully formallyverified toolchain, but take a "doityourself" approach to increase our confidence in compilercorrectness without too much effort. Copilot is an EDSL developed by Galois, Inc. and the National Institute of Aerospace under contract to NASA for the purpose of runtime monitoring of flightcritical avionics. We report our experience in using typechecking, QuickCheck, and modelchecking "offtheshelf" to quickly increase confidence in our EDSL toolchain.
p468
aVThe Glasgow Haskell Compiler is an optimizing compiler that expresses and manipulates firstclass equality proofs in its intermediate language. We describe a simple, elegant technique that exploits these equality proofs to support deferred type errors. The technique requires us to treat equality proofs as possiblydivergent terms; we show how to do so without losing either soundness or the zerooverhead cost model that the programmer expects.
p469
aVSecure multiparty computation (SMC) permits a collection of parties to compute a collaborative result, without any of the parties gaining any knowledge about the inputs provided by other parties. Specifications for SMC are commonly presented as boolean circuits, where optimizations come mostly from reducing the number of multiplyoperations (including andgates)  these are the operations which incur significant cost, either in computation overhead or in communication between the parties. Instead, we take a languageoriented approach, and consequently are able to explore many other kinds of optimizations. We present an efficient and general purpose SMC tablelookup algorithm that can serve as a direct alternative to circuits. Looking up a private (i.e. shared, or encrypted) nbit argument in a public table requires log(n) paralleland operations. We use the advanced encryption standard algorithm (AES) as a driving motivation, and by introducing different kinds of parallelization techniques, produce the fastest current SMC implementation of AES, improving the best previously reported results by well over an order of magnitude.
p470
aVWe present VeriStar, a verified theorem prover for a decidable subset of separation logic. Together with VeriSmall [3], a provedsound Smallfootstyle program analysis for C minor, VeriStar demonstrates that fully machinechecked static analyses equipped with efficient theorem provers are now within the reach of formal methods. As a pair, VeriStar and VeriSmall represent the first application of the Verified Software Toolchain [4], a tightly integrated collection of machineverified program logics and compilers giving foundational correctness guarantees. VeriStar is (1) purely functional, (2) machinechecked, (3) endtoend, (4) efficient and (5) modular. By purely functional, we mean it is implemented in Gallina, the pure functional programming language embedded in the Coq theorem prover. By machinechecked, we mean it has a proof in Coq that when the prover says "valid", the checked entailment holds in a provedsound separation logic for C minor. By endtoend, we mean that when the static analysis+theorem prover says a C minor program is safe, the program will be compiled to a semantically equivalent assembly program that runs on real hardware. By efficient, we mean that the prover implements a stateoftheart algorithm for deciding heap entailments and uses highly tuned verified functional data structures. By modular, we mean that VeriStar can be retrofitted to other static analyses as a plugcompatible entailment checker and its soundness proof can easily be ported to other separation logics.
p471
aVWhen termination of a program is observable by an adversary, confidential information may be leaked by terminating accordingly. While this termination covert channel has limited bandwidth for sequential programs, it is a more dangerous source of information leakage in concurrent settings. We address concurrent termination and timing channels by presenting a dynamic informationflow control system that mitigates and eliminates these channels while allowing termination and timing to depend on secret values. Intuitively, we leverage concurrency by placing such potentially sensitive actions in separate threads. While termination and timing of these threads may expose secret values, our system requires any thread observing these properties to raise its informationflow label accordingly, preventing leaks to lowerlabeled contexts. We implement this approach in a Haskell library and demonstrate its applicability by building a web server that uses informationflow control to restrict untrusted web applications.
p472
aVWe present a framework of dynamic programming combinators that provides a highlevel environment to describe the recursions typical of dynamic programming over sequence data in a style very similar to algebraic dynamic programming (ADP). Using a combination of typelevel programming and stream fusion leads to a substantial increase in performance, without sacrificing much of the convenience and theoretical underpinnings of ADP. We draw examples from the field of computational biology, more specifically RNA secondary structure prediction, to demonstrate how to use these combinators and what differences exist between this library, ADP, and other approaches. The final version of the combinator library allows writing algorithms with performance close to handoptimized C code.
p473
aVHaskell gives computational biologists the flexibility and rapid prototyping of a scripting language, plus the performance of native code. In our experience, higherorder functions, lazy evaluation, and monads really worked, but profiling and debugging presented obstacles. Also, Haskell libraries vary greatly: memoization combinators and parallelevaluation strategies helped us a lot, but other, nameless libraries mostly got in our way. Despite the obstacles and the uncertain quality of some libraries, Haskell's ecosystem made it easy for us to develop new algorithms in computational biology.
p474
aVModern parallel computing hardware demands increasingly specialized attention to the details of scheduling and load balancing across heterogeneous execution resources that may include GPU and cloud environments, in addition to traditional CPUs. Many existing solutions address the challenges of particular resources, but do so in isolation, and in general do not compose within larger systems. We propose a general, composable abstraction for execution resources, along with a continuationbased metascheduler that harnesses those resources in the context of a deterministic parallel programming library for Haskell. We demonstrate performance benefits of combined CPU/GPU scheduling over either alone, and of combined multithreaded/distributed scheduling over existing distributed programming approaches for Haskell.
p475
aVGraphics processing units (GPUs) provide both memory bandwidth and arithmetic performance far greater than that available on CPUs but, because of their SingleInstructionMultipleData (SIMD) architecture, they are hard to program. Most of the programs ported to GPUs thus far use traditional datalevel parallelism, performing only operations that operate uniformly over vectors. NESL is a firstorder functional language that was designed to allow programmers to write irregularparallel programs  such as parallel divideandconquer algorithms  for widevector parallel computers. This paper presents our port of the NESL implementation to work on GPUs and provides empirical evidence that nested dataparallelism (NDP) on GPUs significantly outperforms CPUbased implementations and matches or beats newer GPU languages that support only flat parallelism. While our performance does not match that of handtuned CUDA programs, we argue that the notational conciseness of NESL is worth the loss in performance. This work provides the first language implementation that directly supports NDP on a GPU.
p476
aVExisting approaches to higherorder vectorisation, also known as flattening nested data parallelism, do not preserve the asymptotic work complexity of the source program. Straightforward examples, such as sparse matrixvector multiplication, can suffer a severe blowup in both time and space, which limits the practicality of this method. We discuss why this problem arises, identify the mishandling of index space transforms as the root cause, and present a solution using a refined representation of nested arrays. We have implemented this solution in Data Parallel Haskell (DPH) and present benchmarks showing that realistic programs, which used to suffer the blowup, now have the correct asymptotic work complexity. In some cases, the asymptotic complexity of the vectorised program is even better than the original.
p477
aVWe rely on a computational infrastructure that is a densely interwined mass of software and hardware: programming languages, network protocols, operating systems, and processors. It has accumulated great complexity, from a combination of engineering design decisions, contingent historical choices, and sheer scale, yet it is defined at best by prose specifications, or, all too often, just by the common implementations. Can we do better? More specifically, can we apply rigorous methods to this mainstream infrastructure, taking the accumulated complexity seriously, and if we do, does it help? My colleagues and I have looked at these questions in several contexts: the TCP/IP network protocols with their Sockets API; programming language design, including the Java module system and the C11/C++11 concurrency model; the hardware concurrency behaviour of x86, IBM POWER, and ARM multiprocessors; and compilation of concurrent code. In this talk I will draw some lessons from what did and did not succeed, looking especially at the empirical nature of some of the work, at the social process of engagement with the various different communities, and at the mathematical and software tools we used. Domainspecific modelling languages (based on functional programming ideas) and proof assistants were invaluable for working with the large and loose specifications involved: idioms within HOL4 for TCP, our Ott tool for programming language specification, and Owens's Lem tool for portable semantic definitions, with HOL4, Isabelle, and Coq, for the relaxedmemory concurrency semantics work. Our experience with these suggests something of what is needed to make fullscale rigorous semantics a commonplace reality.
p478
aVContinuing a line of work by Abramsky (1994), by Bellin and Scott (1994), and by Caires and Pfenning (2010), among others, this paper presents CP, a calculus in which propositions of classical linear logic correspond to session types. Continuing a line of work by Honda (1993), by Honda, Kubo, and Vasconcelos (1998), and by Gay and Vasconcelos (2010), among others, this paper presents GV, a linear functional language with session types, and presents a translation from GV into CP. The translation formalises for the first time a connection between a standard presentation of session types and linear logic, and shows how a modification to the standard presentation yield a language free from deadlock, where deadlock freedom follows from the correspondence to linear logic.
p479
aVUnmarshalling primitives in statically typed language require, in order to preserve type safety, to dynamically verify the compatibility between the incoming values and the statically expected type. In the context of programming languages based on parametric polymorphism and uniform data representation, we propose a relation of compatibility between (unmarshalled) memory graphs and types. It is defined as constraints over nodes of the memory graph. Then, we propose an algorithm to check the compatibility between a memory graph and a type. It is described as a constraint solver based on a rewriting system. We have shown that the proposed algorithm is sound and semicomplete in presence of algebraic data types, mutable data, polymorphic sharing, cycles, and functional values, however, in its general form, it may not terminate. We have implemented a prototype tailored for the OCaml compiler [17] that always terminates and still seems sufficiently complete in practice.
p480
aVStronglytyped functional languages provide a powerful framework for embedding DomainSpecific Languages (DSLs). However, building typesafe functions defined over an embedded DSL can introduce applicationspecific type constraints that end up being imposed on the DSL data types themselves. At best, these constraints are unwieldy and at worst they can limit the range of DSL expressions that can be built. We present a simple solution to this problem that allows applicationspecific constraints to be specified at the point of use of a DSL expression rather than when the DSL's embedding types are defined. Our solution applies equally to both tagged and tagless representations and, importantly, also works in the presence of higherrank types.
p481
aVWe present techniques for reasoning about constructor classes that (like the monad class) fix polymorphic operations and assert polymorphic axioms. We do not require a logic with firstclass type constructors, firstclass polymorphism, or type quantification; instead, we rely on a domaintheoretic model of the type system in a universal domain to provide these features. These ideas are implemented in the Tycon library for the Isabelle theorem prover, which builds on the HOLCF library of domain theory. The Tycon library provides various axiomatic type constructor classes, including functors and monads. It also provides automation for instantiating those classes, and for defining further subclasses. We use the Tycon library to formalize three Haskell monad transformers: the error transformer, the writer transformer, and the resumption transformer. The error and writer transformers do not universally preserve the monad laws; however, we establish datatype invariants for each, showing that they are valid monads when viewed as abstract datatypes.
p482
aVLanguages with support for metaprogramming, like MetaOCaml, offer a principled approach to code generation by guaranteeing that welltyped metaprograms produce welltyped programs. However, many problem domains where metaprogramming can fruitfully be applied require generating code in languages like C, CUDA, or assembly. Rather than resorting to addhoc code generation techniques, these applications should be directly supported by explicitly heterogeneous metaprogramming languages. We present MetaHaskell, an extension of Haskell 98 that provides modular syntactic and type system support for type safe metaprogramming with multiple object languages. Adding a new object language to MetaHaskell requires only minor modifications to the host language to support typelevel quantification over object language types and propagation of type equality constraints. We demonstrate the flexibility of our approach through three object languages: a core ML language, a linear variant of the core ML language, and a subset of C. All three languages support metaprogramming with open terms and guarantee that welltyped MetaHaskell programs will only produce closed object terms that are welltyped. The essence of MetaHaskell is captured in a type system for a simplified metalanguage. MetaHaskell, as well as all three object languages, are fully implemented in the mhc bytecode compiler.
p483
aVRepresenting a syntax tree using a data type often involves having many similarlooking constructors. Functions operating on such types often end up having many similarlooking cases. Different languages often make use of similarlooking constructions. We propose a generic model of abstract syntax trees capable of representing a wide range of typed languages. Syntactic constructs can be composed in a modular fashion enabling reuse of abstract syntax and syntactic processing within and across languages. Building on previous methods of encoding extensible data types in Haskell, our model is a pragmatic solution to Wadler's "expression problem". Its practicality has been confirmed by its use in the implementation of the embedded language Feldspar.
p484
aVThis paper describes the first successful attempt, of which we are aware, to define an automatic, typebased static analysis of resource bounds for lazy functional programs. Our analysis uses the automatic amortisation approach developed by Hofmann and Jost, which was previously restricted to eager evaluation. In this paper, we extend this work to a lazy setting by capturing the costs of unevaluated expressions in type annotations and by amortising the payment of these costs using a notion of lazy potential. We present our analysis as a proof system for predicting heap allocations of a minimal functional language (including higherorder functions and recursive data types) and define a formal cost model based on Launchbury's natural semantics for lazy evaluation. We prove the soundness of our analysis with respect to the cost model. Our approach is illustrated by a number of representative and nontrivial examples that have been analysed using a prototype implementation of our analysis.
p485
aVIn the static analysis of functional programs, pushdown flow analysis and abstract garbage collection skirt just inside the boundaries of soundness and decidability. Alone, each method reduces analysis times and boosts precision by orders of magnitude. This work illuminates and conquers the theoretical challenges that stand in the way of combining the power of these techniques. The challenge in marrying these techniques is not subtle: computing the reachable control states of a pushdown system relies on limiting access during transition to the top of the stack; abstract garbage collection, on the other hand, needs full access to the entire stack to compute a root set, just as concrete collection does. Introspective pushdown systems resolve this conflict. Introspective pushdown systems provide enough access to the stack to allow abstract garbage collection, but they remain restricted enough to compute controlstate reachability, thereby enabling the sound and precise product of pushdown analysis and abstract garbage collection. Experiments reveal synergistic interplay between the techniques, and the fusion demonstrates "betterthanbothworlds" precision.
p486
aVHigherorder model checking  the model checking of trees generated by higherorder recursion schemes (HORS)  is a natural generalisation of finitestate and pushdown model checking. Recent work has shown that it can serve as a basis for software model checking for functional languages such as ML and Haskell. In this paper, we introduce higherorder recursion schemes with cases (HORSC), which extend HORS with a definitionbycases construct (to express program branching based on data) and nondeterminism (to express abstractions of behaviours). This paper is a study of the universal HORSC model checking problem for deterministic trivial automata: does the automaton accept every tree in the tree language generated by the given HORSC? We first characterise the model checking problem by an intersection type system extended with a carefully restricted form of union types. We then present an algorithm for deciding the model checking problem, which is based on the notion of traversals induced by the fully abstract game semantics of these schemes, but presented as a goaldirected construction of derivations in the intersection and union type system. We view HORSC model checking as a suitable backend engine for an approach to verifying functional programs. We have implemented the algorithm in a tool called TravMC, and demonstrated its effectiveness on a test suite of programs, including abstract models of functional programs obtained via an abstractionrefinement procedure from patternmatching recursion schemes.
p487
aVDesigning and implementing typed programming languages is hard. Every new type system feature requires extending the metatheory and implementation, which are often complicated and fragile. To ease this process, we would like to provide general mechanisms that subsume many different features. In modern type systems, parametric polymorphism is fundamental, but intersection polymorphism has gained little traction in programming languages. Most practical intersection type systems have supported only refinement intersections, which increase the expressiveness of types (more precise properties can be checked) without altering the expressiveness of terms; refinement intersections can simply be erased during compilation. In contrast, unrestricted intersections increase the expressiveness of terms, and can be used to encode diverse language features, promising an economy of both theory and implementation. We describe a foundation for compiling unrestricted intersection and union types: an elaboration type system that generates ordinary \u03bbcalculus terms. The key feature is a Forsythelike merge construct. With this construct, not all reductions of the source program preserve types; however, we prove that ordinary callbyvalue evaluation of the elaborated program corresponds to a typepreserving evaluation of the source program. We also describe a prototype implementation and applications of unrestricted intersections and unions: records, operator overloading, and simulating dynamic typing.
p488
aVConditional compilation and software product line technologies make it possible to generate a huge number of different programs from a single software project. Typing each of these programs individually is usually impossible due to the sheer number of possible variants. Our previous work has addressed this problem with a type system for variational lambda calculus (VLC), an extension of lambda calculus with basic constructs for introducing and organizing variation. Although our type inference algorithm is more efficient than the bruteforce strategy of inferring the types of each variant individually, it is less robust since type inference will fail for the entire variational expression if any one variant contains a type error. In this work, we extend our type system to operate on VLC expressions containing type errors. This extension directly supports locating illtyped variants and the incremental development of variational programs. It also has many subtle implications for the unification of variational types. We show that our extended type system possesses a principal typing property and that the underlying unification problem is unitary. Our unification algorithm computes partial unifiers that lead to result types that (1) contain errors in as few variants as possible and (2) are most general. Finally, we perform an empirical evaluation to determine the overhead of this extension compared to our previous work, to demonstrate the improvements over the bruteforce approach, and to explore the effects of various error distributions on the inference process.
p489
aVMany substructural type systems have been proposed for controlling access to shared state in higherorder languages. Central to these systems is the notion of a *resource*, which may be split into disjoint pieces that different parts of a program can manipulate independently without worrying about interfering with one another. Some systems support a *logical* notion of resource (such as permissions), under which two resources may be considered disjoint even if they govern the *same* piece of state. However, in nearly all existing systems, the notions of resource and disjointness are fixed at the outset, baked into the model of the language, and fairly coarsegrained in the kinds of sharing they enable. In this paper, inspired by recent work on "fictional disjointness" in separation logic, we propose a simple and flexible way of enabling any module in a program to create its own custom type of splittable resource (represented as a commutative monoid), thus providing finegrained control over how the module's private state is shared with its clients. This functionality can be incorporated into an otherwise standard substructural type system by means of a new typing rule we call *the sharing rule*, whose soundness we prove semantically via a novel resourceoriented Kripke logical relation.
p490
aVMost complex software projects are compiled using a build tool (e.g. make), which runs commands in an order satisfying userdefined dependencies. Unfortunately, most build tools require all dependencies to be specified before the build starts. This restriction makes many dependency patterns difficult to express, especially those involving files generated at build time. We show how to eliminate this restriction, allowing additional dependencies to be specified while building. We have implemented our ideas in the Haskell library Shake, and have used Shake to write a complex build system which compiles millions of lines of code.
p491
aVUntil now there has been no support for specifying and enforcing contracts within a lazy functional program. That is a shame, because contracts consist of pre and postconditions for functions that go beyond the standard static types. This paper presents the design and implementation of a small, easytouse, purely functional contract library for Haskell, which, when a contract is violated, also provides more useful information than the classical blaming of one contract partner. From now on lazy functional languages can profit from the assurances in the development of correct programs that contracts provide.
p492
aVThis paper presents a new functional programming model for graph structures called structured graphs. Structured graphs extend conventional algebraic datatypes with explicit definition and manipulation of cycles and/or sharing, and offer a practical and convenient way to program graphs in functional programming languages like Haskell. The representation of sharing and cycles (edges) employs recursive binders and uses an encoding inspired by parametric higherorder abstract syntax. Unlike traditional approaches based on mutable references or node/edge lists, wellformedness of the graph structure is ensured statically and reasoning can be done with standard functional programming techniques. Since the binding structure is generic, we can define many useful generic combinators for manipulating structured graphs. We give applications and show how to reason about structured graphs.
p493
aVIn dependently typed languages runtime values can appear in types, making it possible to give programs more precise types than in languages without dependent types. This can range from keeping track of simple invariants like the length of a list, to full functional correctness. In addition to having some correctness guarantees on the final program, assigning more precise types to programs means that you can get more assistance from the type checker while writing them. This is what I focus on here, demonstrating how the programming environment of Agda can help you when developing dependently typed programs.
p494
aVDescribing a problem using classical linear algebra is a very wellknown problemsolving technique. If your question can be formulated as a question about real or complex matrices, then the answer can often be found by standard techniques. It's less wellknown that very similar techniques still apply where instead of real or complex numbers we have a closed semiring, which is a structure with some analogue of addition and multiplication that need not support subtraction or division. We define a typeclass in Haskell for describing closed semirings, and implement a few functions for manipulating matrices and polynomials over them. We then show how these functions can be used to calculate transitive closures, find shortest or longest or widest paths in a graph, analyse the data flow of imperative programs, optimally pack knapsacks, and perform discrete event simulations, all by just providing an appropriate underlying closed semiring.
p495
aVWe present a divideandconquer algorithm for parsing contextfree languages efficiently. Our algorithm is an instance of Valiant's (1975), who reduced the problem of parsing to matrix multiplications. We show that, while the conquer step of Valiant's is O(n3) in the worst case, it improves to O(logn3), under certain conditions satisfied by many useful inputs. These conditions occur for example in program texts written by humans. The improvement happens because the multiplications involve an overwhelming majority of empty matrices. This result is relevant to modern computing: divideandconquer algorithms can be parallelized relatively easily.
p496
aVWe describe a functional programming approach to the design of outlines of eighteenthcentury string instruments. The approach is based on the research described in Franois Denis's book, Trait de lutherie. The programming vernacular for Denis's instructions, which we call functional geometry, is meant to reiterate the historically justified language and techniques of this musical instrument design. The programming metaphor is entirely Euclidean, involving straightedge and compass constructions, with few (if any) numbers, and no Cartesian equations or grid. As such, it is also an interesting approach to teaching programming and mathematics without numerical calculation or equational reasoning. The advantage of this languagebased, functional approach to lutherie is founded in the abstract characterization of common patterns in instrument design. These patterns include not only the abstraction of common straightedge and compass constructions, but of higherorder conceptualization of the instrument design process. We also discuss the role of arithmetic, geometric, harmonic, and subharmonic proportions, and the use of their rational approximants.
p497
aVOne often cited benefit of pure functional programming is that pure code is easier to test and reason about, both formally and informally. However, real programs have sideeffects including state management, exceptions and interactions with the outside world. Haskell solves this problem using monads to capture details of possibly sideeffecting computations   it provides monads for capturing state, I/O, exceptions, nondeterminism, libraries for practical purposes such as CGI and parsing, and many others, as well as monad transformers for combining multiple effects. Unfortunately, useful as monads are, they do not compose very well. Monad transformers can quickly become unwieldy when there are lots of effects to manage, leading to a temptation in larger programs to combine everything into one coarsegrained state and exception monad. In this paper I describe an alternative approach based on handling algebraic effects, implemented in the IDRIS programming language. I show how to describe side effecting computations, how to write programs which compose multiple finegrained effects, and how, using dependent types, we can use this approach to reason about states in effectful programs.
p498
aVPlotkin and Pretnar's handlers for algebraic effects occupy a sweet spot in the design space of abstractions for effectful computation. By separating effect signatures from their implementation, algebraic effects provide a high degree of modularity, allowing programmers to express effectful programs independently of the concrete interpretation of their effects. A handler is an interpretation of the effects of an algebraic computation. The handler abstraction adapts well to multiple settings: pure or impure, strict or lazy, static types or dynamic types. This is a position paper whose main aim is to popularise the handler abstraction. We give a gentle introduction to its use, a collection of illustrative examples, and a straightforward operational semantics. We describe our Haskell implementation of handlers in detail, outline the ideas behind our OCaml, SML, and Racket implementations, and present experimental results comparing handlers with existing code.
p499
aVComputer science is one of the richest, most exciting disciplines on the planet, yet any teenager will tell you that ICT (as it is called in UK schools   "information and communication technology") is focused almost entirely on the use and application of computers, and in practice covers nothing about how computers work, nor programming, nor anything of the discipline of computer science as we understand it. Over the last two decades, computing at school has drifted from writing adventure games on the BBC Micro to writing business plans in Excel. This is bad for our young people's education, and it is bad for our economy. Nor is this phenomenon restricted to the UK: many countries are struggling with the same issues. Our young people should be educated not only in the application and use of digital technology, but also in how it works, and its foundational principles. Lacking such knowledge renders them powerless in the face of complex and opaque technology, disenfranchises them from making informed decisions about the digital society, and deprives our nations of a wellqualified stream of students enthusiastic and able to envision and design new digital systems. Can anything be done, given the enormous inertia of our various countries' educational systems? Sometimes, yes. After a decade of stasis, change has come to the UK. Over the last 18 months, there has been a wholesale reform of the English school computing curriculum, and substantial movement in Scotland and Wales. It now seems likely that computer science will, for the first time, become part of every child's education. This change has been driven not by institutions or by the government, but by a grassroots movement of parents, teachers, university academics, software developers, and others. A key agent in this grassroots movement although not the only one is the Computing At School Working Group (CAS). In this talk I will describe how CAS was born and developed, and the radical changes that have taken place since in the UK. I hope that this may be encouraging for those pushing water uphill in other parts of the world, and I will also try to draw out some lessons from our experience that may be useful to others.
p500
aVA concurrent implementation of software transactional memory in Concurrent Haskell using a callbyneed functional language with processes and futures is given. The description of the smallstep operational semantics is precise and explicit, and employs an early abort of conflicting transactions. A proof of correctness of the implementation is given for a contextual semantics with may and shouldconvergence. This implies that our implementation is a correct evaluator for an abstract specification equipped with a bigstep semantics.
p501
aVWe present Mezzo, a typed programming language of ML lineage. Mezzo is equipped with a novel static discipline of duplicable and affine permissions, which controls aliasing and ownership. This rules out certain mistakes, including representation exposure and data races, and enables new idioms, such as gradual initialization, memory reuse, and (type)state changes. Although the core static discipline disallows sharing a mutable data structure, Mezzo offers several ways of working around this restriction, including a novel dynamic ownership control mechanism which we dub "adoption and abandon".
p502
aVIn this paper, we study strong normalization of a core language based on System Fomega which supports programming with finite and infinite structures. Building on our prior work, finite data such as finite lists and trees are defined via constructors and manipulated via pattern matching, while infinite data such as streams and infinite trees is defined by observations and synthesized via copattern matching. In this work, we take a typebased approach to strong normalization by tracking size information about finite and infinite data in the type. This guarantees compositionality. More importantly, the duality of pattern and copatterns provide a unifying semantic concept which allows us for the first time to elegantly and uniformly support both wellfounded induction and coinduction by mere rewriting. The strong normalization proof is structured around Girard's reducibility candidates. As such our system allows for nondeterminism and does not rely on coverage. Since System Fomega is general enough that it can be the target of compilation for the Calculus of Constructions, this work is a significant step towards representing observationcentric infinite data in proof assistants such as Coq and Agda.
p503
aVTotal functional programming offers the beguiling vision that, just by virtue of the compiler accepting a program, we are guaranteed that it will always terminate. In the case of programs that are not intended to terminate, e.g., servers, we are guaranteed that programs will always be productive. Productivity means that, even if a program generates an infinite amount of data, each piece will be generated in finite time. The theoretical underpinning for productive programming with infinite output is provided by the category theoretic notion of final coalgebras. Hence, we speak of coprogramming with nonwellfounded codata, as a dual to programming with wellfounded data like finite lists and trees. Systems that offer facilities for productive coprogramming, such as the proof assistants Coq and Agda, currently do so through syntactic guardedness checkers. Syntactic guardedness checkers ensure that all selfrecursive calls are guarded by a use of a constructor. Such a check ensures productivity. Unfortunately, these syntactic checks are not compositional, and severely complicate coprogramming. Guarded recursion, originally due to Nakano, is tantalising as a basis for a flexible and compositional typebased approach to coprogramming. However, as we show, by itself, guarded recursion is not suitable for coprogramming due to the fact that there is no way to make finite observations on pieces of infinite data. In this paper, we introduce the concept of clock variables that index Nakano's guarded recursion. Clock variables allow us to "close over" the generation of infinite data, and to make finite observations, something that is not possible with guarded recursion alone.
p504
aVMonadic secondorder logic on finite words (MSO) is a decidable yet expressive logic into which many decision problems can be encoded. Since MSO formulas correspond to regular languages, equivalence of MSO formulas can be reduced to the equivalence of some regular structures (e.g. automata). This paper presents a verified functional decision procedure for MSO formulas that is not based on automata but on regular expressions. Functional languages are ideally suited for this task: regular expressions are data types and functions on them are defined by pattern matching and recursion and are verified by structural induction. Decision procedures for regular expression equivalence have been formalized before, usually based on Brzozowski derivatives. Yet, for a straightforward embedding of MSO formulas into regular expressions an extension of regular expressions with a projection operation is required. We prove total correctness and completeness of an equivalence checker for regular expressions extended in that way. We also define a languagepreserving translation of formulas into regular expressions with respect to two different semantics of MSO. Our results have been formalized and verified in the theorem prover Isabelle. Using Isabelle's code generation facility, this yields purely functional, formally verified programs that decide equivalence of MSO formulas.
p505
aVFolds over inductive datatypes are well understood and widely used. In their plain form, they are quite restricted; but many disparate generalisations have been proposed that enjoy similar calculational benefits. There have also been attempts to unify the various generalisations: two prominent such unifications are the 'recursion schemes from comonads' of Uustalu, Vene and Pardo, and our own 'adjoint folds'. Until now, these two unified schemes have appeared incompatible. We show that this appearance is illusory: in fact, adjoint folds subsume recursion schemes from comonads. The proof of this claim involves standard constructions in category theory that are nevertheless not well known in functional programming: EilenbergMoore categories and bialgebras.
p506
aVFunctional reactive programming (FRP) is an elegant approach to declaratively specify reactive systems. However, the powerful abstractions of FRP have historically made it difficult to predict and control the resource usage of programs written in this style. In this paper, we give a new language for higherorder reactive programming. Our language generalizes and simplifies prior type systems for reactive programming, by supporting the use of streams of streams, firstclass functions, and higherorder operations. We also support many temporal operations beyond streams, such as terminatable streams, events, and even resumptions with firstclass schedulers. Furthermore, our language supports an efficient implementation strategy permitting us to eagerly deallocate old values and statically rule out spacetime leaks, a notorious source of inefficiency in reactive programs. Furthermore, these memory guarantees are achieved without the use of a complex substructural type discipline. We also show that our implementation strategy of eager deallocation is safe, by showing the soundness of our type system with a novel stepindexed Kripke logical relation.
p507
aVFunctional Reactive Programming (FRP) is an approach to the development of reactive systems which provides a pure functional interface, but which may be implemented as an abstraction of an imperative eventdriven layer. FRP systems typically provide a model of behaviours (total timeindexed values, implemented as pull systems) and event sources (partial timeindexed values, implemented as push systems). In this paper, we investigate a type system for eventdriven FRP programs which provide liveness guarantees, that is every input event is guaranteed to generate an output event. We show that FRP can be implemented on top of a model of sets and relations, and that the isomorphism between event sources and behaviours corresponds to the isomorphism between relations and setvalued functions. We then implement sets and relations using a model of continuations using the usual doublenegation CPS transform. The implementation of behaviours as pull systems based on futures, and of event sources as push systems based on the observer pattern, thus arises from first principles. We also discuss a Java implementation of the FRP model.
p508
aVThe third listhomomorphism theorem states that if a function is both foldr and foldl, it has a divideandconquer parallel implementation as well. In this paper, we develop a theory for obtaining such parallelization theorems. The key is a new proof of the third listhomomorphism theorem based on shortcut deforestation. The proof implies that there exists a divideandconquer parallel program of the form of h(x 'merge' y) = h1 x odot h2 y, where h is the subject of parallelization, merge is the operation of integrating independent substructures, h1 and h2 are computations applied to substructures, possibly in parallel, and odot merges the results calculated for substructures, if (i) h can be specified by two certain forms of iterative programs, and (ii) merge can be implemented by a function of a certain polymorphic type. Therefore, when requirement (ii) is fulfilled, h has a divideandconquer implementation if h has two certain forms of implementations. We show that our approach is applicable to structureconsuming operations by catamorphisms (folds), structuregenerating operations by anamorphisms (unfolds), and their generalizations called hylomorphisms.
p509
aVThis pearl presents a novel technique for constructing a firstorder syntax tree directly from a higherorder interface. We exploit circular programming to generate names for new variables, resulting in a simple yet efficient method. Our motivating application is the design of embedded languages supporting variable binding, where it is convenient to use higherorder syntax when constructing programs, but firstorder syntax when processing or transforming programs.
p510
aVIn this paper we investigate laziness and optimal evaluation strategies for functional programming languages. We consider the weak lambdacalculus as a basis of functional programming languages, and we adapt to this setting the concepts of optimal reductions that were defined for the full lambdacalculus. We prove that the usual implementation of callbyneed using sharing is optimal, that is, normalizing any lambdaterm with callbyneed requires exactly the same number of reduction steps as the shortest reduction sequence in the weak lambdacalculus without sharing. Furthermore, we prove that optimal reduction sequences without sharing are not computable. Hence sharing is the only computable means to reach weak optimality.
p511
aVSystem FC, the core language of the Glasgow Haskell Compiler, is an explicitlytyped variant of System F with firstclass type equality proofs called coercions. This extensible proof system forms the foundation for type system extensions such as type families (typelevel functions) and Generalized Algebraic Datatypes (GADTs). Such features, in conjunction with kind polymorphism and datatype promotion, support expressive compiletime reasoning. However, the core language lacks explicit kind equality proofs. As a result, typelevel computation does not have access to kindlevel functions or promoted GADTs, the typelevel analogues to expressionlevel features that have been so useful. In this paper, we eliminate such discrepancies by introducing kind equalities to System FC. Our approach is based on dependent type systems with heterogeneous equality and the "TypeinType" axiom, yet it preserves the metatheoretic properties of FC. In particular, type checking is simple, decidable and syntax directed. We prove the preservation and progress theorems for the extended language.
p512
aVIn Haskell, there are many data types that would form monads were it not for the presence of typeclass constraints on the operations on that data type. This is a frustrating problem in practice, because there is a considerable amount of support and infrastructure for monads that these data types cannot use. Using several examples, we show that a monadic computation can be restructured into a normal form such that the standard monad class can be used. The technique is not specific to monads, and we show how it can also be applied to other structures, such as applicative functors. One significant use case for this technique is domainspecific languages, where it is often desirable to compile a deep embedding of a computation to some other language, which requires restricting the types that can appear in that computation.
p513
aVWhen writing embedded domain specific languages in Haskell, it is often convenient to be able to make an instance of the Monad class to take advantage of the donotation and the extensive monad libraries. Commonly it is desirable to compile such languages rather than just interpret them. This introduces the problem of monad reification, i.e. observing the structure of the monadic computation. We present a solution to the monad reification problem and illustrate it with a small robot control language. Monad reification is not new but the novelty of our approach is in its directness, simplicity and compositionality.
p514
aVStructural recursion, in the form of, for example, folds on lists and catamorphisms on algebraic data structures including trees, plays an important role in functional programming, by providing a systematic way for constructing and manipulating functional programs. It is, however, a challenge to define structural recursions for graph data structures, the most ubiquitous sort of data in computing. This is because unlike lists and trees, graphs are essentially not inductive and cannot be formalized as an initial algebra in general. In this paper, we borrow from the database community the idea of structural recursion on how to restrict recursions on infinite unordered regular trees so that they preserve the finiteness property and become terminating, which are desirable properties for query languages. We propose a new graph transformation language called lambdaFG for transforming and querying ordered graphs, based on the welldefined bisimulation relation on ordered graphs with special epsilonedges. The language lambdaFG is a higher order graph transformation language that extends the simply typed lambda calculus with graph constructors and more powerful structural recursions, which is extended for transformations on the sibling dimension. It not only gives a general framework for manipulating graphs and reasoning about them, but also provides a solution to the open problem of how to define a structural recursion on ordered graphs, with the help of the bisimilarity for ordered graphs with epsilonedges.
p515
aVHigherorder recursion schemes (HORS) have recently received much attention as a useful abstraction of higherorder functional programs with a number of new verification techniques employing HORS modelchecking as their centrepiece. This paper contributes to the ongoing quest for a truly scalable modelchecker for HORS by offering a different, automata theoretic perspective. We introduce the first practical modelchecking algorithm that acts on a generalisation of pushdown automata equiexpressive with HORS called collapsible pushdown systems (CPDS). At its core is a substantial modification of a recently studied saturation algorithm for CPDS. In particular it is able to use information gathered from an approximate forward reachability analysis to guide its backward search. Moreover, we introduce an algorithm that prunes the CPDS prior to modelchecking and a method for extracting counterexamples in negative instances. We compare our tool with the stateoftheart verification tools for HORS and obtain encouraging results. In contrast to some of the main competition tackling the same problem, our algorithm is fixedparameter tractable, and we also offer significantly improved performance over the only previously published tool of which we are aware that also enjoys this property. The tool and additional material are available from http://cshore.cs.rhul.ac.uk.
p516
aVThis paper presents 3MT, a framework for modular mechanized metatheory of languages with effects. Using 3MT, individual language features and their corresponding definitions   semantic functions, theorem statements and proofs  can be built separately and then reused to create different languages with fully mechanized metatheory. 3MT combines modular datatypes and monads to define denotational semantics with effects on a perfeature basis, without fixing the particular set of effects or language constructs. One wellestablished problem with type soundness proofs for denotational semantics is that they are notoriously brittle with respect to the addition of new effects. The statement of type soundness for a language depends intimately on the effects it uses, making it particularly challenging to achieve modularity. 3MT solves this longstanding problem by splitting these theorems into two separate and reusable parts: a feature theorem that captures the welltyping of denotations produced by the semantic function of an individual feature with respect to only the effects used, and an effect theorem that adapts welltypings of denotations to a fixed superset of effects. The proof of type soundness for a particular language simply combines these theorems for its features and the combination of their effects. To establish both theorems, 3MT uses two key reasoning techniques: modular induction and algebraic laws about effects. Several effectful language features, including references and errors, illustrate the capabilities of 3MT. A case study reuses these features to build fully mechanized definitions and proofs for 28 languages, including several versions of miniML with effects.
p517
aVLanguage extensions introduce highlevel programming constructs that protect programmers from lowlevel details and repetitive tasks. For such an abstraction barrier to be sustainable, it is important that no errors are reported in terms of generated code. A typical strategy is to check the original user code prior to translation into a lowlevel encoding, applying the assumption that the translation does not introduce new errors. Unfortunately, such assumption is untenable in general, but in particular in the context of extensible programming languages, such as Racket or SugarJ, that allow regular programmers to define language extensions. In this paper, we present a formalism for building and automatically verifying the typesoundness of syntactic language extensions. To build a typesound language extension with our formalism, a developer declares an extended syntax, type rules for the extended syntax, and translation rules into the (possibly further extended) base language. Our formalism then validates that the userdefined type rules are sufficient to guarantee that the code generated by the translation rules cannot contain any type errors. This effectively ensures that an initial type check prior to translation precludes type errors in generated code. We have implemented a core system in PLT Redex and we have developed a syntactically extensible variant of System Fw that we extend with let notation, monadic do blocks, and algebraic data types. Our formalism verifies the soundness of each extension automatically.
p518
aVContemporary compilers must typically handle sophisticated highlevel source languages, generate efficient code for multiple hardware architectures and operating systems, and support sourcelevel debugging, profiling, and other program development tools. As a result, compilers tend to be among the most complex of software systems. Nanopass frameworks are designed to help manage this complexity. A nanopass compiler is comprised of many singletask passes with formally defined intermediate languages. The perceived downside of a nanopass compiler is that the extra passes will lead to substantially longer compilation times. To determine whether this is the case, we have created a plug replacement for the commercial Chez Scheme compiler, implemented using an updated nanopass framework, and we have compared the speed of the new compiler and the code it generates against the original compiler for a large set of benchmark programs. This paper describes the updated nanopass framework, the new compiler, and the results of our experiments. The compiler produces faster code than the original, averaging 1527% depending on architecture and optimization level, due to a more sophisticated but slower register allocator and improvements to several optimizations. Compilation times average well within a factor of two of the original compiler, despite the slower register allocator and the replacement of five passes of the original 10 with over 50 nanopasses.
p519
aVAs programmers, programming in typed languages increases our confidence in the correctness of our programs. As type system designers, soundness proofs increase our confidence in the correctness of our type systems. There is more to typed languages than their typing rules, however. To be usable, a typed language needs to provide a wellfurnished standard library and to specify types for its exports. As software artifacts, these base type environments can rival typecheckers in complexity. Our experience with the Typed Racket base environment which accounts for 31% of the code in the Typed Racket implementation teaches us that writing type environments can be just as errorprone as writing typecheckers. We report on our experience over the past two years of using random testing to increase our confidence in the correctness of the Typed Racket base environment.
p520
aVA modular framework for the development of medical applications that promotes deterministic, robust and correct code is presented. The system is based on the portable Gambit Scheme programming language and provides a flexible crossplatform environment for developing graphical applications on mobile devices as well as medical instrumentation interfaces running on embedded platforms. Real world applications of this framework for mobile diagnostics, telemonitoring and automated drug infusions are reported. The source code for the core framework is open source and available at: https://github.com/partcw/lambdanative.
p521
aVContinuations are programming abstractions that allow for manipulating the "future" of a computation. Amongst their many applications, they enable implementing unstructured program flow through higherorder control operators such as callcc. In this paper we develop a Hoarestyle logic for the verification of programs with higherorder control, in the presence of dynamic state. This is done by designing a dependent type theory with first class callcc and abort operators, where pre and postconditions of programs are tracked through types. Our operators are algebraic in the sense of Plotkin and Power, and Jaskelioff, to reduce the annotation burden and enable verification by symbolic evaluation. We illustrate working with the logic by verifying a number of characteristic examples.
p522
aVModular programming and modular verification go hand in hand, but most existing logics for concurrency ignore two crucial forms of modularity: *higherorder functions*, which are essential for building reusable components, and *granularity abstraction*, a key technique for hiding the intricacies of finegrained concurrent data structures from the clients of those data structures. In this paper, we present CaReSL, the first logic to support the use of granularity abstraction for modular verification of higherorder concurrent programs. After motivating the features of CaReSL through a variety of illustrative examples, we demonstrate its effectiveness by using it to tackle a significant case study: the first formal proof of (partial) correctness for Hendler et al.'s "flat combining" algorithm.
p523
aVWe report on the design and implementation of an extensible programming language and its intrinsic support for formal verification. Our language is targeted at lowlevel programming of infrastructure like operating systems and runtime systems. It is based on a crossplatform core combining characteristics of assembly languages and compiler intermediate languages. From this foundation, we take literally the saying that C is a "macro assembly language": we introduce an expressive notion of certified lowlevel macros, sufficient to build up the usual features of C and beyond as macros with no special support in the core. Furthermore, our macros have integrated support for strongest postcondition calculation and verification condition generation, so that we can provide a highproductivity formal verification environment within Coq for programs composed from any combination of macros. Our macro interface is expressive enough to support features that lowlevel programs usually only access through external tools with no formal guarantees, such as declarative parsing or SQLinspired querying. The abstraction level of these macros only imposes a compiletime cost, via the execution of functional Coq programs that compute programs in our intermediate language; but the runtime cost is not substantially greater than for more conventional C code. We describe our experiences constructing a full Clike language stack using macros, with some experiments on the verifiability and performance of individual programs running on that stack.
p524
aVLanguageintegrated query is receiving renewed attention, in part because of its support through Microsoft's LINQ framework. We present a practical theory of languageintegrated query based on quotation and normalisation of quoted terms. Our technique supports join queries, abstraction over values and predicates, composition of queries, dynamic generation of queries, and queries with nested intermediate data. Higherorder features prove useful even for constructing firstorder queries. We prove a theorem characterising when a host query is guaranteed to generate a single SQL query. We present experimental results confirming our technique works, even in situations where Microsoft's LINQ framework either fails to produce an SQL query or, in one case, produces an avalanche of SQL queries.
p525
aVCoercions and threesomes both enable a language to combine static and dynamic types while avoiding castbased space leaks. Coercion calculi elegantly specify spaceefficient cast behavior, even when augmented with blame tracking, but implementing their semantics directly is difficult. Threesomes, on the other hand, have a straightforward recursive implementation, but endowing them with blame tracking is challenging. In this paper, we show that you can use that elegant spec to produce that straightforward implementation: we use the coercion calculus to derive threesomes with blame. In particular, we construct novel threesome calculi for blame tracking strategies that detect errors earlier, catch more errors, and reflect an intuitive conception of safe and unsafe casts based on traditional subtyping.
p526
aVExpressing algorithms using immutable arrays greatly simplifies the challenges of automatic SIMD vectorization, since several important classes of dependency violations cannot occur. The Haskell programming language provides libraries for programming with immutable arrays, and compiler support for optimizing them to eliminate the overhead of intermediate temporary arrays. We describe an implementation of automatic SIMD vectorization in a Haskell compiler which gives substantial vector speedups for a range of programs written in a natural programming style. We compare performance with that of programs compiled by the Glasgow Haskell Compiler.
p527
aVBidirectional typechecking, in which terms either synthesize a type or are checked against a known type, has become popular for its scalability (unlike DamasMilner type inference, bidirectional typing remains decidable even for very expressive type systems), its error reporting, and its relative ease of implementation. Following design principles from proof theory, bidirectional typing can be applied to many type constructs. The principles underlying a bidirectional approach to polymorphism, however, are less obvious. We give a declarative, bidirectional account of higherrank polymorphism, grounded in proof theory; this calculus enjoys many properties such as etareduction and predictability of annotations. We give an algorithm for implementing the declarative system; our algorithm is remarkably simple and wellbehaved, despite being both sound and complete.
p528
aVThe technique of abstracting abstract machines (AAM) provides a systematic approach for deriving computable approximations of evaluators that are easily proved sound. This article contributes a complementary stepbystep process for subsequently going from a naive analyzer derived under the AAM approach, to an efficient and correct implementation. The end result of the process is a two to three orderofmagnitude improvement over the systematically derived analyzer, making it competitive with handoptimized implementations that compute fundamentally less precise results.
p529
aVStream fusion is a powerful technique for automatically transforming highlevel sequenceprocessing functions into efficient implementations. It has been used to great effect in Haskell libraries for manipulating byte arrays, Unicode text, and unboxed vectors. However, some operations, like vector append, still do not perform well within the standard stream fusion framework. Others, like SIMD computation using the SSE and AVX instructions available on modern x86 chips, do not seem to fit in the framework at all.  In this paper we introduce generalized stream fusion, which solves these issues. The key insight is to bundle together multiple stream representations, each tuned for a particular class of stream consumer. We also describe a stream representation suited for efficient computation with SSE instructions. Our ideas are implemented in modified versions of the GHC compiler and vector library. Benchmarks show that highlevel Haskell code written using our compiler and libraries can produce code that is faster than both compiler and handvectorized C.
p530
aVPurely functional, embedded array programs are a good match for SIMD hardware, such as GPUs. However, the naive compilation of such programs quickly leads to both code explosion and an excessive use of intermediate data structures. The resulting slowdown is not acceptable on target hardware that is usually chosen to achieve high performance. In this paper, we discuss two optimisation techniques, sharing recovery and array fusion, that tackle code explosion and eliminate superfluous intermediate structures. Both techniques are well known from other contexts, but they present unique challenges for an embedded language compiled for execution on a GPU. We present novel methods for implementing sharing recovery and array fusion, and demonstrate their effectiveness on a set of benchmarks.
p531
aVDependent typetheory aims to become the standard way to formalize mathematics at the same time as displacing traditional platforms for highassurance programming. However, current implementations of type theory are still lacking, in the sense that some obvious truths require explicit proofs, making typetheory awkward to use for many applications, both in formalization and programming. In particular, notions of erasure are poorly supported. In this paper we propose an extension of typetheory with colored terms, color erasure and interpretation of colored types as predicates. The result is a more powerful typetheory: some definitions and proofs may be omitted as they become trivial, it becomes easier to program with precise types, and some parametricity results can be internalized.
p532
aVWe present a novel set of metaprogramming primitives for use in a dependentlytyped functional language. The types of our metaprograms provide strong and precise guarantees about their termination, correctness and completeness. Our system supports typesafe construction and analysis of terms, types and typing contexts. Unlike alternative approaches, they are written in the same style as normal programs and use the language's standard functional computational model. We formalise the new metaprogramming primitives, implement them as an extension of Agda, and provide evidence of usefulness by means of two compelling applications in the fields of datatypegeneric programming and proof tactics.
p533
aVEffective support for custom proof automation is essential for large scale interactive proof development. However, existing languages for automation via *tactics* either (a) provide no way to specify the behavior of tactics within the base logic of the accompanying theorem prover, or (b) rely on advanced typetheoretic machinery that is not easily integrated into established theorem provers. We present Mtac, a lightweight but powerful extension to Coq that supports dependentlytyped tactic programming. Mtac tactics have access to all the features of ordinary Coq programming, as well as a new set of typed tactical primitives. We avoid the need to touch the trusted kernel typechecker of Coq by encapsulating uses of these new tactical primitives in a *monad*, and instrumenting Coq so that it executes monadic tactics during type inference.
p534
aVNetworked embedded systems are ubiquitous in modern society. Examples include SCADA systems that manage physical infrastructure, medical devices such as pacemakers and insulin pumps, and vehicles such as airplanes and automobiles. Such devices are connected to networks for a variety of compelling reasons, including the ability to access diagnostic information conveniently, perform software updates, provide innovative features, and lower costs. Researchers and hackers have shown that these kinds of networked embedded systems are vulnerable to remote attacks and that such attacks can cause physical damage and can be hidden from monitors [1, 4]. DARPA launched the HACMS program to create technology to make such systems dramatically harder to attack successfully. Specifically, HACMS is pursuing a cleanslate, formal methodsbased approach to the creation of highassurance vehicles, where high assurance is defined to mean functionally correct and satisfying appropriate safety and security properties. Specific technologies include program synthesis, domainspecific languages, and theorem provers used as program development environments. Targeted software includes operating system components such as hypervisors, microkernels, file systems, and device drivers as well as control systems such as autopilots and adaptive cruise controls. Program researchers are leveraging existing highassurance software including NICTA's seL4 microkernel and INRIA's CompCert compiler. Although the HACMS project is less than halfway done, the program has already achieved some remarkable success. At program kickoff, a Red Team easily hijacked the baseline opensource quadcopter that HACMS researchers are using as a research platform. At the end of eighteen months, the Red Team was not able to hijack the newlyminted "SMACCMCopter" running highassurance HACMS code, despite being given six weeks and full access to the source code of the copter. An expert in penetration testing called the SMACCMCopter "the most secure UAV on the planet." In this talk, I will describe the HACMS program: its motivation, the underlying technologies, current results, and future directions.
p535
aVParigot's \u03bb\u03bccalculus, a system for computational reasoning about classical proofs, serves as a foundation for control operations embodied by operators like Scheme's callcc. We demonstrate that the callbyvalue theory of the \u03bb\u03bccalculus contains a latent theory of delimited control, and that a known variant of \u03bb\u03bc which unshackles the syntax yields a calculus of composable continuations from the existing constructs and rules for classical control. To relate to the various formulations of control effects, and to continuationpassing style, we use a form of compositional program transformations which preserves the underlying structure of equational theories, contexts, and substitution. Finally, we generalize the callbyname and callbyvalue theories of the \u03bb\u03bccalculus by giving a single parametric theory that encompasses both, allowing us to generate a callbyneed instance that defines a calculus of classical and delimited control with lazy evaluation and sharing.
p536
aVThe notion of context in functional languages no longer refers just to variables in scope. Context can capture additional properties of variables (usage patterns in linear logics; caching requirements in dataflow languages) as well as additional resources or properties of the execution environment (rebindable resources; platform version in a crossplatform application). The recently introduced notion of coeffects captures the latter, wholecontext properties, but it failed to capture finegrained pervariable properties. We remedy this by developing a generalized coeffect system with annotations indexed by a coeffect shape. By instantiating a concrete shape, our system captures previously studied flat (wholecontext) coeffects, but also structural (pervariable) coeffects, making coeffect analyses more useful. We show that the structural system enjoys desirable syntactic properties and we give a categorical semantics using extended notions of indexed comonad. The examples presented in this paper are based on analysis of established language features (liveness, linear logics, dataflow, dynamic scoping) and we argue that such contextaware properties will also be useful for future development of languages for increasingly heterogeneous and distributed platforms.
p537
aVProgrammers embrace contracts. They can use the language they know and love to formulate logical assertions about the behavior of their programs. They can use the existing IDE infrastructure to log contracts, to test, to debug, and to profile their programs. The keynote presents the challenges and rewards of supporting contracts in a modern, fullspectrum programming language. It covers technical challenges of contracts while demonstrating the nontechnical motivation for contract system design choices and showing how contracts and contract research can serve practicing programmers. The remainder of this article is a literature survey of contract research, with an emphasis on recent work about higherorder contracts and blame.
p538
aVBehavioral software contracts are a widely used mechanism for governing the flow of values between components. However, runtime monitoring and enforcement of contracts imposes significant overhead and delays discovery of faulty components to runtime. To overcome these issues, we present soft contract verification, which aims to statically prove either complete or partial contract correctness of components, written in an untyped, higherorder language with firstclass contracts. Our approach uses higherorder symbolic execution, leveraging contracts as a source of symbolic values including unknown behavioral values, and employs an updatable heap of contract invariants to reason about flowsensitive facts. We prove the symbolic execution soundly approximates the dynamic semantics and that verified programs can't be blamed. The approach is able to analyze firstclass contracts, recursive data structures, unknown functions, and controlflowsensitive refinements of values, which are all idiomatic in dynamic languages. It makes effective use of an offtheshelf solver to decide problems without heavy encodings. The approach is competitive with a wide range of existing tools  including type systems, flow analyzers, and model checkers  on their own benchmarks.
p539
aVThis paper presents a personal, qualitative case study of a first course using How to Design Programs and its functional teaching languages. The paper reconceptualizes the book's sixstep design process as an eightstep design process ending in a new "review and refactor" step. It recommends specific approaches to students' difficulties with function descriptions, function templates, data examples, and other parts of the design process. Itconnects the process to interactive "world programs." It recounts significant, informative missteps in course design and delivery. Finally, it identifies some unsolved teaching problems and some potential solutions.
p540
aVThis paper reports on our industryacademia project of using a functional language in business software production. The general motivation behind the project is our ultimate goal of adopting an MLstyle higherorder typed functional language in a wide range of ordinary software development in industry. To probe the feasibility and identify various practical problems and needs, we have conducted a 15 month pilot project for developing an enterprise resource planning (ERP) system in SML#. The project has successfully completed as we have planned, demonstrating the feasibility of SML#. In particular, seamless integration of SQL and direct C language interface are shown to be useful in reliable and efficient development of a data intensive business application. During the program development, we have found several useful functional programming patterns and a number of possible extensions of an MLstyle language with records. This paper reports on the project details and the lessons learned from the project.
p541
aVRecent years have seen remarkable successes in rigorous engineering: using mathematically rigorous semantic models (not just idealised calculi) of realworld processors, programming languages, protocols, and security mechanisms, for testing, proof, analysis, and design. Building these models is challenging, requiring experimentation, dialogue with vendors or standards bodies, and validation; their scale adds engineering issues akin to those of programming to the task of writing clear and usable mathematics. But language and tool support for specification is lacking. Proof assistants can be used but bring their own difficulties, and a model produced in one, perhaps requiring many personyears effort and maintained over an extended period, cannot be used by those familiar with another. We introduce Lem, a language for engineering reusable largescale semantic models. The Lem design takes inspiration both from functional programming languages and from proof assistants, and Lem definitions are translatable into OCaml for testing, Coq, HOL4, and Isabelle/HOL for proof, and LaTeX and HTML for presentation. This requires a delicate balance of expressiveness, careful library design, and implementation of transformations  akin to compilation, but subject to the constraint of producing usable and humanreadable code for each target. Lem's effectiveness is demonstrated by its use in practice.
p542
aVGenerative type abstractions   present in Haskell, OCaml, and other languages   are useful concepts to help prevent programmer errors. They serve to create new types that are distinct at compile time but share a runtime representation with some base type. We present a new mechanism that allows for zerocost conversions between generative type abstractions and their representations, even when such types are deeply nested. We prove type safety in the presence of these conversions and have implemented our work in GHC.
p543
aVType inference  the problem of determining whether a program is welltyped  is wellunderstood. In contrast, elaboration  the task of constructing an explicitlytyped representation of the program  seems to have received relatively little attention, even though, in a nonlocal type inference system, it is nontrivial. We show that the constraintbased presentation of HindleyMilner type inference can be extended to deal with elaboration, while preserving its elegance. This involves introducing a new notion of "constraint with a value", which forms an applicative functor.
p544
aVFunctional Reactive Programming (FRP) provides a method for programming continuous, reactive systems by utilizing signal functions that, abstractly, transform continuous input signals into continuous output signals. These signals may also be streams of events, and indeed, by allowing signal functions themselves to be the values carried by these events (in essence, signals of signal functions), one can conveniently make discrete changes in program behavior by "switching" into and out of these signal functions. This higherorder notion of switching is common among many FRP systems, in particular those based on arrows, such as Yampa. Although convenient, the power of switching is often an overkill and can pose problems for certain types of program optimization (such as causal commutative arrows [14]), as it causes the structure of the program to change dynamically at runtime. Without a notion of justintime compilation or related idea, which itself is beset with problems, such optimizations are not possible at compile time. This paper introduces two new ideas that obviate, in a predominance of cases, the need for switching. The first is a noninterference law for arrows with choice that allows an arrowized FRP program to dynamically alter its own structure (within statically limited bounds) as well as abandon unused streams. The other idea is a notion of a settable signal function that allows a signal function to capture its present state and later be restarted from some previous state. With these two features, canonical uses of higherorder switchers can be replaced with a suitable firstorder design, thus enabling a broader range of static optimizations.
p545
aVWe report on our experiences in synthesizing a fullyfeatured autopilot from embedded domainspecific languages (EDSLs) hosted in Haskell. The autopilot is approximately 50k lines of C code generated from 10k lines of EDSL code and includes control laws, mode logic, encrypted communications system, and device drivers. The autopilot was built in less than two engineer years. This is the story of how EDSLs provided the productivity and safety gains to do largescale lowlevel embedded programming and lessons we learned in doing so.
p546
aVCombining type theory, language design, and empirical work, we present techniques for computing with large and dynamically changing datasets. Based on lambda calculus, our techniques are suitable for expressing a diverse set of algorithms on large datasets and, via selfadjusting computation, enable computations to respond automatically to changes in their data. To improve the scalability of selfadjusting computation, we present a type system for precise dependency tracking that minimizes the time and space for storing dependency metadata. The type system eliminates an important assumption of prior work that can lead to recording spurious dependencies. We present a typedirected translation algorithm that generates correct selfadjusting programs without relying on this assumption. We then show a probabilisticchunking technique to further decrease space usage by controlling the fundamental spacetime tradeoff in selfadjusting computation. We implement and evaluate these techniques, showing promising results on challenging benchmarks involving large graphs.
p547
aVIs Haskell a dependently typed programming language? Should it be? GHC's many typesystem features, such as Generalized Algebraic Datatypes (GADTs), datatype promotion, multiparameter type classes, and type families, give programmers the ability to encode domainspecific invariants in their types. Clever Haskell programmers have used these features to enhance the reasoning capabilities of static type checking. But really, how far have we come? Could we do more? In this talk, I will discuss dependently typed programming in Haskell, through examples, analysis and comparisons with modern fullspectrum dependently typed languages, such as Coq, Agda and Idris. What sorts of dependently typed programming can be done in Haskell now? What could GHC learn from these languages? Conversely, what lessons can GHC offer in return?
p548
aVHomotopy type theory is an extension of MartinLf type theory, based on a correspondence with homotopy theory and higher category theory. In homotopy type theory, the propositional equality type becomes proofrelevant, and corresponds to paths in a space. This allows for a new class of datatypes, called higher inductive types, which are specified by constructors not only for points but also for paths. In this paper, we consider a programming application of higher inductive types. Version control systems such as Darcs are based on the notion of patches  syntactic representations of edits to a repository. We show how patch theory can be developed in homotopy type theory. Our formulation separates formal theories of patches from their interpretation as edits to repositories. A patch theory is presented as a higher inductive type. Models of a patch theory are given by maps out of that type, which, being functors, automatically preserve the structure of patches. Several standard tools of homotopy theory come into play, demonstrating the use of these methods in a practical programming context.
p549
aVDependent pattern matching is an intuitive way to write programs and proofs in dependently typed languages. It is reminiscent of both pattern matching in functional languages and case analysis in onpaper mathematics. However, in general it is incompatible with new type theories such as homotopy type theory (HoTT). As a consequence, proofs in such theories are typically harder to write and to understand. The source of this incompatibility is the reliance of dependent pattern matching on the socalled K axiom  also known as the uniqueness of identity proofs  which is inadmissible in HoTT. The Agda language supports an experimental criterion to detect definitions by pattern matching that make use of the K axiom, but so far it lacked a formal correctness proof. In this paper, we propose a new criterion for dependent pattern matching without K, and prove it correct by a translation to eliminators in the style of Goguen et al. (2006). Our criterion both allows more good definitions than existing proposals, and solves a previously undetected problem in the criterion offered by Agda. It has been implemented in Agda and is the first to be supported by a formal proof. Thus it brings the benefits of dependent pattern matching to contexts where we cannot assume K, such as HoTT. It also points the way to new forms of dependent pattern matching, for example on higher inductive types.
p550
aVSMTbased checking of refinement types for callbyvalue languages is a wellstudied subject. Unfortunately, the classical translation of refinement types to verification conditions is unsound under lazy evaluation. When checking an expression, such systems implicitly assume that all the free variables in the expression are bound to values. This property is trivially guaranteed by eager, but does not hold under lazy, evaluation. Thus, to be sound and precise, a refinement type system for Haskell and the corresponding verification conditions must take into account which subset of binders actually reduces to values. We present a stratified type system that labels binders as potentially diverging or not, and that (circularly) uses refinement types to verify the labeling. We have implemented our system in LIQUIDHASKELL and present an experimental evaluation of our approach on more than 10,000 lines of widely used Haskell libraries. We show that LIQUIDHASKELL is able to prove 96% of all recursive functions terminating, while requiring a modest 1.7 lines of terminationannotations per 100 lines of code.
p551
aVEffect systems have the potential to help software developers, but their practical adoption has been very limited. We conjecture that this limited adoption is due in part to the difficulty of transitioning from a system where effects are implicit and unrestricted to a system with a static effect discipline, which must settle for conservative checking in order to be decidable. To address this hindrance, we develop a theory of gradual effect checking, which makes it possible to incrementally annotate and statically check effects, while still rejecting statically inconsistent programs. We extend the generic typeandeffect framework of Marino and Millstein with a notion of unknown effects, which turns out to be significantly more subtle than unknown types in traditional gradual typing. We appeal to abstract interpretation to develop and validate the concepts of gradual effect checking. We also demonstrate how an effect system formulated in Marino and Millstein's framework can be automatically extended to support gradual checking.
p552
aVI present a datatypegeneric treatment of recursive container types whose elements are guaranteed to be stored in increasing order, with the ordering invariant rolled out systematically. Intervals, lists and binary search trees are instances of the generic treatment. On the journey to this treatment, I report a variety of failed experiments and the transferable learning experiences they triggered. I demonstrate that a total element ordering is enough to deliver insertion and flattening algorithms, and show that (with care about the formulation of the types) the implementations remain as usual. Agda's instance arguments and pattern synonyms maximize the proof search done by the typechecker and minimize the appearance of proofs in program text, often eradicating them entirely. Generalizing to indexed recursive container types, invariants such as size and balance can be expressed in addition to ordering. By way of example, I implement insertion and deletion for 23 trees, ensuring both order and balance by the discipline of type checking.
p553
aVWe propose the integration of a relational specification framework within a dependent type system capable of verifying complex invariants over the shapes of algebraic datatypes. Our approach is based on the observation that structural properties of such datatypes can often be naturally expressed as inductivelydefined relations over the recursive structure evident in their definitions. By interpreting constructor applications (abstractly) in a relational domain, we can define expressive relational abstractions for a variety of complex data structures, whose structural and shape invariants can be automatically verified. Our specification language also allows for definitions of parametricrelations for polymorphic data types that enable highly composable specifications and naturally generalizes to higherorder polymorphic functions. We describe an algorithm that translates relational specifications into a decidable fragment of firstorder logic that can be efficiently discharged by an SMT solver. We have implemented these ideas in a type checker called CATALYST that is incorporated within the MLton SML compiler. Experimental results and case studies indicate that our verification strategy is both practical and effective.
p554
aVWe describe a new programming idiom for concurrency, based on Applicative Functors, where concurrency is implicit in the Applicative  While it is generally applicable, our technique was designed with a particular application in mind: an internal service at Facebook that identifies particular types of content and takes actions based on it. Our application has a large body of business logic that fetches data from several different external sources. The framework described in this paper enables the business logic to execute efficiently by automatically fetching data concurrently; we present some preliminary results.
p555
aVA domainspecific language can be implemented by embedding within a generalpurpose host language. This embedding may be deep or shallow, depending on whether terms in the language construct syntactic or semantic representations. The deep and shallow styles are closely related, and intimately connected to folds; in this paper, we explore that connection.
p556
aVIn a SoftwareDefined Network (SDN), a central, computationally powerful controller manages a set of distributed, computationally simple switches. The controller computes a policy describing how each switch should route packets and populates packetprocessing tables on each switch with rules to enact the routing policy. As network conditions change, the controller continues to add and remove rules from switches to adjust the policy as needed. Recently, the SDN landscape has begun to change as several proposals for new, reconfigurable switching architectures, such as RMT [5] and FlexPipe [14] have emerged. These platforms provide switch programmers with many, flexible tables for storing packetprocessing rules, and they offer programmers control over the packet fields that each table can analyze and act on. These reconfigurable switch architectures support a richer SDN model in which a switch configuration phase precedes the rule population phase [4]. In the configuration phase, the controller sends the switch a graph describing the layout and capabilities of the packet processing tables it will require during the population phase. Armed with this foreknowledge, the switch can allocate its hardware (or software) resources more efficiently. We present a new, typed language, called Concurrent NetCore, for specifying routing policies and graphs of packetprocessing tables. Concurrent NetCore includes features for specifying sequential, conditional and concurrent controlflow between packetprocessing tables. We develop a finegrained operational model for the language and prove this model coincides with a higherlevel denotational model when programs are welltyped. We also prove several additional properties of welltyped programs, including strong normalization and determinism. To illustrate the utility of the language, we develop linguistic models of both the RMT and FlexPipe architectures and we give a multipass compilation algorithm that translates graphs and routing policies to the RMT model.
p557
aVWe define a new approach to compilation to distributed architectures based on networks of abstract machines. Using it we can implement a generalised and fully transparent form of Remote Procedure Call that supports calling higherorder functions across node boundaries, without sending actual code. Our starting point is the classic Krivine machine, which implements reduction for untyped callbyname PCF. We successively add the features that we need for distributed execution and show the correctness of each addition. Then we construct a twolevel operational semantics, where the high level is a network of communicating machines, and the low level is given by local machine transitions. Using these networks, we arrive at our final system, the Krivine Net. We show that Krivine Nets give a correct distributed implementation of the Krivine machine, which preserves both termination and nontermination properties. All the technical results have been formalised and proved correct in Agda. We also implement a prototype compiler which we compare with previous distributing compilers based on Girard's Geometry of Interaction and on Game Semantics.
p558
aVThe root cause for confidentiality and integrity attacks against computing systems is insecure information flow. The complexity of modern systems poses a major challenge to secure endtoend information flow, ensuring that the insecurity of a single component does not render the entire system insecure. While information flow in a variety of languages and settings has been thoroughly studied in isolation, the problem of tracking information across component boundaries has been largely out of reach of the work so far. This is unsatisfactory because tracking information across component boundaries is necessary for endtoend security. This paper proposes a framework for uniform tracking of information flow through both the application and the underlying database. Key enabler of the uniform treatment is recent work by Cheney et al., which studies database manipulation via an embedded languageintegrated query language (with Microsoft's LINQ on the backend). Because both the host language and the embedded query languages are functional F#like languages, we are able to leverage informationflow enforcement for functional languages to obtain informationflow control for databases "for free", synergize it with informationflow control for applications and thus guarantee security across applicationdatabase boundaries. We develop the formal results in the form of a security type system that includes a treatment of algebraic data types and pattern matching, and establish its soundness. On the practical side, we implement the framework and demonstrate its usefulness in a case study with a realistic movie rental database.
p559
aVPrevious research on static analysis for program families has focused on lifting analyses for single, plain programs to program families by employing idiosyncratic representations. The lifting effort typically involves a significant amount of work for proving the correctness of the lifted algorithm and demonstrating its scalability. In this paper, we propose a parameterized static analysis framework for program families that can automatically lift a class of typebased static analyses for plain programs to program families. The framework consists of a parametric logical specification and a parametric variational constraint solver. We prove that a lifted algorithm is correct provided that the underlying analysis algorithm is correct. An evaluation of our framework has revealed an error in a previous manually lifted analysis. Moreover, performance tests indicate that the overhead incurred by the general framework is bounded by a factor of 2.
p560
aVCurrent languages for safely manipulating values with names only support term languages with simple binding syntax. As a result, no tools exist to safely manipulate code written in those languages for which name problems are the most challenging. We address this problem with Romeo, a language that respects \u03b1equivalence on its values, and which has access to a rich specification language for binding, inspired by attribute grammars. Our work has the complexbinding support of David Herman's \u03bbm, but is a fullfledged bindingsafe language like Pure FreshML.
p561
aVIncreasing sharing in programs is desirable to compactify the code, and to avoid duplication of reduction work at runtime, thereby speeding up execution. We show how a maximal degree of sharing can be obtained for programs expressed as terms in the lambda calculus with letrec. We introduce a notion of 'maximal compactness' for \u03bbletrecterms among all terms with the same infinite unfolding. Instead of defined purely syntactically, this notion is based on a graph semantics. \u03bbletrecterms are interpreted as firstorder term graphs so that unfolding equivalence between terms is preserved and reflected through bisimilarity of the term graph interpretations. Compactness of the term graphs can then be compared via functional bisimulation. We describe practical and efficient methods for the following two problems: transforming a \u03bbletrecterm into a maximally compact form; and deciding whether two \u03bbletrecterms are unfoldingequivalent. The transformation of a \u03bbletrecterms L into maximally compact form L0 proceeds in three steps: (i) translate L into its term graph G = [[L]] ; (ii) compute the maximally shared form of G as its bisimulation collapse G0 ; (iii) read back a \u03bbletrecterm L0 from the term graph G0 with the property [[L0]] = G0. Then L0 represents a maximally shared term graph, and it has the same unfolding as L. The procedure for deciding whether two given \u03bbletrecterms L1 and L2 are unfoldingequivalent computes their term graph interpretations [[L1]] and [[L2]], and checks whether these are bisimilar. For illustration, we also provide a readily usable implementation.
p562
aVInlining is an optimization that replaces a call to a function with that function's body. This optimization not only reduces the overhead of a function call, but can expose additional optimization opportunities to the compiler, such as removing redundant operations or unused conditional branches. Another optimization, copy propagation, replaces a redundant copy of a stilllive variable with the original. Copy propagation can reduce the total number of live variables, reducing register pressure and memory usage, and possibly eliminating redundant memorytomemory copies. In practice, both of these optimizations are implemented in nearly every modern compiler. These two optimizations are practical to implement and effective in firstorder languages, but in languages with lexicallyscoped firstclass functions (aka, closures), these optimizations are not available to code programmed in a higherorder style. With higherorder functions, the analysis challenge has been that the environment at the call site must be the same as at the closure capture location, up to the free variables, or the meaning of the program may change. Olin Shivers' 1991 dissertation called this family of optimizations super\u0392 and he proposed one analysis technique, called reflow, to support these optimizations. Unfortunately, reflow has proven too expensive to implement in practice. Because these higherorder optimizations are not available in functionallanguage compilers, programmers studiously avoid uses of higherorder values that cannot be optimized (particularly in compiler benchmarks). This paper provides the first practical and effective technique for super\u0392 (higherorder) inlining and copy propagation, which we call unchanged variable analysis. We show that this technique is practical by implementing it in the context of a real compiler for an MLfamily language and showing that the required analyses have costs below 3% of the total compilation time. This technique's effectiveness is shown through a set of benchmarks and example programs, where this analysis exposes additional potential optimization sites.
p563
aVMuch research in program optimization has focused on formal approaches to correctness: proving that the meaning of programs is preserved by the optimisation. Paradoxically, there has been comparatively little work on formal approaches to efficiency: proving that the performance of optimized programs is actually improved. This paper addresses this problem for a generalpurpose optimization technique, the worker/wrapper transformation. In particular, we use the callbyneed variant of improvement theory to establish conditions under which the worker/wrapper transformation is formally guaranteed to preserve or improve the time performance of programs in lazy languages such as Haskell.
p564
aVThis paper describes Symbolics' newly redesigned objectoriented programming system, Flavors. Flavors encourages program modularity, eases the development of large, complex programs, and provides high efficiency at run time. Flavors is integrated into Lisp and the Symbolics program development environment. This paper describes the philosophy and some of the major characteristics of Symbolics' Flavors and shows how the above goals are addressed. Full details of Flavors are left to the programmers' manual, Reference Guide to Symbolics Common Lisp. (5)
p565
aVLOOM (Large ObjectOriented Memory) is a virtual memory implemented in software that supports the Smalltalk80(\u2122) programming language and environment on the Xerox Dorado computer. LOOM provides 8 billion bytes of secondary memory address space and is specifically designed to run on computers with a narrow word size (16bit wide words). All storage is viewed as objects that contain fields. Objects may have an average size as small as 10 fields. LOOM swaps objects between primary and secondary memory, and addresses each of the two memories with a different sized object pointer. When objects are cached in primary memory, they are known only by their short pointers. On a narrow word size machine, the narrow object pointers in primary memory allow a program such as the Smalltalk80 interpreter to enjoy a substantial speed advantage. Interesting design problems and solutions arise from the mapping between the two address spaces and the temporary nature of an object's short address. The paper explains why the unusual design choices in LOOM were made, and provides an interesting example of the process of designing an integrated virtual memory and storage management system.
p566
aVWe have implemented Smalltalk80 on an instructionlevel simulator for a RISC microcomputer called SOAR. Measurements suggest that even a conventional computer can provide high performance for Smalltalk80 by abandoning the 'Smalltalk Virtual Machine' in favor of compiling Smalltalk directly to SOAR machine code, linearizing the activation records on the machine stack, eliminating the object table, and replacing reference counting with a new technique called Generation Scavenging. In order to implement these techniques, we had to find new ways of hashing objects, accessing oftenused objects, invoking blocks, referencing activation records, managing activation record stacks, and converting the virtual machine images.
p567
aVA new, high performance Smalltalk80\u2122 implementation is described which builds directly upon two previous implementation efforts. This implementation supports a large object space while retaining compatibility with previous Smalltalk80\u2122 images. The implementation utilizes a interpreter which incorporates a generation based garbage collector and which does not have an object table. This paper describes the design decisions which lead to this implementation and reports preliminary performance results.
p568
aVA processor for the Smalltalk80\u2191 programming language is described. This machine is implemented using a standard bit slice ALU and sequencer, TTL MSI, and NMOS LSI RAMS. It executes an instruction set similar to the Smalltalk80 virtual machine instruction set. The data paths of the machine are optimized for rapid Smalltalk80 execution by the inclusion of a context cache, tag checking, and a hardware method cache. Each context is only partly initialized when created, and has no memory allocated for it until a possibly nonLIFO reference to it is created. The machine is microprogrammed, and uses a simple next microaddress prediction strategy to obtain most of the performance of pipelining without the attendant complexity. The machine can execute simple instructions at over 7M bytecodes per second and has a predicted average throughput of 1.9M bytecodes per second.A processor for the Smalltalk80\u2191 programming language is described. This machine is implemented using a standard bit slice ALU and sequencer, TTL MSI, and NMOS LSI RAMS. It executes an instruction set similar to the Smalltalk80 virtual machine instruction set. The data paths of the machine are optimized for rapid Smalltalk80 execution by the inclusion of a context cache, tag checking, and a hardware method cache. Each context is only partly initialized when created, and has no memory allocated for it until a possibly nonLIFO reference to it is created. The machine is microprogrammed, and uses a simple next microaddress prediction strategy to obtain most of the performance of pipelining without the attendant complexity. The machine can execute simple instructions at over 7M bytecodes per second and has a predicted average throughput of 1.9M bytecodes per second.
p569
aVQUICKTALK is a dialect of Smalltalk80 that can be compiled directly into native machine code, instead of virtual machine bytecodes. The dialect includes \u201chints\u201d on the class of method arguments, instance variables, and class variables. We designed the dialect to describe primitive Smalltalk methods. QUICKTALK achieves improved performance over bytecodes by eliminating the interpreter loop on bytecode execution, by reducing the number of message send/returns via binding some target methods at compilation, and by eliminating redundant class checking. We identify changes to the Smalltalk80 system and compiler to support the dialect, and give performance measurements.
p570
aVWe discuss a recent attempt at providing a more efficient compilation system for the Smalltalk80\u2122 language than currently exists. The approach is unique in that it attempts to work within the existing semantics of the language. A type declaration and inference mechanism is developed in order to make the problem tractable. Details concerning the compiler implemented to date are given, as well as preliminary speed tests of compiled object code.
p571
aVKnowledge bases built in objectoriented systems use networks of interconnected objects in their representations. The mechanism described here provides a way to use such a network as a prototype by making virtual copies of it. The virtual copy is created incrementally. Values of instance variables in the virtual copy are inherited from the prototype until locally overridden in the copy, similar to inheritance of defaults between instances and classes in Loops. A virtual copy preserves the topology of the original network. Virtual copies can be made from virtual copies. Alternative implementations of virtual copies allow different tradeoffs in space and lookup time. Virtual copies can be used for building knowledge bases for design, for representing contexts in a problem solving system, and have other uses in ordinary programming.
p572
aVImpulse86 provides a general and extensible substrate upon which to construct a wide variety of interactive user interfaces for developing, maintaining, and using knowledgebased systems. The system is based on five major building blocks: Editor, Editor Window, PropertyDisplay, Menu, and Operations. These building blocks are interconnected via a uniform framework and each has a welldefined set of responsibilities in an interface.Customized interfaces can be designed by declaratively replacing some of the building blocks in existing Impulse86 templates. Customization may involve a wide range of activities, ranging from simple override of default values or methods that control primitive operations (e.g., font selection), to override of more central Impulse86 methods (e.g., template instantiation). Most customized interfaces require some code to be written\u2014to handle domainspecific commands. However, in all cases, the Impulse86 substrate provides considerable leverage by taking care of the lowlevel details of screen, mouse, and keyboard manipulation.Impulse86 is implemented in Strobe, a language that provides objectoriented programming support for Lisp. This simplifies customization and extension.
p573
aVThe Flamingo Window Management System is based on a remote method invocation mechanism that provides separate processes running in a heterogeneous, distributed computing environment with complete access to Flamingo's objects and methods. This objectoriented interface has made Flamingo a kernel window manager into which device drivers, graphics libraries, window managers and user interfaces can be dynamically loaded. This paper discusses the strengths and weaknesses of Flamingo's system architecture, and introduces a new architecture which will provide a networkwide object space with object protection, migration, and garbage collection.
p574
aVThis article presents a case study of the development of the Intermedia system, a large, objectoriented hypermedia system and associated applications development framework providing sophisticated document linkages. First it presents the educational and technological objectives underlying the project. Subsequent sections capture the process of developing the Intermedia product and detail its architecture and construction, concentrating on the areas in which objectoriented technology has had a significant role. Finally, the successes and failures of the development approach are examined, and several areas of standardization and research that would enhance the process are proposed.
p575
aVTrellis/Owl is an objectbased language incorporating a type hierarchy with multiple inheritance and compiletime type checking. The combination of features in the language facilitates the design, implementation, and evolution of large computer programs. This paper provides an brief introduction to the Trellis/Owl language. It discusses the basic elements of the language, objects, and shows how these are specified and implemented using types, operations, and components. The notion of a type hierarchy is introduced by a discussion of subtyping and inheritance. Other elements of the Trellis/Owl language such as type generators, iterators, and exceptions are briefly presented.
p576
aVThis article describes InterVal, a software tool that allows authors to create dynamic timelines. It is one tool in Intermedia, a framework developed at Brown University's institute for Research in Information and Scholarship (IRIS) that allows professors and students to create linked multimedia documents and encourages exploration, connectivity, and visualization of ideas. The system was written using an objectoriented extension to C, MacApp, and a set of underlying building blocks, or functional groups of objects. This paper describes InterVal and discusses the architecture of the InterVal application, focusing on the design of the objectoriented architecture and on the use of appropriate building blocks. Concluding sections evaluate objectoriented programming and outline future work.
p577
aVA traditional philosophical controversy between representing general concepts as abstract sets or classes and representing concepts as concrete prototypes is reflected in a controversy between two mechanisms for sharing behavior between objects in object oriented programming languages. Inheritance splits the object world into classes, which encode behavior shared among a group of instances, which represent individual members of these sets. The class/instance distinction is not needed if the alternative of using prototypes is adopted. A prototype represents the default behavior for a concept, and new objects can reuse part of the knowledge stored in the prototype by saying how the new object differs from the prototype. The prototype approach seems to hold some advantages for representing default knowledge, and incrementally and dynamically modifying concepts. Delegation is the mechanism for implementing this in object oriented languages. After checking its idiosyncratic behavior, an object can forward a message to prototypes to invoke more general knowledge. Because class objects must be created before their instances can be used, and behavior can only be associated with classes, inheritance fixes the communication patterns between objects at instance creation time. Because any object can be used as a prototype, and any messages can be forwarded at any time, delegation is the more flexible and general of the two techniques.
p578
aVThis paper presents an experience with a programming language SPOOL which is based on the combination of objectoriented programming and logic programming. This language inherits the capability of knowledge base organization from objectoriented programming and its expressive power from logic programming.The experience of the application of SPOOL to the program annotation system showed that this combination was quite useful to formalize domain knowledge into declarative data types and make them reusable in different contexts. It also showed the need for further study such as better linguistic support to exploit the full power of this combination.
p579
aVOrient84/K is an object oriented concurrent programming language for describing knowledge systems. In Orient84/K, an object is composed of the behavior part, the knowledgebase part, and the monitor part, in order to provide objectoriented, logicbased, demonoriented, and concurrentprogramming paradigms in the object framework. Every object is capable of concurrent execution in Orient84/K.In this paper, after describing an overview of Orient84/K, we will describe implementation issues in a concurrent object oriented language. Then, a new method for an efficient implementation of concurrent objects is proposed and formally described. A new virtual machine for Orient84/K is designed using this method, and some preliminary results of evaluation are presented.
p580
aVConcurrent Prolog supports objectoriented programming with a clean semantics and additional programming constructs such as incomplete messages, unification, direct broadcasting, and concurrency synchronization [Shapiro 1983a]. While it provides excellent computational support, we claim it does not provide good notation for expressing the abstractions of objectoriented programming. We describe a preprocessor that remedies this problem. The resulting language, Vulcan, is then used as a behicle for exploring new variants of objectoriented programming which become possible in this framework.
p581
aVAn objectoriented computation model is presented which is designed for modelling and describing a wide variety of concurrent systems. In this model, three types of message passing are incorporated. An overview of a programming language called ABCL/1, whose semantics faithfully reflects this computation model, is also presented. Using ABCL/1, a simple scheme of distributed problem solving is illustrated. Furthermore, we discuss the reply destination mechanism and its applications. A distributed \u201csame fringe\u201d algorithm is presented as an illustration of both the reply destination mechanism and the future type message passing which is one of the three message passing types in our computation model.
p582
aVWe describe an objectoriented architecture for intelligent tutoring systems. The architecture is oriented around objects that represent the various knowledge elements that are to be taught by the tutor. Each of these knowledge elements, called bites, inherits both a knowledge organization describing the kind of knowledge represented and tutoring components that provide the functionality to accomplish standard tutoring tasks like diagnosis, student modeling, and task selection. We illustrate the approach with several tutors implemented in our lab.
p583
aVThis paper describes the design of an algebra system Views implemented in Smalltalk. Views contains facilities for dynamic creation and manipulation of computational domains, for viewing these domains as various categories such as groups, rings, or fields, and for expressing algorithms generically at the level of categories. The design of Views has resulted in the addition of some new abstractions to Smalltalk that are quite useful in their own right. Parameterized classes provide a means for runtime creation of new classes that exhibit generally very similar behavior, differing only in minor ways that can be described by different instantiations of certain parameters. Categories allow the abstraction of the common behavior of classes that derives from the class objects and operations satisfying certain laws independently of the implementation of those objects and operations. Views allow the runtime association of classes with categories (and of categories with other categories), facilitating the use of code written for categories with quite different interpretations of operations. Together, categories and views provide an additional mechanism for code sharing that is richer than both single and multiple inheritance. The paper gives algebraic as well as nonalgebraic examples of the abovementioned features.
p584
aVThis paper presents an objectoriented approach for building distributed systems. An example taken from the field of computer integrated manufacturing systems is taken as a guideline. According to this approach a system is built up through three steps: control and synchronization aspects for each class of objects are treated first using PROT nets, which are a highlevel extension to Petri nets; then data are introduced specifying the internal states of the objects as well as the messages they send each other; finally the connections between the objects are introduced by means of a data flow diagram between classes. The implementation uses ADA as the target language, exploiting its tasking and structuring mechanisms. The flexibility of the approach and the possibility of using a knowledgebased user interface promote rapid prototyping and reusability.
p585
aVThe Application Accelerator Illustration System is a prototype of an integrated CAD environment that supports the development of applicationspecific integrated circuits. The current implementation features a hardware description language compiler, timing analyzer, functional simulator, waveform tracer, and data path place and route facility. The system is implemented in Smalltalk80\u2122.
p586
aVCommonLoops blends objectoriented programming smoothly and tightly with the procedureoriented design of Lisp. Functions and methods are combined in a more general abstraction. Message passing is invoked via normal Lisp function call. Methods are viewed as partial descriptions of procedures. Lisp data types are integrated with object classes. With these integrations, it is easy to incrementally move a program between the procedure and objectoriented styles.One of the most important properties of CommonLoops is its extensive use of metaobjects. We discuss three kinds of metaobjects: objects for classes, objects for methods, and objects for discriminators. We argue that these metaobjects make practical both efficient implementation and experimentation with new ideas for objectoriented programming.CommonLoops' small kernel is powerful enough to implement the major objectoriented systems in use today.
p587
aVVirtual Instruments1 is an experimental programming environment for developing electronic test and measurement (T&M) applications. Intended users are test engineers, who are not programmers, but computer literate domain specialists. Unlike traditional programming environments, that provide weak support for a broad range of applications, virtual instruments provides strong support for a specific application. The programming paradigm is bottomup synthesis of layers of virtual machines \u2014 called virtual instruments \u2014 using human interface models from the application domain, so that software development occurs without writing code. The objectoriented view of the world has proven a natural fit. Implementation was in Berkeley Smalltalk on a SUN workstation.
p588
aVAlthough most attempts to speedup Smalltalk have focused on providing more efficient interpreters, code optimization is probably necessary for further increases in speed. A typesystem for Smalltalk is a prerequisite for building an optimizing compiler. Unfortunately, none of the typesystems so far proposed for Smalltalk are adequate; they either cause nearly all Smalltalk programs to be type incorrect, allow runtime type errors, or do not provide enough information for optimization. This paper presents a typesystem for Smalltalk that is suitable for code optimization.
p589
aVTwo varieties of objectoriented systems exist: one based on classes as in Smalltalk and another based on exemplars (or prototypical objects) as in Act/1. By converting Smalltalk from a class based orientation to an exemplar base, independent instance hierarchies and class hierarchies can be provided. Decoupling the two hierarchies in this way enables the user's (logical) view of a data type to be separated from the implementer's (physical) view. It permits the instances of a class to have a representation totally different from the instances of a superclass. Additionally, it permits the notion of multiple representations to be provided without the need to introduce specialized classes for each representation. In the context of multiple inheritance, it leads to a novel view of inheritance (orinheritance) that differentiates it from the more traditional multiple inheritance notions (andinheritance). In general, we show that exemplar based systems are more powerful than class based systems. We also describe how an existing class based Smalltalk can be transformed into an exemplarbased Smalltalk and discuss possible approaches for the implementation of both andinheritance and orinheritance.
p590
aVConcurrentSmalltalk is a programming language/system which incorporates the facilities of concurrent programming in Smalltalk801. Such facilities are realized by providing concurrent constructs and atomic objects. This paper first gives an outline of ConcurrentSmalltalk. Then, the design of ConcurrentSmalltalk is described. The implementation of ConcurrentSmalltalk is presented in detail.
p591
aVCertain situations arise in programming that lead to multiply polymorphic expressions, that is, expressions in which several terms may each be of variable type. In such situations, conventional objectoriented programming practice breaks down, leading to code which is not properly modular. This paper describes a simple approach to such problems which preserves all the benefits of good objectoriented programming style in the face of any degree of polymorphism. An example is given in Smalltalk80 syntax, but the technique is relevant to all objectoriented languages.
p592
aVCertain situations arise in programming that lead to multiply polymorphic expressions, that is, expressions in which several terms may each be of variable type. In such situations, conventional objectoriented programming practice breaks down, leading to code which is not properly modular. This paper describes a simple approach to such problems which preserves all the benefits of good objectoriented programming style in the face of any degree of polymorphism. An example is given in Smalltalk80 syntax, but the technique is relevant to all objectoriented languages.
p593
aVPi is a debugger written in C + +. This paper explains how objectoriented programming in C + + has influenced Pi's evolution. The motivation for objectoriented programming was to experiment with a browserlike graphical user interface. The first unforeseen benefit was in the symbol table: lazy construction of an abstract syntaxbased tree gave a clean interface to the remainder of Pi, with an efficient and robust implementation. Next, though not in the original design, Pi was easily modified to control multiple processes simultaneously. Finally, Pi was extended to control processes executing across multiple heterogeneous target processors.
p594
aVWe introduce a notation for diagramming the message sending dialogue that takes place between objects participating in an objectoriented computation. Our representation takes a global point of view which emphasizes the collaboration between objects implementing the behavior of individuals. We illustrate the diagram's usage with examples drawn from the Smalltalk80\u2122 virtual image. We also describe a mechanism for automatic construction of diagrams from Smalltalk code.
p595
aVWe used an objectoriented design to build a large scientific application: simulation of radiation therapy treatments for cancer. We provide features familiar in the graphics workstation world, including graphic editing of the proposed treatment, multiple views of the treatment in different windows, and computations which proceed concurrently as the input data are being edited. To make our system practical for the typical clinic we used a popular minicomputer and the vendor's operating system and compiler. This paper describes how we implemented objects, inheritance, message passing, windows, and concurrency in (almost) standard Pascal on a VAX under VMS.
p596
aVA set of concepts for modeling large real time systems is discussed informally. The concepts support the design of centralized as well as distributed systems. They are object oriented in that they correspond to entities of the 'real world', and they are 'change oriented' in that they support not only the first development stage of a system but also its continuous change and evolution. In particularly, the concepts give a promising solution to 'on the fly' changes of existing, active entities.
p597
aVThe Scheme papers demonstrated that lisp could be made simpler and more expressive by elevating functions to the level of first class objects. Oaklisp shows that a message based language can derive similar benefits from having first class types.
p598
aVThe ASP package, a spreadsheet implemented in Smalltalk80, is discussed. A description of the unique data manipulation features of ASP is given. A discussion of how these features arise from the Smalltalk80 environment is included, with emphasis on features not common to all object oriented languages.
p599
aVGenericity, as in Ada or ML, and inheritance, as in objectoriented languages, are two alternative techniques for ensuring better extendibility, reusability and compatibility of software components. This article is a comparative analysis of these two methods. It studies their similarities and differences and assesses to what extent each may be simulated in a language offering only the other. It shows what features are needed to successfully combine the two approaches in a statically typed language and presents the main features of the programming language Eiffel, whose design, resulting in part from this study, includes multiple inheritance and limited form of genericity under full static typing.
p600
aVIdentity is that property of an object which distinguishes each object from all others. Identity has been investigated almost independently in generalpurpose programming languages and database languages. Its importance is growing as these two environments evolve and merge.We describe a continuum between weak and strong support of identity, and argue for the incorporation of the strong notion of identity at the conceptual level in languages for general purpose programming, database systems and their hybrids. We define a data model that can directly describe complex objects, and show that identity can easily be incorporated in it. Finally, we compare different implementation schemes for identity and argue that a surrogatebased implementation scheme is needed to support the strong notion of identity.
p601
aVWe extend the notion of class so that any Boolean combinations of classes is also a class. Boolean classes allow greater precision and conciseness in naming the class of objects governed a particular method. A class can be viewed as a predicate which is either true or false of any given object. Unlike predicates however classes have an inheritance hierarchy which is known at compile time. Boolean classes extend the notion of class, making classes more like predicates, while preserving the compile time computable inheritance hierarchy.
p602
aVSmalltalk80 obtains some of its expressive power from arranging classes in a hierarchy. Inheritance is an important aspect of this hierarchy. An alternative organization of classes is proposed that emphasizes description instead of inheritance. This alternative can be used with compiletime type checking and retains the important characteristics of Smalltalk's hierarchy.
p603
aVObjectoriented programming and abstract data type (ADT) theory have emerged from the same origin of computer science: the inability to deal efficiently with 'programming in the large' during the early seventies. Each of the approaches has led to significant practical and theoretical results resp. Nevertheless it is still unsatisfactory that up to now the mutual influence seems to be limited to more or less syntactical issues (e.g. the provision of packages, clusters, forms). In this paper we report on the objectoriented language ModPascal that was developed as part of the Integrated Software Development and Verification (ISDV) Project. We show how the essence of concepts of ADT theory as algebraic specifications, enrichments, parameterized specifications or signature morphisms as well as their semantics can be consistently integrated in an imperative objectoriented language. Furthermore, as the experience of using ModPascal as target language of the ISDV System has shown, we claim that without similar support of theoretical concepts techniques like formal specification of programs or algebraic verification loose their power and even applicability.
p604
aVThis paper describes the design of a distributed object manager which allows several Smalltalk80 systems to share objects over a localarea network. This object manager is based on the following principles: location transparency and uniform object naming, unique object representation and use of symbolic links for remote access, possibility of object migration and distributed garbage collection. A version of the object manager has been implemented and is currently being integrated on a two nodes configuration.
p605
aVJasmine is an objectoriented system for programminginthelarge. Jasmine describes software using system model objects. These objects are persistent (they have lifetimes of days or decades) and immutable (since system models act as historical records). This paper describes JStore, a distributed, replicated repository for system model objects. JStore provides robust, transactional, writeonce storage.Designs are presented for the serialization, location, and replication of objects. Description procedures serialize objects for network transmission and permanent storage. An expanding ring multicast search algorithm locates saved objects. JStore replicates objects using a lazy replication algorithm built on top of the location mechanism. Decision procedures determine the replication policy used at each storage site.
p606
aVThere are a number of reasons why a user might want to move data structures between Smalltalk images. Unfortunately, the facilities for doing this in the standard Smalltalk image are inadequate: they do not handle circular structures properly, for example. We have implemented a collection of Smalltalk methods that handles circular structures; in addition, these methods have a number of other advantages over those provided in the standard image. This paper is largely a discussion of the issues that arose during their design, implementation, and use.
p607
aVWe describe the results of developing the GemStone objectoriented database server, which supports a model of objects similar to that of Smalltalk80. We begin with a summary of the goals and requirements for the system: an extensible data model that captures behavioral semantics, no artificial bounds on the number or size of database objects, database amenities (concurrency, transactions, recovery, associative access, authorization) and an interactive development environment. Objectoriented languages, Smalltalk in particular, answer some of these requirements. We discuss satisfying the remaining requirements in an object oriented context, and report briefly on the status of the development efforts. This paper is directed at an audience familiar with objectoriented languages and their implementation, but perhaps unacquainted with the difficulties and techniques of database system development. It updates the original report on the project [CM], and expands upon a more recent article [MDP].
p608
aVObjectoriented programming is a practical and useful programming methodology that encourages modular design and software reuse. Most objectoriented programming languages support data abstraction by preventing an object from being manipulated except via its defined external operations. In most languages, however, the introduction of inheritance severely compromises the benefits of this encapsulation. Furthermore, the use of inheritance itself is globally visible in most languages, so that changes to the inheritance hierarchy cannot be made safely. This paper examines the relationship between inheritance and encapsulation and develops requirements for full support of encapsulation with inheritance.
p609
aVThis paper discusses an objectoriented interface from the Smalltalk80\u2122 programming environment to a Unixlike operating system. This interface imposes an objectoriented paradigm on operating system facilities. We discuss some of the higher order abstractions that were created to make use of these facilities, and discuss difficulties we encountered implementing this interface. Several examples of cooperating Smalltalk and operating system processes are presented.
p610
aVMach, a multiprocessor operating system kernel providing capabilitybased interprocess communication, and Matchmaker, a language for specifying and automating the generation of multilingual interprocess communication interfaces, are presented. Their usage together providing a heterogeneous, distributed, objectoriented programming environment is described. Performance and usage statistics are presented. Comparisons are made between the Mach/Matchmaker environment and other related systems. Possible future directions are examined.
p611
aVEmerald is an objectbased language for the construction of distributed applications. The principal features of Emerald include a uniform object model appropriate for programming both private local objects and shared remote objects, and a type system that permits multiple userdefined and compilerdefined implementations. Emerald objects are fully mobile and can move from node to node within the network, even during an invocation. This paper discusses the structure, programming, and implementation of Emerald objects, and Emerald's use of abstract types.
p612
aVOne of the most promising automatic storage reclamation techniques, generationbased storage reclamation, suffers poor performance if many objects live for a fairly long time and then die. We have investigated the severity of this problem by simulating Generation Scavenging automatic storage reclamation from traces of actual fourhour sessions. There was a wide variation in the sample runs, with garbagecollection overhead ranging from insignificant, during the interactive runs, to severe, during a single noninteractive run. All runs demonstrated that performance could be improved with two techniques: segregating large bitmaps and strings, and mediating tenuring with demographic feedback. These two improvements deserve consideration for any generationbased storage reclamation strategy.
p613
aVLanguages like Snobol, Prolog, and Icon were designed with backtracking facilities from the outset and these facilities are deeply intertwined with the implementation. Retrofitting a backtracking facility in a language that wasn't designed for it has never been achieved. We report on an experiment to retrofit Smalltalk with a backtracking facility. The facility is provided through a small number of primitives written in the language (no modifications to the kernel were made). The ability to do this is a direct result of the power provided by the objectification of contexts.
p614
aVThis paper introduces the programming language Modular Smalltalk, a descendant of the Smalltalk80 programming language. Modular Smalltalk was designed to support teams of software engineers developing production application programs that can run independently of the environment in which they are developed. We first discuss our motivation for designing Modular Smalltalk. Specifically, we examine the properties of Smalltalk80 that make it inappropriate for our purposes. We then present an overview of the design of Modular Smalltalk, with an emphasis on how it overcomes these weaknesses.
p615
aVIn this paper, we describe the design and implementation of Orwell, a configuration management tool for multiperson Smalltalk projects. Although the system described has been implemented for Smalltalk, the design is applicable to other languages such as C++, ObjectiveC or ADA. Smalltalk is well recognized as a productive programming environment for an individual programmer, but its lack of team support is currently a major obstacle in using Smalltalk for a large software project. To support multiperson Smalltalk programming, Orwell provides both source and object code sharing as well as version control on a network of personal workstations. Class ownership is used as the primary means for dividing work among programmers during the lifecycle of a project. Orwell also supports groups of programmers not physically connected to a common file server. We describe our implementation which preserves the productive exploratory environment of Smalltalk. Seamless integration and performance are essential for Orwell to be accepted and used by Smalltalk programmers.
p616
aVThere are two major issues to address to achieve integration of an objectoriented programming system with a database system. One is the language issue: an objectoriented programming language must be augmented with semantic data modeling concepts to provide a robust set of data modeling concepts to allow modeling of entities for important realworld applications. Another is the computationalmodel issue: application programmers should be able to access and manipulate objects as though the objects are in an infinite virtual memory; in other words, they should not have to be aware of the existence of a database system in their computations with the data structures the programming language allows. This paper discusses these issues and presents the solutions which we have incorporated into the ORION objectoriented database system at MCC.
p617
aVA general concern about objectoriented systems has been whether or not they are able to meet the performance demands required to be useful for the development of significant production software systems. Attempts to evaluate this assertion have been hampered by a lack of meaningful performance benchmarks that compare database operations across different kinds of databases.\u000aIn this paper, we utilize the Sun Benchmark [Rube87] as a means for assessing the performance of an object database and comparing it with existing relational systems. We discuss the benchmark, and many of the implementation issues involved in introducing a relationally oriented benchmark into an objectoriented paradigm. We demonstrate the performance of an object database using Ontologic's Vbase object database platform as an example of a commercially available object database, and we compare these benchmark results against those of existing relational database systems. The results offer strong evidence that object databases are capable of performing as well as, or better than, existing relational database systems.
p618
aVThere is a requirement for a stronger treatment of intentional concepts and more general inferential ability within database systems. A framework for achieving this will be described in terms of extensions to an object data model.\u000aThe type Concept is introduced into the model as a subtype of Action. Intentional concepts may then be defined as filters or generators, depending on the nature of the defining formula. In either case, the dual action should be available\u2014provided automatically by the system or explicitly by the user.\u000aFinally, sets and types are treated as special cases of extensional concepts, leading to a novel structure of the hierarchy of system types.
p619
aVFabrik is a visual programming environment  a kit of computational and userinterface components that can be \u201cwired\u201d together to build new components and useful applications. Fabrik diagrams utilize bidirectional dataflow connections as a shorthand for multiple paths of flow. Built on objectoriented foundations. Fabrik components can compute arbitrary objects as outputs. Music and animation can be programmed in this way and the user interface can even be extended by generating graphical structures that depend on other data. An interactive type system guards against meaningless connections. As with simple dataflow, each Fabrik component can be compiled into an object with access methods corresponding to each of the possible paths of data propagation.
p620
aVObjectOriented programming is a powerful means of developing large complex systems. In this paper we address the need to understand the behavior of objects in order to facilitate code sharing and reusability. We describe GraphTrace, a tool we have developed that has allowed us to experiment with new ways of visualizing the dynamic behavior of objectoriented programs. Based on our experience with the GraphTrace tool we suggest that being able to present many different views of an objectoriented system and then animating these views concurrently represents a powerful means for understanding such systems.
p621
aVAVANCE1 is an integrated application development and runtime system. It provides facilities for programming with shared and persistent objects, transactions and processes. The architecture is designed with decentralization in mind by having a large object identifier space and a remote procedure call interface to objects. Emphasis in this paper is on the programming language PAL and its relation with the underlying virtual machine.
p622
aVKyma is an objectoriented environment for music composition written in Smalltalk80, which, in conjunction with a microprogrammable digital signal processor called the Platypus, provides the composer with a means for creating and manipulating Sound objects graphically with realtime sonic feedback via software synthesis. Kyma draws no distinctions between the materials and the structure of a composition; both are Sound objects. When a Sound object receives a message to play, it transforms itself into a microSound object, i.e. an object representation of itself in the microcode of the Platypus. Thus an object paradigm is used not only in the representation of Sound objects in Smalltalk80 but also in the microcode representation of those Sound objects on the Platypus.
p623
aVTS (Typed Smalltalk) is a portable optimizing compiler that produces native machine code for a typed variant of Smalltalk, making Smalltalk programs much faster. This paper describes the structure of TS, the kinds of optimizations that it performs, the constraints that it places upon Smalltalk, the constraints placed upon it by an interactive programming environment, and its performance.
p624
aVThis paper describes the use of Smalltalk in the design and implementation of OFMspert. OFMspert is an architecture for an intelligent operator's associate. In order to verify the architecture, an OFMspert was implemented to act as an assistant to an operator of a NASA satellite ground control system. OFMspert is a large system that utilizes a Smalltalk implementation of a knowledgebased problem solving method known as the blackboard architecture. The entire system was designed and implemented in Smalltalk/V\u2122 and later ported to the Smalltalk80\u2122 system. The objectoriented paradigm in general, and Smalltalk\u2021 in particular, greatly facilitated the rapid design and implementation of this system. This paper summarizes the OFMspert architecture, emphasizing its implementation in an objectoriented paradigm.
p625
aVThe programming of the interrupt handling mechanisms, process switching primitives, scheduling mechanisms, and synchronization primitives of an operating system for a multiprocessor require both efficient code in order to support the needs of highperformance or realtime applications and careful organization to facilitate maintenance. Although many advantages have been claimed for objectoriented class hierarchical languages and their corresponding design methodologies, the application of these techniques to the design of the primitives within an operating system has not been widely demonstrated.\u000aTo investigate the role of class hierarchical design in systems programming, the authors have constructed the Choices multiprocessor operating system architecture using the C++ programming language. During the implementation, it was found that many operating system design concerns can be represented advantageously using a class hierarchical approach, including: the separation of mechanism and policy; the organization of an operating system into layers, each of which represents an abstract machine; and the notions of process and exception management. In this paper, we discuss an implementation of the lowlevel primitives of this system and outline the strategy by which we developed our solution.
p626
aVIn this paper, we describe a purely objectoriented framework of pattern recognition systems. Its aim is in dealing with knowledge representation issues in pattern recognition. In our approach, everything works in an entirely autonomous and decentralized manner. Even a search procedure for sampleconcept matching is distributed onto every concept object itself by being implemented in what we introduced as the recursive agentblackboard model. We developed an experimental prototype of character recognition systems in Smalltalk80, which proved the ability of the objectoriented framework and the cooperative search procedure.
p627
aVThis paper describes a new data abstraction mechanism in an objectoriented model of computing. The data abstraction mechanism described here has been devised in the context of the design of Sina/st language. In Sina/st no language constructs have been adopted for specifying inheritance or delegation, but rather, we introduce simpler mechanisms that can support a wide range of code sharing strategies without selecting one among them as a language feature. Sina/st also provides a stronger data encapsulation than most of the existing objectoriented languages. This language has been implemented on the SUN 3 workstation using Smalltalk.
p628
aVWhen ObjectOriented languages are applied to distributed problem solving, the form of communication restricted to direct message sending is not flexible enough to naturally express complex interactions among the objects. We transformed the Tuple Space Communication Model[29] for better affinity with ObjectOriented computation, and integrated it as an alternative method of communication among the distributed objects. To avoid the danger of potential bottleneck, we formulated an algorithm that makes concurrent pattern matching activities within the Tuple Space possible.
p629
aVControlling the propagation of operations through a collection of objects connected by various relationships has been a problem both for the objectoriented and the data base communities. Operations such as copy, destroy, print, and save must propagate to some, but not all, of the objects in a collection. Such operations can be implemented using ad hoc methods on objects, at the cost of extra work and loss of clarity. The use of propagation attributes on the relationships between objects permits a concise, intuitive specification of the manner in which operations should propagate from one object to another. These concepts have been implemented in the objectoriented language DSM and have been used to write applications.
p630
aVA design and verification technique for implementation schemes of distributed software is presented. In this technique, first, the specification is modelled by a concurrent object system, that is, one which is constituted of computational agents with capability of concurrent execution and message passing. Then, such a concurrent object system is transformed into another concurrent object system, which models a sophisticated implementation scheme.\u000aOur transformation technique is mainly based on fusing and splitting concurrent objects. The correctness of transformation rules can be proven formally.
p631
aVOur work is along the line of the work of B. Smith and P. Maes. We first discuss our notion of reflection in objectoriented concurrent computation and then present a reflective objectoriented concurrent language ABCL/R. We give several illustrative examples of reflective programming such as (1) dynamic concurrent acquisition of \u201cmethods\u201d from other objects, (2) monitoring the behavior of concurrently running objects, and (3) augmentation of the time warp mechanism to a concurrent system. Also the definition of a metacircular interpreter of this language is given as the definition of a metaobject. The language ABCL/R has been implemented. All the examples given in this paper are running on our ABCL/R system.
p632
aVThis paper describes an execution model being developed for distributed objectoriented in a messagepassing multipleinstruction/multipledatastream (MIMD) environment. The objective is to execute an objectoriented program as concurrently as possible. Some opportunities for concurrency can be identified explicitly by the programmer. Others can be identified at compile time. There are some opportunities for concurrency, however, that can only be discovered at runtime because they are datadependent. The model of execution we describe attempts to discover and exploit the datadependent concurrency that exists in a given program execution.
p633
aVWe introduce a simple, programming language independent rule (known inhouse as the Law of Demeter\u2122) which encodes the ideas of encapsulation and modularity in an easy to follow form for the objectoriented programmer. You tend to get the following related benefits when you follow the Law of Demeter while minimizing simultaneously code duplication, the number of method arguments and the number of methods per class: Easier software maintenance, less coupling between your methods, better information hiding, narrower interfaces, methods which are easier to reuse, and easier correctness proofs using structural induction. We discuss two important interpretations of the Law (strong and weak) and we prove that any objectoriented program can be transformed to satisfy the Law. We express the Law in several languages which support objectoriented programming, including Flavors, Smalltalk80, CLOS, C++ and Eiffel.
p634
aVOne of the most difficult aspects of creating graphical, direct manipulation user interfaces is managing the relationships between the graphical objects on the screen and the application data structures that they represent. Coral (Constraintbased Objectoriented Relations And Language) is a new user interface toolkit under development that uses efficientlyimplemented constraints to support these relationships. Using Coral, user interface designers can construct interaction techniques such as menus and scroll bars. More importantly, Coral makes it easy to construct directmanipulation user interfaces specialized to particular applications. Unlike previous constraintbased toolkits, Coral supports defining constraints in the abstract, and then applying them to different object instances. In addition, it provides iteration constructs where lists of items (such as those used in menus) can be constrained as a group. Coral has two interfaces: a declarative interface that provides a convenient way to specify the desired constraints, and a procedural interface that will allow a graphical user interface management system (UIMS) to automatically create Coral calls.
p635
aVET++ is an objectoriented application framework implemented in C++ for a UNIX\u2020 environment and a conventional window system. The architecture of ET++ is based on MacApp and integrates a rich collection of user interface building blocks as well as basic data structures to form a homogeneous and extensible system. The paper describes the graphic model and its underlying abstract window system interface, shows composite objects as a substrate for declarative layout specification of complex dialogs, and presents a model for editable text allowing the integration of arbitrary interaction objects.
p636
aVThis paper describes the evolution of the Transportable Applications Executive (TAE) (developed at NASA/Goddard Space Flight Center) from a traditional procedural menu and commandoriented system to an objectoriented, modeless user interface management system, known as TAE Plus. The impetus for developing this environment and early experiments which led to its current implementation are addressed. The current version of TAE Plus provides design and prototyping functions, working in tandem with a mature application management system. The main components are (1) a user interface designers' WorkBench that allows an application developer to interactively layout an application screen and define the static and/or dynamic areas of the screen; (2) an application programmer subroutine package that provides runtime services used to display and control WorkBenchdesigned \u201cinteraction objects\u201d on the screen; and (3) an extension to the existing TAE command language that provides commands for displaying and manipulating interaction objects, thus providing a means to quickly prototype an application's user interface. During TAE Plus development, many design and implementation decisions were based on the stateoftheart within graphics workstations, windowing systems and objectoriented programming languages, and this paper shares some of the problems and issues experienced during implementation. Some of the topics discussed include: lessons learned in using the Smalltalk\u2122 language to prototype the initial WorkBench; why C++ was selected (over other languages) to build the WorkBench; and experiences in using X Window System\u2122 and Stanford's InterViews object library. The paper concludes with open issues and a description of the next steps involved in implementing the \u201ctotally modern\u201d TAE.
p637
aVThe Smalltalk80\u2122 user interface and graphics model are based on monochromatic graphics. One natural step in the evolution of the Smalltalk80 system is the addition of color. This paper describes an implementation of color Smalltalk. Classes have been defined to manipulate visual color models and colored graphics objects. The extensive collaboration between classes which describe color, classes which perform basic graphics operations, and classes in the user interface is explored. Issues in the design and implementation are examined. Potential future directions for objectoriented color systems are discussed.
p638
aVWe describe the design of a constraintbased window system for Smalltalk. This window system uses constraints to specify attributes of windows and relationships between them. Three classes of constraints are supported, one of which is implicit and not available for general use. The system extends the current Smalltalk system, providing support for both fixedsize and fixedscale windows. It also provides the capability to dynamically reorganize the layout of a window. A goal of the design is to produce a system with realtime response that is fast enough to be substituted for the existing system. A prototype with response times of approximately 1/4 second has been implemented to demonstrate the feasibility of the design as well as to point out several important optimizations.
p639
aVThe Smalltalk80\u2122 programming environment, though powerful for prototyping applications, does not have any mechanisms for constructing a standalone version of an application. Traditionally, the application is bundled with an image including the entire development environment. Production applications frequently require that the only interface visible to the end user be that of the application. A common misperception among Smalltalk80 application developers is that it is impossible to:\u000a\u000adevelop and deliver applications containing proprietary algorithms,\u000aprevent inspection and modification of the application,\u000aseparate the development environment from the delivered application,\u000aprovide annotation of the application classes and methods without actually revealing the source code to the end user.\u000a\u000aIn this paper, we introduce various techniques and mechanisms for meeting these requirements.
p640
aVIt is difficult to introduce both novice and experienced procedural programmers to the anthropomorphic perspective necessary for objectoriented design. We introduce CRC cards, which characterize objects by class name, responsibilities, and collaborators, as a way of giving learners a direct experience of objects. We have found this approach successful in teaching novice programmers the concepts of objects, and in introducing experienced programmers to complicated existing designs.
p641
aVPROCOL is a parallel Cbased objectoriented language with communication based on oneway synchronous messages. Objects execute in parallel unless engaged in communication. Communication partners are defined by object instance identifiers, or by type. Therefore sendreceive mappings may be 11, n1, or 1n, though only 1 message is transferred. PROCOL controls object access by a novel concept: an explicit perobject protocol. This protocol is a specification of the occurrence and sequencing of the communication between the object and its partners. Thus protocols support structured, safer and potentially verifiable information exchange between objects. Protocols also act as a composition rule over client objects, thereby offering a 'partof' hierarchy of these cooperating objects.
p642
aVWe discuss several issues related to the integration of inheritance and concurrency in an objectoriented language to support finegrain parallel algorithms. We present a reflective extension of the actor model to implement inheritance mechanisms within the actor model. We demonstrate that a particularly expressive and inheritable synchronization mechanism must support local reasoning, be composable, be firstclass, and allow parameterization based on message content. We present such a mechanism based on the concept of enabledsets, and illustrate each property. We have implemented enabledsets in the Rosette prototyping testbed.
p643
aVComandos is a project within the European Strategic Programme for Research on Information Technology  ESPRIT and it stems from the identified need of providing simpler and more integrated environments for application development in large distributed systems.\u000aThe fundamental goal of the project is the definition of an integrated platform providing support for distributed and concurrent processing in a LAN environment, extensible and distributed data management and tools for monitoring and administrating the distributed environment.\u000aAn object oriented approach was used as the ground level for the integration of the multidisciplinary concepts addressed in the project.\u000aThis paper starts by describing the basic model and architecture of Comandos, which results from a common effort by all the partners in the project. We focus then on the description of a first prototype of the system, which implements a subset of the architecture and is currently running on a set of personal computers and workstations at INESC. The prototype is a testbed for the architecture, providing dynamic linking, access to persistent objects and transparent distribution. Special attention is given to the performance aspects of object invocation in virtual memory.
p644
aVProgramming languages for children have been limited by primitive control and data structures, indirect user interfaces, and artificial syntax. Playground is a childoriented programming language that uses objects to structure data and has a modular control structure, a directmanipulation user interface, and an Englishlike syntax. Integrating Playground into the curriculum of a classroom of 9 to 10yearolds has given us valuable insights from the programs intended users, and confirmed many of our design decisions.
p645
aVSeveral recent theories have been advanced to model complex heuristic behavior. From Minsky's Society of Mind to neural networks, these theories all share a common architecture consisting of many highly connected elements that together exhibit sophisticated macro behavior. This paper describes NeuralAgents, a framework developed in Smalltalk80 that combines the high level interaction and collaboration of rulebased agents with the topological connectivity of neural nets. The ClientFacilitatorConsultant model for agencies will be presented, along with the object oriented design and implementation of the NeuralAgents system. PetWorld, an animated environment for the simulation of animal behavior is presented as an example application.
p646
aVThis paper describes several extensions to the traditional frame based programming semantics. We generalize the treatment of slots to include framelike semantics; in addition we allow slots to be referenced by complex terms, whereas previously described frame based systems typically allow only scalar slot references. The resulting frame system naturally accommodates complex data structures and provides an extremely powerful mechanism for specifying complex relationships between objects represented by frames.
p647
aVThis paper describes the design of a scientific prototyping environment in Smalltalk and discusses implementation strategies to achieve high performance interactive modeling of computationally intensive physical problems. Classes for scientific visualization, including contour plotting and 3D surface representations which incorporate the modelviewcontroller paradigm are presented. Techniques for inclusion of user primitives written in C to support computationally intense methods are described in detail in their current implementation on advanced Smalltalk workstations (SUN4, Ardent Titan, Tektronix 4317).
p648
aVFunction minimization techniques often require values for the first partial derivatives of the function at a point. Certain techniques, such as Newton's method, require the values of the second partial derivatives as well. The methods commonly used to obtain these values have certain drawbacks which can be eliminated using a technique known as automatic differentiation. When this technique is implemented in an object oriented language with operator overloading capabilities, the problem of differentiation and minimization can be mapped into a code which fits well with the problem space.
p649
aVMany numerical analysts and Lisp/Smalltalk programmers share the assumption that languages like Fortran are more appropriate for traditional, quantitative scientific programming than objectoriented languages. To show how straightforward application of objectoriented design to standard algorithms in numerical linear algebra improves clarity and expressiveness, without sacrificing speed or accuracy, I describe parts of Cactus, a system for numerical linear algebra and constrained optimization, implemented in Common Lisp and CLOS.
p650
aVThis paper describes an application framework, called Vamp, which supports Interactive Compound Documents and applications as collections of Interactive Objects. Aldus developed Vamp as a practical experiment in solving several software engineering and project management problems including software reusability, portability, and management of large program teams. Vamp is written in C ++ to run on the Macintosh and Microsoft Windows. This paper discusses the decisions that led to the architecture of Vamp and what Aldus stands to gain by its use.
p651
aVSix expert Smalltalk programmers and three expert procedural programmers were observed as they worked on a gourmet shopping design problem; they were asked to think aloud about what was going through their minds as they worked. These verbal protocols were recorded and examined for ways in which the programmers' understanding of the problem domain affected the design process; most of our examples are from the three Smalltalk programmers who focussed most on the mapping from problem to solution. We characterize the problem entities that did appear as solution objects, the active nature of the mapping process, and ways in which the resultant objects went beyond their problem analogs.
p652
aVThe Data Structure Manager (DSM) combines objectoriented programming with semantic data modeling concepts in the context of the C language. DSM is a fullfeatured objectoriented language which includes single and multiple inheritance, class descriptor objects, metaclasses, choice of method binding time, error handling, persistent objects, modularity, and an interactive interpreter in an efficient manner. In addition, DSM supports the association and aggregation relationships as part of the Object Modeling Technique (OMT) used for conceptual design. DSM has been used since 1986 to build a variety of research and productionquality software such as an advanced CAF/CAD system.
p653
aVA multilevel secure objectoriented data model (using the ORION data model) is proposed for which mandatory security issues in the context of a database system is discussed. In particular the following issues are dealt with: (1) the security policy for the system, (2) handling polyinstantiation, and (3) handling the inference problem.\u000aA set of security properties that has been established in this paper is more complete than those that have been proposed previously. Finally we describe how certain security constraints are handled by our model.
p654
aVThis paper addresses the problem of an efficient dispatch mechanism in an objectoriented system with multiple inheritance. The solution suggested is a direct table indexed branch such as is used in C++. The table slot assignments are made using a coloring algorithm. The method is applicable to strongly typed languages such as C++ (with multiple inheritance added) and Eiffel, and in a slightly slower form to less strongly typed languages like Objective C.
p655
aVThis paper uses the persistent object system PCLOS to survey some problems and benefits of object persistence. The system is analyzed along several relevant dimensions. PCLOS provides object persistence for an objectoriented language. The insights gained on desirable and detrimental components of the system are presented. The intent is to outline some of the expected and unexpected problems encountered in the construction of support for object persistence.
p656
aVSolo is a portable window interface written in the Common Lisp Object System (CLOS) objectoriented programming language. Solo provides a virtual window machine which is targeted to a host window system by implementing a set of host window system specific classes and methods for Solo's host window system driver protocol. The interface presented by Solo to an application insulates it from differences in the host window system, facilitating application portability. Solo distinguishes itself from other objectoriented window systems by exploiting certain features of CLOS. CLOS method combination simplifies initialization of windows while preserving easy extensibility of the basic classes. Generic dispatch on multiple arguments, a feature unique to CLOS, allows a simpler and more flexible input event dispatching protocol. A powerful event description language simplifies the specification of keyboard and mouse events. A prototype implementation runs on the server based XII and NeWS host systems, and on the frame buffer based Lucid Window Toolkit.
p657
aVThis paper presents a case study of AMEP, a prototype ESM signal processor which has been implemented in Smalltalk using an actorbased design methodology, Arguments for choosing an OOPS for implementing such applications are reviewed. AMEP is a large system which includes both hard realtime and knowledgebased subsystems. Extensive software metrics are presented for each subsystem and used to compare the characteristics of code designed for different purposes. For example, analysis of this data suggests that knowledgebased applications may be more difficult to port to an objectbased language such as Ada than hard realtime systems. Team programming, productivity, documentation standards and other software engineering issues are also addressed.
p658
aVThe Choices operating system architecture [3, 4, 15] uses class hierarchies and objectoriented programming to facilitate the construction of customized operating systems for shared memory and networked multiprocessors. The software is being used in the Tapestry Parallel Computing Laboratory at the University of Illinois to study the performance of algorithms, mechanisms, and policies for parallel systems. This paper describes the architectural design and class hierarchy of the Choices memory and secondary storage management system.\u000aThe mechanisms and policies of a virtual memory system implement a memory hierarchy that exploits the tradeoffs between response times and storage capacities. In Choices, the notion of a memory hierarchy is represented by layers in which abstract classes define interfaces between and internal to the layers. Concrete subclasses implement new algorithms or data structures or specializations of existing ones. This paper describes the motivation for an objectoriented, classhierarchical approach to virtual memory system design, and describes the overall architecture of such an approach, as it has been applied to the Choices system. Special attention is paid to the advantages in both design and implementation that have resulted from using objectoriented techniques.
p659
aVIn this paper we are concerned with addressing techniques for statically typed languages with multiple inheritance. The addressing techniques are responsible for the efficient implementation of record field selection. In objectoriented languages, this record selection is equivalent to the access of methods. Thus, the efficiency of these techniques greatly affects the overall performance of an objectoriented language. We will demonstrate that addresses, in such systems, cannot always be calculated statically and show how symbol tables have been used as address maps at run time. The essence of the paper is a new addressing technique that can statically calculate either the address of a field or the address of the address of the field. This technique is powerful enough to support an efficient implementation of multiple inheritance with implicit subtyping as described by Cardelli.
p660
aVMany tools and techniques exist for the modeling and analysis of computer and communication systems. These tools are often complex and tailored to a narrow range of problems. The system analysis task often requires coordinated use of multiple tools and techniques which is not supported by currently available systems. The Tangram project goal is to develop an environment which makes a large set of tools and techniques readily accessible and is easily tailored to specialized applications.\u000aThis system has been prototyped in an objectoriented extension to Prolog. The impact that these two paradigms, logic and objects, have had on the design is discussed. Several example applications are presented to illustrate the extensibility of the system.
p661
aVWe provide a formal agenda for teaching the objectoriented paradigm in a programming language independent manner, and a tool which supports our teaching approach. Our proposal for a comprehensive study of the subject includes an ordered set of objectives designed to guide the uninitiated user from zero knowledge about objectoriented programming through class definitions, inheritance, subtyping, and the parameterization of classes. This set of graded objectives provides both a useful metric for gauging a student's progress, and a facility through which users can begin their studies at a level commensurate with their experience.
p662
aVThree important goals of next generation software development environments (SDEs) are extensibility, integration and broad scope. Our work on OROS is predicated on the hypothesis that a type model, incorporated into an environment's object manager, can contribute to achieving those goals. This paper reports on an attempt at applying objectoriented typing concepts in the domain of software development environments. We believe that the result is a type model that has properties of interest both to software environment builders and also to builders and users of objectoriented systems in general.
p663
aVMetaclasses provide control of object description, method compilation and more generally class behaviour. Consequently, they have been widely used in languages such as Smalltalk80 or more recently CLOS by both users and implementers. Due to their powerful control of instances, careless inheritance of classes with different metaclasses can lead to unsound or inefficient implementations of instance methods. Using examples, we will pinpoint these problems. Then we will introduce compatibility rules which need to be fulfilled by metaclasses of inherited classes in order to allow reasonable instance method description.
p664
aVThis paper describes various models of computational reflection in class based object oriented language. Two different approaches are covered: the metaobject approach which supposes that every object can have a metaobject describing and monitoring its behavior, and the message reification approach which describes a message as an object. The metaobject approach is discussed more fully showing that it is important to differentiate between structural reflection and computational reflection. We will see that, whereas classes and metaclasses are very important for the former, they cannot cope adequately with the later. Therefore we introduce a model of computational reflection where metaobjects are instances of a class METAOBJECT or of one of its subclasses.
p665
aVComputational reflection makes it easy to solve problems that are otherwise difficult to address in Smalltalk80, such as the construction of monitors, distributed objects, and futures, and can allow experimentation with new inheritance, delegation, and protection schemes. Full reflection is expensive to implement. However, the ability to override method lookup can bring much of the power of reflection to languages like Smalltalk80 at no cost in efficiency.
p666
aVThe programming environment described in this paper is an adaptation of Donald Knuth's concept of literate programming, applied to Smalltalk programs. The environment provides a multimedia document production system including media for Smalltalk class and method definitions.\u000aThere are two outputs from the system. The first output is a document which contains general descriptions and discussions intermixed with precise definitions of program fragments, test inputs and test results. The second output consists of compiled Smalltalk programs installed and ready for execution.\u000aThe main idea was to produce program documentation that was just as interesting and fascinating to read as ordinary literature. Our experience showed an added benefit, namely that the literate programming environment was an active aid in the problem solving process. The simultaneous programming and documentation lead to significantly improved quality of both programs and documentation.
p667
aVThe Smalltalk ModelViewController (MVC) user interface paradigm uses polling for its input control. The polling loops consume CPU cycles even when the user is not interacting with the interface. Applications using Smalltalk as their frontend often suffer unnecessary performance loss. This paper presents a prototype eventdriven MVC framework to solve these problems. A solution to the compatibility problem is also provided to allow interface objects built under both polling and eventdriven mechanisms to be used by each other with no modification and no performance penalty.
p668
aVAt the core of any sophisticated software development and maintenance environment is a large mass of complex data. The data (the central data of the environment) is composed of smaller sets of data that can be related in complicated and often subtle ways. The user or developer of the environment will be more effective if they are able to deal with conceptual slices, or views, of the large, complex structure. This paper presents an architectural building block for objectbased software environments based on the views concept. The building block allows the construction of global abstractions that describe unified behavior of large sets of objects. The basis of the architecture relies on extending the objectoriented paradigm in three steps: (1) defining multiple interfaces in object classes; (2) controlling visibility of instance variables; and (3) allowing multiple copies of an instance variable to occur within an object instance. This paper focuses on the technical aspects of the views approach.
p669
aVArguments have been given recently for providing the functionality of prototypes in objectoriented languages. Prototypes allow more flexible sharing of code and data by delegating messages to parent objects without the rigid structure of a class hierarchy. Prototypes can implement classes, and delegation can be used to model both single and multiple inheritance. However, one drawback with delegation is the difficulty in enforcing the semantics that delegation is used to model. This paper proposes a novel mechanism to control the delegation of messages with rules. In this system, the delegation of messages is governed by a set of rules possessed by each object. Rules can be used to implement classical single inheritance and can implement various solutions to multiple inheritance. In addition, rules can be created dynamically to model applicationspecific semantics. This paper describes how rulebased delegation works and illustrates various rules for rulebased delegation that have been implemented.
p670
aVWe believe that the regime governing the patterns of sharing between objects, and of exchange of messages between them, should not be \u201chardwired\u201d into a programming language, but should be specifiable by the builders of a system to fit its particular requirements. This thesis has been the primary motivation behind our general concept of lawgoverned system, which serves as the foundation for this paper.\u000aWe show how the law can be used to impose a variety of useful constraints over the structure and behavior of delegationbased systems. Such a law may contain some very specific rules that apply only to small parts of a system; it may also impose some very general regimes, such as class inheritance, on the entire system. We also argue that the constraints established by the law can be easily adapted to the changing requirements of an evolving system. This should be very useful in managing the process of software development from its design stage, through prototyping, construction, and beyond.
p671
aVThingLab II is an objectoriented constraint programming system designed specifically for interactive user interface construction and implemented in Smalltalk80 For constraints to be effective in building user interfaces, they must not impede the responsiveness of the user interface either at run time or during construction. The necessary speed is attained in ThingLab II by making judicious tradeoffs between compilation and interpretation, and by using a fast, incremental algorithm for constraint satisfaction. The resulting system allows user interface components to be assembled, tested, and modified expediently while maintaining interactive responsiveness.
p672
aVThe Opportunistic Garbage Collector (OGC) is a generational garbage collector for stock hardware and operating systems. While incorporating important features of previous systems, the OGC includes several innovations. A new bucket brigade heap organization supports advancement thresholds between one and two scavenges, using only two or three spaces per generation, and without requiring perobject counts. Opportunistic scavenging decouples scavenging from the filling of available memory, in order to hide potentially disruptive scavenge pauses and improve efficiency. Card marking efficiently records which small areas of the heap may contain pointers into younger generations, and is supported by a refinement of the crossing map technique, to enable scanning of arbitrary cards.
p673
aVThingLab II, a rewrite of ThingLab, provides two representations of objects: fullyexposed and interpreted Things, or hidden and compiled Modules. Both representations provide the full power of the ThingLab II constraint hierarchy (an ordering of constraint preferences), and both can be manipulated by the graphical userinterface. This paper briefly describes Modules and their environmental support in ThingLab II. It also describes the process by which the ModuleCompiler translates a collection of objects (a ThingLab II Thing) into a single object with compiled and optimized Smalltalk80 methods (a Module).
p674
aVThe notions of class, subclass and virtual procedure are fairly well understood and recognized as some of the key concepts in objectoriented programming. The possibility of modifying a virtual procedure in a subclass is a powerful technique for specializing the general properties of the superclass.\u000aIn most objectoriented languages, the attributes of an object may be references to objects and (virtual) procedures. In Simula and BETA it is also possible to have class attributes. The power of class attributes has not yet been widely recognized. In BETA a class may also have virtual class attributes. This makes it possible to defer part of the specification of a class attribute to a subclass. In this sense virtual classes are analogous to virtual procedures. Virtual classes are mainly interesting within strongly typed languages where they provide a mechanism for defining general parameterized classes such as set, vector and list. In this sense they provide an alternative to generics.\u000aAlthough the notion of virtual class originates from BETA, it is presented as a general language mechanism.
p675
aVThe arrangement of classes in a specialization hierarchy has proved to be a useful abstraction mechanism in classbased object oriented programming languages. The success of the mechanism is based on the high degree of code reuse that is offered, along with simple type conformance rules.\u000aThe opposite of specialization is generalization. We will argue that support of generalization in addition to specialization will improve class reusability. A language that only supports specialization requires the class hierarchy to be constructed in a top down fashion. Support for generalization will make it possible to create superclasses for already existing classes, hereby enabling exclusion of methods and creation of classes that describe commonalties among already existing ones.\u000aWe will show how generalization can coexist with specialization in classbased object oriented programming languages. Furthermore, we will verify that this can be achieved without changing the simple conformance rules or introducing new problems with name conflicts.
p676
aVThis paper discusses the introduction of explicit metaclasses  la ObjVlisp into the Smalltalk80 language. The rigidity of Smalltalk metaclass architecture motivated this work. We decided to implement the ObjVlisp model into the standard Smalltalk80 system. The resulting combination defines the Classtalk platform. This platform provides a fullsize environment to experiment with classoriented programming by combining implicit metaclasses  la Smalltalk and explicit metaclasses  la ObjVlisp. Obviously, these experiments are not limited to the Smalltalk world and will be useful to understand and practice the metaclass concept advocated by modern objectoriented languages such as ObjVlisp and CLOS.
p677
aVThis paper presents a denotational model of inheritance. The model is based on an intuitive motivation of the purpose of inheritance. The correctness of the model is demonstrated by proving it equivalent to an operational semantics of inheritance based upon the methodlookup algorithm of objectoriented languages. Although it was originally developed to explain inheritance in objectoriented languages, the model shows that inheritance is a general mechanism that may be applied to any form of recursive definition.
p678
aVCentral features of objectoriented programming are method inheritance and data abstraction attained through hierarchical organization of classes. Recent studies show that method inheritance can be nicely supported by ML style type inference when extended to labeled records. This is based on the fact that a function that selects a field \u0192 of a record can be given a polymorphic type that enables it to be applied to any record which contains a field \u0192. Several type systems also provide data abstraction through abstract type declarations. However, these two features have not yet been properly integrated in a statically checked polymorphic type system.\u000aThis paper proposes a static type system that achieves this integration in an MLlike polymorphic language by adding a class construct that allows the programmer to build a hierarchy of classes connected by multiple inheritance declarations. Moreover, classes can be parameterized by types allowing \u201cgeneric\u201d definitions. The type correctness of class declarations is statically checked by the type system. The type system also infers a principal scheme for any type correct program containing methods and objects defined in classes.
p679
aVA new algorithm for distributed garbage collection is presented. This algorithm collects distributed garbage incrementally and concurrently with user activity. It is the first incremental algorithm that is capable of collecting cyclic distributed garbage. Computational and network communication overhead are acceptable. Hosts may be temporarily inaccessible and synchronization between hosts is not necessary. The algorithm is based on asynchronous distribution of timestamp packets each containing a list of lastaccess times of some relevant remotely referenced objects. Finally, the correctness and time complexity of the algorithm are discussed.
p680
aVWe have developed and implemented techniques that double the performance of dynamicallytyped objectoriented languages. Our SELF implementation runs twice as fast as the fastest Smalltalk implementation, despite SELF's lack of classes and explicit variables.\u000aTo compensate for the absence of classes, our system uses implementationlevel maps to transparently group objects cloned from the same prototype, providing data type information and eliminating the apparent space overhead for prototypebased systems. To compensate for dynamic typing, userdefined control structures, and the lack of explicit variables, our system dynamically compiles multiple versions of a source method, each customized according to its receiver's map. Within each version the type of the receiver is fixed, and thus the compiler can statically bind and inline all messages sent to self. Message splitting and type prediction extract and preserve even more static type information, allowing the compiler to inline many other messages. Inlining dramatically improves performance and eliminates the need to hardwire lowlevel methods such as +,==, and ifTrue:.\u000aDespite inlining and other optimizations, our system still supports interactive programming environments. The system traverses internal dependency lists to invalidate all compiled methods affected by a programming change. The debugger reconstructs inlined stack frames from compilergenerated debugging information, making inlining invisible to the SELF programmer.
p681
aVObjectoriented programming languages support encapsulation, thereby improving the ability of software to be reused, refined, tested, maintained, and extended. The full benefit of this support can only be realized if encapsulation is maximized during the design process.\u000aWe argue that design practices which take a datadriven approach fail to maximize encapsulation because they focus too quickly on the implementation of objects. We propose an alternative objectoriented design method which takes a responsibilitydriven approach. We show how such an approach can increase the encapsulation by deferring implementation issues until a later stage.
p682
aVDevelopers of application software must often work with \u201clegacy systems.\u201d These are systems that have evolved over many years and are considered irreplaceable, either because it is thought that duplicating their function would be too expensive, or because they are trusted by users. Because of their age, such systems are likely to have been implemented in a conventional language with limited use of data abstraction or encapsulation. The lack of abstraction complicates adding new applications to such systems and the lack of encapsulation impedes modifying the system because applications depend on system internals. We describe our experience providing and using an objectoriented interface to a legacy system.
p683
aVThis paper presents our experience building an extendible software development environment using the objectoriented paradigm. We have found that object instances provide a natural way to model program constructs, and to capture complex relationships between different aspects of a software system. The objectoriented paradigm can be efficiently implemented on standard hardware and software, and provides some degree of extendibility without requiring major modifications to the existing implementation.\u000aUnfortunately, we have also found that some natural extensions that we would like to make to the environment are not easily incorporated. We argue that the lack of extendibility is due to the objectoriented paradigm's lack of support for providing modifications and extensions to the objectoriented paradigm itself.
p684
aVThe objectoriented model has become exceedingly attractive as the best answer to the increasingly complex needs of the software development community. The initial boasts regarding quality, reuse, concurrency, and scalability are now being substantiated by documented software development experience. It is certainly appropriate, then, for educators who teach computer science (the current preparation for emerging software developers) to examine where objectoriented development best fits into the computer science curriculum. How much of the paradigm\u2014language, analysis, design, management\u2014ought to be addressed? At what level in an undergradute program are objectoriented techniques appropriate? What are the risks involved? How is the objectoriented approach effectively taught?
p685
aVThis paper describes the effort undertaken at the University of Iowa to institute a software engineering curriculum at the graduate level. We describe our experiences over 2 years in developing a pilot 2semester course in objectoriented software engineering. This pilot course had the goals of introducing practical and realistic objectoriented software engineering to students, and developed several innovative techniques to achieve this goal on a very limited budget.
p686
aVA model for verification and testing in an objectoriented CS2 course is presented. The model has four stages\u2014two for verification and two for testing\u2014at which student progress is evaluated. Students thereby see verification and testing as integral parts of the programming process, rather than as addenda. The impact of this model on students' programming behavior is discussed.
p687
aVThis presentation discusses the evolution of the curriculum in objectoriented methods in the main undergraduate degree in the Department of Computer Science at Swinburne University of Technology in Melbourne, Australia. The degree has a strong Software Engineering emphasis, and objectoriented analysis and design methods are complemented by a programming curriculum which uses Ada. The forthcoming Ada9X will have a substantial impact on the curriculum.
p688
aVCurrently objectoriented technology (OOT) plays only a minor role in the computer science curriculum at most universities. This is because the curriculum is already crowded and OOT is just one of many new topics competing for a place. This paper describes reasons for including OOT in the curriculum and strategies for doing so. The experiences of two universities which teach OOT are described. The first university emphasizes OOT, while the second offers only a single course on the subject.
p689
aVA 92 Vancouver, British Columbia, Canada 5  10 October 1992\u000aAddendum to the Proceedings Panel Report The 00 Software\u000aDevelopment Process Report by: Dennis de Champeaux HPLabs Members\u000aof the panel Dermis de Champeaux, HPLab, moderator Bob Balzer,\u000aInformation Sciences Institute Dave Bulman, Pragmatics Kathleen\u000aCulverLozo, AT&T Bell Laboratories Ivar Jacobson, Objectory\u000aStephen J. Mellor, Project Technology Pankaj Garg, HPLab,\u000acoorganizer of the panel Aim of the panel Improving development\u000aprocesses is a key issue for the software community in general. The\u000aliterature gives us some guidance with respect to major activities\u000aof the development process. The waterfall, spiral and fountain\u000amodel all agree upon distinctions such as: requirements capture,\u000aanalysis, design, implementation, testing, maintenance. They also\u000aagree in that they do not provide a finer granularity of the inside\u000aactivities of analysis and design. We postulate that this omission\u000ais a key inhibitor to a more widespread acceptance of the 00\u000aparadigm by project managers. They do not know how to plan a\u000anontrivial 00 software development project. The purpose of this\u000apanel was to take the first steps towards developing such process\u000amodels. More 510 October 1992 61 specifically the panelists have\u000abeen asked to address the following questions (among others): . Can\u000ayou envision a generic OOA/ OOD development process? If so what are\u000athe major steps and deliverables of these steps. If not, why not? l\u000aIf a software development organization had to choose between either\u000adoing a paradigm shift to 00, or improving its rating on the SE1\u000amaturity scale what would you recommend? What are the relationships\u000abetween the two? Qualify, if necessary, the answer with respect to\u000aan extant maturity level. It turned out that these, and the other\u000aquestions, were quite hard for the panellists. Hence, we envision a\u000afollow on panel in the future where these topics will be raised\u000aagain. In addition to this panel, I organized in 1992, workshops on\u000a00 process: one at ECOOP and one also at OOPSLA. All the events had\u000ain common that the presenterslpanelists had hardly anything to say\u000aabout process. The most tangible contributions were modest\u000aextensions of the standard macro process: requirements capture,\u000aanalysis, design, implementation, test, maintenance, enhanced, of\u000acourse, by the recommendation to iterate. Addendum to the\u000aProceedings Hence, although the panel had lively exchanges, I\u000arefrain from giving the details because I do not feel that the\u000astate of the art was advanced. Instead I would like to point the\u000areader to a report that proposes some details on the 00 analysis\u000aand design process: Dennis de Champeaux, Towards an 00 SW\u000aDevelopment Process, HPLabs Tech Report HPL92149, November 1992.\u000a[The contributors Doug Lea, Penelope Faure, and Mohammed Fayad\u000acould not be listed as authors for legal reasons.] For a copy\u000acontact: HPLabs/ Technical Publications Dept./ 1501 Page Mill Rd\u000aPalo Alto, CA 94304 (415) 857 4573 OOPSLA92 62  Vancouver,\u000aBritish Columbia
p690
aVA combined workshop drawing on the faculties of the established \u201cThe Bottom Line: Using ObjectOriented Development in the Commercial Environment\u201d workshop and the new \u201cDevelopment Processes for Use of the Object Paradigm\u201d covered issues of process, architecture, and tool support for iterative development. Keynote talks described:\u000a\u2022 farreaching multidimensional development methods\u000a\u2022the problems of semantic coupling between objects\u000a\u2022a highly iterative development project using a symbolic language\u000a\u2022a pioneering development where highlevel management was given a traditional view of development, while firstlevel management enjoyed the benefits of  more loosely structured iteration\u000a\u2022the use of formal specifications as a universal development tool facilitating compactness and analysis\u000aDiscussion on architecture, tools and reuse, and management techniques followed. Key areas of consensus included the multidimensionality of architecture, the inadequacy of class models alone to encapsulate change, the importance of tool support, and the need for local standards such as \u201csoftware backplanes.\u201d
p691
aVThe AUTOSPEC system is an automatic motor specification software system that primarily serves to noninteractively produce bill of materials from sales orders. At the core, the system applies Expert System and coordinated Relational Database technology to effect an objectoriented implementation. Computer Aided Software Engineering (CASE) tools supporting objectoriented analysis (OOA) and an objectoriented design (OOD) approach were used to provide a smooth transition of the software design effort into production at motor manufacturing plants. The powerful mechanisms of class inheritance, and forward chaining, as well as graphical program construction provided by the ProKappa Knowledge Base System are employed. Motif Graphical User Interface (GUI) development was carried out using UIM/X, a WYSIWYG screen development tool. The ORACLE Database Management System was used to provide data independence necessary for engineering system interfacing.
p692
aVAssembling classes to form systems is a key concept in objectoriented software development. However, in order to be composed, classes have to be compatible in a certain way. In current objectoriented languages the compatibility with a specific behaviour is expressed by inheritance from a class which describes this behaviour. Unfortunately inheritance is neither intended to express compatibility nor does it enforce compatibility. This deficiency may lead to inconsistent systems. Therefore, in the Project \u201c10\u201314\u201d inheritance is restricted to express compatibility between classes. Two kinds of compatibilities are distinguished: compatibility required for the integration of subsystems (conformance) and compatibility required for reuse (imitation). A formal basis for the discipline is developed in order to get mathematically precise notions and to enable the verification of objectoriented systems. A design strategy is built on top of these compatibility notions. In actual case studies the discipline has already shown to support a clean design and to promote the reusability of class hierarchies.
p693
aVThe objective of this poster is to show the evolution of the application framework ET++ from a university research project to a seamless platform for commercial application development in a banking environment. The presentation of our recent projects should stimulate the interaction between the ET++ developers and current and future ET++ users, application framework users and developers in general.
p694
aVGenetic algorithms (GA) mimic natural reproduction to search for complex problem solutions. Their principles are shortly explained. A point of interest is the regular and repetititve structure of computation involving communication, data exchanges, and control phases. Interaction with presentation and analysis tools is also a requirement. This make sense for the definition of a general framework allowing fast building of parallel applications in an objectoriented system. A GA workbench is developed using the Smalltalk80 system with parallel machine code generation in mind.
p695
aVIn a class hierarchy, a method should have access to only as many attributes as it needs to do its job. In general case, object programmers do not have regard to such an optimizing criterion. Consequent defaults are displayed and measured. In analyzing the way class attributes are used by methods, can we define alternative classes, not containing more attributes than are needed? Is it possible to translate an application into one achieving the same purpose but with attributes more efficiently spread in class hierarchy? We propose a reversible technique for restructuring that may answer these questions. Indirectly, it provides by measurement a way for localizing attributes and restricting their use in an object system. [Lieb 88], [Parn 86].
p696
aVThe development of large, complex systems consisting of both hardware and software normally involves several companies and/or development teams for the development of the different subsystems forming the entire system. Depending on the kind of subsystem to be developed, these different development teams normally use different system engineering methodologies and/or tool sets to produce their subsystems. However, the usage of different methodologies and tools makes technical management and quality assurance for the entire development effort a difficult task. Therefore the development as well as the maintenance of future systems with their increasing size and complexity requires a common frame for the entire system development and maintenance effort covering different kinds of  subsystems, like graphical user interfaces, database systems, expert systems, realtime systems, and so on. This common frame should support the separate and independent development of the different subsystems by different subcontractors using different system engineering methods, the integration of these subsystems to the entire system as well as the maintenance of the entire system. A hierarchical objectoriented model for the encapsulation and integration of different system engineering methods can serve as this common frame, providing traceability from the system requirements model to the implemented system while giving each subsystem development team full scope in choosing the system engineering method most suitable for a specific subsystem. The approach underlying this objectoriented  model is the encapsulation of the different subsystems in \u201chighlevel objects,\u201d which can be either further decomposed into \u201chighlevel object\u201d or described in more detail using any appropriate system engineering method. The integration of these \u201chighlevel objects\u201d to the entire system is supported by a \u201cdefinition object\u201d on each layer of each system engineering models and the different \u201chighlevel objects.\u201d
p697
aVWe want to talk about the importance of lowlevel software architectures and how to use them. In order to do this we look at the example of carrying out operations on elements in a collection. We look at the possible solutions and the properties of each of these solutions.
p698
aVThis poster presentation illustrates the use of SOM (the IBM System Object Model) for interfacing, different objectoriented programming (OOP) languages. Our approach allows classes defined in one OOP language to be used by different (possibly nonOOP) languages\u2014both for subclassing, and for object creation. This extends the utility of OOP class libraries and makes it possible to define \u201cmultilanguage\u201d objects, whose supporting methods and instance variables are provided by different languages.
p699
aVThe objectoriented model is very successful to represent various phenomena in our real world from an entitycategory point of view. However, it is not sufficient to model the dynamic actions of autonomous entities under this paradigm. This model is in short of representing the relationships between entities and the environments. In this paper, we propose an experimental objectoriented programming language to manage the relationships together.
p700
aVHow do we know when we have a well formed inheritance scheme? What are the criteria that we can use to judge? These are important questions that we must consider. For most people, inheritance is a rather new design discipline, and its implementation in C++ allows ample opportunity to create both good and bad examples of its use. In this paper we explore how the design criteria for inheritance is both similar to, and different from, conventional design considerations. We also discuss a set of anomalies that should be removed from proposed C++ inheritance schemes [1], which can be used to judge whether inheritance constructs are well formed or not.
p701
aVA Comparison of ObjectOriented Analysis and Design Methods\u000aMartin Fowler\u000a\u000aThis tutorial shows how to look inside design and analysis\u000amethods to see how they differ, and more importantly, how they are\u000athe same. It examines several of the most wellknown methods,\u000aincluding Booth, Coad/Yourdon, Odell/Martin, Rumbaugh, and\u000aSchlaer/Mellor, but its analysis techniques can be applied to any\u000amethod.\u000a\u000aIn general, each method is made up of several techniques (e.g.\u000aER modeling, state transition diagrams) each of which emphasizes a\u000aparticular aspect of the system. These techniques can be classified\u000aas being for structural (data), behavioral architectural modeling.\u000aThe tutorial shows the techniques each method uses and how\u000adifferent methods use different dialects of the same I techniques,\u000avarying the notation and introducing new concepts. It uses examples\u000ato show how the same system is expressed in different ways by the\u000avarious methods. The tutorial also compares the design approach\u000aadvocated by each method.\u000a\u000aThis tutorial will help you decide which method will work best\u000afor you. In addition, you will learn how ideas from different\u000amethods can be combined to better suit the system under\u000aanalysis.
p702
aVConcepts of ObjectOriented Programming\u000aRaimund K. Ege, Florida International University\u000a\u000aThis tutorial defines and teaches the basic concepts of\u000aenvironment, and gives an overview of the features of\u000aobjectoriented programming, illustrates the objectoriented\u000alanguages and environments. This advantages of objectoriented\u000atechniques over tutorial will let you make an informed decision\u000aconventional programming, introduces the about what\u000alanguage/environment will best serve components of an\u000aobjectoriented programming your programming needs.
p703
aVObject Design for Modularity, Reuse and Quality\u000aDouglas Bennett, Design Ways\u000a\u000aTurning buzzwords like modularity, reusable components,\u000aextensibility, testability, and robustness into reality requires\u000amore than just a compiler for an objectoriented language, or even\u000aan integrated development environment. These properties must be\u000adesigned into the software product. This tutorial shows how to make\u000adesign decisions so the product will do what its users want and so\u000athe buzzwords actually show up in the product. The tutorial will\u000awork through the steps of a design project, documenting user and\u000aproducer needs, modeling the thing objects, describing behavior\u000awith event response models, and, finally, developing an\u000aarchitecture for the product. Part of each step is evaluating the\u000adesign against the magic words. The result will be a product\u000astructure that is probably very different from conventional\u000asoftware architectures, but one that can be measured against the\u000adesign criteria.\u000a\u000aThis tutorial teaches the explicit design of software. It uses\u000anotations from several existing methods. You may use your own\u000anotations, if you wish.
p704
aVObjectOriented Concurrent Programming\u000aJeanPierre Briot, Institute Blaise Pascal, France\u000a\u000aThis tutorial treats objectoriented concurrent programming\u000a(OOCP) as the natural generalization of objectoriented\u000aprogramming. OOCP decomposes a large program into a collection of\u000asmall modules that run and interact concurrently and are capable of\u000aexploiting parallel hardware. The tutorial describes various levels\u000aof integration between objectoriented programming and concurrency,\u000aleading to the notion of an active object, which unifies object and\u000aactivity, message passing and synchronization.\u000a\u000aThe tutorial introduces Concurrent Smalltalk to describe\u000aconcepts, constructs, and methodology. Examples include programming\u000awith continuations, divide and conquer, and pipelining. It also\u000ashows how to implement active objects, and uses Actalk as an\u000aexample. Finally, it compares various OOP models and languages,\u000awith a special focus on the Actor computation model.\u000a\u000aAlthough the tutorial uses Smalltalk for examples, you dont need\u000ato know Smalltalk to understand it; a quick introduction to\u000aSmalltalk syntax is included. It assumes that you understand\u000aobjectoriented programming well, but little more about concurrency\u000athan intuitive concepts of processes and synchronization.
p705
aVIntegrating Analysis and Design Methods\u000aDerek Coleman & Paul Jeremaes, HP Labs, England\u000a\u000aMost of those who practice objectoriented analysis and design\u000ado not follow any standard method exactly, but combine different\u000atechniques to suit their own unique requirements. Each method\u000aemploys its own set of models, notations, and processes, so it can\u000abe difficult to combine them. This tutorial shows how to design a\u000amethod by providing a framework for understanding and evaluating\u000acurrent methods, applying it to three recent methods (OMT (Rumbaugh\u000aet al), Responsibility Driven Design/CRC cards, Booth 91 Method),\u000aand combining them to produce a new method, FUSION, that builds on\u000atheir best aspects.\u000a\u000aThe tutorial introduces a set of criteria for evaluating methods\u000afrom the viewpoint of the objectoriented concepts they support,\u000athe kinds of models and notations they employ and the process steps\u000athat they recommend. The criteria provides a way to understand the\u000aunderlying similarities and differences between methods.\u000a\u000aThe criteria were developed as part of a program to assess what\u000akind of method should be made available to HP engineers, which led\u000ato the development of the FUSION method. The tutorial ends with a\u000abrief account of the usefulness of FUSION in practice.
p706
aVTypes for the Language Designer\u000aMichael Schwartzbach & Jens Palsberg, Aarhus University,\u000aDenmark>/p>\u000a\u000aThe type systems of objectoriented languages have specific\u000agoals: they serve as partial documentation, they provide\u000amodularity, and they ensure safety and efficiency at runtime.\u000aThere are many choices in the design of a type system, and it is\u000ahard to evaluate their tradeoffs. This tutorial teaches a coherent\u000atheory of type systems for objectoriented languages that can be\u000aused for both explicit systems (where the programmer supplies type\u000aannotations) and implicit systems (where the compiler must perform\u000atype inference), and forty systems based on interfaces, classes,\u000aand sets of classes.\u000a\u000aThe tutorial defines an idealized objectoriented language,\u000ainspired by Smalltalk, and develops several type systems for it.\u000aThis uniform framework makes it easy to compare different\u000aapproaches. The tutorial explores the limitations of static type\u000achecking and shows how dynamic type checking can be introduced. It\u000ademonstrates how subclassing is different from subtyping, how\u000aspecification types differ from implementation types, and the\u000ainfluence of a type system on separate compilation.
p707
aVTypes for the Working Programmer\u000aAndrew Black, DEC\u000a\u000aThere has been recent progress in understanding types for\u000aobjectoriented languages, but most of it has had little or no\u000aimpact on real programmers. This tutorial aims to extract from the\u000aconfusion those topics that are important to programmers who must\u000ause or choose an objectoriented language. The tutorial explains\u000athe role of types in objectoriented languages, what types can do\u000afor programmers, and how the trend towards distributed and\u000aheterogeneous systems and objectoriented databases influences what\u000ait means for a program to be typecorrect.\u000a\u000aThe tutorial describes the difference between objects and\u000avalues, what types are and why they are good for you, what\u000arefinement and subtyping are, what problems they solve, and what\u000aproblems they do not; why contravariance isnt an unnatural act; why\u000ainheritance is a relationship between programs, not between\u000aclasses.\u000a\u000aBy the end of the tutorial, you will understand how abstract\u000aconcepts like subtyping can help you solve practical problems such\u000aas deciding when one piece of code can be substituted for\u000aanother.
p708
aVSimulation with DEVSCLOS\u000aSuleyman Sevinc, University of Sydney , Australia\u000a\u000aThis tutorial has two purposes: to show how the features of CLOS\u000aaffect a design and to describe the design of a generalpurpose\u000asimulation system. The tutorial is designed for those who want to\u000aunderstand the practical importance of the unique features of CLOS\u000aand also those who would like to learn how to use or build\u000aobjectoriented simulation systems.\u000a\u000aThe tutorial emphasizes the underlying model of objectoriented\u000aprogramming in CLOS and distinguishes it from other models. It also\u000adescribes the requirements of a simulation system and the design of\u000aDEVSCLOS, a publicly available simulation system. DEVSCLOS is an\u000aextension to CLOS that supports modeling and simulation. It\u000asupports visualization, supports hierarchical design of\u000asimulations, allows firstorder logic in models, and supports\u000aadaptive system simulation. The tutorial shows how a system like\u000aDEVSCLOS can be used to solve typical simulation problems, and how\u000aCLOS concepts like multimethods, multiple inheritance, pre and\u000apostmethods and dispatching algorithms were used in the design of\u000aDEVSCLOS.
p709
aVObjectOriented Software Development with the Demeter Method\u000aKarl Lieberherr, Northeastern University\u000a\u000aThe Demeter Method is a formal method that lifts objectoriented\u000asoftware development to a higher level of abstraction by using a\u000agraphical specification language for describing objectoriented\u000aprograms. Executable programs are automatically generated by a CASE\u000atool (the Demeter System/C++) from the graphical highlevel\u000adescriptions. Unlike other specification languages, the Demeter\u000aMethod allows you to keep the binding of methods to classes\u000aflexible under changing class structures. The higher level of\u000aabstraction leads to shorter and more reusable programs than by\u000aprogramming directly in one of todays objectoriented languages\u000asuch as C++, Smalltalk, Eiffel or CLOS.\u000a\u000aBecause the Demeter Method is regularly taught at NU both at the\u000agraduate and undergraduate level with the 10 week quarter system,\u000ait has become, by nnecessity, very easy to learn.\u000a\u000aThis tutorial is for professionals who want to learn powerful,\u000aformally defined highlevel concepts that describe the programming\u000atask in terms of datamodelbased graphs and their subgraphs.
p710
aVAdvanced CLOS and MetaObject Protocols\u000aJon L. White, Lucid\u000a\u000aOne of the important characteristics of CLOS is its dynamic\u000aflexibility to change descriptions, programs, and even data objects\u000aon the fly. A typical CLOS system is implemented by a set of\u000ametaobjects, which can be changed by the programmer, essentially\u000aletting you create a new language. Thus, CLOS is a reflective\u000aprogramming language.\u000a\u000aThe main purpose of this tutorial is to teach you how to put a\u000ametaobject protocol to a practical use. In addition to describing\u000athe implementation of a typical CLOS system and discussing the\u000avarious issues raised by the book Art of the MetaObject Protocol,\u000ait gives practical examples of metaobject extensions including\u000atechniques for making userdefined metaclasses, for making\u000aalternations to SLOTVALUE so that certain slots can be persistent,\u000aand for making metaclasses whose classes automatically keep a table\u000aof all their instances.\u000a\u000aThe tutorial is aimed at the CLOS programmer or the programming\u000alanguage expert who would like to learn more about reflective\u000aprogramming using a metaobject protocol. The emphasis is on\u000apractical uses of a metaobject protocol, not on the philosophy of\u000areflection.
p711
aVIntroduction to ObjectOriented Database Management Systems\u000aDavid Maier, Oregon Graduate Institute\u000a\u000aThis tutorial begins by explaining objectoriented database\u000amanagement systems (OODBMS) in terms of what is the value added\u000abeyond recordoriented database systems and objectoriented\u000aprogramming languages. It presents most of the current commercial\u000aOODBMS and several advanced prototypes with particular attention to\u000adistinguishing them in their data models, application interfaces\u000aand system architectures. It will also contrast the OODBMS approach\u000awith extended relational systems. A goal of the tutorial is to give\u000aparticipants an appreciation for the consequences of design choices\u000amade in the different systems. It concludes with a critique of\u000acurrent market offerings, and suggests there are significant\u000aregions of the design space to explore, and needs of advanced\u000aapplications that are still largely unmet by any database\u000aproduct.\u000a\u000aThis tutorial is designed both for those considering investing\u000ain OODBs and those who just want to understand the technology. It\u000aassumes knowledge of objectoriented programming concepts, and some\u000afamiliarity with conventional database systems, particularly\u000arelational databases.
p712
aVObjectOriented User Interfaces\u000aDave Collins, IBM,\u000a\u000aThe principles of objects, polymorphism, classes, and\u000ainheritance can apply to the end users external view of a user\u000ainterface, just as it can apply to the language that is used to\u000aimplement it. This tutorial shows how to design (not implement) a\u000auser interface that is truly objectoriented. It gives guidance on\u000athe design of the externals of objectoriented user interfaces, and\u000ashows how developers can capitalize on the isomorphism between the\u000ausers conceptual model of the interface and the constructs provided\u000aby objectoriented programming languages. The tutorial exposes a\u000a(perhaps surprisingly) deep analogy between objectoriented\u000aprogramming languages and objectoriented user interfaces. This\u000aanalogy is valuable because it can be used to map external user\u000ainterface design in a clear and natural way onto objectoriented\u000aapplication frameworks. The tutorial is illustrated with historical\u000aexamples, many on videotape.
p713
aVTeaching ObjectOriented Programming and Design\u000aJames C. McKim Jr., Hartford Graduate Center\u000a\u000aA course in objectoriented programming and design oriented\u000aparadigm, namely that it promotes reuse, should address the claims\u000amade for the object models the problem space, facilitates\u000amaintenance, incorporates changes easily, and shortens the\u000adevelopment lifecycle. One way (perhaps the only way) for students\u000ato test such claims is to build a small but high quality product as\u000apart of the course.\u000a\u000aThis tutorial shows how to have students in a conventional\u000acomputer science program build such a product, and addresses such\u000aissues as how to pick good projects, whether and how students\u000ashould work together in teams, how to keep students on schedule,\u000aand how to divide a project into a sequence of deliverables within\u000athe context of a one semester course.\u000a\u000aThe tutorial also describes how to scale back the approach so\u000athat students in short courses, common in industry, can get maximum\u000abenefit in the limited time that is allowed.
p714
aVEvaluating Reusable Class Libraries\u000aTimothy Korson, Clemson University\u000a\u000aObjectoriented technology not only affects the way we design\u000aindividual applications, it holds the potential of ushering in the\u000asoftware industrial revolution. A key to the success of object\u000atechnology is the widespread availability of high quality class\u000alibraries. These class libraries are the reusable parts of the\u000asoftware industry.\u000a\u000aThis tutorial teaches how to evaluate objectoriented libraries.\u000aThe same criteria can be used for selecting commercial libraries or\u000afor designing libraries for use inhouse. The tutorial also\u000adescribes practical criteria for specifying and cataloging inhouse\u000acorporate software libraries. In addition to these technical\u000acriteria, the tutorial also shows you the corporate infrastructure\u000athat is necessary for enabling large scale reuse.
p715
aVObject Engineering\u000aR. Stonewall Ballard, Component Software Corporation\u000a\u000aConverting an objectoriented design into class definitions in\u000aan objectoriented language often requires many engineering\u000adecisions. It is easy to make these decisions incorrectly for\u000alanguages that give the programmer a lot of control over data\u000arepresentation, such as C++. This tutorial covers lowlevel issues\u000ain building extensible and evolvable programs, issues that are\u000aimportant if you want to build wellbalanced abstractions. It\u000adescribes how to use multiple inheritance properly in spite of the\u000acompiler, when to use forwarding instead of inheritance, how to\u000aencapsulate state, and when to use references instead of copying\u000aobjects.\u000a\u000aThis tutorial teaches how to convert objectoriented designs\u000ainto code that is easy to extend and reuse. Many of the issues are\u000amore interesting to C++ programmers than Smalltalk programmers, and\u000aC++ is the language used for examples, but most of the issues are\u000alanguage independent.
p716
aVThe Pragmatics of Building ObjectOriented Systems\u000aGrady Booth, Rational\u000a\u000aThis tutorial expands upon the process described in Booths\u000aObjectOriented Design with Applications to provide a prescriptive,\u000aiterative process for objectoriented development. It describes\u000aboth the macro and micro process of development. It gives\u000aguidelines for managing iterative development, and addresses the\u000apragmatic issues of milestones and planning, staffing, integration\u000aand release management, reuse, testing, quality assurance and\u000ametrics, documentation, tools, and technology transfer.\u000a\u000aThis tutorial assumes that you are familiar with Booths design\u000amethod.
p717
aVA Case Study of Domain Analysis: Health Care\u000aMartin Fowler & Thomas Cairns, St Marys Hospital Medical School\u000a, England\u000a\u000aThree years ago the UK National Health Service decided to\u000adevelop a generic model to describe all aspects of its management\u000aand operation. Several projects were launched to describe the\u000ahealth care process using the objectoriented modeling technique\u000aPtech (now published by James Martin and Jim Odell). One of these\u000aprojects, the Cosmos project, produced the Cosmos Clinical Process\u000aModel (or CCPM), which describes the medical record. The CCPM\u000acontains approximately 70 object types in a highly abstract\u000astructure describing clinical procedures, observations, evidence\u000aand assessment, accountabilities and contracts, care planning, and\u000aclinical knowledge. The model has gained widespread interest in\u000aboth the UK and Europe and will be used as the focus for a number\u000aof EEC sponsored projects, and is taking a leading standardization\u000arole in Europe.\u000a\u000aThe CCPM is an example of the result of domain analysis. Its use\u000aof abstractions, some of which are applicable outside health care,\u000aand its use of an operational/knowledge divide can guide those\u000aworking on largescale generic models in other areas. The tutorial\u000adescribes the relationship between generic and specific models\u000asince these issues are key in the acceptance of a highly generic\u000aapplication area model by those with specific applications to\u000adevelop. The question of adapting an objectoriented method for\u000aparticular needs is also addressed.
p718
aVObjectOriented Project Management\u000aKenneth Rubin & Adele Goldberg, ParcPlace\u000a\u000aObjectoriented projects have to be managed properly to obtain\u000athe most benefits from objectoriented technology. This tutorial\u000aexplains the effect of objectoriented technology on costs,\u000astaffing and choice of methodology. It describes the pitfalls that\u000aattend objectoriented projects so that you can avoid them, and\u000aexplains the key processes, including development style, use of\u000aprototyping, and reuse. It gives specific guidelines for organizing\u000aprojects, managing a reuse library, estimating the size and cost of\u000aprojects, and introducing objectoriented technology into an\u000aorganization.
p719
aVIntroduction to ObjectOriented Design\u000aLori Stipp & Grady Booth, Rational\u000a\u000aThis tutorial describes the Booth method for objectoriented\u000adesign, including details of the notation and the design process.\u000aIt describes the principles that are necessary for thinking and\u000aabstracting in terms of classes and objects. The tutorial includes\u000aa number of examples, including war stories from specific projects.\u000aIt also covers extensions to the notation beyond that described in\u000aObjectOriented Design with Applications.\u000a\u000aThis tutorial is a condensed version of Booths popular tutorial\u000agiven at past OOPSLAs. If you have attended that in the past then\u000ayou should attend tutorial 18 instead of this one.
p720
aVWriting Efficient C++ Programs\u000aScott Meyers, Brown University\u000a\u000aThis tutorial teaches the competing meanings of high\u000aperformance; the characteristics of objectoriented systems that\u000acan decrease performance; how to locate and eliminate computational\u000abottlenecks in C++ programs; and the tradeoffs between high\u000aperformance and system reusability, maintainability, and\u000aportability. It describes in detail the factors that affect the\u000aperformance of C++ software. Whether your primary concern is high\u000asystem speed, small system size, fast recompilation, or a\u000acombination of these, this tutorial provides you with the tools\u000anecessary to come up with an appropriate object oriented design,\u000ato implement it efficiently in C++, and to finetune it for maximum\u000aperformance.\u000a\u000aThe reasons for bottlenecks in C++ programs are often\u000asurprising. Contrary to popular belief, virtual functions usually\u000aexact a negligible performance cost, while unexpected calls to\u000aconstructors and destructors frequently hamstring applications.\u000aThis tutorial teaches you what is really important if you want to\u000adeliver high performance, and the techniques you need to achieve\u000ait.
p721
aVThe Analysis and Design of Distributed Systems\u000aMehmet Aksit, University of Twente\u000a\u000aThe design of distributed objectoriented systems involves a\u000anumber of considerations that rarely arise in sequential\u000aobjectoriented design or in nonobjectoriented languages. The\u000atutorial describes analysis and design techniques for data\u000aabstraction, inheritance, delegation, persistence, atomicity,\u000aconcurrency, synchronization, and coordinated behavior in a\u000adistributed objectoriented framework. Special attention will be\u000apaid to the uniform integration of these concepts with the\u000aobjectoriented paradigm. Discussions will be accompanied by\u000aexamples that arose from constructing such systems.
p722
aVSpecification Techniques for ObjectOriented Software\u000aMahesh H. Dodani, University of Iowa\u000a\u000aThere are many techniques for specifying software. However, they\u000aare not usually presented in a way that is directly applicable to\u000aobjectoriented software. This tutorial surveys both operational\u000aand descriptive specification techniques, shows how to use them in\u000aseveral popular objectoriented software engineering methods, and\u000aprovides case studies of developing useful, formal specification\u000amechanisms that are appropriate for objectoriented software\u000aengineering.\u000a\u000aThe tutorial first describes criteria for choosing specification\u000amechanisms from both a theoretical (syntax and semantics,\u000aspecifying properties, reasoning about properties, verification)\u000aand practical (ease of use, modularity, applicability, supporting\u000atools) perspective. These criteria are then used to survey several\u000apopular operational (data flow diagrams, finite state machines, and\u000apetri nets) and descriptive (entityrelationship models, logic, and\u000aalgebraic specifications) specification mechanisms. Finally it\u000ashows how to extend two of these techniques (algebraic\u000aspecifications and finite state machines) so that they are suitable\u000afor objectoriented software engineering.
p723
aVThe Design and Management of C++ Class Libraries\u000aArthuir Riel, Vanguard Training\u000a\u000aProducing reusable C++ class libraries takes more than just\u000aknowing the language: it requires careful design. This tutorial\u000ashows how to create truly reusable C++ class libraries. It includes\u000ahow to design a minimal public interface, variablesize objects,\u000amemory leakage, heuristics for operator overloading, designing\u000areusable baseclasses, using inheritance and containment, and\u000adeciding how to place classes in a class library.
p724
aVEfficient Implementation of ObjectOriented Programming\u000aLanguages\u000aCraig Chambers, University of Washington, David Ungar, Sun Labs\u000a\u000aThis tutorial is for those who like to know how objectoriented\u000alanguages work under the hood, why some things are expensive while\u000aother things are cheap, and why language definitions include\u000acertain restrictions. This tutorial provides that, and more. It\u000adescribes features of objectoriented languages that are difficult\u000ato implement efficiently, and how this has affected language\u000adesigns. It describes stateofthepractice and stateoftheart\u000atechniques for implementing languages like C++ and Smalltalk.\u000a\u000aLanguage features include dynamic binding and generic\u000aoperations, inheritance, userdefined control structures, static\u000atype systems, multiple inheritance and virtual base classes, and\u000amixing objectoriented and nonobjectoriented programming.\u000a\u000aThe tutorial also addresses questions like: What are the\u000atradeoffs in using the various implementation techniques? What\u000aproblems remain that block more efficient objectoriented language\u000aimplementations? What might be promising areas for future\u000aresearch?
p725
aVHardware Support for ObjectOriented Systems\u000aMario Wolczko, University of Manchester, England\u000a\u000aWhen the performance penalty of objectoriented systems is\u000amentioned, a common response is to blame antiquated hardware\u000adesigns for not supporting objectoriented languages as they\u000adeserve, To what extent can the performance gap between\u000aconventional languages and objectoriented languages be closed\u000ausing hardware? What architectural changes benefit objectoriented\u000asystems, and by how much?\u000a\u000aThere have been many attempts to make hardware that better\u000asupports objectoriented programming. This tutorial describes these\u000asystems, and the extent that they have succeeded or failed in their\u000aaims. These systems include SOAR, Rekursiv and MUSHROOM, as well as\u000asome features from mainstream architectures such as SPARC. Issues\u000acovered include: choice of instruction set, design of the memory\u000asystem including caches and virtual memory hardware, scaleability,\u000ause of parallelism, and hardware/software tradeoffs.\u000a\u000aA common misconception is that if something is implemented in\u000ahardware then it must be fast. One aspect of the tutorial is to\u000ashow that there are limits to what hardware support can achieve.\u000aHowever, better hardware can reduce the cost of some language\u000afeatures, such as dynamic binding, and can make a system more\u000ascaleable.
p726
aVVisual Programming Languages from an ObjectOriented\u000aPerspective\u000aAllen L. Ambler, University of Kansas\u000aMargaret M. Burnett, Michigan Technical University\u000a\u000aVisual programming language research has evolved greatly since\u000aits early days. At first, attempts at visual programming mostly\u000atook the form of flowchartlike diagrams. But in recent years, a\u000awide number of innovative approaches have been incorporated into\u000avisual languages, including objectoriented programming, formbased\u000aprogramming, programming by demonstration, and dataflow\u000aprogramming. Unfortunately, while many of these systems represent\u000aimportant ideas, only a few have been successful as complete visual\u000aprogramming languages. This tutorial explains why this is true, and\u000adescribes ways in which the problem can be addressed.\u000a\u000aThis tutorial explores the issues behind the successes and\u000afailures of earlier approaches from a design perspective. It\u000aidentifies characteristics of successful visual programming\u000alanguages, and explains how to design an objectoriented language\u000athat maintains those characteristics. It shows solutions to a\u000anumber of problems by looking at existing visual programming\u000alanguages, including Prograph.
p727
aVIntermediate Smalltalk: Practical Design and Implementation\u000aTrygve Reenskaug, TASKON , Norway\u000a\u000aCareful design before programming is as important in Smalltalk\u000aas in other languages. This tutorial describes techniques that have\u000aproven useful in commercial development of large Smalltalk systems.\u000aThe tutorial shows how to develop a language independent design and\u000athen go to detailed programming and testing. The examples will be\u000abased on Smalltalk80 and use the release 4.1 graphical user\u000ainterface framework.
p728
aVConstraintBased Languages and Systems\u000aBjom FreemanBenson , University of Victoria, Canada\u000aAlan Boming, University of Washington\u000a\u000aA constraint is a relation that should be satisfied, for\u000aexample, that a line remain horizontal, that a resistor in an\u000aelectrical circuit simulation obey Ohms Law, or that the height of\u000aa bar in a bar chart be proportional to some number in an\u000aapplication program. Constraints have been used in a variety of\u000alanguages and systems, particularly in user interface tool kits, in\u000aplanning and scheduling, and in simulation. They provide an\u000aintuitive declarative style of programming that integrates well\u000awith objectoriented systems.\u000a\u000aThis tutorial teaches what constraints are, how to use them in\u000aapplications such as user interfaces, how to implement them\u000a(including how to implement constraint hierarchies), and how to\u000aembed them in objectoriented and logic programming languages. You\u000adont have to know anything about constraints, but it would be\u000ahelpful to have a strong background in programming languages.
p729
aVWriting Efficient Smalltalk Programs\u000aKen Auer, Knowledge Systems Corp.\u000a\u000aSmalltalk has a reputation for being slow and a offers many ways\u000ato create inefficient code, and memory hog. In reality, the\u000aSmalltalk environment programmers often exploit those\u000aopportunities.\u000a\u000aHowever, there are just as many ways to create efficient code,\u000asome of which may not be available in more traditional languages.\u000aWhether an application is efficient or not has more to do with the\u000away the programmer has used the available tools than the tools\u000athemselves.\u000a\u000aThis tutorial teaches Smalltalk programmers how to write\u000aefficient programs. The emphasis is on how to exploit, rather than\u000asacrifice, the benefits of good objectoriented design. You should\u000ahave at least six months experience with Smalltalk writing nontoy\u000aprograms to get the most benefit from the tutorial.
p730
aVThe Design of an ObjectOriented Operating System: A Case Study\u000aof Choices\u000aRoy H. Campbell & Nayeem Islam, University of Illinois\u000aPeter W. Madany, SUN Microsystems\u000a\u000aThis tutorial describes the objectoriented design of a complete\u000aoperating system, written to be objectoriented, with a user and\u000aapplication interface that is objectoriented. The main objective\u000ais to illustrate objectoriented design tradeoffs by studying a\u000alarge objectoriented system, the Choices operating system.\u000a\u000aChoices is an objectoriented multiprocessor operating system\u000athat runs native on SPARC stations, Encore Multimaxes, and IBM PCs.\u000aThe system is built from a number of frameworks that implement a\u000ageneral file system, persistent store for persistent objects,\u000aprocess switching, parallel processing, distributed processing,\u000ainterrupt handling, virtual memory, networking, and interprocess\u000acommunication.\u000a\u000aIf you bring an IBM/PC 386based portable computer running\u000aMSDOS to the course then you may experiment by writing application\u000aprograms for PCChoices. All participants will receive a copy of\u000aPCChoices on a floppy.\u000a\u000aParticipants should have experience with building\u000aobjectoriented systems and have a basic understanding of operating\u000asystems design. Reading knowledge of C++ is helpful, but not\u000anecessary.
p731
aVObjectOriented Geometry and Graphics\u000aJan KrcJediny & Augustin Mrazik, ArtInAppleS,\u000aCzechoslovakia\u000a\u000aSpatial information is an important part of geographic\u000ainformation systems, CAD and CAM systems, and user interfaces for\u000avisualization of any kind of information. The objectoriented\u000aapproach to spatial object modelling results in much more\u000aunderstandable designs than nonobjectoriented approaches.\u000a\u000aThis tutorial describes the design of several key components of\u000aa system for dealing with spatial information which are: analytical\u000ageometry closed with respect to union, intersection and complement\u000aoperations; hierarchically structured objects for maintaining\u000atopological information; object identity and problems arising from\u000achange of spatial objects in the database and user interface,\u000atransactions and copies of objects; dependencies of spatial objects\u000aand their copies in the MVC userinterface architecture.\u000a\u000aYou should have experience in building userinterfaces and\u000ashould have some background in analytical geometry. Examples are\u000ashown in Smalltalk, so a basic knowledge of Smalltalk would be\u000ahelpful.
p732
aVTesting ObjectOriented Software\u000aEdward Berard, Berard Software Engineering\u000a\u000aTesting of objectoriented software is just as important as\u000atesting nonobjectoriented software, but the process is\u000afundamentally different because of such factors as information\u000ahiding, encapsulation, and inheritance. This tutorial teaches you\u000athe terms and concepts of testing software in general, and\u000aobjectoriented software in particular; how to apply a number of\u000adifferent testing techniques to objectoriented software; how to\u000aconstruct test cases; and an appreciation of what is involved in\u000aplanning a successful software testing effort. The testing\u000atechniques that are covered include both whitebox testing such as\u000abasis path testing and coverage testing, and blackbox testing\u000atechniques such as equivalence class partitioning and boundary\u000avalue analysis.\u000a\u000aThis course is designed for those with experience in\u000aobjectoriented software engineering who would like to follow a\u000amore rigorous approach to testing.
p733
aVTeaching ObjectThink with MultiSensory Engagement\u000aPeter Coad, Object International, Inc.\u000a\u000aThis is a tutorial about how to teach the big picture of\u000aobjects, especially in an industrial environment where time is\u000ashort. It is based on the whole brain theory and multisensory\u000aengagement. The tutorial shows over twenty specific techniques for\u000ateaching using multisensory engagement, many of these unified into\u000aThe Object Game, which lets those new to objects see and manipulate\u000athem. The tutorial describes a set of specific techniques for\u000aleading someone into more effective object think, and compares and\u000acontrasts wholebrain multisensory engagement with other "object\u000athink" techniques, such as graphics, languages, and CRC cards.\u000a\u000aThis tutorial is designed for those trying to teach\u000aobjectoriented development who have been frustrated by the\u000ainability of some people to reach effective object think. It will\u000aalso be of interest to those planning to teach objectoriented\u000aprogramming, or if you are responsible for selecting such a\u000acourse.
p734
aVSchema Updates for ObjectOriented Database Systems\u000aRoberto Zicari , Johann Wolfgang Goethe University, Germany\u000a\u000aOODBMS are often used for complex data whose structure is likely\u000ato change over time, yet the problem of schema updates has not been\u000acompletely solved by any commercial or research OODBMS. This\u000atutorial describes the schema modification problem and why it is\u000aimportant, what is really offered by products, what a good solution\u000awould be like, and whether we are likely to see it soon.\u000a\u000aThe tutorial reviews several commercial OODBMS products that\u000aprovide facilities for updating the schema, namely: Gemstone\u000a(Servio Corporation), ITASCA(Itasca), 02(02Technology), ObjectStore\u000a(ObjectDesign), Ontos (Ontologic) and Statice (Symbolics). It also\u000adescribes the solutions proposed in some experimental research\u000aprototypes and the open problems that remain.
p735
aVSniff is a pragmatic C++ programming environment which has been\u000aimplemented during the last fifteen months. Sniff is implemented in\u000aC++ with the ET++ application framework. It runs on a large number\u000aof UNIX workstations under several window management systems such\u000aas OSFMotif, OpenWindows, and Sunview. Sniff is an open\u000aenvironment providing browsing, crossreferencing, design\u000avisualization, documentation, and editing support. It delegates\u000acompilation and debugging to any C++ compiler and debugger of\u000achoice. Sniff has been in internal use at UBS (Union Bank of\u000aSwitzerland) since August 1991. Since then several developers have\u000aapplied Sniff in developing non trivial software systems as well\u000aas in evolving Sniff. Public domain distribution off Sniff will\u000acommenced in May 1992.\u000a\u000aThe main goal in developing Sniff was to create an efficient\u000aportable C++ programming environment which makes it possible to\u000aedit and browse large software systems textually and graphically\u000awith a high degree of comfort, without wasting huge amounts of RAM\u000aor slowing down in an annoying way. A further goal was to achieve\u000aapplicable results within one person year.
p736
aVAt the University of Twente, the Netherlands, we have been\u000adeveloping methods and techniques in the area of object oriented\u000aanalysis and design. Our major contribution is the socalled\u000aComposition Based Object Oriented Software Development (COOSD)\u000amethodology, which includes methods, computational models, and\u000asupporting tools. We have applied this methodology in about 20\u000apilot studies and we have been organizing tutorials in Europe.\u000a\u000aThe supporting tools, organized in an Integrated CASE tool\u000aenvironment, support the developer both in the analysis and in the\u000adesign process, whereafter automatic translation of the design,\u000adescribed in a highlevel object oriented language, called SINA, to\u000aC++ is offered.
p737
aVObjectOriented Programming (OOP) has been recognized to have\u000agreat potential in producing high quality, large reusable software.\u000aHowever, we often find it difficult to write good C++ or Smalltalk\u000aprograms. Without good program designs we will not reap the\u000abenefits of OOP. On the other hand, ObjectOriented Design (OOD) is\u000aconsiderably more complex than that of the traditional design\u000adisciplines such as Structured Design. Therefore, without the help\u000aof powerful CASE tools, we may not achieve the benefits of OOD and\u000aOOP.\u000a\u000aThis demonstration will present TurboCASE, a multimethodology\u000asupporting CASE tool that supports both ObjectOriented Analysis\u000a(OOA) and OOD. TurboCASE provides multiple views to help software\u000adesigners create good OOD models.\u000a\u000aTurboCASE allows you to create and maintain Class Hierarchy,\u000aClass Collaboration, Class Definition, and Class Design Diagrams.\u000aThe Class Hierarchy Diagram not only shows the class hierarchy, but\u000aalso helps resolve how polymorphism works. The class Collaboration\u000aDiagram gives three levels of abstraction showing how objects\u000acommunicate with each other. The Class Definition Diagram shows the\u000amethods and instance variables of the class, as well as providing\u000athe ability to browse the capabilities of the class inherited from\u000athe class hierarchy. The Class Design Diagram allows you to design\u000aa class similar to that of the Structured Chart with full support\u000aof the method signature specification. These diagrams are\u000aintegrated through a project database, thereby providing multiple\u000aviews of the software design.\u000a\u000aTurboCASE also provides intelligent help to refine the design\u000asuch as abstracting the common behaviors of classes into an\u000aabstracted class. Subsystems can be designed topdown as well as\u000abottomup by abstracting from system components. These capabilities\u000agreatly reduce the complexities of the designs and help achieve\u000aoptimal solutions.
p738
aVIn France, there is a generalized file transfer method used for\u000acustomerbank information exchange, on X25 networks and on\u000atelephone switched lines (BSC 2780/3780 like protocol). It is used\u000aby (almost) all Francebased banks, and thousands of customers.\u000a\u000aRONDO is an 00 file server, supporting these protocols. It was\u000adeveloped in BORLAND C++, with TURBO VISION graphics library.\u000aWithout the included files, the overall code lines, as reported by\u000athe project manager, are 22,000. The server works fine on a 386\u000abased PC, DOS 5.0. RONDOs capabilities are:\u000a\u000a\u000a\u000asupport up to four file transfers on public switched\u000acommunication lines, and four file transfers on X25 virtual\u000acircuits, simultaneously,\u000a\u000aonline verification of incoming files for format correctness,\u000aand\u000a\u000aonline surveillance and administration.\u000a\u000a\u000aThe demonstrations purposes are:\u000a\u000a\u000a\u000apresent a realtime application of the 00 technology, including\u000ainterrupt control and encapsulation of an OEM X25 driver (not\u000a00);\u000a\u000apresent a brand new user comfort for DOS communication\u000aapplications;\u000a\u000areal time information about incoming transfers;\u000a\u000aoptional operator zoom on different monitors: status,\u000astatistics, log, traffic, trace, file verification;\u000a\u000aconfiguration control;\u000a\u000aclient diagnosis and support;\u000a\u000apresent the internal objects grouped in 4 domains:\u000acommunication, administration, user interface, and system\u000aextension.\u000a\u000a\u000a\u000a\u000a \u000a\u000aThrough graphic objects, this technology brings a spectacular\u000anew comfort to DOS users with less overhead than in graphical\u000aenvironments. For developers it provides reusable objects and\u000asimple building logic.
p739
aVWhich is bettertext or graphicsfor exploiting and communicating\u000aobjectoriented results? Both! To illustrate the desirability,\u000aeffectiveness, and impact of using both mediums, this demonstration\u000aworks through a small OOA/OOD model with text and graphicson two\u000alarge screens, sidebyside, in parallel. The demonstration\u000aincludes practical insights on how to use both text and graphics\u000afor better developing and communicating objectoriented results. It\u000aalso includes a look at the theories which point to a dramatic need\u000afor a more multifaceted approach. Special emphasis is placed on\u000athe theory of seven intelligenceslinguistic, mathematical/logical,\u000aspatial, musical, bodykinesthetic, interpersonal, and\u000aintrapersonal.\u000a\u000aTwo tools for visualizing objectstext (Smalltalk) and graphics\u000a(from the ObjecThink Series)are applied sidebyside, in parallel.\u000aParticipants experience firsthand the needand the impactof\u000aapplying multimedia approaches to stimulate more effective object\u000athink.
p740
aVWe have built an extension to a prototype CASE tool called\u000aESPEX, that makes it possible to execute graphically Wassermans\u000aObjectOriented Structured Design (OOSD) notation. The tool itself\u000ais implemented in the Smalltalk 80 programming environment. The\u000atool contains a windowbased menudriven graph editor and an\u000aintegrated graph simulator with animation support. Animation means\u000athat the movements of data items and messages between objects are\u000avisualized on a workstation display.\u000a\u000aWe have implemented simulation for object instances. We are\u000ausing moving tokens with data values attached on top of them.\u000aTokens flow from objects to other objects, carrying the essential\u000adata with them. The moving tokens indicate clearly the active parts\u000aof the model, while the data values also make it possible to\u000avisualize the actual computations that take place in the objects.\u000aAnother advantage of moving tokens is that it is possible to attach\u000atime stamps to tokens. With the help of the time stamps it is\u000apossible to simulate experimenting with animation techniques for\u000adynamic creation and deletion of objects.\u000a\u000aWe model objects hierarchically, so that an object can be\u000acomposed of other lower level objects. At the lowest level objects\u000aare described with predicate/transition Petri nets, and the\u000acomputations with IPTBS MetaIV, Smalltalk80, or C.\u000a\u000aThe work presented demonstrates that it is possible and feasible\u000ato animate objectoriented design notation using a token moving\u000aapproach. However, there is still much work to be done on the\u000aanimation of dynamic instantiations and deletion of objects. We\u000abelieve that graphical animation tools are the enabling technology\u000afor teamwork and for conceptual communication in the future\u000aconcurrent engineering working environments.\u000a\u000aThe work presented in this demo was carried out in the IPTES\u000aproject. IPTBS is partially funded by the European Communities\u000aunder the ESPRIT programme.
p741
aVThis demonstration presents Audition, an objectoriented\u000alanguage and environment designed to allow the easy creation of\u000avisual and interactive programs.\u000a\u000aIt uses a metaphor of Performers and Stages, as first inspired\u000aby the XEROX Pam Rehearsal World project. Performers are the main\u000ainteraction units. They are visible and mouse or message activated;\u000asample types include DisplayPerformers, Turtles, Graphs, and\u000aGauges. Performers are grouped on Stages; stages maintain local\u000aname spaces so that Performers can be named and intercommunicate\u000ausing their local names. While Performers are members of a class,\u000athey may also be specialised through the addition of\u000ainstancespecific behaviour and structure; this may also be\u000ainherited from other performers.\u000a\u000aJust as inheritance models iskindof relationships, performers\u000acollected on a stage visibly and logically model ispartof\u000arelationships, in a more flexible way than instance variables. An\u000aapplication is usually just a specialised stage and its specialised\u000aperformers. The performerstage paradigm gives rise to models that\u000aare at once well factored across their participating performers and\u000ayet are fully encapsulated within a stage. Audition is a threaded\u000ainterpreted language implemented in itself. At present we are using\u000aAudition to investigate planning aids and simulations for\u000aindustrial situations.
p742
aVThe Universities of Kaiserlautern and Karlsruhe are\u000acooperating with the CEC Karlsruhe, a research center of Digital\u000aEquipment CmbH, in the area of software engineering for distributed\u000aapplications.\u000a\u000aThis demonstration will showcase DOCASE, a software engineering\u000aenvironment for distributed applications, using objectoriented\u000atechnology. The demonstration will provide insight into the\u000amethods, languages, tools and environments used in the lifecycle\u000aof distributed applications. With the term distributed\u000aapplications, we put an emphasis on structured parallelism (which\u000acomes more or less naturally with the logical structure and\u000ainherent parallelism and distribution of an application), not so\u000amuch on the finegrained parallelism exploited by applications such\u000aas image processing etc.\u000a\u000aWith more and more complex (dynamic, multiparty, cooperative)\u000adistributed applications to be developed in computer integrated\u000amanufacturing, office automation, CSCW, and other domains, we think\u000athat this topic is both very much up to date and a topic which is\u000anot sufficiently taught.\u000a\u000aThe live demonstration will show a distributed application which\u000awill allow researchers of arbitrary disciplines to coordinate their\u000awork with other members of a research team. The underlying meta\u000amodel is generic and instantiated by developing a set of concrete\u000aobject types together with a GUI.
p743
aVHotDraw is a framework for structured drawing editors that has\u000abeen floating around the Smalltalk community for several years\u000aunder several incarnations. It was originally developed by Kent\u000aBeck and Ward Cunningham, but has been reimplemented many times\u000asince then. Most of these implementations were proprietary. Our\u000adesign extends the original one by making constraints more\u000acomposable and by adding metafigures, but the system is instantly\u000arecognizable by anyone familiar with one of the earlier\u000aimplementations.\u000a\u000aWe wrote a version of HotDraw for ObjectWorks\u005cSmalltalk Release\u000a4.0 that we are distributing. We will describe and demonstrate the\u000abasic framework and several applications built from it, such as a\u000avisual programming language, a tool for inspecting Smalltalk\u000aobjects by drawing pictures of them, an environment for learning\u000aSmalltalk by issuing commands and writing scripts for visible\u000aobjects, and a PERT chart editor. These applications illustrate\u000aseveral advanced Smalltalk techniques, such as lightweight\u000aclasses, compiling code on the fly, and dynamically redirecting\u000amessages by overriding doesNotUnderstand:. Time permitting, well\u000aexplore the implementation of HotDraw and how it uses these\u000atechniques.
p744
aVWe are demonstrating the Demeter Tools/C++, a CASE research\u000aprototype developed at Northeastern University, including the\u000alatest tools described in papers presented at ECOOP 91, ER 90, CSM\u000a89 and OOPSLA 89 and in the Software Engineering Journal\u000a(1991).\u000a\u000aThe Demeter system has been under development since 1985 and has\u000abeen regularly used and refined with support from companies such as\u000aIBM, Mettler Toledo and SAIC.\u000a\u000aDemeter Tools/C++ consists of the following tools:\u000a\u000a\u000a1. Design level (programming language independent):\u000a\u000a\u000a\u000acddraw: to draw a class dictionary\u000a\u000alearn: Learns class dictionary for object examples\u000a\u000aoptimize: Optimizes class dictionary\u000a\u000asemcheck: Checks class dictionary for violations and proposes\u000achanges to the class dictionary\u000a\u000aparsegenerate, runparser: Checks conformance between class\u000adictionary and objects\u000a\u000acdcompare: Compares two class dictionaries\u000a\u000acdabstract: Finds the largest class dictionary\u000a\u000acommon to both input class\u000adictionaries\u000a\u000axref: Draws a graphical representation of a class\u000adictionary\u000a\u000a\u000a\u000a2. Generation level (C++ specific):\u000a\u000agenerate: Generates C++ class library from input class\u000adictionary\u000a\u000apropagate: Generates C++ member functions from propagation\u000apattern\u000a\u000aheaders: Generate C++ function interfaces from\u000aimplementation\u000a\u000a\u000a\u000a 3. Implementation level (C++ specific):\u000a\u000aClass library of predefined member functions for object\u000amanipulation (e.g., reading, writing, drawing, accessing,\u000aetc.)
p745
aVThis paper presents an equational formulation of an objectoriented data model. In this model, a database is represented as a system of equations over a set of oid's, and a database query is a transformation of a system of equations into another system of equations. During the query processing, our model maintains an equivalence relation over oid's that relates oid's corresponding to the same "realworld entity." By this mechanism, the model achieves a declarative setbased query language and views for objects with identity. Moreover, the query primitives are designed so that queries including object traversal can be evaluated in a dataparallel fashion.
p746
aVObjectbased (i.e. classless) models are very effective for elucidating requirements from users, and they support exploratory programming and rapid prototyping, providing a direct manipulation approach. On the other hand, classbased models have powerful mechanisms to control redundancy, exploit sharing, express extension, and propagate changes to instances.The price objectbased approaches pay is loss of control over change propagation, and potential redundancy. Two mechanisms to overcome this are sharing among objects and definition of objects as extension of others. We examine these mechanisms, and consider the effect that interacting policies for objects sharing and definitionbyextension have on change propagation and replication control. An implication is that, in absence of metaobjects or extralanguage support, monolithic shared parts cannot coexist with prototypes represented as split objects.
p747
aVIn this paper, the authors share their experiences gathered during the design and implementation of the CORBA Persistent Object Service. There are two problems related to a design and implementation of the Persistence Service: first, OMG intentionally leaves the functionality core of the Persistence Service unspecified; second, OMG encourages reuse of other Object Services without being specific enough in this respect. The paper identifies the key design issues implied both by the intentional lack of OMG specification and the limits of the implementation environment characteristics. At the same time, the paper discusses the benefits and drawbacks of reusing other Object Services, particularly the Relationship and Externalization Services, to support the Persistence Service. Surprisingly, the key lesson learned is that a direct reuse of these Object Services is impossible.
p748
aVIt has been difficult to objectively assess the real value or maturity of the Object Management Group's Object Management Architecture (OMA). While experience reports have appeared in the literature, these have focused more on the functionality of the endsystem than on systematically exploring the strengths and weaknesses of the OMA, and providing practical guidelines on the effective use of the OMA for specific softwareengineering problems. In this paper we describe a case study in the use of the OMA to integrate legacy software components into a distributed object system. We assess the OMA in this problem context, and indicate strengths and weaknesses of the specification and current implementations. We extrapolate our experience to a broader class of componentbased software systems, and recommend an architectural strategy for the effective use of the OMA to this class of systems.
p749
aVThis paper describes an algorithm for slicing class hierarchies in C++ programs. Given a C++ class hierarchy (a collection of C++ classes and inheritance relations among them) and a program P that uses the hierarchy, the algorithm eliminates from the hierarchy those data members, member functions, classes, and inheritance relations that are unnecessary for ensuring that the semantics of P is maintained.Class slicing is especially useful when the program P is generated from a larger program P' by a statement slicing algorithm. Such an algorithm eliminates statements that are irrelevant to a set of slicing criteria program points of particular interest. There has been considerable previous work on statement slicing, and it will not be the concern of this paper. However, the combination of statement slicing and class slicing for C++ has two principal applications: First, class slicing can enhance statement slicing's utility in program debugging and understanding applications, by eliminating both executable and declarative program components irrelevant to the slicing criteria. Second, the combination of the two slicing algorithms can be used to decrease the space requirements of programs that do not use all the components of a class hierarchy. Such a situation is particularly common in programs that use class libraries.
p750
aVThe paper motivates the facilities provided by Description Logics in an objectoriented programming scenario. It presents a unification approach of Description Logics and objectoriented programming that allows both views to be conveniently used for different subproblems in a modern softwareengineering environment. The main thesis of this paper is that in order to use Description Logics in practical applications, a seamless integration with objectoriented system development methodologies must be realized.
p751
aVThe class of an object is not necessarily the only determiner of its runtime behaviour. Often it is necessary to have an object behave differently depending upon the other objects to which it is connected. However, as it currently stands, objectoriented programming provides no support for this concept, and little recognition of its role in common, practical programming situations. This paper investigates a new programming paradigm, environmental acquisition in the context of object aggregation, in which objects acquire behaviour from their current containers at runtime. The key idea is that the behaviour of a component may depend upon its enclosing composite(s). In particular, we propose a form of feature sharing in which an object "inherits" features from the classes of objects in its environment. By examining the declaration of classes, it is possible to determine which kinds of classes may contain a component, and which components must be contained in a given kind of composite. These relationships are the basis for language constructs that supports acquisition. We develop the theory of acquisition that includes topics such as the kinds of links along which acquisition may occur, and the behaviour of routine (methods) and attribute features under acquisition. The proposed model for acquisition as a hierarchical abstraction mechanism is a strongly typed model that allows static type checking of programs exploiting this mechanism. We compare it to several other mechanisms including inheritance and delegation, and show that it is significantly different than these.
p752
aVThe key to making object technology work isn't technology. So what is it? This panel will cover a lot of the "soft" issues about dealing with people and teams. It's partly about managing software development, and mostly about doing it.
p753
aVMost, objectoriented programs have imperfectly designed inheritance hierarchies and imperfectly factored methods, and these imperfections tend to increase with maintenance. Hence, even objectoriented programs are more expensive to maintain, harder to understand and larger than necessary. Automatic restructuring of inheritance hierarchies and refactoring of methods can improve the design of inheritance hierarchies, and the factoring of methods. This results in programs being smaller, having better code reuse and being more consistent. This paper describes Guru, a prototype tool for automatic inheritance hierarchy restructuring and method refactoring of Self programs. Results from realistic applications of the tool are presented.
p754
aVSeveral algorithms [Cas92, MS89, Run92, DDHL94a, DDHL95, GMM95] have been proposed to automatically insert a class into an inheritance hierarchy. But actual hierarchies all include overriden and overloaded properties that these algorithms handle either very partially or not at all. Partially handled means handled provided there is a separate given function f able to compare overloaded properties [DDHL95, GMM95].In this paper, we describe a new version of our algorithm (named Ares) which handles automatic class insertion more efficiently using such a function f. Although impossible to fully define, this function can be computed for a number of well defined cases of overloading and overriding. We give a classification of such cases and describe the computation process for a welldefined set of nontrivial cases.The algorithm preserves these important properties: preservation of the maximal factorization of properties preservation of the underlying structure (Galois lattice) of the input hierarchy conservation of relevant classes of the input hierarchy with their properties.
p755
aVA critical concern in the reuse of software is the propagation of changes made to reusable artifacts. Without techniques to manage these changes, multiple versions of these artifacts will propagate through different systems and reusers will not be able to benefit from improvements to the original artifact. We propose to codify the management of change in a software system by means of reuse contracts that record the protocol between managers and users of a reusable asset. Just as real world contracts can be extended, amended and customised, reuse contracts are subject to parallel changes encoded by formal reuse operators: extension, refinement and concretisation. Reuse contracts and their operators serve as structured documentation and facilitate the propagation of changes to reusable assets by indicating how much work is needed to update previously built applications, where and how to test and how to adjust these applications.
p756
aVThis report describes an experiment carried out at ParcPlaceDigitalk which sought to increase the lookandfeel compliance of portable applications built using the company's Smalltalkbased VisualWorks product. We outline the structure of the current VisualWorks user interface framework, and the precise requirements which the experimental system sought to fulfill. We go on to show how we were able to reuse design patterns from the literature in a generative fashion, to direct the evolution of the new framework. This contrasts with most patternrelated work to date, which has concentrated on discerning design patterns in existing systems. Finally, we draw generalizations from our experience concerning the evolution of software architecture using patterns.
p757
aVAnthropology is the study of civilization, particularly its societies, customs, structure, and evolution. Our premise is that there are cultureal "chasms" to be crossed to ensure the success of the technological beachhead established by innovators and early adopters of the OO paradigm. Our panelists will address the following questions:&bull;What anthropological cultural factors have to be "matured" (and how?) to foster the success of the OO paradigm?&bull;What mechanisms are required to facilitate communications between cultures of differing maturity?&bull;What cultural chasms must be crossed to develop a successful organization/culture within the framework supported by the objectoriented paradigm (or more perrversely, is there anything "special" about the OO paradigm)?This panel will interest pracitioners as a forum to share and debate experiences related to our currentday software culture metamorphosis.
p758
aVTo use modern hardware effectively, compilers need extensive controlflow information. Unfortunately, the frequent method invocations in objectoriented languages obscure control flow. In this paper, we describe and evaluate a range of analysis techniques to convert method invocations into direct calls for staticallytyped objectoriented languages and thus improve controlflow information in objectoriented languages. We present simple algorithms for type hierarchy analysis, aggregate analysis, and interprocedural and intraprocedural type propagation. These algorithms are also fast, O(|procedures| * &sum;pprocedure np * vp) worst case time (linear in practice) for our slowest analysis, where np is the size of procedure p and vp is the number of variables in procedure p, and are thus practical for use in a compiler. When they fail, we introduce cause analysis to reveal the source of imprecision and suggest where more powerful algorithms may be warranted. We show that our simple analyses perform almost as well as an oracle that resolves all method invocations that invoke only a single procedure.
p759
aVWe study the direct cost of virtual function calls in C++ programs, assuming the standard implementation using virtual function tables. We measure this overhead experimentally for a number of large benchmark programs, using a combination of executable inspection and processor simulation. Our results show that the C++ programs measured spend a median of 5.2% of their time and 3.7% of their instructions in dispatch code. For "all virtuals" versions of the programs, the median overhead rises to 13.7% (13% of the instructions). The "thunk" variant of the virtual function table implementation reduces the overhead by a median of 21% relative to the standard implementation. On future processors, these overheads are likely to increase moderately.
p760
aVVirtual functions make code easier for programmers to reuse but also make it harder for compilers to analyze. We investigate the ability of three static analysis algorithms to improve C++ programs by resolving virtual function calls, thereby reducing compiled code size and reducing program complexity so as to improve both human and automated program understanding and analysis. In measurements of seven programs of significant size (5000 to 20000 lines of code each) we found that on average the most precise of the three algorithms resolved 71% of the virtual function calls and reduced compiled code size by 25%. This algorithm is very fast: it analyzes 3300 source lines per second on an 80 MHz PowerPC 601. Because of its accuracy and speed, this algorithm is an excellent candidate for inclusion in production C++ compilers.
p761
aVEvolution of software systems is prompted by all sorts of changes. This paper demonstrates how the use case, a well known construct in objectoriented analysis, is adapted to form the change case, to identify and articulate anticipated system changes. A change case provides the ability to identify and incorporate expected future change into a design to enhance the longterm robustness of that design. In this paper, we define change cases and demonstrate how change cases are captured by the analyst. We present examples to illustrate how change cases can influence present system design and point the way toward designs that more easily accommodate expected future changes. Change cases can be effectively employed in the context of any methodology that supports use cases and traceability links.
p762
aVIn this paper we present a method of code implementation that works in conjunction with collaboration and responsibility based analysis modeling techniques to achieve better code reuse and resilience to change. Our approach maintains a closer mapping from responsibilities in the analysis model to entities in the implementation. In so doing, it leverages the features of flexible design and design reuse found in collaborationbased design models to provide similar adaptability and reuse in the implementation. Our approach requires no special development tools and uses only standard features available in the C++ language. In an earlier paper we described the basic mechanisms used by our approach and discussed its advantages in comparison to the framework approach. In this paper we show how our approach combines code and design reuse, describing specific techniques that can be used in the development of larger applications.
p763
aVLarge software systems are often built on system platforms that support or enforce specific characteristics of the source code or actual design. These characteristics are either captured informally in design guideline documents or in specialized design and implementation languages.In our view, both approaches are unsatisfactory. Informal descriptions do not allow automated analysis and lead to vague constraint descriptions. The languagebased approach leads to different languages for different platforms and even for different versions of the same basic platform.Our approach is to describe and name the constraints separately in a design constraint language called CDL, which is based on an extraordinarily concise logic of parse trees. Designs are then annotated with the names of the constraints they are supposed to satisfy.We discuss how the design constraint language is integrated into a design language environment. We exhibit industrial and experimental evidence that our choice of design constraint language allows us to formalize naturally and succinctly common design characteristics.
p764
aVObjectoriented concepts such as reuse and encapsulation offer many benefits to application development, particularly in managing complexity and change. All the benefits of OO that apply to the client can apply equally well to the server. However, until recently, these concepts have proven to benefit only the client side. Could there be any intrinsic or conceptual reason for this? Or could it be a result of timing, the availability of the tools, and the conservative restraints that result from the missioncritical nature of server computing?This panel will explore the roles of objects on the server by examining the experiences of the panelists from a technology consumer's perspective. By doing so, we would like to provide insights for corporations that are making decisions on OO technology, point out pitfalls along the way, and identify potential opportunities for technology providers.All panelists have developed production level object servers. The panel will answer the question of whether we are ready for object servers by discussing the following issues:&bull; Different server types: transactional server, data server, application server, web server, etc. Are they really that much different? Which one is the most important kind?&bull; Experience in implementing and maintaining object systems on the server: the configurations of the systems and how they were arrived at; the tools and programming languages used.&bull; Benefits and drawbacks of server object systems.&bull; What's hard? What's easy?&bull; What are the prerequisites for pervasive deployments of objects on the server? (e.g., standards, application types, customer situations, tools, and languages.)&bull; What would be desirable for technology providers (researchers and vendors) to provide? (Tools, languages, execution environments that represents vendor opportunities.)&bull; Practical advice to organizations interested in introducing objects to the server.
p765
aVTracking the changing dynamics of objectoriented frameworks[5], design patterns[7], architectural styles[8], and subsystems during the development and reuse cycle can aid producing complex systems. Unfortunately, current objectoriented programming tools are relatively oblivious to the rich architectural abstractions in a system.This paper shows that architectureoriented visualization, the graphical presentation of system statics and dynamics in terms of its architectural abstractions, is highly beneficial in designing complex systems. In addition, the paper presents architectureaware instrumentation, a new technique for building efficient online instrumentation to support architectural queries. We demonstrate the effectiveness and performance of the scheme with case studies in the design of the Choices objectoriented operating system.
p766
aVFrameworks and domainspecific visual languages are two different reuse techniques, the first targeted at expert programmers, the second at domain experts. In fact, these techniques are closely related. This paper shows how to develop a domainspecific visual language by first developing a whitebox framework for the domain, then turning it into a blackbox framework, and finally building a graphical front end for it. We used this technique in a compiler to specify runtime systems.
p767
aVMany parallel and distributed programming models are based on some form of shared objects, which may be represented in various ways (e.g., singlecopy, replicated, and partitioned objects). Also, many different operation execution strategies have been designed for each representation. In programming systems that use multiple representations integrated in a single object model, one way to provide multiple execution strategies is to implement each strategy independently from the others. However, this leads to rigid systems and provides little opportunity for code reuse. Instead, we propose a flexible operation execution model that allows the implementation of many different strategies, which can even be changed at runtime. We present the model and a distributed implementation of it. Also, we describe how various execution strategies can be expressed using the model, and we look at applications that benefit from its flexibility.
p768
aVWe present a graphical environment for parallel objectoriented programming. It provides visual tools to develop and debug objectoriented programs as well as parallel or concurrent systems. This environment was derived from a structural operational semantics of an extension of the Eiffel language, Eiffel//. Objectrelated features of the language (inheritance, polymorphism) are formalized using a bigstep semantics, while the interleaving model of concurrency is expressed with smallstep semantics.Without user instrumentation, the interactive environment proposes features such as stepbystep animated executions, graphical visualization of object and process topology, futures and pending requests, control of interleaving, deadlock detection.
p769
aVIn the realm of OO methodologies there are two major schools of thought. Both schools claim to define mechanisms whereby software applications can be created that are reusable, maintainable, and robust. Moreover, both schools claim to use abstraction as a key mechanism for achieving these benefits. At issue is whether or not these two schools are fundamentally different, or just variations on an objectoriented theme.Shlaer and Mellor have dubbed one of these schools "Translational". In the translational approach, two models are created. One is an abstract model of the application domain which is devoid of any design dependencies. The other model is an abstract model of the design which is devoid of any application dependencies. These two models are composed automatically to yield the code for the system.The other school  supported by Booch, Rumbaugh, Jacobson, and Martin  views the architecture of a system from several different perspectives of abstraction, e.g. logical, physical. These abstractions typically form a layer; abstractions in the logical sense manifest themselves as individual classes as well as collaborations of classes. There may be one layered model, at different layers of abstraction, or, especially given the Objectory view point, there may be multiple models, with an analysis model that's nearly independent from the design model.The panel will explore:&bull; Is there a seamless transition between analysis and design?&bull; Should there be a single model or should there be two  one for the analysis and one for the design?&bull; If there are two models, how are they "bridged"?&bull; What, if any, are the differences in process between the two schools?&bull; How does architecture manifest itself!&bull; Is there, in fact, a real difference between the two schools of thought?As a result of this exploration, we hope to answer the question: Is translation a myth or is it a reality?
p770
aVThis paper describes how the cmcc compiler reuses code both internally (reuse between different modules) and externally (reuse between versions for different target machines). The key to reuse are the application frameworks developed for global dataflow analysis, code generation, instruction scheduling, and register allocation.The code produced by cmcc is as good as the code produced by the native compilers for the MIPS and SPARC, although significantly less resources have been spent on cmcc (overall, about 6 man years by 2.5 persons). cmcc is implemented in C++, which allowed for a compact expression of the frameworks as class hierarchies. The results support the claim that suitable frameworks facilitate reuse and thereby significantly improve developer effectiveness.
p771
aVObjectoriented languages with multiple inheritance and automatic conflict resolution typically use a linearization of superclasses to determine which version of a property to inherit when several superclasses provide definitions. Recent work has defined several desirable characteristics for linearizations, the most important being monotonicity, which prohibits inherited properties from skipping over direct superclasses. Combined with Dylan's sealing mechanism, a monotonic linearization enables some compiletime method selection that would otherwise be impossible in the absence of a closedworld assumption.The Dylan linearization is monotonic, easily described, strictly observes local precedence order, and produces the same ordering as CLOS when that is monotonic. We present an implementation based on merging and a survey of class heterarchies from several large programs, analyzing where commonly used linearizations differ.
p772
aVPreviously, techniques such as class hierarchy analysis and profileguided receiver class prediction have been demonstrated to greatly improve the performance of applications written in pure objectoriented languages, but the degree to which these results are transferable to applications written in hybrid languages has been unclear. In part to answer this question, we have developed the Vortex compiler infrastructure, a languageindependent optimizing compiler for objectoriented languages, with frontends for Cecil, C++, Java, and Modula3. In this paper, we describe the Vortex compiler's intermediate language, internal structure, and optimization suite, and then we report the results of experiments assessing the effectiveness of different combinations of optimizations on sizable applications across these four languages. We characterize the benchmark programs in terms of a collection of static and dynamic metrics, intended to quantify aspects of the "objectorientedness" of a program.
p773
aVReuse is about more than sharing code. As technology, standards and ideas evolve: so do the artifacts available for reuse. Our understanding of the nontechnical issues associated with reuse also progresses. This panel will look at reuse from 5 different perspectives. We will look at the reuse of design patterns and of services in a servicebased, distributed architecture as examples of artifacts relatively new to the corporate world. We will also focus on the psychological factors affecting the success of reuse programs along with organizational modifications and measurement techniques that will help make reuse work. Lastly, we will take a look at how certain kinds of reuse can affect the success of a project at the user level, and what this might imply for measuring the effectiveness of our reuse programs. The audience will leave with some concrete ideas about how to implement and measure reuse program as well as new thoughts on what artifacts may be reused.
p774
aVBOS is a prototypebased, objectoriented toolkit aimed at better supporting evolutionary software development. BOS attempts to support a spectrum of activities in one environment ranging from rapid prototyping to code hardening. Features enabling rapid prototyping include a prototypebased object model, an interpreted language, runtime argument constraints, position and keyword arguments, and a user interface toolkit. BOS also provides features for code hardening such as multimethods, multiple inheritance, external code wrapping mechanisms, and interfaces to other packages such as database management systems. BOS thus enables the endtoend programming of software in an integrated and unified environment. BOS has been used to develop several fullsize applications which have been evaluated and delivered externally.
p775
aVThis paper's primary aim is to improve the understanding of the delegation mechanism as defined in [18]. We propose a new characterization of delegation based on the notions of name sharing, property sharing and value sharing. It allows us (1) to clearly differentiate delegation from classinheritance in particular and more generally from other inheritance mechanisms and (2) to explain how a founded use of delegation relies on a correct semantics of variable property sharing between objects connected by a delegation link. We then describe a model of split objects which is proposed as an example of a disciplined and semantically founded use of delegation, where property sharing expresses viewpoints within objects.
p776
aVObjectoriented languages such as Java and Smalltalk provide a uniform object reference model, allowing objects to be conveniently shared. If implemented directly, these uniform reference models can suffer in efficiency due to additional memory dereferences and memory management operations. Automatic inline allocation of child objects within parent objects can reduce overheads of heapallocated pointerreferenced objects.We present three compiler analyses to identify inlinable fields by tracking accesses to heap objects. These analyses span a range from local data flow to adaptive wholeprogram, flowsensitive interprocedural analysis. We measure their cost and effectiveness on a suite of moderatesized C++ programs (up to 30,000 lines including libraries). We show that aggressive interprocedural analysis is required to enable object inlining, and our adaptive interprocedural analysis [23] computes precise information efficiently. Object inlining eliminates typically 40% of object accesses and allocations (improving performance up to 50%). Furthermore,
p777
aVThe rapid evolution of Design Patterns has hampered the benefits gained from using Design Patterns. The increase in the number of Design Patterns makes a common vocabulary unmanageable, and the tracing problem obscures the documentation that should be enhanced by using Design Patterns. We present an analysis of Design Patterns that will strongly reduce the number of Fundamental Design Patterns and show how strong language abstractions can solve the tracing problem and thereby enhance the documentation.
p778
aVThis paper explores the interpretation of specifications in the context of an objectoriented programming language with subclassing and method overrides. In particular, the paper considers annotations for describing what variables a method may change and the interpretation of these annotations. The paper shows that there is a problem to be solved in the specification of methods whose overrides may modify additional state introduced in subclasses. As a solution to this problem, the paper introduces data groups, which enable modular checking and rather naturally capture a programmer's design decisions.
p779
aVWe show how finitestate machines can standardize the protocol used by a component object to notify other interested objects of its state changes, resulting in a more effective use of static types to constrain both parties, and a more efficient dissemination of information. The enhanced component specification is called a logical observable entity, or LOE. We address two key issues in effectively applying such a strategy: how to extend subtyping to consider the state machines, and how to ensure some kind of compliance between the statemachine specification and its implementation. This leads to an unusual subtyping predicate for LOEs on the one hand, and a prototype code generation strategy on the other.
p780
aVIn this paper we investigate, in the context of functional prototypebased languages, objects which might extend themselves upon receiving a message. The possibility for an object of extending its own "self", referred to by Cardelli, as a selfinflicted operation, is novel in the context of typed objectbased languages. We present a sound type system for this calculus which guarantees that evaluating a welltyped expression will never yield a messagenotfound runtime error. We give several examples which illustrate the increased expressive power of our system with respect to existing calculi of objects. The new type system allows also for a flexible widthsubtyping, still permitting sound method override, and a limited form of object extension. The resulting calculus appears to be a good starting point for a rigorous mathematical analysis of classbased languages.
p781
aVWe present GJ, a design that extends the Java programming language with generic types and methods. These are both explained and implemented by translation into the unextended language. The translation closely mimics the way generics are emulated by programmers: it erases all type parameters, maps type variables to their bounds, and inserts casts where needed. Some subtleties of the translation are caused by the handling of overriding.GJ increases expressiveness and safety: code utilizing generic libraries is no longer buried under a plethora of casts, and the corresponding casts inserted by the translation are guaranteed to not fail.GJ is designed to be fully backwards compatible with the current Java language, which simplifies the transition from nongeneric to generic programming. In particular, one can retrofit existing library classes with generic interfaces without changing their code.An implementation of GJ has been written in GJ, and is freely available on the web.
p782
aVThe most serious impediment to writing substantial programs in the Java&trade; programming language is the lack of a gentricity mechanism for abstracting classes and methods with respect to type. During the past two years, several research groups have developed Java extensions that support various forms of genericity, but none has succeeded in accommodating general type parameterization (akin to Java arrays) while retaining compatibility with the existing. Java Virtual Machine. In this paper, we explain how to support general type parameterization including both nonvariant and covariant subtyping on top of the existing Java Virtual Machine at the cost of a larger code footprint and the forwarding of some method calls involving parameterized classes and methods. Our language extension is forward and backward compatible with the Java 1.2 language and runtime environment: programs in the extended language will run on existing Java 1.2 virtual machines (relying only on the unparameterized Java core libraries) and all existing Java 1.2 programs have the same binary representation and semantics (behavior) in the extended language.
p783
aVA number of inadequacies of existing implementation techniques for extending Java&trade; with parametric polymorphism are revealed. Homogeneous translations are the most spaceefficient but they are not compatible with reflection, some models of persistence, and multiple dispatch. Heterogeneous translations, on the other hand, can potentially produce large amounts of redundant information. Implementation techniques that address these concerns are developed. In languages that support runtime reflection, an adequate implementation of parametric, bounded and Fbounded polymorphism is shown to require (reflective) runtime support. In Java, extensions to the core classes are needed. This is in spite of the fact that parametric polymorphism is intended to be managed statically.
p784
aVWe consider the problem of delivering an effective finegrained clustering tool to implementors and users of objectoriented database systems. This work emphasizes online clustering mechanisms, as contrasted with earlier work that concentrates on clustering policies (deciding which objects should be near each other). Existing online clustering methods can be ineffective and/or difficult to use and may lead to poor space utilization on disk and in the disk block cache, particularly for small to mediumsize groups of objects. We introduce variablesize clusters (Vclusters), a finegrained object clustering architecture that can be used directly or as the target of an automatic clustering algorithm. We describe an implementation of Vclusters in the Shore OODBMS and present experimental results that show that Vclusters significantly outperform other mechanisms commonly found in object database systems (fixedsize clusters and near hints). Vclusters deliver excellent clustering and space utilization with only a modest cost for maintaining clustering during updates.
p785
aVWith the spread of the Internet the computing model on server systems is undergoing several important changes. Recent research ideas concerning dynamic operating system extensibility are finding their way into the commercial domain, resulting in designs of extensible databases and Web servers. In addition, both ordinary users and service providers must deal with untrusted downloadable executable code of unknown origin and intentions.Across the board, Java has emerged as the language of choice for Internetoriented software. We argue that, in order to realize its full potential in applications dealing with untrusted code, Java needs a flexible resource accounting interface. The design and prototype implementation of such an interface   JRes   is presented in this paper. The interface allows to account for heap memory, CPU time, and network resources consumed by individual threads or groups of threads. JRes allows limits to be set on resources available to threads and it can invoke callbacks when these limits are exceeded. The JRes prototype described in this paper is implemented on top of standard Java virtual machines and requires only a small amount of native code.
p786
aVThis paper draws several observations from our experiences in building support for object groups. These observations actually go beyond our experiences and may apply to many other developments of object based distributed systems.Our first experience aimed at building support for Smalltalk object replication using the Isis process group toolkit. It was quite easy to achieve group transparency but we were confronted with a strong mismatch between the rigidity of the process group model and the flexible nature of object interactions. Consequently, we decided to build our own object oriented protocol framework, specifically dedicated to support object groups (instead of using a process group toolkit). We built our framework in such a way that basic distributed protocols, such as failure detection and multicasts, are considered as first class entities, directly accessible to the programmers. To achieve flexible and dynamic protocol composition, we had to go beyond inheritance and objectify distributed algorithms.Our second experience consisted in building a CORBA service aimed at managing group of objects written on different languages and running on different platforms. This experience revealed a mismatch between the asynchrony of group protocols and the synchrony of standard CORBA interaction mechanisms, which limited the portability of our CORBA object group service. We restricted the impact of this mismatch by encapsulating asynchrony issues inside a specific messaging subservice.We dissect the cost of object group transparency in our various implementations, and we point out the recurrent sources of overheads, namely message indirection, marshaling/unmarshaling and strong consistency.
p787
aVIn this paper we extend the ODMG object data model with composite objects. A composite object is an object built by aggregating other component objects. Exclusiveness and dependency constraints, as well as referential integrity, can be associated with composition relationships among objects. Our composite object model is developed in the framework of the ODMG object database standard data model, but can be used in both objectoriented and objectrelational database systems. In the paper, we propose a language for defining composite objects and we define the semantics of update operations on composite objects.
p788
aVDynamic information collected as a software system executes can help software engineers perform some tasks on a system more effectively. To interpret the sizable amount of data generated from a system's execution, engineers require tool support. We have developed an offline, flexible approach for visualizing the operation of an objectoriented system at the architectural level. This approach complements and extends existing profiling and visualization approaches available to engineers attempting to utilize dynamic information. In this paper, we describe the technique and discuss preliminary qualitative studies into its usefulness and usability. These studies were undertaken in the context of performance tuning tasks.
p789
aVIn this paper we describe our experiences with the design, the deployment, and the initial operation of a distributed system for the remote monitoring and operation of multiple heterogeneous commercial buildings across the Internet from a single control center. Such systems can significantly reduce building energy usage.Our system is distinguished by its ability to interface to multiple heterogeneous legacy building Energy Management Control Systems (EMCSs), its use of the Common Object Request Broker Architecture (CORBA) standard communication protocols for the former task, development of a standardized naming system for monitoring points in buildings, the use of a relational DBMS to store and process time series data, automatic time and unit conversion, and a scripted time series visualization system.We describe our design choices and our experiences in development and operation. We note requirements for future distributed systems software for interoperability of heterogeneous realtime data acquisition and control systems.
p790
aVThis paper describes a lightweight yet powerful approach for writing distributed applications using shared variables. Our approach, called SHAREHOLDER, is inspired by the flexible and intuitive model of information access common to the World Wide Web. The distributed applications targeted by our approach all share a weak consistency model and loose transaction semantics, similar to a user's model of accessing email, bulletin boards, chat rooms, etc. on the Internet. The SHAREHOLDER infrastructure has several advantages. Its highly objectoriented view of shared variables simplifies their initialization and configuration. A shared variable's distribution mechanism is specified through an associated configuration object, and the programmer does not need to write any extra code to implement the sharing mechanism. These configuration objects can be initialized at runtime, allowing tremendous flexibility in dynamic control of distribution of shared variables. Finally, the programmer can treat shared variables and local variables interchangeably, thus simplifying conversion of a serial application into a distributed application.
p791
aVIn the standard Java implementation, a Java language program is compiled to Java bytecode. This bytecode may be sent across the network to another site, where it is then interpreted by the Java Virtual Machine. Since bytecode may be written by hand, or corrupted during network transmission, the Java Virtual Machine contains a bytecode verifier that performs a number of consistency checks before code is interpreted. As illustrated by previous attacks on the Java Virtual Machine, these tests, which include type correctness, are critical for system security. In order to analyze existing bytecode verifiers and to understand the properties that should be verified, we develop a precise specification of staticallycorrect Java bytecode, in the form of a type system. Our focus in this paper is a subset of the bytecode language dealing with object creation and initialization. For this subset, we prove that for every Java bytecode program that satisfies our typing constraints, every object is initialized before it is used. The type system is easily combined with a previous system developed by Stata and Abadi for bytecode subroutines. Our analysis of subroutines and object initialization reveals a previously unpublished bug in the Sun JDK bytecode verifier.
p792
aVWe present the first results of a project called LOOP, on formal methods for the objectoriented language Java. It aims at verification of program properties, with support of modern tools. We use our own frontend tool (which is still partly under construction) for translating Java classes into higher order logic, and a backend theorem prover (namely PVS, developed at SRI) for reasoning. In several examples we demonstrate how nontrivial properties of Java programs and classes can be proven following this twostep approach.
p793
aVSeparate compilation allows the decomposition of programs into units that may be compiled separately, and linked into an executable. Traditionally, separate compilation was equivalent to the compilation of all units together, and modification and recompilation of one unit required recompilation of all importing units.Java suggests a more flexible framework, in which the linker checks the integrity of the binaries to be combined. Certain source code modifications, such as addition of methods to classes, are defined as binary compatible. The language description guarantees that binaries of types (i.e. classes or interfaces) modified in binary compatible ways may be recompiled and linked with the binaries of types that imported and were compiled using the earlier versions of the modified types.However, this is not always the case: some of the changes considered by Java as binary compatible do not guarantee successful linking and execution. In this paper we study the concepts around binary compatibility. We suggest a formalization of the requirement of safe linking and execution without recompilation, investigate alternatives, demonstrate several of its properties, and propose a more restricted definition of binary compatible changes. Finally, we prove for a substantial subset of Java, that this restricted definition guarantees errorfree linking and execution.
p794
aVType compatibility can be defined based on name equivalence, that is, explicit declarations, or on structural matching. We argue that component software has demands for both. For types expressing individual contracts, name equivalence should be used so that references are made to external semantical specifications. For types that are composed of several such contracts, the structure of this composition should decide about compatibility.We introduce compound types as the mechanism to handle such compositions. To investigate the integrability into a strongly typed language, we add compound types to Java and report on a mechanical soundness proof of the resulting type system.Java users benefit from the higher expressiveness of the extended type system. We introduce compound types as a strict extension of Java, that is without invalidating existing programs. In addition, our proposal can be implemented on the existing Java Virtual Machine.
p795
aVMany popular objectoriented programming languages, such as C++, Smalltalk80, Java, and Eiffel, do not support multiple dispatch. Yet without multiple dispatch, programmers find it difficult to express binary methods and design patterns such as the "visitor" pattern. We describe a new, simple, and orthogonal way to add multimethods to singledispatch objectoriented languages, without affecting existing code. The new mechanism also clarifies many differences between single and multiple dispatch.
p796
aVClass loaders are a powerful mechanism for dynamically loading software components on the Java platform. They are unusual in supporting all of the following features: laziness, typesafe linkage, userdefined extensibility, and multiple communicating namespaces.We present the notion of class loaders and demonstrate some of their interesting uses. In addition, we discuss how to maintain type safety in the presence of userdefined dynamic class loading.
p797
aVWe present a static type system for objectoriented languages which strives to provide static typechecking without resorting to dynamic "type casts," restricting what code the programmer can write, or being too verbose or difficult to use in practice. The type system supports bounded parametric polymorphism where the bounds on type variables can be expressed using general recursive subtype or signature constraints, with Fbounded polymorphism and covariant type parameters being special cases.We implemented this type system in the Cecil language and used it to successfully typecheck a 100,000line Cecil program, the Vortex optimizing compiler. Our experience was very good: dynamicallytyped code needed very little rewriting. We also observed several common programming situations that presented a challenge for our type system. We discuss these situations and ways to typecheck them statically.
p798
aVGroups of people working in concert perform most commercial, industrial or inhouse software development. These groups are often quite diverse. This panel brings together objectoriented consultants and developers as well as practitioners and researchers interested in human factors and usercentered design, project management and technical writing. The panel will address the question, "To successfully complete today's projects, does objectoriented development as practiced today do an adequate job of supporting ALL of the participants who must collaborate?"
p799
aVObjectoriented programming languages allow interobject aliasing. Although necessary to construct linked data structures and networks of interacting objects, aliasing is problematic in that an aggregate object's state can change via an alias to one of its components, without the aggregate being aware of any aliasing.Ownership types form a static type system that indicates object ownership. This provides a flexible mechanism to limit the visibility of object references and restrict access paths to objects, thus controlling a system's dynamic topology. The type system is shown to be sound, and the specific aliasing properties that a system's object graph satisfies are formulated and proven invariant for welltyped programs.
p800
aVDynamic memory management in C++ is complex, especially across the boundaries of library abstract data types. C++ libraries designed in the orthodox canonical form (OCF) alleviate some of the problems by ensuring that classes which manage any kind of heap structures faithfully copy and delete these. However, in certain common circumstances, OCF heap structures are wastefully copied multiple times. General reference counting is not an option in OCF, since a shared body violates the intended value semantics; although a copyonwrite policy can be made to work with borrowed heap structures. A simpler ownership policy, based on larceny, allows lowlevel memory manager objects to steal heap structures from temporary variables, in properly isolated circumstances. Various strategies for regulating theft are presented, ranging from pilferconstructors to locks on heap data. Larceny has similarities with other transfer of ownership patterns, but is more a core implementation technique designed to improve the efficiency and effectiveness of OCFconformant libraries.
p801
aVIn a system where classes are treated as first class objects, classes are defined as instances of other classes called metaclasses. An important benefit of using metaclasses is the ability to assign properties to classes (e.g. being abstract, being final, tracing particular messages, supporting multiple inheritance), independently from the baselevel code. However, when both inheritance and instantiation are explicitly and simultaneously involved, communication between classes and their instances raises the metaclass compatibility issue. Some languages (such as SMALLTALK) address this issue but do not easily allow the assignment of specific properties to classes. In contrast, other languages (such as CLOS) allow the assignment of specific properties to classes but do not tackle the compatibility issue well.In this paper, we describe a new model of metalevel organization, called the compatibility model, which overcomes this difficulty. It allows safe metaclass programming since it makes it possible to assign specific properties to classes while ensuring metaclass compatibility. Therefore, we can take advantage of the expressive power of metaclasses to build reliable software. We extend this compatibility model in order to enable safe reuse and composition of class specific properties. This extension is implemented in NEOCLASSTALK, a fully reflective SMALLTALK.
p802
aVIn several works on design methodologies, design patterns, and programming language design, the need for program entities that capture the patterns of collaboration between several classes has been recognized. The idea is that in general the unit of reuse is not a single class, but a slice of behavior affecting a set of collaborating classes. The absence of largescale components for expressing these collaborations makes objectoriented programs more difficult to maintain and reuse, because functionality is spread over several methods and it becomes difficult to get the "big picture". In this paper, we propose Adaptive Plug and Play Components to serve this need. These components are designed such that they not only facilitate the construction of complex software by making the collaborations explicit, but they do so in a manner that supports the evolutionary nature of both structure and behavior.
p803
aVToday, any large objectoriented software system is built using frameworks. Yet, designing frameworks and defining their interaction with clients remains a difficult task. A primary reason is that today's dominant modeling concept, the class, is not well suited to describe the complexity of object collaborations as it emerges in framework design and integration. We use role modeling to overcome the problems and limitations of classbased modeling. Using role models, the design of a framework and its use by clients can be described succinctly and with much better separation of concerns than with classes. Using role objects, frameworks can be integrated into usecontexts that have not been foreseen by their original designers.
p804
aVThis paper presents a simple and efficient data flow algorithm for escape analysis of objects in Java programs to determine (i) if an object can be allocated on the stack; (ii) if an object is accessed only by a single thread during its lifetime, so that synchronization operations on that object can be removed. We introduce a new program abstraction for escape analysis, the connection graph, that is used to establish reachability relationships between objects and object references. We show that the connection graph can be summarized for each method such that the same summary information may be used effectively in different calling contexts. We present an interprocedural algorithm that uses the above property to efficiently compute the connection graph and identify the nonescaping objects for methods and threads. The experimental results, from a prototype implementation of our framework in the IBM High Performance Compiler for Java, are very promising. The percentage of objects that may be allocated on the stack exceeds 70% of all dynamically created objects in three out of the ten benchmarks (with a median of 19%), 11% to 92% of all lock operations are eliminated in those ten programs (with a median of 51%), and the overall execution time reduction ranges from 2% to 23% (with a median of 7%) on a 333 MHz PowerPC workstation with 128 MB memory.
p805
aVSeveral recent studies have introduced lightweight versions of Java: reduced languages in which complex features like threads and reflection are dropped to enable rigorous arguments about key properties such as type safety. We carry this process a step further, omitting almost all features of the full language (including interfaces and even assignment) to obtain a small calculus, Featherweight Java, for which rigorous proofs are not only possible but easy.Featherweight Java bears a similar relation to full Java as the lambdacalculus does to languages such as ML and Haskell. It offers a similar computational \u201cfeel,\u201d providing classes, methods, fields, inheritance, and dynamic typecasts, with a semantics closely following Java's. A proof of type safety for Featherweight Java thus illustrates many of the interesting features of a safety proof for the full language, while remaining pleasingly compact. The syntax, type rules, and operational semantics of Featherweight Java fit on one page, making it easier to understand the consequences of extensions and variations.As an illustration of its utility in this regard, we extend Featherweight Java with generic classes in the style of GJ (Bracha, Odersky, Stoutamire, and Wadler) and sketch a proof of type safety. The extended system formalizes for the first time some of the key features of GJ.
p806
aVThis paper presents a sound type system for a large subset of the Java bytecode language including classes, interfaces, constructors, methods, exceptions, and bytecode subroutines. This work serves as the foundation for developing a formal specification of the bytecode language and the Java Virtual Machine's bytecode verifier. We also describe a prototype implementation of a type checker for our system and discuss some of the other applications of this work. For example, we show how to extend our work to examine other program properties, such as the correct use of object locks.
p807
aVIn this paper we propose a new form of polymorphism for objectoriented languages, called correspondence polymorphism. It lies in a different dimension than either parametric or subtype polymorphism. In correspondence polymorphism, some methods are declared to correspond to other methods, via a correspondence relation. With this relation, it is possible to reuse nongeneric code in various type contexts\u2014not necessarily subtyping or matching contexts\u2014without having to plan ahead for this reuse. Correspondence polymorphism has advantages over other expressive object type systems in that programmerdeclared types still may be simple, firstorder types that are easily understood. We define a simple language LCP that reflects these new ideas, illustrating its behavior with multiple examples. We present formal type rules and an operational semantics for LCP, and establish soundness of the type system with respect to reduction.
p808
aVThis paper presents a combined pointer and escape analysis algorithm for Java programs. The algorithm is based on the abstraction of pointsto escape graphs, which characterize how local variables and fields in objects refer to other objects. Each pointsto escape graph also contains escape information, which characterizes how objects allocated in one region of the program can escape to be accessed by another region. The algorithm is designed to analyze arbitrary regions of complete or incomplete programs, obtaining complete information for objects that do not escape the analyzed regions.We have developed an implementation that uses the escape information to eliminate synchronization for objects that are accessed by only one thread and to allocate objects on the stack instead of in the heap. Our experimental results are encouraging. We were able to analyze programs tens of thousands of lines long. For our benchmark programs, our algorithms enable the elimination of between 24% and 67% of the synchronization operations. They also enable the stack allocation of between 22% and 95% of the objects.
p809
aVPrograms written in concurrent objectoriented languages, especially ones that employ threadsafe reusable class libraries, can execute synchronization operations (lock, notify, etc.) at an amazing rate. Unless implemented with utmost care, synchronization can become a performance bottleneck. Furthermore, in languages where every object may have its own monitor, perobject space overhead must be minimized. To address these concerns, we have developed a metalock to mediate access to synchronization data. The metalock is fast (lock + unlock executes in 11 SPARC\u2122 architecture instructions), compact (uses only two bits of space), robust under contention (no busywaiting), and flexible (supports a variety of higherlevel synchronization operations). We have validated the metalock with an implementation of the synchronization operations in a highperformance productquality Java\u2122 virtual machine and report performance data for several large programs.
p810
aVObject locking can be efficiently implemented by bimodal use of a field reserved in an object. The field is used as a lightweight lock in one mode, while it holds a reference to a heavyweight lock in the other mode. A bimodal locking algorithm recently proposed for Java achieves the highest performance in the absence of contention, and is still fast enough when contention occurs.However, mode transitions inherent in bimodal locking have not yet been fully considered. The algorithm requires busywait in the transition from the light mode (inflation), and does not make the reverse transition (deflation) at all.We propose a new algorithm that allows both inflation without busywait and deflation, but still maintains an almost maximum level of performance in the absence of contention. We also present statistics on the synchronization behavior of real multithreaded Java programs, which indicate that busywait in inflation and absence of deflation can be problematic in terms of robustness and performance. Actually, an implementation of our algorithm shows increased robustness, and achieves performance improvements of up to 13.1% in serveroriented benchmarks.
p811
aVThe speed of message dispatching is an important issue in the overall performance of objectoriented programs. We have developed an algorithm for constructing efficient dispatch functions that combines novel algorithms for efficient single dispatching, multiple dispatching, and predicate dispatching. Our algorithm first reduces methods written in the general predicate dispatching model (which generalizes single dispatching, multiple dispatching, predicate classes and classifiers, and patternmatching) into ones written using a simpler multimethod dispatching model. Our algorithm then computes a strategy for implementing multiple dispatching in terms of sequences of single dispatches, representing the strategy as a lookup DAG. Finally, our algorithm computes an implementation strategy separately for each of the single dispatches, producing for each dispatch a dispatch tree, which is a binary decision tree blending class identity tests, class range tests, and table lookups. Our algorithm exploits any available static information (from type declarations or class analysis) to prune unreachable paths from the lookup DAG, and uses any available dynamic profile information to minimize the expected time to search the dispatch trees. We measure the effectiveness of our dispatching algorithms on a collection of large Cecil programs, compiled by the Vortex optimizing compiler, showing improvements of up to 30% over already heavily optimized baseline versions.
p812
aVTraditional implementations of multiple inheritance bring about not only an overhead in terms of runtime but also a significant increase in object space. For example, the number of compilergenerated fields in a certain object can be as large as quadratic in the number of its subobjects. The problem of efficient object layout is compounded by the need to support two different semantics of multiple inheritance: shared, in which a base class inherited along distinct paths occurs only once in the derived class, and repeated, in which this base has multiple distinct occurrences in the derived. In this theoretical and foundational paper, we introduce two new techniques to optimize memory layout for multiple inheritance. The main ideas behind these techniques are the inlining of virtual bases and bidirectional memory layout. Our techniques never increase time overhead, and usually even decrease it. We show that in some example hierarchies, more than tenfold reduction in the space overhead can be achieved. We analyze the complexity of the algorithms to apply these techniques, and give theorems to estimate the efficacy of this application. For concreteness, techniques and examples are discussed in the context of C++.
p813
aVThe proliferation of the Internet is fueling the development of mobile computing environments in which mobile code is executed on remote sites. In such environments, the end user must often wait while the mobile program is transferred from the server to the client where it executes. This downloading can create significant delays, hurting the interactive experience of users.We propose Java class file splitting and class file prefetching optimizations in order to reduce transfer delay. Class file splitting moves the infrequently used part of a class file into a corresponding cold class file to reduce the number of bytes transferred. Java class file prefetching is used to overlap program transfer delays with program execution. Our splitting and prefetching compiler optimizations do not require any change to the Java Virtual Machine, and thus can be used with existing Java implementations. Class file splitting reduces the startup time for Java programs by 10% on average, and class file splitting used with prefetching reduces the overall transfer delay encountered during a mobile program's execution by 25% to 30% on average.
p814
aVJava programs are routinely transmitted over lowbandwidth network connections as compressed class file archives (i.e., zip files and jar files). Since archive size is directly proportional to download time, it is desirable for applications to be as small as possible. This paper is concerned with the use of program transformations such as removal of dead methods and fields, inlining of method calls, and simplification of the class hierarchy for reducing application size. Such \u201cextraction\u201d techniques are generally believed to be especially useful for applications that use class libraries, since typically only a small fraction of a library's functionality is used. By \u201cpruning away\u201d unused library functionality, application size can be reduced dramatically. We implemented a number of application extraction techniques in Jax, an application extractor for Java, and evaluate their effectiveness on a set of realistic benchmarks ranging from 27 to 2,332 classes (with archives ranging from 56,796 to 3,810,120 bytes). We report archive size reductions ranging from 13.4% to 90.2% (48.7% on average).
p815
aVEscape analysis [27, 14, 5] is a static analysis that determines whether the lifetime of data exceeds its static scope.The main originality of our escape analysis is that it determines precisely the effect of assignments, which is necessary to apply it to object oriented languages with promising results, whereas previous work [27, 14, 5] applied it to functional languages and were very imprecise on assignments. Our implementation analyses the full Java\u2122 Language.We have applied our analysis to stack allocation and synchronization elimination. We manage to stack allocate 13% to 95% of data, eliminate more than 20% of synchronizations on most programs (94% and 99% on two examples) and get up to 44% speedup (21% on average). Our detailed experimental study on large programs shows that the improvement comes from the decrease of the garbage collection and allocation times than from improvements on data locality [7], contrary to what happened for ML [5].
p816
aVThis paper presents a performance evaluation of the mobile agent paradigm in comparison to the client/server paradigm. This evaluation has been conducted on top of the Java environment, using respectively RMI, the Aglets mobile agents platform and a mobile agents prototype that we implemented. The measurements give the cost of the basic mechanisms involved in the implementation of a mobile agent platform, and a comparative evaluation of the two considered models (client/server and mobile agents) through two application scenarios. The results show that significant performance improvements can be obtained using mobile agents.
p817
aVJalapeo is a virtual machine for Java\u2122 servers written in Java.A running Java program involves four layers of functionality: the user code, the virtualmachine, the operating system, and the hardware. By drawing the Java / nonJava boundary below the virtual machine rather than above it, Jalapeo reduces the boundarycrossing overhead and opens up more opportunities for optimization.To get Jalapeo started, a boot image of a working Jalapeo virtual machine is concocted and written to a file. Later, this file can be loaded into memory and executed. Because the boot image consists entirely of Java objects, it can be concocted by a Java program that runs in any JVM. This program uses reflection to convert the boot image into Jalapeo's object format.A special MAGIC class allows unsafe casts and direct access to the hardware. Methods of this class are recognized by Jalapeo's three compilers, which ignore their bytecodes and emit specialpurpose machine code. User code will not be allowed to call MAGIC methods so Java's integrity is preserved.A small nonJava program is used to start up a boot image and as an interface to the operating system.Java's programming features \u2014 object orientation, type safety, automatic memory management \u2014 greatly facilitated development of Jalapeo. However, we also discovered some of the language's limitations.
p818
aVThe Advanced Teaching and Learning Academic Server (Atlas) is a software system that supports webbased learning. Students can register for courses, and can navigate through personalized views of course material. Atlas has been built according to Sun Microsystem's Java\u2122 Servlet specification using Xerox PARC's aspectoriented programming support called Aspect\u2122. Since aspectoriented programming is still in its infancy, little experience with employing this paradigm is currently available. In this paper, we start filling this gap by describing the aspects we used in Atlas and by discussing the effect of aspects on our objectoriented development practices. We describe some rules and policies that we employed to achieve our goals of maintainability and modifiability, and introduce a straightforward notation to express the design of aspects. Although we faced some obstacles along the way, this combination of technology helped us build a fast, wellstructured system in a reasonable amount of time.
p819
aVThis paper describes research in applications of aspectoriented programming (AOP) as captured in the AspectJ\u2122 language. In particular, it compares objectoriented and aspectoriented designs and implementations of role models.Sections 1, 2, and 3 provide background information on role models, objectoriented role model implementations, and aspectoriented programming, respectively. New aspectoriented designs for role models are explored in sections 4, 5, and 6.The base reference for this exploration is the Role Object pattern. Although useful for role models, this pattern introduces some problems at the implementation level, namely object schizophrenia, significant interface maintenance, and no support for role composition. Our research has resulted in alternative aspectoriented designs that alleviate some of these problems.Section 7 discusses how an agent framework that implements role models has been partially reengineered with aspects. The reengineering addressed concerns that are orthogonal or cross cut both the core and the role behavior. The aspect oriented redesign significantly reduced code tangling, overall method and module count, and total lines of code. These results and other conclusions are presented in section 8.
p820
aVModern generational garbage collectors look for garbage among the young objects, because they have high mortality; however, these objects include the very youngest objects, which clearly are still live. We introduce new garbage collection algorithms, called agebased, some of which postpone consideration of the youngest objects. Collecting less than the whole heap requires write barrier mechanisms to track pointers into the collected region. We describe here a new, efficient write barrier implementation that works for agebased and traditional generational collectors. To compare several collectors, their configurations, and program behavior, we use an accurate simulator that models all heap objects and the pointers among them, but does not model cache or other memory effects. For objectoriented languages, our results demonstrate that an olderfirst collector, which collects older objects before the youngest ones, copies on average much less data than generational collectors. Our results also show that an olderfirst collector does track more pointers, but the combined cost of copying and pointer tracking still favors an olderfirst over a generational collector in many cases. More importantly, we reopen for consideration the question where in the heap and with which policies copying collectors will achieve their best performance.
p821
aVWe describe how reachabilitybased orthogonal persistence can be supported even in uncooperative implementations of languages such as C++ and Modula3, and without modification to the compiler. Our scheme extends Bartlett's mostlycopying garbage collector to manage both transient objects and resident persistent objects, and to compute the reachability closure necessary for stabilization of the persistent heap. It has been implemented in our prototype of reachabilitybased persistence for Modula3, yielding performance competitive with that of comparable, but nonorthogonal, persistent variants of C++. Experimental results, using the 007 object database benchmarks, reveal that the mostlycopying approach offers a straightforward path to efficient orthogonal persistence in these uncooperative environments. The results also characterize the performance of persistence implementations based on virtual memory protection primitives.
p822
aVIn this paper we present the Generic Graph Component Library (GGCL), a generic programming framework for graph data structures and graph algorithms. Following the theme of the Standard Template Library (STL), the graph algorithms in GGCL do not depend on the particular data structures upon which they operate, meaning a single algorithm can operate on arbitrary concrete representations of graphs. To attain this type of flexibility for graph data structures, which are more complicated than the containers in STL, we introduce several concepts to form the generic interface between the algorithms and the data structures, namely, Vertex, Edge, Visitor, and Decorator. We describe the principal abstractions comprising the GGCL, the algorithms and data structures that it provides, and provide examples that demonstrate the use of GGCL to implement some common graph algorithms. Performance results are presented which demonstrate that the use of novel lightweight implementation techniques and static polymorphism in GGCL results in code which is significantly more efficient than similar libraries written using the objectoriented paradigm.
p823
aVThis paper describes a novel approach to managing the evolution of distributed, persistent systems at runtime. This is achieved by partitioning a system into disjoint zones, each of which can be evolved without affecting code in any other. Contracts are defined between zones, making typelevel interdependencies and interzone communication explicit. Programmer supplied code is added to the running system, at the boundary between zones, to constrain the scope of changes. A change methodology is presented which the software engineer uses to help describe and manage the evolution of the system. Knowledge of the application semantics is essential when evolving a system and our approach allows the engineer to concentrate on these semantic aspects of change. Our Javabased demonstration platform and methodology reduce or remove some of the burdensome tasks the software engineer is normally expected to perform when changing a system, making evolution more tractable.
p824
aVIn programming distributed objectoriented systems, there are several approaches for achieving binary interactions in a multiprocess environment. Usually these approaches take care only of synchronisation or communication. In this paper we describe a way of designing and implementing a more general concept: multiparty interactions. In a multiparty interaction, several parties (objects or processes) somehow \u201ccome together\u201d to produce an intermediate and temporary combined state, use this state to execute some activity, and then leave this interaction and continue their normal execution. The concept of multiparty interactions has been investigated by several researchers, but to the best of our knowledge none have considered how failures in one or more participants of the multiparty interaction can be dealt with. In this paper, we propose a general scheme for constructing dependable multiparty interactions in a distributed objectoriented system, and describe its implementation in Java. In particular, we extend the notion of multiparty interaction to include facilities for handling exceptions. To show how our scheme can be used, we use our framework to build an abstraction mechanism that supports cooperative and competitive concurrency in distributed systems. This mechanism is then applied to program a system in which multiparty interactions are more than simple synchronisations or communications.
p825
aVJava programs perform many synchronization operations on data structures. Some of these synchronization are unnecessary; in particular, if an object is reachable only by a single thread, concurrent access is impossible and no synchronization is needed. We describe an interprocedural, flow and contextinsensitive dataflow analysis that finds such situations. A global optimizing transformation then eliminates synchronizations on these objects. For every program in our suite of ten Java benchmarks consisting of SPECjvm98 and others, our system optimizes over 90% of the alias sets containing at least one synchronized object. As a result, the dynamic frequency of synchronizations is reduced by up to 99%. For two benchmarks that perform synchronizations very frequently, this optimization leads to speedups of 36% and 20%.
p826
aVInspections can be used to identify defects in software artifacts. In this way, inspection methods help to improve software quality, especially when used early in software development. Inspections of software design may be especially crucial since design defects (problems of correctness and completeness with respect to the requirements, internal consistency, or other quality attributes) can directly affect the quality of, and effort required for, the implementation.We have created a set of \u201creading techniques\u201d (so called because they help a reviewer to \u201cread\u201d a design artifact for the purpose of finding relevant information) that gives specific and practical guidance for identifying defects in ObjectOriented designs. Each reading technique in the family focuses the reviewer on some aspect of the design, with the goal that an inspection team applying the entire family should achieve a high degree of coverage of the design defects.In this paper, we present an overview of this new set of reading techniques. We discuss how some elements of these techniques are based on empirical results concerning an analogous set of reading techniques that supports defect detection in requirements documents. We present an initial empirical study that was run to assess the feasibility of these new techniques, and discuss the changes made to the latest version of the techniques based on the results of this study.
p827
aVThe Unified Modeling Language (UML) is a standard modeling language in which some of the best objectoriented (OO) modeling experiences are embedded. In this paper we illustrate the role formal specification techniques can play in developing a precise semantics for the UML. We present a precise characterization of requirementslevel (problemoriented) Class Diagrams and outline how the characterization can be used to semantically analyze requirements Class Diagrams.
p828
aVWe present a domainspecific language for specifying recursive traversals of object structures, for use with the visitor pattern. Traversals are traditionally specified as iterations, forcing the programmer to adopt an imperative style, or are hardcoded into the program or visitor. Our proposal allows a number of problems best approached by recursive means to be tackled with the visitor pattern, while retaining the benefits of a separate traversal specification.
p829
aVSharing and transfer of object references is difficult to control in objectoriented languages. Unconstrained sharing poses serious problems for writing secure components in objectoriented languages. In this paper, we present a set of inexpensive syntactic constraints that strengthen encapsulation in objectoriented programs and facilitate the implementation of secure systems. We introduce two mechanisms: confined types to impose static scoping on dynamic object references and, for technical reasons, anonymous methods which are methods that do not reveal the identity of the current instance (this). Confined types protect objects from use by untrusted code, while anonymous methods allow standard classes to be reused from confined classes. We have implemented a verifier which performs a modular analysis of Java programs and provides a static guarantee that confinement is respected. We present security related programming examples.
p830
aVAn analysis is provided for Java programs that reverse engineers parameterized types into existing Java code. This analysis propagates precise type information about the contents of container objects. As an application, the analysis can be used to justify the safe removal of downcasts that are guaranteed to succeed. Another application is in automatically reverse engineering parameterized types into existing Java libraries, so that they can be used in Java dialects with parameterized types.
p831
aVVirtual classes and nested classes are distinguishing features of BETA. Nested classes originated from Simula, but until recently they have not been part of main stream object oriented languages. C++ has a restricted form of nested classes and they were included in Java 1.1. Virtual classes is the BETA mechanism for expressing generic classes and virtual classes is an alternative to parameterized classes. There has recently been an increasing interest in virtual classes and a number of proposals for adding virtual classes to other languages, extending virtual classes, and unifying virtual classes and parameterized classes have been made. Although virtual classes and nested classes have been used in BETA for more than a decade, their implementation has not been published. The purpose of this paper is to contribute to the understanding of virtual classes and nested classes by presenting the central elements of the semantic analysis used in the Mjlner BETA compiler.
p832
aVThe Educators' Symposium is a unique forum for educators from both academia and industry who have a vested interest in OO education and training. The Educators' Symposium facilitates the exchange of ideas in a number of ways, including featured talks by professionals on the cutting edge of OO education, paper presentations, and activity sessions. Furthermore the successful poster session has been expanded to an exhibition. This exhibition provides the ideal opportunity for exchanging teching and learning techniques, networking with professionals facing similar challenges, and exploring teaching and learning tools. All attendees are invited to actively participate in the exhibition by bringing any related material they would like to share.
p833
aVIs it possible to do fixedprice eXtreme Programming projects? How does one bid on a project with changing scope? When is the project finished? These questions and others were addressed in the context of a successful project modeling infection and vaccination scenarios implemented in VisualAge Smalltalk for a noted pharmaceutical company. Interesting results and answers, as well as a short demonstration, will be presented.
p834
aVFollow along the implementation of an actual webbased application developed using Extreme Programming (XP). The application implements the user area for a commercial web site, using Java Servlets and JDBC. This report highlights the practices of XP that worked well on the project as well as those that could be improved.
p835
aVThis paper describes our experiences with evolving a system infrastructure while continuing to support new functionality. In it we explore the architecture, technology decisions made, and difficulties encountered in porting a core business application while it is being used and extended to support the enterprise.
p836
aVThis article describes a flexible architecture for developing servicesbased business applications that use componentbased architectural services to demarcate the application into various tiers that enable the decoupling of each layer, both logically and physically. This provides the speed and flexibility needed to embrace new product ideas, channels, and markets.
p837
aVWhile building nine objectoriented financial systems since 1989, we evolved techniques for modeling \u201cfinancial vehicles\u201d (bonds, contracts, real estate, racehorses). We specify each particular financial vehicle and vehicle type by combining objects that model the fundamental ideas behind financial vehicles. These techniques enable installed systems to accommodate new kinds of financial vehicles. Many normal software design techniques proved ineffective.
p838
aVWhile designing a global bank's MoneyLink, the clustering of nonfunctional properties suggested two \u201clobes\u201d: (1) transactional (enabling payments among affiliate banks), which required highavailability, highsecurity, and geographical distribution; and (2) analytic, with traditional database requirements. These purely architectural components coincided with business processes separately identified by business analysts. Architecture provided problem domain insight.
p839
aVCan the benefits of reuse and patterns that are so well known in software design and implementation be carried over into analysis and database design? The fledgling field of Analysis Patterns has provided only a partial affirmative answer. We have instead attempted to import the CYC upper ontology (an AIderived \u201cmodel of everything\u201d) into an object model expressed in Java. The result is an object model that is extremely resilient to change and should be extensible to practically any domain. We describe the difficulties involved and some of the most important lessons learned.
p840
aVBoeing has nearly completed a major program for business process redesign. Given the scale of this project and the large number of existing, heterogeneous systems, it has been essential that a standardsbased architecture be implemented. The Common Object Request Broker Architecture (CORBA) standard has worked well as the application integration baseline for this new system
p841
aVWe present the key lessons learned from the ESCORT European project. ESCORT is concerned with integrating diverse applications and devices for traffic control at the intersection level. The paper emphasises the difficulty of breaking existing, vertical products to build an integration platform, and the role of object technology (and UML in particular) in this context.
p842
aVApplication development with an endtoend declarative Application Framework has shown the following significant benefits: 80% error reduction, 4080% code reduction, and predictable development cycles by first time object/Java developers. These Application Frameworks may at any point in the development/deployment process sit on top of System Frameworks such as CORBA or EJB.
p843
aVAlthough there has been an ANSI standard for Smalltalk for some years, there has not been a verification suite for it. This has allowed the various dialects to deviate from the standard without much notice. One of the first Camp Smalltalk projects was a test suite for the ANSI standard protocols. It has several thousand tests based on SUnit, a way of testing protocol conformance, and way of reusing protocolbased tests for each of the concrete classes that is supposed to support a protocol.
p844
aVSeveral coupling frameworks have been introduced in the literature to identify and measure the design complexity of objectoriented software systems. The coupling metric COF presented by the unified framework considers inheritance, polymorphism, method overriding, and direct methods of invocations to identify possible interactions in the system that contribute to the software complexity. We evaluate the COF metric in the context of Java interfaces. Interactions are identified, the effectiveness of the model is discussed, and an extension of the model is offered for more accurate measurement of complexity.
p845
aVThe paper introduces two concepts based on use cases: business patterns and automation patterns. It describes a connection between the business and automation patterns with classes. Finally, it presents an idea of a multilevel library that allows global reuse of software development models and code.
p846
aVThis research presents a reengineering workbench and architecture that allows for legacy systems written in procedural languages to be migrated to new object oriented platforms. This methodology allows for specific design and quality requirements of the target migrant system to be considered during the reengineering process through an iterative and incremental process. The migration process recovers an object model from the procedural code and incrementally refines it by taking into consideration the design requirements for the migrant system. A case study is presented as a proof of concept.
p847
aVRealtime objectoriented modeling tools (e.g. Rational RoseRT, iLogix Rhapsody) allow developers to focus on software architecture by abstracting away lowlevel implementation details. We believe that design patterns can be very beneficial in this context, and present the rationale and concepts behind a proposal for the extension of such a toolset to support them explicitly.
p848
aVWith software systems such as operating systems, the interaction of their components becomes more complex. This interaction may limit reusability, adaptability, and make it difficult to validate the design and correctness of the system. As a result, reengineering of these systems might be inevitable to meet future requirements. There is a general feeling that OOP promotes reuse and expandability by its very nature. This is a misconception as none of these issues is enforced. Rather, a software system must be specifically designed for reuse, expandability, and adaptability [4]. Operating systems are dominated in many aspects. Supporting separation of concerns and aspectual decomposition in the design of operating systems provides a number of benefits such as reusability, expandability, adaptability and reconfigurability. However, such support is difficult to accomplish. AspectOriented Programming (AOP) [7] is a paradigm proposal that aims at separating components and aspects from the early stages of the software life cycle, and combines them together at the implementation phase. Besides, AspectOriented Programming promotes the separation of the different aspects of components in the system into their natural form. However, AspectOriented software engineering can be supported well if there is an operating system, which is built based on an aspectoriented design. Therefore aspects can be created in applications, reused and adapted from the aspects provided by the operating systems. ObjectOriented Operating Systems treat aspects, components, and layers as a two dimensional models, which is not a good design model. Aspects in the operating system cannot be captured in the design and implementation. Twodimensional models lead to inflexibility, limit possibilities for reuse and adaptability, and make it hard to understand and modify. The poster will show an AspectOriented Framework [1, 8], which simplifies system design by expressing its design at a higher level of abstraction, for supporting the design of adaptable operating systems. A framework is more than a class hierarchy and it is a reusable to produce custom systems and applications [5]. AspectOriented Framework is based on a threedimensional design that consists of components, aspects, and layers.\u000aComponents consist of the basic functionality modules of the system. Aspects are the properties in the systems that cut across the components in the operating systems. Some aspects in operating systems such as synchronization, scheduling, faulttolerance cut across, in horizontal and vertical, the basic functionality of the systems. Layers consist of the components and aspects. By separating aspects and components of the operating systems in every layer, we can provide a better generic design model of the operating systems. The framework uses design patterns [6]. The overall architecture is divided into two frameworks: Base Layer and Application Layer Framework. The poster will show The UML model of frameworks and how to maximize separation of aspects, components, and layers from each other. Our goal is to achieve a better design model and implementation of operating systems, in terms of reusability, adaptability, extensibility, and reconfigurability.
p849
aVAs the size and requirements of software systems increase, their design has reached a complexity that requires software engineers to revisit the principle of separation of concerns [5]. Traditional software organization has been performed along some form of functional decomposition. Different paradigms and languages support the implementation, and composition of subparts into whole systems through the availability of some modular unit of functionality (component). In essence, traditional software decomposition and current programming languages have been mutually supportive [4]. At the same time, separation of concerns can only be beneficial if the different concerns can be effectively composed to produce the overall system. The OOP paradigm seems to work well only if the problem to be solved can be described with relatively simple interfaces among objects. Unfortunately, this is not the case when we move from sequential programming to concurrent programming where the component interaction violates simple object interfaces. One of the reasons behind this is the inherent structure of today's software systems that conceptually does not lead itself to a safe decomposition. As a result, the benefits associated with OOP no longer hold. Component interactions limit reuse and make it difficult to validate the design and correctness of software systems. Reengineering of these systems is needed in order to meet future changes in requirements. This component interaction is based on a number of properties that affect the semantics or the performance of the system and do not localize well in one modular unit, but tend to cutacross groups of components resulting in a \u201ccode tangling\u201d [4]. Example crosscutting properties (or aspects) include synchronization, scheduling, and fault tolerance. This code tangling destroys modularity, making the source code difficult to develop and difficult to understand. It also limits reuse, making the source code difficult to evolve. It further makes programs more error prone. In essence, it destroys the quality of the software. In [1] the authors refer to these phenomena as \u201ccomposition anomalies\u201d. This composition anomaly requires a shift in the methodologies used to separate concerns. In conjunction with modular composition, adaptability and reuse remain major issues to be considered while building complex software systems. AspectOriented Programming (AOP) [4] is an emerging methodology that addresses components and aspects at the analysis and design phase of the software lifecycle, using mechanisms to compose them at the implementation level with a growing number of different technologies.
p850
aVGeographic Information Systems (GIS) are used to collect, store, analyze and present information describing physical and logical aspects of real world entities. They deal with several areas, such as georeferencing, topology, visualization, representation of continuous information and so on.\u000aMost of GIS applications are hardly adapted to new requirements, like changing of the reference system, integrating vector and raster models, or handling continuous information, among others.\u000aThis work presents an pattern driven architecture for building GIS applications. The result is a framework that is focused on flexibility and adaptability features.
p851
aVCurrent programming environments for user interface development provide visual programming techniques to aid in using object oriented and component technology concepts. We suggest a clear separation of visualizations for component assembly on the one hand and object oriented programming on the other hand, present metaphors and a programming environment.
p852
aVWe present an assemblydesign environment that supports the JavaBeans extensible runtime containment and services protocol. The environment provides: a vehicle for demonstrating the Java component model; a thirdparty client for testing BeanContext and BeanContextChild components; and a prototype illustrating how a visual builder might unify visual and context nesting during component assembly.
p853
aVJava Beans provides a robust mechanism for combining software components. However, to a large extent Java Beans components are currently not being combined to create documents in an environment such as a word processor or illustration program. This is not due to a weakness in the Beans property model, but rather to limitations in how Java Beans' user interfaces are currently implemented. Verdantium is a prototype Javabased API (for JDK 1.2.2 and above) for document embedding that takes a novel approach to implementing software components. This environment retains the propertybased semantics from Java Beans, but implements different approaches for onscreen presentation, visual embedding, document printing, and persistence. Fully functional applications (i.e., word processors, illustration programs, equation editors, etc.) can be implemented as individual software components that can either run independently or be embedded in other applications using the same API.\u000aVerdantium's presentation API works on top of Java JFC/Swing and is implemented using the ModelViewController (MVC) philosophy. That is to say, the user interface for a certain kind of document is kept separate from its application logic. The VerdantiumComponent is a Java interface implemented by the model class, with the GUI stored in a separate object (though one may optionally integrate the two). By contrast, Java Beans that are displayed in the Bean Box are subclasses of a view class, with no inherent support given to the underlying model.\u000aOne important aspect of using this interface is that an implementation of VerdantiumComponent does not need to subclass a descendent of java.awt.Component. For instance if the component's GUI is a scrolling pane, one does not have to subclass JScrollPane. Instead, the user interface of the VerdantiumComponent is the Swing JComponent returned by the VerdantiumComponent's getGUI() method.\u000aBecause Verdantium components do not need to subclass java.awt.Component, subclassing can be used instead to enhance design and code reuse. For instance, a scrolling component can be implemented as a straightforward subclass of a simpler drawing component that renders onto a pane. The subclass overrides the getGUI() method so that it returns a JScrollPane containing the user interface of the superclass VerdantiumComponent. That is to say, the subclass wraps a JScrollPane around what the superclass originally returned from its own getGUI() method. Note that the JScrollPane returned by the subclass and the rendering pane returned by the superclass are two objects that can come from completely different class hierarchies (as long as they both have JComponent as a common root).\u000aIn order to provide appropriate support for embedded documents and take full advantage of JFC/Swing functionality, Verdantium uses an embedding implementation based on subclassing Swing's existing JDesktopPane and JInternalFrame. The subclass of JInternalFrame modifies the internal frame's MDI user interface so that it displays itself in a fashion more consistent with an embedding frame in a typical container application. This provides a flexible and robust means for moving and resizing embedded components. Unlike typical MDI windows, Verdantium embedding frames have a transparent content area, allowing multiple components with transparent backgrounds to overlay each other in a container application. Verdantium also uses a different semantics, compared to most other document frameworks, for the display of embedding frames. Instead of displaying frames on a percomponent basis, Verdantium uses a global mode that turns them all on, or turns them all off.\u000aCurrently, the primary printing functionality for visual Java Beans is through the use of the java.awt.Component print() method, which does not support printing on multiple pages. Verdantium retains the use of this method, but adds an additional API for multipage printing using the java.awt.print. Printable interface. This allows a \u201cmultipage\u201d component such as a text editor to be printed by another component in a standard way. Verdantium also provides the option for a component to take greater control over the printing process, allowing the component to do things such as display customized print dialogs.\u000aA component running as a standalone application often needs to be printed in a different manner from one than has been visually embedded in some other component. Verdantium addresses this by allowing a component to both implement the multipage printing interface and have its UI support the java.awt.Component print() method. For instance, a visually embedded component might be printed by its parent using the print() method. Meanwhile, a standalone component displaying the same content might print itself on multiple pages.\u000aTypical preJava document applications can load or save data of more than one flavor from persistent storage. For instance, Microsoft Word can save its data to either plain text, RTF, or to its own Microsoft Word format. Java Beans persistence APIs currently provide no means for a bean to load and/or save its state in multiple flavors. To compensate for this, the ability to load or save in multiple data flavors is provided as a standard part of a Verdantium Component. Such a data flavor can either refer to an object in a serialized object stream or to a set of raw stream data in a legacy format. This allows a Verdantium component to load data formats such as plain text that are currently outside the Java Beans persistence model. In order to determine which component can load a particular legacy format, the file's extension is mapped to a MIME type, and then the mapped MIME type is compared with the MIME types each component supports to determine which one(s) can handle the load request.\u000aIn addition to legacy file formats, Verdantium also provides a mechanism for a component to save itself to a property list of serializable objects (where each such object can be externalized in terms of another property list of objects). This allows an object to serialize itself by writing a series of namevalue pairs (more precisely nameobject pairs), while retaining the ability to utilize Java's existing serialization. In addition, the use of property names is advantageous when attempting to maintain different versions of the same persistence format over time. A single reader can scan for property names from several formats, and maintain compatibility with each.
p854
aVThe unrelenting pace of change that confronts contemporary software developers compels them to make their applications more configurable, flexible, and adaptable. A possible way to meet such requirements is to use an Adaptive ObjectModel (AOM). This poster describes common architectures for adaptive objectmodels and summarizes the results from our ECOOP 2000 workshop [9].
p855
aVAs industry begins to invest in OO/CBD (ComponentBased Development) processes and the OMG considers whether/how to standardize on process, many issues arise. Those to be debated include process framework versus process; high ceremony versus low ceremony; the need for flexibility and tailorability; and the role of automation.
p856
aVThis work presents a software architecture that is especially useful for managing the evolution of web applications. Webbased systems are a range of applications for which there are no technological standards and new concepts and tools are currently under evolution. Examples of this lack of standards include the transition from CGI scripts to Java Servlets and to Java Server Pages (JSP). Therefore, the maintenance and evolution of web applications is an important topic for software developers and the software research community. The proposed architecture combines the ntier, broker, and blackboard architectural patterns.
p857
aVSeparation of concerns is a key goal in achieving software reusability. MetaLevel Programming approaches pave the way to separation of concerns by handling functional and nonfunctional aspects in different levels, but provide little help for software composition, verification and evolution activities. Approaches based on Software Architecture Description Languages can overcome these deficiencies and additionally may discipline, and make explicit, the deployment of metalevel programming. RRIO combines both approaches providing a useful framework to develop, implement and maintain applications.
p858
aVObjectOriented (OO) computing is fast becoming the defacto standard for software development. Currently, many OO systems consist of a single, large object server and multiple client applications. Optimizations can be accomplished if these large monolithic servers are decomposed into numerous smaller object servers and spread across the network.\u000aMatching object servers to network assets isn't a trivial task. The more criteria one considers in determining an optimal assignment, the more complex the processing becomes. Complicating matters is the fact that many of these criteria are not fixed.\u000aOptimal deployment strategies for object servers can change given deviations in object servers, client applications, operational missions, hardware modifications, and various other changes to the environment.\u000aOnce distributed object servers become more prevalent, there will be a need to optimize the deployment of object servers to best serve the end user's changing needs. Having the ability to generate object server deployment strategies would allow users to take full advantage of their network of computers.\u000aState of the art load balancing techniques consists of scheduling a given number of independent tasks to a set of machines. Object servers do not have independent tasks. All methods in an object are related. Also, the number of times a method is called is usually dependent on the interaction with the end user.\u000aObject servers, client applications, user inputs, and network resources can be profiled. A system of nonlinear equations can be derived from these profiles. Solving this system of equations can produce more optimal deployment strategies for the object servers.
p859
aVOur research involves the creation of a plug and play environment where different devices and services can be made to function together effortlessly. Currently the connection of any entity to another entity, through a network, needs prior setup and configuration. We aim to design an architecture, which requires minimal setup processes or none, for easy access to remote services using the Wireless Application Protocol (WAP) and Jini.
p860
aVThe tools and processes used to design XML vocabularies (DTD or XML Schema) are generally different from those used for application design using UML. In addition, large XML vocabularies are often difficult to understand and communicate with business users. This research summary describes the use of UML class diagrams for modeling XML vocabularies and generating XML Schemas from the UML.
p861
aVMany software projects fail. The main reason they fail is that people don't understand them well enough to make them succeed. The Project/Process Evaluation and Development Framework (PEDF) allows both customers and developers to understand their project and its process. It allows them to understand what they're doing, why it's supposed to work (or why it won't work), and how to modify it to make it more successful.
p862
aVApplying current objectoriented (OO) methods in an industrial environment is by itself not sufficient for a certification according to the norm EN ISO 9001, which requires the application of a comprehensive process model that includes, among other things, quality assurance. We present ooSEM in this poster, a process model for supporting OO development in an industrial environment based on a meta process model that is consistent with this norm. ooSEM is not a usual OO method but deals primarily with complementary aspects like project management and control, comprehensive documentation, quality assurance and configuration management, as required by EN ISO 9001. So, ooSEM is also flexible with respect to existing OO notations and methods. Specifically for OO modeling, ooSEM provides a clean solution to the problem of the transition from OO analysis to design, which is also the topic of a panel at OOPSLA 2000. ooSEM is available to the software developers in our environment through the company's intranet in the form of a Web application.
p863
aVThis poster examines the challenges of developing a refactoring tool for a weakly typed language such as Smalltalk as opposed to a strongly typed language such as Java. To explore this, we will compare the push up field refactoring in each language. This refactoring was selected because it is relatively simple conceptually, but difficult to implement for Java.\u000aIn a weakly typed language such as Smalltalk, push up field is simple. The user simply determines that the parent class needs the variable. The refactoring tool moves the field to the parent class. Then the tool searches all the subclasses of the parent class, if the classes have a variable with the same name, the refactoring tool removes the variable from the subclass. The task is complete.\u000aFor Java, a description of classes and types is necessary. Let's start with a base class A. A has a number of child classes, B, C, D, E, F, and G. Each of BG has a single instance variable named var. The only difference between the classes is the type of var. B and C both have a variable named var with type X. D has a variable named var with a type Y. E has a variable named var with type W. F has a variable named var with a type of Z. And G has a variable named var with the type int. W, X, Y, and Z are classes. W is the base class, X is a subclass of W, and Y is a subclass of X. Z is unrelated to all of the other classes. Since all subclasses of A have a variable named var, a programmer might suspect that they could reduce the amount of code by moving the variable into the parent class A.\u000aLet's move the field named var from class B into class A. Like in Smalltalk, the Java refactoring tool can remove the variable var from B and C, since the var variable in both classes have the same type. The source file for A would get the declaration of type X named var. The source file for A might also gain an import for the type X. Let's postpone the discussion of the scope of the variable var.\u000aThe refactoring tool can remove var from D since the storage in A is sufficient. However, the impact on the source code depends on whether there is a difference between the interface of X and Y. If there are methods or fields added in Y that are not in X, then by removing the variable var from D, the refactoring tool must be sure to cast var back to a Y in code that refers to var in a way that accesses the differences. For instance, if X and Y have a method m, then in D the invocation var.m() remains unchanged. If Y has a method n and no similar method is present in X or a parent class of X, then D must change the code that looks like var.n(); to ((Y) var).n();\u000aThe refactoring tool cannot remove var from E, since a variable of type W can not be stored in a class X. A refactoring tool might change the type of the object in class A to W. The effect of casting var to an X or a Y type would be more widespread.\u000aTo remove var from F, the refactoring tool would need to change the variable var's type to Object, since Object is the base class for all classes thus would be a common storage type for Ws and Zs. Everywhere that var is used in any source files, it must be cast to the appropriate type, eliminating the benefit of compile time type checking.\u000aRemoving var from G is even more difficult, since a primitive type such as int does not derive from Object, the refactoring tool must find a way to store var for G. Java provides a series of immutable types that store the primitive types, i.e. java.lang.Integer. To access the variable's value, \u201cvar\u201d would be replaced with \u201cvar.intValue()\u201d. To set var, the tool would create a new instance of the Integer class, and assign it to var. Since multithreaded programming is encouraged, any operation that replaces an atomic action with a series of steps must be considered very carefully. In this instance, the assignment of a value to var is atomic, but now includes the creation of an Integer object followed by an assignment. Object instantiation is not atomic, therefore if might be necessary to wrap the assignment of var in an appropriate synchronized block. Synchronized blocks are computationally very expensive, and should not be used indiscriminately. Therefore, an automated tool should not remove the variable var from G as a result of pushing var from B to A Instead, a refactoring tool might recommend renaming the variable var in G.
p864
aVOne of the claimed advantages of objectoriented (OO) development is that developers can use objects in a uniform modeling approach throughout the process. In particular, they can coherently apply the same notation for representing these objects and their relations in both analysis and design. Given this, the claims by many OO methodologists (see, e.g., [1, 5]) that the transition from OO analysis (OOA) to OO design (OOD) is easy and smooth may seem convincing. However, the contrasting view can be found in [2, 4] that it is actually difficult to go from OOA to OOD and, recognizing the differences between what is modeled in the analysis and design phases can lead to a more conscious development approach.\u000aIn the light of such controversial views, it seems to be necessary to widely discuss this issue. The prospective panelists represent a wide spectrum of related views. So, there is some hope that this panel might more or less resolve this important issue.\u000aA consequence of resolving this issue might be a contribution to a better understanding of a paradox in the current software business: software is wanted faster and at the same time with higher quality than ever before. Does the view that the transition from OOA to OOD might be easy promise too much in the direction of quick solutions?
p865
aVJML is a notation for specifying the detailed design of Java classes and interfaces. JML's assertions are stated using a slight extension of Java's expression syntax. This should make it easy to use. Tools for JML aid in static analysis, verification, and runtime debugging of Java code.
p866
aVThis paper describes an objectoriented language called MixJuice, which uses the differences of class hierarchies as a unit of information hiding and reuse. In this language, classes \u2014 templates of objects \u2014 and modules \u2014 units of information hiding and reuse \u2014 are completely orthogonal, thus supporting separation of concerns. MixJuice is basically based on Java. However, its module mechanism is simpler and more powerful than that of Java. The MixJuice programmers can make use of all Java libraries and execution platforms1.
p867
aVIn this extended abstract, we describe an experimental, UML model driven, Java prototyping and development tool.
p868
aVMainstream objectoriented languages, such as C++ and Java, provide only a restricted form of polymorphic methods, namely singlereceiver dispatch. In common programming situations, programmers must workaround this limitation. We detail how to extend the Java Virtual Machine to support multipledispatch and examine the complications that Java imposes on multipledispatch in practice. Our technique avoids changes to the Java programming language itself, maintains sourcecode and library compatibility, and isolates the performance penalty and semantic changes of multipledispatch to the program sections which use it. We have microbenchmark and applicationlevel performance results for a dynamic Most Specific Applicable (MSA) dispatcher, two tablebased dispatchers (Multiple Row Displacement (MRD) and Single Receiver Projections (SRP)), and a tuned SRP dispatcher. Our generalpurpose technique provides smaller dispatch latency than equivalent programmerwritten doubledispatch code.
p869
aVC++ programmers who make intensive use of templates for developing applications get poor support from the compilers they use. Typical error reporting associated with template analysis ranges from long, nested, and therefore complex expressions, to short but cryptic messages. This poster introduces a graphical notation that can be used as an intermediate abstract medium on which the result of template analysis can be expressed. The motivation of our approach is to make template analysis and error reporting two orthogonal activities. If compiler implementors adopt a standard notation to express the result of template analysis, tools to help programmers visualize/understand errors can be independently developed, and used as compiler \u201cplugins\u201d, which will hopefully have a positive impact on issues such as programmer's productivity and product quality. We have used this notation to manually parse examples taken from the STL, and less known powerful uses of templates, such as expression templates, and template metaprogramming. The preliminary stage we are reporting on has helped us to (a) identify elements that are needed in the notation, (b) identify limitations that are due to the instantiation model inherently associated with the language, and (d) have a sense of how useful the notation can be to pinpoint errors associated with the analysis of nontrivial cases such as expression templates.
p870
aVThe research displayed in this poster showcases issues surrounding adding parametric polymorphism to CORBA. The merits of parametric polymorphism are widely published, yet there is no support for the parametric polymorphism paradigm in CORBA. This research should be of special interest to C++ programmers, and other programmers accustomed to generic programming, frustrated by this lack of support.
p871
aVKava is a powerful and portable reflective Java that uses byte code rewriting to implement behavioural reflection. In our poster we briefly overview the Kava system, give an example of its use, and contrast it with simple byte code rewriting. More details about Kava are available from http://www.cs.ncl .ac.uk /research/dependability/reflection.
p872
aVFunctional programming, AI, patterns, OO, structured programming  they were promising, and yet they seem to have failed to deliver. Did we lose interest too soon? Is the best too good for our industry? Is there \u201ca\u201d best for our industry or is our endless search for the silver bullet driving us? Do we want the \u201cbest\u201d to (again) be a popular goal? How?
p873
aVThis practitioner report focuses on a project, named \u201cRemote Access to Clinical Data\u201d (RACD). The report describes the techniques used to clarify the requirements and deliver the project successfully. In addition, it covers the team's experiences in using Sun's JINI to provide a robust distributed infrastructure for RACD. The presentation will cover these areas in equal depth.
p874
aVChecking pre and postconditions of procedures and methods at runtime helps improve software reliability. In the procedural world, pre and postconditions have a straightforward interpretation. If a procedure's precondition doesn't hold, the caller failed to establish the proper context. If a postcondition doesn't hold, the caller failed to establish the proper context. If a postcondition doesn't hold, the procedure failed to compute the expected result. In the objectoriented world, checking pre and postconditions for methods, often called contracts in this context, poses complex problems. Because methods may be overridden, it is not sufficient to check only pre and postconditions. In addition, the contract hierarchy must be checked to ensure that the contracts on overridden methods are properly related to the contracts on overriding methods. Otherwise, a class hierarchy may violate the substitution principle, that is, it may no longer be true that an instance of a class is substitutable for objects of the superclass. In this paper, we study the problem of contract enforcement in an objectoriented world from a foundational perspective. More specifically, we study contracts as refinements of types. Pushing the analogy further, we state and prove a contract soundness theorem that captures the essential properties of contract enforcement. We use the theorem to illustrate how most existing tools suffer from a fundamental flaw and how they can be improved.
p875
aVThe multitasking virtual machine (called from now on simply MVM) is a modification of the Java virtual machine. It enables safe, secure, and scalable multitasking. Safety is achieved by strict isolation of application from one another. Resource control augment security by preventing some denialofservice attacks. Improved scalability results from an aggressive application of the main design principle of MVM: share as much of the runtime as possible among applications and replicate everything else. The system can be described as a 'no compromise'approach   all the known APIs and mechanisms of the Java programming language are available to applications. MVM is implemented as a series of carefully tuned modifications to the Java HotSpot virtual machine, including the dynamic compiler. this paper presents the design of MVM, focusing on several novel and general techniques: an inruntime design of lightweight isolation, an extension of a copying, generational garbage collector to provide besteffort management of a portion of the heap space, and a transparent and automated mechanism for safe execution of userlevel native code. MVM demonstrates that multitasking in a safe language can be accomplished with a high degree of protection, without constraining the language, and and with competitive performance characteristics
p876
aVPreventing abusive resource consumption is indispensable for all kinds of systems that execute untrusted mobile coee, such as mobile object sytems, extensible web servers, and web browsers. To implement the required defense mechanisms, some support for resource control must be available: accounting and limiting the usage of physical resources like CPU and memory, and of logical resources like threads. Java is the predominant implementation language for the kind of systems envisaged here, even though resource control is a missing feature on standard Java platforms. This paper describes the model and implementation mechanisms underlying the new resourceaware version of the JSEAL2 mobile object kernel. Our fundamental objective is to achieve complete portability, and our approach is therefore based on Java bytecode transformations. Whereas resource control may be targeted towards the provision of quality of service or of usagebased billing, the focus of this paper is on security, and more specificlly on prevention of denialofservice attacks orginating from hostile or poorly implemented mobile code.
p877
aVThe need for incremental algorithms for evaluating database queries is well known, but constructing algorithms that work on objectoriented databases (OODBs) has been difficult. The reason is that OODB query languages involve complex data types including composite objects and nested collections. As a result, existing algorithms have limitations in that the kinds of database updates are restricted, the operations found in many query languages are not supported, or the algorithms are too complex to be described precisely. We present an incremental computation algorithm that can handle any kind of database updates, can accept any expressions in complex query languages such as OQL, and can be described precisely. By translating primitive values and records into collections, we can reduce all query expressions comprehension. This makes the problems with incremental computation less complicated and thus allows us to decribe of two parts: one is to maintain the consistency in each comprehension occurrence and the other is to update the value of an entire expression. The algorithm is so flexible that we can use strict updates, lazy updates, and their combinations. By comparing the performance of applications built with our mechanism and that of equivalent hand written update programs, we show that our incremental algorithm can be iplemented efficiently.
p878
aVThe traditional tradeoff when performing dynamic compilation is that of fast compilation time versus fast code performance. Most dynamic compilation systems for Java perform selective compilation and/or optimization at a method granularity. This is the not the optimal granularity level. However, compiling at a submethod granularity is thought to be too complicated to be practical. This paper describes a straightforward technique for performing compilation and optimizations at a finer, submethod granularity. We utilize dynamic profile data to determine intramethod code regions that are rarely or never executed, and compile and optimize the code without those regions. If a branch that was predicted to be rare is actually taken at run time, we fall back to the interpreter or dynamically compile another version of the code. By avoiding compiling and optimizing code that is rarely executed, we are able to decrease compile time significantly, with little to no degradation in performance. Futhermore, ignoring rarelyexecuted code can open up more optimization opportunities on the common paths. We present two optimizations partial dead code elimination and rarepathsensitive pointer and escape analysis that take advantage of rare path information. Using these optimizations, our technique is able to improve performance beyond the compile time improvements
p879
aVThe high performance implementation of Java Virtual Machines (JVM) and justintime (JIT) compilers is directed toward adaptive compilation optimizations on the basis of online runtime profile information. This paper describes the design and implementation of a dynamic optimization framework in a productionlevel Java JIT compiler. Our approach is to employ a mixed mode interpreter and a three level optimizing compiler, supporting quick, full, and special optimization, each of which has a different set of tradeoffs between compilation overhead and execution speed. a lightweight sampling profiler operates continuously during the entire program's exectuion. When necessary, detailed information on runtime behavior is collected by dynmiacally generating instrumentation code which can be installed to and uninstalled from the specified recompilation target code. Value profiling with this instrumentation mechanism allows fully automatic code specialization to be performed on the basis of specific parameter values or global data at the highest optimization level. The experimental results show that our approach offers high performance and a low code expansion ratio in both program startup and steady state measurements in comparison to the compileonly approach, and that the code specialization can also contribute modest performance improvement
p880
aVIn this paper, we address the problem of dynamic optimistic interprocedural analysis. Our goal is to build on past work on static interprocedural analysis and dynamic optimization by combining their advantages. We present a framework for performing dynamic optimistic interprocedural analysis. the framework is designed to be used in the context of dynamic class loading and dynamic compilation, and includes mechanisms for event notification (on class loading and method compilation) and dependence tracking (to enable invalidation of optimistic assumptions). We illustrate the functionality of the framework by using it to be used in the context of dynamic class loading and dynamic compilation, and includes mechanisms for event notification (on class loading and method compilaton) and dependence tracking (to enable invalidation of optimistic assumptions). We illustrate the functionality of the framework by using it to implement a dynamic optimistic interprocedural type (DOIT) analysis algorithm. The DOIT algorithm uses a new global data structure called the Value Graph. The framework and DOIT analysis of the IBM Jalapeo Java virtual machine. Our experimental results for the SPECjvm benchmarks and two larger programs show promising benefits due to dynamic optimistic analysis. Compared to pessimistic analysis, the reduction in number of methods and fields analyzed was in the 2.7x4.6x and 1.62.4x ranges respectively. The average fraction of polymorphic virtual calls decreased from 39.5% to 24.4% due to optimistic analysis, with a bestcase decrease from 47.0% to 8.1%. The average fraction of polymorphic interface calls decreased from 96.4% to 36.2% due to optimistic analysis, with a bestcase decrease from 100.0% to 0.0%. These benefits were obtained with a low dynamic analysis overhead in the range of 570930 bytecode bytes/millisecond (about 2.5x5.4x faster than the Jalapeo baseline compiler)
p881
aVWe present Jiazzi, a system that enables the construction of largescale binary components in Java. Jiazzi components can be thought of as generalizations of Java packages with added support for external linking and separate compilation. Jiazzi components are practical becuase they are constructed out of standard Java source code. Jiazzi requires neither extensions to the Java language nor special conventions for writing Java source code that will go inside a component. Our components are expressive becuase Jiazzi supports cyclic component linking and mixins, which are used together in an open class pattern that enables the modular addition of new features to existing classes. This paper describes Jiazzi, how it enhances Java with components, its implementation, and how type checking works. An implementation of Jiazzi is available for download.
p882
aVMixin modules are proposed as an extension of a classbased programming language. Mixin modules combine parallel extension of classes, including extension of the self types for those classes, with mixinbased inheritance. For soundness of sybtyping purposes, they require an explicit distinction between mixinbased objects and classbased objects. Applications of mixin modules are in statically typesafe monadbased aspectoriented programming, and in modular mixinbased Internet programming.
p883
aVObjectoriented languages provide little support for encapsulating objects. Reference semantics allows objects to escape their defining scope. The pervasive aliasing that ensues remains a major source of software defects. This paper introduces Kacheck/J a tool for inferring object encapulation properties in large Java programs. Our goal is to develop practical tools to assist software engineers, thus we focus on simple and scalable techniques. Kacheck/J is able to infer confinement for Java classes. A class and its sublasses are confined if all of their instances are encapsulated in their defining package. This simple property can be used to identify accidental leaks of sensitive objects. The analysis is scalable and efficient; Kacheck/J is able t infer confinement on a corpus of 46,000 classes (115 MB) in 6 minutes
p884
aVThis paper presents linguistic primitives for publish/subscribe programming using events and objects. We integrate our primitives into a strongly typed objectoriented language through four mechnisms: (1) serialization, (2) multiple subtyping, (3)closures, and (4) deferred code evaluation. We illustrate our primitives through Java, showing how we have overcome its respective lacks. A precompiler transforms statements based on our publish/subscribe primitives into calls to specifically generated typed adapters, which resemble the typed stubs and skeletons by the rmic precompiler for remote method invocations in Java
p885
aVIn this paper we present a simple calculus (called CJE) in ourder to fully investigate the exception mechanism of Java, and in particular its interaction with inheritance, which turns out to be non trivial. Moreover, we show that the type system for the calculus directly dirven by the Java language specification (called FULL) uses too many types, in the sense that there are different types which rpovide exactly the same information. Hence, we obtain from FULL a simplified type system called MIN where equivalent types have been identified. We show that is useful both for typechecking optimization and for clarifying the static semantics of the language. The two type systems are proved to satisfy the subject reduction property
p886
aVThe Visitor design pattern allows the encapsulation of polymorphic behavior outside the class hierarchy on which it operates. A common application of Visitor is the encapsulation of tree traversals. A clean separation can be made between the generic parts of the combinator set and the parts that are specific to a particular class hierarchy. The generic parts form a reusable framework. The generic parts form a reusable framework. The specific parts can be generated from a (tree) grammar. Due to this separation, programming with visitor combinators becomes a form of generic programming with significant reuse of (visitor) code.
p887
aVObjectoriented languages come with predefined composition mechansims, such as inheritance, object composition, or delegation, each characterized by a certain set of composition properties, which do not themselves individually exist as abstractions at the language level. However, often nonstandard composition semantics is needed, with a mixture of composition mechanisms. Such nonstandard semantics are simulated by complicated architectures that are sensitive to requirement changes and cannot easily be adapted without invalidating existing clients. In this paper, we propose compound references, a new abstraction for object references, that allows us to provide explicit linguistic means for expressing and combining individual composition properties ondemand. The model is statically typed and allows the programmer to express a seamless spectrum of composition semantics in the interval between object composition and inheritance. The resulting programs are better understandable, due to explicity expressed design decisions, and less sensitive to requirement changes.
p888
aVThe reengineering and reverse engineering of software systems is gaining importance in software industry, because the accelerated turnover in software industry, because the accelerated turnover in software companies creates legacy systems in a shorter period of time. Especially understanding classes is a key activity in objectoriented programming, since classes represent the primary abstractions from which applications are built. The main problem of this task is to quickly grasp the purpose of a class and its inner structure. To help the reverse engineers in their first contact with a foreign system, we propose a categorization of classes based on the visualization of their internal structure. The contributions of this paper are a novel categorization of classes and a visualization of the which we call the class blueprint. We have validated the categorization on several case studies, two of which we present here.
p889
aVRegression testing is applied to modified software to provide confidence that the changed parts behave as intended and that the unchanged parts have not been adversely affected by the modifications. To reduce the cost of regression testing, test cases are selected from the test suite that was used to test the original version of the software this process is called regression test selection. A safe regressiontestselection algorithm selects every test case in the test suite that may reveal a fault in the modified software. Safe regressiontestselection technique that, based on the use of a suitable representation, handles the features of the Java language. Unlike other safe regression test selection techniques, the presented technique also handles incomplete programs. The technique can thus be safely applied in the (very common) case of Java software that uses external libraries of components; the analysis of the external code is note required for the technique to select test cases for such software. The paper also describes RETEST, a regressiontestselection algorithm can be effective in reducing the size of the test suite.
p890
aVCurrent software development tools let developers model a software system and generate program code from the models to run the system. However, generating code and installing a nontrivial system induces a time delay between changing the model and executing it that makes rapid model prototyping awkard if not impossible. This paper presents the architecture of a virtual machine for UML that interprets UML models without any intermediate codegeneration step. The paper shows how to embed UML in a metalevel architecture so that a key property of modelbased systems, the casual connection between models and model instances, is guaranteed. With this architecture, changes to a model have immediate effects on its execution, providing users with rapid feedback about the model's structure and behavior. This approach supports model innovation better than today's codegeneration approaches.
p891
aVPretenuring can reduce copying costs in garbage collectors by allocating longlived objects into regions that the garbage collector with rarely, if ever, collect. We extend previous work on pretenuring as follows. (1) We produce pretenuring advice that is neutral with respect to the garbage collector algorithm and configuration. We thus can and do combine advice from different applications. We find that predictions using object lifetimes at each allocation site in Java prgroams are accurate, which simplifies the pretenuring implementation. (2) We gather and apply advice to applications and the Jalapeo JVM, a compiler and runtime system for Java written in Java. Our results demonstrate that building combined advice into Jalapeo from different application executions improves performance regardless of the application Jalapeo is compiling and executing. This buildtime advice thus gives user applications some benefits of pretenuring without any application profiling. No previous work pretenures in the runtime system. (3) We find that applicationonly advice also improves performance, but that the combination of buildtime and applicationspecific advice is almost always noticeably better. (4) Our same advice improves the performance of generational and Older First colleciton, illustrating that it is collector neutral.
p892
aVIn systems that support garbage collection, a tension exists between collecting garbage too frequently and not collecting garbage frequently enough. Garbage collection that occurs too frequently may introduce unnecessary overheads at the rist of not collecting much garbage during each cycle. On the other hand, collecting garbage too infrequently can result in applications that execute with a large amount of virtual memory (i.e., with a large footprint) and suffer from increased execution times die to paging. In this paper, we use a large colleciton of Java applications and the highly tuned and widely used BoehmDemersWeiser (BDW) conservative markandsweep garbage collector to experimentally examine the extent to which the frequency of garbage collectio impacts an application's execution time, footprint, and pause times. We use these results to devise some guidelines for controlling garbage and heap growth in a conservative garbage collection in order to minimize application execution times. Then we describe new strategies for controlling in order to minimize application execution times.
p893
aVThe ability to extend a language with new syntactic forms is a powerful tool. A sufficiently flexible macro system allows programmers to build from a common base towards a language designed specifically for their problem domain. However, macro facilities that are integrated, capable, and at the same time simple enough to be widely used have been limited to the Lisp family of languages to date. In this paper we introduce a macro facility, called the Java Syntactic Extender (JSE), with the superior power and ease of use of Lisp macro sytems, but for Java, a language with a more conventional algebraic syntax. The design is based on the Dylan macro system, but exploits Java's compilation model to offer a full procedural macro engine. In other words, syntax expanders may be implemented in, and so use all the facilities of, the full Java language
p894
aVThe goal of pointto analysis for Java is to determine the set of objects pointed by a reference variable or a reference object field. This information has a wide variety of client applications in optimizing compilers and software engineering tools. In this paper we present a pointto analysis for Java based on Andersen's pointto analysis for C [5]. We implement the analysis by using a constraintbased approach which employs annotated inclusion constraints. Constraint annotations allow us model precisely and efficiently the semantics of virtual calls and the flow of values through object fields. By solving systems of annotated inclusion constraints, we have been albe to perform practical and precies pointsto analysis for Java
p895
aVThis paper presents a new static type system for multithreaded programs; any welltyped program in our system is free of data races. Our type system is significantly more expressive than previous such type systems. In particular, our system lets programmers write generic code to implement a class, then create different objects of the same class that have different objects of the same class that have different protection mechanisms. This flexibility enables programmers to reduce the number of unnecessary synchronizationoperations in a program without risking data races. We also support default types which reduce the burden of writing extra type annotations. Our experience indicates that our system provides a promising approach to make multithreaded programs more reliable and efficient
p896
aVWe present an onthefly mechanism that detects access conflicts in executions of multithreaded Java programs. Access conflicts are a conservative approximation of data races. The checker tracks access information at the level of objects (object races) rather than at the level of individual variables. This viewpoint allows the checker to exploit specific properties of objectoriented programs for optimization by restricting dynamic checks to those objects that are identified by escape analysis as potentially shared. The checker has been implemented in collaboration with an "aheadoftime"Java compiler. The combination fo static program analysis (escapeanalysis) and inline instrumentation during code generation allows us to reduce the runtime overhead of detecting access conflicts. This overhead amounts to about 16129% in time and less than 25% in space for typical benchmark applications and compares favorably to previously published onthefly mechanism that incurred an overhead of about a factor of 280 in time and up to a factor of 2 in space.
p897
aVOptimizing exception handling is critical for programs that frequently throw exceptions. We observed that there are many such exceptionintensive programs iin various categories of Java programs. There are two commonly used exception handling techniques, stack unwinding optimizes the normal path, while stack cutting optimizes the exception handling path. However, there has been no single exception handling technique to optimize both paths.
p898
aVSubtyping test, i.e., determining whether one type is a subtype of another, are a frequent operation during the execution of objectoriented programs. The challenge is in encoding the hierarchy in a small space, while simulataneously making sure that subtyping tests have efficient implmentation. We present a new scheme for encoding multiple and single inheritance hierarchies, which, in the standardized hierarchies, reduces the footprint of all previsously published schemes. The scheme is called PQencoding after PQtrees, a data structure previously used in graph theory for finding the orderings that satisfy a collection of constraints. In particular, we show that in the traditional object layout model, the extra memory requirement for single inheritance hierarchies is zero. In the PQencoding subtyping tests are constant time, and use only two comparisons. Other than PQtrees, PQencoding uses several novel optimization techniques. These techniques are applicable also in improving the performance of otehr, previously published, encoding schemes.
p899
aVSingle superclass inheritance enables simple and efficient tabledriven virtual method dispathc. However, virtual method table dispatch does not handle multiple inheritance and interfaces. This complication has led to a widespread misimpression that interface method dispatch is inherently inefficient. This paper argues that with proper implementation techniques, Java interfaces need not be a source of significant performance degradation.
p900
aVProgramming languages provide a limited range of mechanisms to represent concepts. This means that the final program lacks important information that the programmer has about the domain. We propose the use of programmerextensible program annotations as a means to represent that information about the domain. Using these program annotations we can specify join points by means of semantic properties of the programs, thereby improving the reusability and robustness of aspects.
p901
aVCLAM (C++ Library for Audio and Music) is a framework for audio and music programming. It may be used for developing any type of audio or music application as well as for doing more complex research related with the field. In this paper we introduce the practicalities of CLAM's first release as well as some of the sample application that have been developed within the framework. See [1] for a more conceptual approach to the description of the CLAM framework.
p902
aVGrammaroriented Object design (GOOD) externalizes the manners of enterprisescale components using a business domainspecific language. This implementation of manners models the flow and constraints on a set of collaborating enterprise components.A Business Compiler (BC) executes manners of components specified in the business domainspecific language; supporting a highly reconfigurable architectural style. The BC is a software asset that can be used to define, parse and execute business flow languages. This provides various advantages including the ability to animate and execute the collaboration of components reflecting the business process steps defined by business modeler. Architects enhance the grammar with component services that serve as actions in the grammar. The combination of flow definition by modelers and component services by software architects provides a powerful collaborative environment for enabling the incremental creation of a highly reconfigurable architectural style.This tool (Business Compiler) is the result of harvesting a set of software assets from multiple projects into an API and a GUI front end that helps modelers by providing dynamic documentation and animation of and can be used to drive business flow for highly adaptive and reconfigurable software architectures.
p903
aVNebras Classifier is a distributed modelview component developed to perform one type of information handling namely classification using both COM and CORBA communication facilities. In this component, model has a hierarchical structure capable of storing multiple instances of domain CORBA objects in its nodes and has facilities to perform special queries on this structure. The view is a graphical representation for this model. A third party named Semantic Engine that is domain dependent, may be used in interaction with these two parts using proxy design pattern to perform required feasibility checks or domain dependent actions on the classifier.
p904
aVUsing platform independent models (PIMs) in integrating enterprise systems is much more concerned nowadays. In this demonstration we are going to present an approach to build and integrate enterprise systems which has many commonalities with OMG MDA [1] concepts. There are two major integrity points of view in this approach. First, integrity from business process point of view and the second, integrity from object model point of view. We are going to present how these two are achieved in Nebras Enterprise Framework.
p905
aVNaked Objects is an opensource Javabased framework designed specifically to encourage the creation of business systems from behaviourallycomplete business objects. In fact, with the Naked Objects framework you have no alternative but to make your business objects behaviourallycomplete. The reason is that the framework exposes core business objects, such as Customer, Product and Order, directly to the user. All user actions consist of invoking methods directly upon those business objects, or sometimes upon the object's class. There are no scripts, no controllers, nor even any dialog boxes in between the user and the 'naked' objects.
p906
aVOur goal is to enable rapid production of static and dynamic object models from natural language description of problems. Rapid modeling is achieved through automation of analysis tasks. This automation captures the cognitive schemes analysts use to build their models of the world through the use of a precise methodology. The methodology is based on the use of proposed technique called role posets, and a seminatural language (called 4W). Original problem statements are automatically translated to 4W language. The produced sentences then, are analyzed with role posets to produce static model views.. Finally the 4W sentences are used to generate dynamic views of the problem. This set of methods maximizes analysis process agility, promotes reusability and constitutes a valuable tool in the learning process of object thinking. The prototype tool: "GOOAL" (Graphic Object Oriented Analysis Laboratory) receives a natural language (NL) description of a problem and produces the object models taking decisions sentence by sentence. The user realizes the consequences of the analysis of every sentence in real time. Unique features of this tool are the underlying methodology and the production of dynamic object models.
p907
aVThis paper describes CATfood (Code Authoring Tool), an application created to automatically test example code that is embedded in word processing files.
p908
aVIn this paper I examine the issue of combining system structure representations and application domain semantic representations to facilitate unanticipated system adaptation.
p909
aVThis paper describes an objectoriented system that is designed to provide access to databases and highperformance computing systems to process images and other objects found in the databases. The system features a Webbased frontend and CORBA middleware to provide convenient access and management of services. The Kerberos authentication system is used for security to avoid IP addressbased approaches that restrict user mobility.
p910
aVIn this demonstration, we are going to present a workflow management system that is built up with distributed objects based on CORBA. In addition to its architecture, we will show how a workflow manager can act as a higherlevel infrastructure over CORBA and how someone can reduce the complexity of a system by rending its use cases and then integrating them on the activities of the flow in a business process by means of such a workflow manager.
p911
aVCLAM (C++ Library for Audio and Music) is a framework that aims to offer extensible, generic and efficient design and implementation solutions for developing Audio and Music applications as well as for doing more complex research related with the field. Although similar libraries exist, some particularities make CLAM of high interest for anyone interested in the field.
p912
aVPolymorphous Computing Architectures (PCA) represent a significant leap in the capabilities of computing systems, but also a dramatic increase in software complexity and fragility. We present our synthesis of technologies generated to date in the DARPAfunded Morphware Forum, to develop a stable and scalable software development platform for PCA devices.
p913
aVUniFrame is a framework for seamlessly assembling heterogeneous distributed components. It is based on the Unified Metacomponent Model (UMM). UniFrame uses twolevel grammar (TLG) for formally specifying components and translation into other component representations for component service export and assembly.
p914
aVThis is an analysis of the mental life a team of software engineers can have as a cognitive entity in itself, distinguishable from the engineers who participate in the team. Literature in cognitive science, philosophy, and software development is cited to support the analysis and provide examples.
p915
aVA Webbased exercise package for engineering is developed for teaching engineering students. It is based on PHP and can be accessible from the Web by both students and administrator. It is very userfriendly and can be updated easily. Various types of exercises are available and found very useful for teaching.
p916
aVAn advanced distance learning method is introduced that is especially programmed for nonnative English speakers. Many language features have been implemented to arouse undergraduate students' interests in learning. The package is also programmed to help professors to teach much easier than before.
p917
aVWhile traditional, onedimensional approaches to the problem of separation of concerns have been adequate for current software development, they are often brittle and resistant to evolutionary change. Aspects and aspectorientation offer a controllable, modular mechanism for describing the separation of concerns that are orthogonal to the object model that is the primary developmental focus of a wide range of software applications. This dissertation research project involves the creation of an aspectoriented infrastructure to support a variety of software development tools. Use of this infrastructure is demonstrated in domain areas such as ecological modeling software and web development in order to establish aspectorientation as a feasible and straightforward solution to the problem of separation of concerns in objectoriented software systems. In the process of establishing the viability of the aspectoriented solution, this dissertation investigates several new directions in aspectorientation: aspects in system software, language independent aspects, aspect integration techniques, and opportunities for aspect reuse. In comparing the twodimensional, aspectoriented approach to the traditional, onedimensional approach, the assertion of this research is that a twodimensional approach offers an inherently more flexible software system while maintaining the advantages of modularity and code reuse that have long been ascribed to objectoriented systems.
p918
aVIn order to teach objects and classes effectively with an "objectfirst" approach, some basic knowledge of UML is required. In this paper, we describe a beginners UML tool that was created to assist in teaching introductory level, objectoriented programming courses  without overwhelming students with the complete UML syntax.
p919
aVStottler Henke Associates Inc.'s ObjectOriented Design Learning Environment (OODLE) is an early prototype intelligent tutoring system aimed at teaching objectoriented design skills by coached practice with realistically complex design problems. OODLE's pedagogical approach is based on the instructional practices of Craig Larman, author of Applying UML and Patterns.
p920
aVManytoMany Invocation (M2MI) is a new paradigm for building collaborative systems that run in wireless proximal ad hoc networks M2MI provides an object oriented method call abstraction based on broadcasting. An M2MI invocation means "Every object out there that implements this interface, call this method." An M2MIbased application is built by defining one or more interfaces, creating objects that implement those interfaces in all the participating devices, and broadcasting method invocations to all the objects on all the devices.
p921
aVThis poster describes the code developed for simulations of disordered electronphonon systems. This code uses the C++ Standard Template Library (STL) and is designed to be easily changed when the physicists want to change aspects of the simulation.
p922
aVThe goal of this workshop is to foster precise and explicit OO specifications of business and system semantics, independently of any (possible) realization. As a result of lip service to these ideas, the systems we build or buy are all too often not what they are supposed to be. Doing better than that requires both a clear understanding of the semantics of the problems together with their business and technological environments, and abstract, precise and explicit specifications of that semantics. The specific theme this year is on serving the customer.
p923
aVCan a Software Engineering Environment (SEE) be built from Open Software? We introduce the requirements for a SEE in a largescale, global, distributed objectoriented software development project and suggest how open software solutions can fulfill these requirements. Our suggestions are being applied in the domain of network development within Nokia.
p924
aVA Universal DataBase Connectivity (UDBC) component has been developed with componentbased approach. This paper presents our design goal, software architecture, and implementation issues for UDBC. Strength, limitations, and possible extension of JavaBean component architecture and its design environment are discussed.
p925
aVIf software is so easy to create, why is it so difficult to change existing software to keep up to date with changing requirements? Discovery costs   the costs of learning what one needs to know in order to evolve an existing software system   has proven to be the dominant (but often unrecognized) cost in many software systems.This subject was discussed in a previous OOPSLA panel session: "Do Patterns and Frameworks Reduce Discovery Costs?" at OOPSLA '97. This workshop will take a fresh look  five years later   at how organizations are coping with discovery costs.
p926
aVIntroductory computer science courses often focus on language specifics as opposed to general concepts applicable in multiple languages. Often design issues are raised and discussed during the last week of a semester long course, or emphasized in tidbits interwoven with discussions of implementation issues. In many cases, students are exposed to design concepts before they are ready to learn design.We propose a measure of assessing "design readiness" an assessment of the cognitive state where one is able to understand design abstractly. We will then use programming and design patterns to assist in teaching critical design concepts. This research is an attempt to address the question, can we improve a student's chance of success in learning design concepts by adjusting instruction to his/her level of design readiness? In an attempt to answer this question, we will concentrate on the following challenges (1) are programming and design patterns an effective approach to teaching objectoriented design (OOD)? If so, will an expertdefined ordering of exposure to patterns enhance learning impacts? (2) Are there characteristics of a student's background or cognitive state that makes him/her "ready" to learn design concepts and skills of OOD? If so, can these measures be used to adjust a student's design instruction to increase its effectiveness?
p927
aVA powerful methodology for specifying scenariobased requirements of reactive systems is described, in which behavioral requirements are "played in" directly from the system's GUI or some abstract version thereof, and full behavior can then be "played out" freely, just as if a conventional system model were present. The approach is supported and illustrated by a tool we have built, which we call the playengine. The ideas appear to be relevant to many stages of system development, including requirements engineering, specification, testing, and implementation.
p928
aVOur goal is to enable rapid production of static and dynamic object models from natural language description of problems. Rapid modeling is achieved through automation of analysis tasks. This automation captures the cognitive schemes analysts use to build their models of the world through the use of a precise methodology. The methodology is based on the use of proposed technique called role posets, and a seminatural language (called 4W). Original problem statements are automatically translated to 4W language. The produced sentences then, are analyzed with role posets to produce static model views. Finally the 4W sentences are used to generate dynamic views of the problem. This set of methods maximizes analysis process agility, promotes reusability and constitutes a valuable tool in the learning process of object thinking.
p929
aVAs the term convergence is heavily used in current publications, we suggest to define it as multidisciplinary, inhomogeneous integration with the aim to reach an added value. We use the convergence between telecommunication and information technology as an example to show different classes of convergence on different levels of abstraction.
p930
aVThe next generation of distributed systems will require individual components to adapt to their environment. Increasingly, developers want architectures that allow the internal structures of the software that comprise their systems to change autonomously with changes in the deployment environment. This has given rise to research in reflective technologies like middleware, languages, etc. However, these technologies only get us part of the way to systems that can adapt.For a system to be truly adaptive, it must also change in support of evolving technologies and ontologies. When engineering to support these requirements, some of goals of reflective technologies can also be met without changes to middleware and languages. The Enterprise Intelligent Distributed Architecture (EIDA) is an attempt to realize these goals by creating a set of development tools that are also deployed with the system as run time servers. By completely automating the development of key components of the runtime system, the goal is to enable the adaptation of these components to changes in the environment, technology and ontology of the system.
p931
aVCurrent modeling languages are based on the concepts taken from programming languages, leading to a poor mapping to an organizations' own domains and duplication of effort in problem solving, design and coding. Domainspecific languages allow faster development, based on models of the product rather than on models of the code. A domainspecific modeling language applies concepts and rules found in the domain. Together with generators and components it can automate a large portion of software production. Industrial applications of this approach show remarkable improvements in productivity: up to ten times faster. This poster describes a framework for implementing domainspecific visual modeling languages and summarizes industrial experiences from the use of domainspecific languages. The results of an OOPSLA workshop in this area will also be reported in this poster.
p932
aVA MOO was modified to permit the writing, compilation and execution of simple Java programs. In addition to serving as an online environment for teaching objectoriented concepts, the MOO can now be used to teach the Java programming language.
p933
aVWe need various higher level services for using PeertoPeer(P2P) network effectively and for building useful applications. We introduce a new distributed name service which enables bottomup combination of local name spaces by complete decentralized control. Our name service provides much benefit for P2P networking and groupware applications.
p934
aVSoftware systems for the analysis of imagebased biomedical data, such as functional magnetic resonance imaging (fMRI), require a flexible data model, fast computational techniques and a graphical user interface. Objectotiented programming languages, such as Java and C++ facilitate software reuse and maintainability, and provide the foundations for the development of complex data analysis applications. This paper explores the advantages and disadvantages of using these two programming environments for scientific computation.
p935
aVCan libraries written without explicit support for Design by Contract\u2122 in the language or the method benefit from a posteriori addition of contracts? To help answer this question, we performed an empirical study of library classes from the .NET Collections library, which doesn't use Design by Contract, to search for unexpressed contracts. This poster reports on what we have found, and discusses whether the results could be used to improve the design of the classes and make them easier to learn and use.
p936
aVAfter many years of waiting, real support for multithreading has been integrated into mainstream programming languages. Inclusion of this longawaited feature brings with it a need for a clear and direct explanation of how threads interact through memory. Java's threading specification, its memory model, is fundamentally flawed [1, 17]. Some language features, like volatile fields, are underspecified: their treatment is so weak as to be useless. Other features, including fields without access modifiers, are overspecified: the memory model prevents almost all optimizations of code containing these "normal" fields. Finally, some features, like final fields, have no specification at all.This work has attempted to remedy these limitations; we provide a clear and concise definition of thread interaction. It is sufficiently simple for programmers to work with, and flexible enough to take advantage of compiler and processorlevel optimizations. We provide techniques for verifying that the model does what we expect it to do, and apply them. These techniques take the form of both rigorous proof and automated simulation. This work is critical for portable, safe, secure and efficient support of multithreading.
p937
aVWe present a cookbook for the development of J2EEWeb applications. This cookbook aims at the minimization of quality costs. It consists of procedures and guidelines in combination with a toolset. The classical software lifecycle presents test as a single phase in the development process. The cookbook presents testing as the motor of every other activity in development.
p938
aVAs researchers share computational resources, such as objects in large distributed environments, it becomes difficult to achieve scalability of synchronization. Concurrency protocols currently lack scalability. Our protocol enhances middleware concurrency services to provide scalability of synchronization enabling resource sharing and computing with distributed objects in systems with a large number of nodes.
p939
aVSoftware analysis patterns are believed to play a major role in reducing the cost and condensing the time of software product lifecycles. However, there are several deficiencies with today's analysis patterns. These deficiencies make it difficult to use analysis patterns as efficient reusable artifacts. This poster discusses the status of today's analysis patterns. It also introduces the novel concept of Stable Analysis Patterns. In addition, it introduces eight essential properties to evaluate analysis pattern reusability.
p940
aVIntroductory computer science courses often focus on language specifics as opposed to general concepts applicable in multiple languages. Often design issues are raised and discussed during the last week of a semester long course, or emphasized in tidbits interwoven with discussions of implementation issues. In many cases, students are exposed to design concepts before they are ready to learn design.We propose a measure of assessing "design readiness" an assessment of the cognitive state where one is able to understand design abstractly. We will then use programming and design patterns to assist in teaching critical design concepts. This research is an attempt to address the question, can we improve a student's chance of success in learning design concepts by adjusting instruction to his/her level of design readiness? In an attempt to answer this question, we will concentrate on the following challenges (1) are programming and design patterns an effective approach to teaching objectoriented design (OOD)? If so, will an expertdefined ordering of exposure to patterns enhance learning impacts? (2) Are there characteristics of a student's background or cognitive state that makes him/her "ready" to learn design concepts and skills of OOD? If so, can these measures be used to adjust a student's design instruction to increase its effectiveness?
p941
aVWe propose a framework that can be used during as well as after development, to identify performance problems, suggest corrections and predict performance in largescale componentbased distributed enterprise systems.
p942
aVThis poster presents a tool for the analysis of Java heap snapshots. The tool supports a flexible query language to measure various aspects of the object graph related to the studies of uniqueness, ownership, encapsulation and confinement. One of the applications of our tool was the verification of the power law dependency in the distribution of incoming and outgoing references to objects.
p943
aVThe use of computers in schools to promote active learning by students is not common. This research studies educational software simulations and utilizes the results to create a visual programming environment that will support the creation of simulations by teachers who are novice programmers. The expectation is that this will increase the accessibility of programming systems and give teachers a tool that will empower them to create and modify their own software.
p944
aVThis paper outlines the major problems that need to be addressed when providing support for selfadaptive systems. A reflexive architecture that provides facilities for managing system adaptation is then described.
p945
aVWe propose a framework that can be used during as well as after development, to identify performance problems, suggest corrections and predict performance in largescale componentbased distributed enterprise systems.
p946
aVComponent methods often produce their final parameter values long before the method body is ready to terminate. To minimize client blocking, EarlyReply can be used to forward invocation results to the caller as soon as they are (safely) available. After executing EarlyReply, the method remainder and the client caller can proceed concurrently, modulo synchronization constraints. The prime motivation for EarlyReply, then, is to improve performance factors such as response time and resource utilization.EarlyReply received previous attention as a construct for explicit concurrent programming. It's value for sequential programming, however, has not been widely recognized. The present research supplies a formal treatment of EarlyReply as a basis for concurrent execution of sequential programs. In particular, we reformulate EarlyReply under local proof obligations that encapsulate concurrency as a (temporal) unit of information hiding. The upshot is that software developers can use EarlyReply to exploit the performance benefits of concurrent execution, without compromising the reasoning benefits of sequential programming.
p947
aVIn this paper, we offer an alternative vision for domain driven development (3D). Our approach is model driven and emphasizes the use of generic and specific domain oriented programming (DOP) languages. DOP uses strong specific languages, which directly incorporate domain abstractions, to allow knowledgeable end users to succinctly express their needs in the form of an application computation. Most domain driven development (3D) approaches and techniques are targeted at professional software engineers and computer scientists. We argue that DOP offers a promising alternative. Specifically we are focused on empowering application developers who have extensive domain knowledge as well as sound foundations in their professions, but may not be formally trained in computer science.We provide a brief survey of DOP experiences, which show that many of the best practices such as patterns, refactoring, and pair programming are naturally and ideally practiced in a Model Driven Development (MDD) setting. We compare and contrast our DOP with other popular approaches, most of which are deeply rooted in the OO community.Finally we highlight challenges and opportunities in the design and implementation of such languages.
p948
aVJPie is a tightly integrated development environment supporting live objectoriented software construction in Java. JPie embodies the notion of a dynamic class whose signature and implementation can be modified at run time, with changes taking effect immediately upon existing instances of the class. The result is complete elimination of the editcompiletest cycle. JPie users create and modify class definitions through direct manipulation of visual representations of program abstractions. This support is provided without modification of the language or runtime system. In this demonstration, we illustrate central features of JPie through the construction of a sample application. These include dynamic declaration of instance variables and methods, dynamic modification of method bodies and threads, dynamic user interface construction and event handling, and onthefly exception handling in JPie's integrated threadoriented debugger.
p949
aVWe propose a selfoptimizing application server design for EJB component technology. Optimizations are driven by the discovery of intercomponent communication patterns and the application of corresponding container refactorings. Our solution attempts to address the impact the application server has on system performance.
p950
aVThe processors normally used for low cost or embedded applications are not well suited for running Smalltalk, so we created our own using programmable circuits (FPGAs). By creating the software and hardware specifically to work with each other it was possible to simplify both to such a degree that the resulting system is competitive in terms of price/performance compared to solutions with traditional processors, despite the inefficiency of FPGAs relative to custom designs.Both a 16 bit and a 32 bit hardware implementation of Neo Smalltalk were created and illustrate the cost and performance tradeoffs possible in this kind of development. The hardware is defined in terms of objects exchanging messages down to the lowest level, which is an interesting contrast to the traditional bytecoded virtual machines used for Smalltalk, Java and similar languages.
p951
aVThis document describes how the Norwegian education portal (www.utdanning.no) uses XML and the Learning Object Metadata standard (LOM) to integrate existing providers of learning objects into one common search portal. LOM is an IEEE standard (IEEE 1484.12.1) for categorizing and describing learning objects [1].
p952
aVWe demonstrate QuickUML, a tool which supports iterative design and code development by providing facilities to draw UML class diagrams, to generate Java code from such diagrams, and also to automatically generate a UML class diagram from a collection of Java source code files. We also discuss how use of the tool provides general support for teaching students the importance of design in software development.
p953
aVFScript is a Smalltalklike interactive scripting language based on the Mac OS X object model. FScript provides scripting and interactive access to Mac OS X frameworks and custom objects. It also introduces an innovative highlevel objectoriented programming model based on APLlike array programming principles.
p954
aVThe development of modern loosely coupled distributed applications requires extensive use of asynchronous processes. The ability to manipulate execution context could simplify development of such applications, helping to separate business logic from handling asynchrony.This paper describes a framework that implements Execution Context Reification for Java Virtual Machine (JVM). The framework uses builtin secondary bytecode interpreter that provides access to Execution Context as a first class serializable object. Asynchronous Transfer of Control Threading (ATCT) mechanism is used to manage the execution process using wellknown thread semantics. The framework allows the process to be suspended for unlimited amount of time without locking system threads. Next, the process can be instructed to resume execution from the point where it was stopped. Described approach will allow to simplify development of asynchronous processes by enabling use of sequential programming style.
p955
aVThe Generative Model Transformer (GMT) project is an Open Source initiative to build a Model Driven Architecure\u2122 tool that allows fully customisable Platform Independent Models, Platform Description Models, Texture Mappings, and Refinement Transformations. The project should result in (a) a tool that fulfils the MDA promise for faster/more accurate/better maintainable application development, (b) a tool for industrial use, and (c) MDA related researchwhich is encouraged and needed. A distinctive feature of GMT is the emphasis of model transformations as "firstclass model citizens". The implementation of model transformations is envisaged to be in conformance with the future OMG modeling standard for Queries, Views, and Transformations (QVT).
p956
aVRecently, the paradigm of software engineering has shifted significantly to service orientation based on Web services. Web Services Description Language interface specifications provide sufficient information to physically access a service. However, these interface descriptions are semantically bleak. This work introduces a number of tools, which were developed to augment strict syntactic service descriptions with semantic information in order to elucidate the meaning of processed data and provided functionality. Semantic Web technologies such as DAML+OIL were supplemented with natural language support for usability improvements both at design and at runtime.
p957
aVMetaEdit+ is an environment that allows building modeling tools and generators fitting to application domains, without having to write a single line of code. The capability to define modeling tools and generators is relevant as it provides the ability to raise the abstraction of design work from code to domain concepts, and a raise in abstraction leads to an imminent raise in productivity, as illustrated by the past years' experiences.In domainspecific modeling and MetaEdit+, one expert defines a domainspecific language as a metamodel containing the domain concepts and rules, and specifies the mapping from that to code in a domainspecific code generator. For the method implementation, MetaEdit+ provides a metamodeling language and tool suite for defining the method concepts, their properties, associated rules, symbols, checking reports, and generators. Once the expert defines a modeling method, or even a partial prototype, the rest of the team can start to use it in MetaEdit+ to make models with the modeling language and the required code is automatically generated from those models. Based on the metamodel, MetaEdit+ automatically provides CASE tool functionality: diagramming editors, browsers, generators, multiuser/project/platform support, etc.The MetaEdit+ demo will focus on showing how the domainspecific languages and generators are made; complete with several examples of domainspecific methods and related code generators.
p958
aVThis paper presents a comprehensive, domaindriven framework for software development. It consists of a metaprogrammable domainspecific modeling environment and a model transformation generator toolset based on graph transformations. The framework allows the creation of custom, domainoriented programming environments that support enduser programmability. In addition, the framework could be considered an early, endtoend implementation of the concepts advocated by the OMG's Model Driven Architecture initiative.
p959
aVWith the rapid increase in complexity of software systems and applications, it becomes necessary to develop tools to simplify the incorporation of selfmanaging features into applications. The use of object and component technologies, together with a policy system which externalizes business logic from an application, plays an important role in enabling systems with greater manageability and variability. In this presentation, we will show an approach to build policy based applications with much greater flexibility, expressiveness and reusability . The key concept in this approach is the separation of business logic expressed as policy or rules into its various subcomponents and these subcomponents are dynamically configurable.
p960
aVAn "Omniscient Debugger" works by recording all state changes in the run of a program, and then allowing the programmer to explore the history of that program  effectively going "backwards in time." Event analysis debuggers work by observing events as they occur, and allowing the programmer to write queries which will pause the program when matched  effectively highly sophisticated breakpoints.Recently we have integrated the two techniques to produce an omniscient debugger which can use event queries to search the history of a program interactively. The query mechanism is designed along the lines of an EMACS incremental search, where the query is typed into a "minibuffer" at the bottom of the debugger window, and the commands "next match" and "previous match" are single keystrokes. The result is instantaneous feedback with no danger of missing an interesting state by going too far.
p961
aVThis paper describes our experience in redeveloping an objectoriented system. Experience working with the system along with new requirements that were introduced as the system matured motivated a significant redesign and reimplementation effort. The initial objectoriented design helped to allow extension as well as restructuring of system objects. Experience in integrating new objects into the system and reimplementing objects in different languages drove a refactoring process. The Web interface to the system was reimplemented as a Grid portal.
p962
aVThis is the overview of the original System Prevalence layer demonstration at OOPSLA'2003.
p963
aVThe Requirements Use case Tool (RUT) provides assistance to managers, customers, and developers in assessing the quality of use cases. In addition, RUT serves as a database repository for requirements developed as use cases. To ensure consistency, the tool provides a standard use case template to be used for all use case entry into the repository. Furthermore, RUT provides integration with Rational Rose, the industrystandard tool for developing UML diagrams. The tool also provides a series of metrics useful for calculating information about the relationships among the captured use cases. RUT performs use case evaluation by searching text and identifying risk indicators such as incomplete or weak phrases. The Requirements Use case Tool is a valuable resource for collecting, evaluating, and maintaining software requirements gathered as use cases.RUT is a webbased, multiuser application that provides project team members with the ability to create, view, and modify use cases and related information for a particular project. The "dashboard" view provided by the tool gives managers and others the ability to quickly ascertain the status of a project by viewing various use case metrics. The tool was developed using multiplatform, open source technologies (PHP and MySQL).All features of the Requirements Use case Tool described above will be demonstrated at the conference.
p964
aVAs the demand for web applications grows, so does the demand for tools that support them. As a general rule, such tools extend general purpose programming languages, like Servlets/Jsp [2] does for Java [4], or define their own programming language, like PHP [3]. But there is no established engine for web applications written with C++. This work presents technical challenges that were faced when developing T++, an engine that supports web application development with C++.
p965
aVCustom graphics processors or GPUs have been available for a few years now. Currently these graphics processors are slowing evolving to generalized stream processors. These custom vector processors [1] have special support for vector and matrix data types supported through "packed arrays". Recently, Microsoft and 3D Labs (Open GL ARB) have independently developed virtual execution environments to abstract these underlying graphics processors [2]. Additionally new languages including Cg [3], HLSL [4], and GLSlang [5] have been developed to target these graphics virtual machines. This paper explores the technical issues involved with building compilers that target DirectX 9.0 compatible programmable graphics processors.
p966
aVIn theory, requirements engineering solves many of software engineering's fundamental problems. The stakeholders know what the developers are building, why they are building it, when they are building it, and even to some degree, how they are building it. If requirements engineering resolves some of the basic communication issues between IT and the business, why aren't more companies actively practicing this discipline? In practice, requirements engineering is almost impractical without a commercial automation tool. The critics argue that the current automation tools do not convincingly demonstrate its value proposition, or fulfill the longstanding promises of the leading requirements engineering experts. This paper describes how the enterprise software development lifecycle management solution, Visual SDLC, addresses some of the outstanding issues of the present requirements engineering tools.
p967
aVThe confluence of component based development, model driven development and software product lines forms an approach to application development based on the concept of software factories. This approach promises greater gains in productivity and predictability than those produced by incremental improvements to the current paradigm of object orientation, which have not kept pace with innovation in platform technology. Software factories promise to make application assembly more cost effective through systematic reuse, enabling the formation of supply chains and opening the door to mass customization.
p968
aVIn most business software systems the time dimension of business objects plays a significant role. Time is a crosscutting concern that is hard to separate from other business logic. We have developed a toolkit that allows existing business application systems to be extended with "timeconscious" behavior in a nonintrusive way by factoring out all aspects of timerelated behavior into a framework and a set of classes that is distinct from the existing code base. The Time Conscious Objects\u2122 (TCO\u2122) toolkit is currently implemented in Java\u2122, but through the use of generation technology the toolkit can easily be made available in any language that supports polymorphism.
p969
aVStatic analysis of systems allows to discover errors at design time and to avoid runtime error detection techniques that negatively impact performance of the systems. In this paper, we present the consent operator, which allows (i) to capture errors resulting from incorrect composition of software components, (ii) to test conformance of behavior descriptions and (iii) to analyze whether conditions of a correct dynamic update are satisfied.
p970
aVThe Model Driven Architecture (MDA) can have a greater impact by expanding its scope to Domain Specific MDA (DSMDA). This helps developers to represent their systems using familiar domain concepts. For each DSMDA, a transformer is needed to convert Domain Specific Platform Independent Models (DSPIMs) to Domain Specific Platform Specific Models (DSPDMs). Such model transformers are time consuming and error prone to develop and maintain. A highlevel specification language to formally specify the behavior of model transformers is required. The language must also have an efficient execution framework. This research proposes to develop such a language and execution framework.
p971
aVSystematic largescale modification of source code is tedious and errorprone, because developers use authoring and editing tools poorly suited to the program maintenance task. We combine the results from psychology of programming, software visualization, program analysis, and program transformation fields to create a novel environment that lets the programmers express operations on program source code at a level above textoriented editing.
p972
aVWe propose a framework that uses component redundancy for enabling selfadaptation, selfoptimisation and selfhealing capabilities in componentbased enterprise software systems.
p973
aVModel Driven Architecture (MDA), which supports the development of softwareintensive systems through the transformation of models to executable components and applications, requires a standard way to express transformations. The approach developed by this research focuses on defining patternbased transformation at the metamodel level. This research has two primary objectives. The first objective is to support systematic application of patterns. The use of patterns as model building blocks (through patternbased transformations) helps raise the level of abstraction at which systems are developed. The second research objective is to support controlled model evolution by specifying patternbased transformations at the metamodel level.
p974
aVA multiagent system modeling language (MASML) that extends the UML (Unified Modeling Language) is proposed here based on a conceptual framework (metamodel) called TAO (Taming Agents and Objects). The most important difference between our approach and others presented in the literature is our clear definition and representation of the abstractions and behavioral features that compose MASs. Extensive experimentation is being used to access the expressiveness of models represented in MASML and the ease to implement a multiagent system from a specification expressed in our notation.
p975
aVObjectoriented programming languages provide effective means to achieve better reuse and extensibility levels, which increases development productivity. However, the objectoriented paradigm has several limitations, sometimes leading to tangled code and spread code. For example, business code tangled with presentation code or data access code, and distribution, concurrency control, and exception handling code spread over several classes. This decreases readability, and therefore, system maintainability. Some extensions of the objectoriented paradigm try to correct those limitations allowing reuse and maintenance in practical situations where the original paradigm does not offer an adequate support. However, in order to guarantee that those benefits will be achieved by those techniques it is necessary to use them together with an implementation method. Our objective is to adapt and to analyze an objectoriented implementation method to use aspectoriented programming in order to implement several concerns to a family of objectoriented system. In particular, we are interested in implementing persistence, distribution, and concurrency control aspects. At the moment we are particularly interested to present some results and get feed back about a performed experiment to identify if and when a progressive approach is better than a nonprogressive one. In a progressive approach, persistence, distribution, and concurrency control are not initially considered in the implementation activities, but are gradually introduced, preserving the application's functional requirements. This approach helps in dealing with the inherent complexity of the modern applications, through the support to gradual implementations and tests of the intermediate versions of the application.
p976
aVThe evolution of programming languages (e.g. machine languages, assembly languages and high level languages) has been the driving force for the evolution of software development from the machinecentric to the applicationcentric. The 4th generation languages (4GLs), languages defined directly by the composition of domain features, serve as the languagebased formalism for the emerging Domain Driven Development paradigm. The 4GLs are defined in TwoLevel Grammar++ and can be compiled into 3GLs using the 4GL compiler framework.
p977
aVThere has always been a gap between what college graduates in any field are taught and what they need to know to work in industry. However, today the gap in computer science has grown into a chasm. Current college hires who join Microsoft development teams only know a small fraction of their jobs and cannot be trusted to write new code until they have received months of indepth training. The cause of this growing gap is a fundamental shift in the software industry, which now demands higher quality and greater attention to customer needs. This paper presents five new courses to add to computer science curriculums to help close this gap.
p978
aVCurrent general aspectoriented programming solutions fall short of helping the problem of separation of concerns for several concern domains. Because of this limitation good solutions for these concern domains do not get used and the opportunity to benefit from separation of these concerns is missed. By using XAspects, a plugin mechanism for domainspecific aspect languages, separation of concerns can be achieved at a level beyond what is possible for objectoriented programming languages. As a result, XAspects allows for certain domainspecific solutions to be used as easily as a new language feature.
p979
aVRelational database management systems are an essential component of many data intensive applications. At USC, a course entitled "File and Database Management" introduces students to fundamental concepts in relational databases. Students are introduced to conceptual, logical and physical organization of data, use of both formal and commercial query languages, e.g., SQL, indexing techniques for efficient retrieval of data, the concept of a transaction, concurrency control and crash recovery techniques. This paper summarizes our experiences with this course and the challenges of educating students on use of objectoriented concepts and their mapping to tables.
p980
aVC++ is a very successful objectoriented language. It is a required language for more and more students. It takes great effort and practice for these students to learn how to program in C++ and how to make objectoriented programs. One potential failure is that they have learned programming in C++ but do not know how to program in an objectoriented (OO) style. To avoid such failures, this paper proposes that first an objectoriented methodology is taught, and then the language itself. A sixstep approach to teach the OO methodology is presented, followed by some innovative ways to teach different mechanisms in C++. In this way, students can master both objectoriented programming and C++ programming. The proposed teaching method is applicable to teaching other languages like Java and C#.
p981
aVDespite our best efforts and intentions as educators, student programmers continue to struggle in acquiring comprehension and analysis skills. Students believe that once a program runs on sample data, it is correct; most programming errors are reported by the compiler; when a program misbehaves, shuffling statements and tweaking expressions to see what happens is the best debugging approach. This paper presents a new vision for computer science education centered around the use of testdriven development in all programming assignments, from the beginning of CS1. A key element to the strategy is comprehensive, automated evaluation of student work, in terms of correctness, the thoroughness and validity of the student's tests, and an automatic coding style assessment performed using industrialstrength tools. By systematically applying the strategy across the curriculum as part of a student's regular programming activities, and by providing rapid, concrete, useful feedback that students find valuable, it is possible to induce a cultural shift in how students behave.
p982
aVTeaching objectoriented programming in CS1 is hard. Keeping the attention of CS1 students is perhaps even harder. In our experience the former can be done successfully with very satisfying results by focusing on the fundamental principles of objectorientation, such as inheritance, polymorphism and encapsulation. The latter can be done by having students create graphical eventdriven programs. Care must be taken, however, since teaching graphics can easily distract students and certainly takes time away from the fundamentals being taught. We use Java as a vehicle for OO instruction, but rather than expose CS1 students to the intricacies of Swing we employ an elegant and small graphics package called NGP. NGP allows students to create eventdriven graphical programs using only inheritance and method overriding. We describe how we use NGP to enhance rather than detract from our teaching of fundamental OO principles.
p983
aVA five week program which involved extensive exposure to a complex objectoriented virtual environment was evaluated for efficacy in learning fundamental objectoriented principles. Students (n=57) from two CS1 classes were asked to write essays on their knowledge of objectoriented programming concepts following their participation in the program. A systematic method for evaluating student knowledge was followed by rating the student's knowledge on a onetofive rating scale. The results indicated that the students understood, by virtue of their ability to write intelligibly, the concepts of abstraction, inheritance and method override. Demonstrated knowledge of the concept of state was far less convincing.
p984
aVPolymorphism is often treated as an advanced topic by educators. Many feel that if statements are in some sense more "fundamental" to computing. On the contrary, polymorphism is both fundamental to object programming and is an elementary topic that can be easily understood by students. Previous papers [1] have shown how roleplay exercises can remind students that they already have a deep understanding of dynamic polymorphism. The question then becomes how do we find effective teaching techniques to present this topic when we move from the level of metaphor to that of programming. A few elementary patterns [2] can be used to teach this topic even before the student is introduced to adhoc selection with if statements. Teaching these patterns early has the added benefit that they are pervasive in the Java libraries, so understanding them eases the student's later work.
p985
aVIn the secondsemester programming course at the University of Utah, we have observed that our students suffer unnecessarily from a mismatch between the course content and the programming environment. The course is typical, in that it exposes students to Java a little at a time. The programming environments are also typical, in that they report compilation and runtime errors in the jargon of professional programmers who use the full Java language. As a result, students rely heavily on teaching assistants to interpret error messages, and valuable classroom time is wasted on syntactic diversions.ProfessorJ is our new programming environment that remedies this problem. Like other pedagogical environments, such as BlueJ and DrJava, ProfessorJ presents the student with a simplified interface to the Java compiler and virtual machine. Unlike existing environments, ProfessorJ tailors the Java language and error messages to the students' needs. Since their needs evolve through the course, ProfessorJ offers several language levels, from Beginner Java to Full Java.
p986
aVJava certification promises to make our students more marketable once they graduate. The truth is that certifications in general offer significant advantages, but it is important not to overestimate their benefits. In this paper, we describe our experiences on teaching a workshop aimed at preparing undergraduate students for the Sun Certified Java Programmer exam. But first, we layout the real value of IT certifications and explain the different certification options available for Java technology.
p987
aVIn this paper we present a model for the representation of academic courses based on the organization and semantic relation of reusable teaching components using the unified modeling language. We describe an application of the model to cs101, but we believed that the model can be applied to any subject content of any discipline. Results indicate that by following this model, the knowledge of more experienced teachers can be stored in its representation and the course components are easier to be reused by less experienced teachers.
p988
aVPair programming is a concept where two programmers work side by side at the same computer, writing code jointly. One of them, called the driver, is in control of the keyboard and mouse. The other, called the navigator, observes what the driver is doing and offers advice. It is the driver's job to write the code. The navigator has a chance to observe the larger picture, evaluating the driver's code for correctness of design and implementation. Studies have shown that pair programming is very effective. Two programmers can finish a task in little over half the elapsed time that a single programmer takes. And the quality of the codemeasured in terms of absence of defectsis much higher.In the past few years, pair programming has made inroads into industry and into programming courses. However, it has not typically been used in courses that teach subjects other than programming or software engineering, nor has it been used in the analysis of experimental results. This paper reports on an experiment in a combined senior/masterslevel computer architecture class, using Hennessy & Patterson's Computer Architecture: A Quantitative Approach as a text. Students were required to implement three projects simulating various aspects of a microarchitecture (cache, branch predictor, dynamic instruction scheduler). Then they engaged in an experimental analysis to find the best configuration in a design space. Students reported high levels of satisfaction with the experience of pair programming. Pair programmers obtained significantly higher grades on Project 1; however, differences on the other projects were not statistically significant.
p989
aVI present the Ohmu language, a unified object model which allows a number of "advanced" techniques such as aspects, mixin layers, parametric polymorphism, and generative components to be implemented cleanly using two basic concepts: block structure and inheritance. I argue that conventional ways of defining classes and objects have created artificial distinctions which limit their expressiveness. The Ohmu model unifies functions, classes, instances, templates, and even aspects into a single construct  the structure. Function calls, instantiation, aspectweaving, and inheritance are likewise unified into a single operation  the structure transformation. This simplification eliminates the distinction between classes and instances, and between compiletime and runtime code. Instead of being compiled, programs are reduced using partial evaluation, during which the interpreter is invoked at compiletime. Within this architecture, standard OO inheritance becomes a natural vehicle for creating metaprograms and automatic code generators  the key to a number of recent domaindriven programming methodologies.
p990
aVThis work presents an ObjectOriented framework for the implementation of language interpreters in an educational context. We use this framework to implement different programming language paradigms, including interpreters for the Functional, ObjectOriented and Logic paradigms. This framework focuses its structure on aiding the comprehension of the similarities and differences between the implementation of different paradigms.
p991
aVSoftware understanding for documentation, maintenance or evolution is one of the longeststanding problems in Computer Science. The use of "highlevel" programming paradigms and objectoriented languages helps, but fundamentally remains far from solving the problem. Most programming languages and systems have fallen prey to the assumption that they are supposed to capture idealized models of computation inspired by deceptively simple metaphors such as objects and mathematical functions. Aspectoriented programming languages have made a significant breakthrough by noticing that, in many situations, humans think and describe in crosscutting terms. In this paper we suggest that the next breakthrough would require looking even closer to the way humans have been thinking and describing complex systems for thousand of years using natural languages. While natural languages themselves are not appropriate for programming, they contain a number of elements that make descriptions concise, effective and understandable. In particular, natural languages referentiality is a key factor in supporting powerful program organizations that can be easier understood by humans.
p992
aVRuntime code generation (RTCG) would be used routinely if application programmers had a facility with which they could easily create their own runtime code generators, because it would offer benefits both in terms of the efficiency of the code that programmers would produce and the ease of producing it. Such a facility would necessarily have the following properties: it would not require that programmers know assembly language; programmers would have full control over the generated code; the code generator would operate entirely at the binary level. In this paper, we offer arguments and examples supporting these assertions. We briefly describe Jumbo, a system we have built for producing runtime code generators for Java.
p993
aVWe discuss a new approach to the construction of software systems. Instead of attempting to build a system that is as free of errors as possible, the designer instead identifies key properties that the execution must satisfy to be acceptable to its users. Together, these properties define the acceptability envelope of the system: the region that it must stay within to remain acceptable. The developer then augments the system with a layered set of components, each of which enforces one of the acceptability properties. The potential advantages of this approach include more flexible, resilient systems that recover from errors and behave acceptably across a wide range of operating environments, an appropriately prioritized investment of engineering resources, and the ability to productively incorporate unreliable components into the final software system.
p994
aVIn the beginning, so our myths and stories tell us, the programmer created the program from the eternal nothingness of the void. In this essay, we recognise that programs these days are like any other assemblage, and suggest that in fact programming has always been about reuse. We also explore the nature of reuse, and claim that Components themselves are not the most important consideration for reuse; it is the end product, the composition. The issues still involve value, investment, and return. But pervasive reuse promotes a change in the method of construction of the program, and in the program itself.
p995
aVSoftware engineering education for working professionals remains a challenge from the perspective of determining relevant content; identifying effective methods for delivery; and maintaining the focus and motivation of students. This panel brings together academic and industry professionals to share their perspectives and experiences. Anticipated points for discussion include: education/training delivery strategies, curriculum definition, certification challenges, marketing issues, collaboration strategies to engage industry sponsorship, value assessments for students and sponsoring organizations, and program success stories. This will be a highly interactive panel and the audience should come prepared to both ask and answer questions.
p996
aVThis panel brings together coaches to discuss all aspects of the practice  how to become a coach, choosing a coach, and describing what is to be an (in) effective coach. A coach watches, provides feedback, and suggests subtle direction. The coach may be more  for example  an architect or team lead. The panelists will describe their positions and offer feedback. Panelists were asked to offer responses to three questions: How did YOU become a coach?What's the toughest thing you've had to do as a coach?What's your advice for teams looking for a coach?
p997
aVThis panel brings together practitioners with experience in Agile and XP methodologies to discuss the approaches and benefits of applying Test Driven Development (TDD). The goal of TDD is clean code that works. The mantra of TDD is: write a test; make it run; and make it right. Open questions to be addressed by the panel include:  How are TDD approaches to be applied to databases, GUIs, and distributed systems? What are the quantitative benchmarks that can demonstrate the value of TDD, and what are the best approaches to solve the ubiquitous issue of scalability.
p998
aVFreedom to innovate is one of the key motivators for many technical workers. Unfortunately, although innovation is often trumpeted as a key company attribute, it seems that many organizations struggle to provide the necessary environment  even those organizations whose original claim to fame lay in their ability to innovate. This panel will look at the barriers to innovation that occur in a variety of environments: large, wellestablished organizations, startups, academia, standards bodies and the open source community. Panelists will propose a set of technical and nontechnical techniques that can be used to foster innovation in even the most lethargic or hostile environment.
p999
aVModel Driven Architecture (MDA) is a technology that has been in the process of evolution for many years. Today, many vendors are now producing products that support MDA. We are hearing more and more success stories that indicate that this technology is the "real deal". But, with the failed promises of CASE in the late 1980's, many people still have questions about how much of an application can really be generated from models and constraint languages. Is MDA really capable of generating enterprise applications? What are the technologies are available to implement MDA? This panel examines the technologies and tools behind Model Driven Architecture. Each panelist has been intricately involved in building the underlying foundations of Model Driven Architecture and implementation.There are many possible implementations of MDA including action semantics and the object constraint language. This panel will examine: What is the future of MDA? Will the "model driven" approach replace "highlevel" languages as "highlevel" languages replaced assembly languages? What are the challenges with using an MDA approach? How can a project get started using MDA?
p1000
aVWeb application development cuts across the HTTP protocol, the clientside presentation language (HTML, XML), the serverside technology (Servlets, JSP, ASP, PHP), and the underlying resource (files, database, information system). Consequently, web development concerns including functionality, presentation, control, and structure crosscut, leading to tangled and scattered code that is hard to develop, maintain, and reuse. In this paper we analyze the cause, consequence, and remedy for this crosscutting. We distinguish between intracrosscutting that results in code tangling and intercrosscutting that results in code scattering. To resolve intercrosscutting, we present a new web application development model named XP that introduces extension points as placeholders for structuredependent code. We present another model named DDD that incorporates XP into the ModelViewController (MVC) model to resolve both intra and intercrosscutting. WebJinn is a novel domaindriven web development framework that implements the DDD model. WebJinn has been used to develop web applications at several web sites. Domain driven web development with WebJinn benefits from a significant improvement in code reuse, adaptability, and maintainability.
p1001
aV"Selfdirected team" is one of the mantras of Agile Methodologies. Selfdirection means that the team's manager is relegated to a facilitator role with little or no influence over daytoday activities. For example, Kent Beck has written that the manager of an XP project can do four things: ask for estimates on cost and results, move people around among projects, ask for status reports, and cancel the project. Agile literature in general says that managers shouldn't be directly involved in analysis, design, coding, testing or integration. They may (but only occasionally!) facilitate the process between the customer and the developers  and it would be nice if they provided food and toys to keep the team happy. It appears, then, that the agile manger is expected to hover on the fringes of a project asking a few questions and throwing in goodies  but with ultimate power (cancellation) in her hip pocket.This scenario makes one wonder. Do managers really matter to the success of an agile project? Are they superfluous? What happens when managers step over the prescribed line  does it mean that the end of Agile Methodology as we know it and as handed down by the Agile Manifesto? The panel will explore this ticklish terrain by answering the following questions.Why Agile Methods and managers don't mix. Or do they?What can/should managers do in an agile environment?Under what conditions are managers an absolute requirement in an agile environment? (e.g. Government applications?)Do good management techniques apply to both Agile and nonAgile environments?Is management a deadend profession in an Agile world?
p1002
aVBeneath the buzz around methodologies, languages and technologies, the last seventeen years at OOPSLA have seen countless objectoriented success and failure stories, large and small. Last year at OOPSLA there was great enthusiasm over the telling of objectoriented success stories. However, we believe that one often learns more from failures than successes. This fishbowl will provide OOPSLA attendees to bear witness to these failure stories, and tell these tales at last.
p1003
aVThis panel (part of the 2003 Onward! program) will discuss libraries, repositories, and reuse. While there is so much hype and noise about repositories and metadata  so little understood about how hard it is to design for reuse and to encourage systematic reuse.
p1004
aVAgile Methods are advocated as a way of producing better software. Advocates of agile methods suggest that practices such as keeping in close communication with your customers, frequent integration, and frequent assessment of project status will enable us to produce software that has value for the customer  quality software. It's hard to argue with that. But why is this any different than simply "good" software development practice? Why does saying "Scrum" "Agile" or "XP" grab peoples' attention? Why does it take a name for useful practices to be accepted.This panel will help us understand the role of hype in getting useful practices accepted or rejected. We will explore why it is that these good ideas have not been more widely used. Some of the questions that the panel and the audience will explore are: Why do we ignore proven practices until we see them packaged as a "method?" Can we do something different in the workplace or in school to teach these practices? Or is it the case that these practices are not universally good?This panel talks about agility in a different context than what is typical: we won't just discuss what agile practices are. We will explore why they are not more widely adopted, especially when not packaged as part of a "named" method like XP. And we will discuss why projects suffer even when the methods that can help them are well known. This panel will provide an entertaining and thought provoking forum for discussing an issue that is ever present in the world of technology: the role of hype. We concentrate on agile practices, moving beyond simply enumerating them, to discussing why they are not more widely adopted.
p1005
aVIn the beginning there was machine language, followed by assembly language, formula translation, and eventually procedural programming, to organize the chaos. And then objects were introduced, to hide information. Soon Client/Server and multitier applications were conceived to separate data concerns from business logic concerns and user interface concerns. Later, these objects were distributed geographically to optimize hardware resources. And now, we have application servers, to simplify scaling up a system for large volumes, improved response times, impeccable reliability, and high availability. Application servers house the business logic, operating on data from a different server, and responding to requests from any source. But these Application Servers come in all shapes, flavors, and sizes. What is a developer to do? This panel will explore issues comparing application server technologies and questions about their appropriate use in different contexts.
p1006
aVOur research promotes the use of a mathematical concept lattice as a novel visualization of the interfaces of Java classes. The binary relation of accesses between methods and fields, from which the lattice is constructed, serves as a heuristic for an automatic feature categorization. We demonstrate in a detailed reallife case study that such a lattice is valuable for understanding and reverseengineering purposes, in that it helps reason about the interface and structure of the class and find errors in the absence of source code. We also show that if the source code of the class is available, then the lattice can be of assistance in selecting an efficient reading order.
p1007
aVIn most business software systems the time dimension of business objects plays a significant role. Time is a crosscutting concern that is hard to separate from other business logic. We have developed a toolkit that allows existing business application systems to be extended with "timeconscious" behavior in a nonintrusive way by factoring out all aspects of timerelated behavior into a framework and a set of classes that is distinct from the existing code base. The Time Conscious ObjectsTM (TCOTM) toolkit is currently implemented in JavaTM, but through the use of generation technology the toolkit can easily be made available in any language that supports polymorphism.
p1008
aVFly is a lightweight version of the Smalltalk programming environment. Fly attempts to preserve the benefits of Smalltalk as a development system while making it feasible to develop applications for embedded systems, PDAs, and other limited resource environments. Here we introduce the Fly project and its current and expected results.
p1009
aVThe Generative Model Transformer (GMT) project is an Open Source initiative to build a Model Driven ArchitecureTM tool that allows fully customisable Platform Independent Models, Platform Description Models, Texture Mappings, and Refinement Transformations. The project should result in (a) a tool that fulfils the MDA promise for faster/more accurate/better maintainable application development, (b) a tool for industrial use, and (c) MDA related researchwhich is encouraged and needed. A distinctive feature of GMT is the emphasis of model transformations as "firstclass model citizens". The implementation of model transformations is envisaged to be in conformance with the future OMG modeling standard for Queries, Views, and Transformations (QVT).
p1010
aVThis paper presents doctoral research on a key problem for ubiquitous computing: implementation of representatives for physical objects, particularly people. This poster outlines an approach to implementing dynamic personal roles suitable for a ubiquitous computing environment.
p1011
aVWe describe "smart playout", a new method for executing and analyzing scenario based behavior, which is part of the PlayIn/PlayOut methodology and the PlayEngine tool. Behavior is "played in" directly from the system's GUI, and as this is being done the PlayEngine continuously constructs Live Sequence Charts (LSCs), a powerful extension of sequence diagrams. Later, behavior can be "played out" freely from the GUI, and the tool executes the LSCs directly, thus driving the system's behavior. An inherent difficulty in constructing a ``playout" mechanism is how to resolve the nondeterminism allowed by the LSC specification in order to obtain an executable model. Smart playout, is a recent strengthening of the playout mechanism, which addresses this problem by using powerful verification methods, mainly modelchecking, to execute and analyze the LSCs, helping the execution to avoid deadlocks and violations. Thus, smart playout utilizes verification techniques to run programs, rather than to verify a program with respect to given requirements, as in traditional verification approaches. The ideas appear to be relevant in various stages of system development, including requirements specification and analysis, implementation and testing.
p1012
aVA Spring 2003 experiment examines the claims that testdriven development or testfirst programming improves software quality and programmer confidence. The results indicate support for these claims and inform larger future experiments.
p1013
aVAgile Software Development and Component Based Software Engineering are two fundamentally different methods to serve today's demands of software engineering. By combining the technical and organizational issues, we introduce an approach for a consequent integration to allow agile component development in the small and system engineering in the large, respectively.
p1014
aVWe present our framework called DUCS. DUCS supports onthefly updates of distributed componentbased applications. DUCS has a layered architecture running on top of a virtual machine extended with a support for object unload and replacement. DUCS is a metalevel framework, so the impact on the application's implementation is minimal. It supports dynamic component replacement, state transfer among components, interface modifications, and automatic propagation of updates. The framework is expandable and can be applied to many programming languages as well as many hardware platforms.
p1015
aVA multiagent system modeling language (MASML) that extends the UML (Unified Modeling Language) is proposed here based on a conceptual framework (metamodel) called TAO (Taming Agents and Objects). The most important difference between our approach and others presented in the literature is our clear definition and representation of the abstractions and behavioral features that compose MASs.
p1016
aVWe describe a complete system for gathering, computing and presenting dynamic metrics from Java programs. The system itself was motivated from our real goals in understanding program behaviour as compiler/runtime developers, and so solves a number of practical and difficult problems related to metric gathering and analysis.
p1017
aVReusing domainindependent knowledge might be hindered if such knowledge is presented as an integral part of domain specific components. This poster presents the concept of Stable Atomic Knowledge (SAK) patterns. A SAK pattern presents a domainindependent knowledge in such a way that makes this knowledge reusable whenever it is needed.
p1018
aVThere are no mature guidelines or methodologies exist for extracting patterns. Software Stability Model [2] can provide a base for extracting patterns. This poster presents the concept of extracting both domainspecific and domain independent patterns from systems that are built using software stability concepts.
p1019
aVAutonomic Computing has gained widespread attention over the last few years for its vision of developing applications with autonomic or selfmanaging behaviors[1]. One of the most important aspects of building autonomic systems is the ability to monitor applications and generate corrective actions should exceptions occur. The problem lies in those applications where source code is not available and therefore it is virtually impossible to modify the application code to include monitoring functions, or the application code is too tangled with other components which make modification difficult. This hinders the inclusion of autonomic features in many of the legacy applications. In this report, we will describe an approach to build generic monitoring systems for legacy applications.
p1020
aVThis paper describes a modelbased development approach for pervasive computing applications. The key concept introduced is the use of the Model Driven Architecture (MDA) for development of system families.
p1021
aVBytecode instrumentation can be used effectively to (a) generate visualizations and (b) to modify the behavior of Eclipse plugins. In this demonstration, we will show two independent techniques that have in common that they obtain their results by modifying the binary representation of a given software system. In the first part of the demo, Chris Laffra will show experiments he performed on visualization of Eclipse plugins in the context of the JikesBT project. In the second part of the demo, Martin Lippert will show how to weave aspects into Eclipse plugins without having access to their source.
p1022
aVA new approach to teaching software testing is proposed: students use testdriven development on programming assignments, and an automated grading tool assesses their testing performance and provides feedback. The basics of the approach, screenshots of the sytem, and a discussion of industrial tool use for grading Java programs are discussed.
p1023
aVWe present a new QoSenabled load management framework for component oriented middleware. It offers the possibility of selecting the optimal load distribution algorithms and changing the load metrics at runtime. The QoS service level agreements are made at user level, transparent to the managed application.
p1024
aVSeparation of concerns and modularity are key elements of software engineering. The work described here presents a combination of two proven techniques that help improve both of these elements: the Eclipse Core Runtime Platform, which introduces plugins to Java programming as a kind of module concept on top of packages, and aspectoriented programming using AspectJ, which aims to improve the modularity of crosscutting concerns. The work presents a combination of these two technologies in an AspectJenabled version of the Eclipse Core Runtime Platform. Unlike the standard implementation of the Eclipse Core Runtime Platform, the AspectJenabled implementation allows aspects to modularize crosscutting concerns beyond the boundaries of plugins (without the need for recompilation across plugins). It allows crosscutting concerns to be modularized by means of aspects and plugins while using the enhanced but compatible version of the Eclipse Core Runtime Platform as promoted by the Eclipse project.
p1025
aVSelection of proper analysis and design methods is crucial to produce cost and time efficient applications. Effective strategies help developers to come up with more stable, reusable, and complete applications, which is the main purpose of software management. In order to express the importance of proper analysis and design methods, this poster compares two different modeling methods: traditional object oriented and software stability. The modeling methods are utilized for development of the same application: MRI visual analyzer (MRIVA).MRIVA is a computer application to help doctors gain a better understanding of Magnetic resonance image (MRI) pictures. It provides them the opportunity to access variant parts of the organ and study it from different points of view. The current poster shows the result of both methods; however, it is evident that SSM model is simpler and more complete. It offers a firm base for later reuses and is stable under changes over time or application area.
p1026
aVThe Model Driven Architecture (MDA) is a framework that is intended to support the development of softwareintensive systems through the transformation of models to executable components and applications. A key facilitator of the MDA is a standard to express model transformations. This poster describes a rigorous approach to modeling patternbased transformations at the metamodel level.
p1027
aVA virtual machine optimization technique that makes use of bytecode sequences is introduced. The process of determining candidate sequences is discussed and performance gains achieved when applied to a Java interpreter are presented. The suitability of this optimization for JVMs that perform justintime compilation is also discussed.
p1028
aVFew large corporate organizations make the decision to use an Object Database Management System (ODBMS) when developing high volume transactional eCommerce web sites. This report examines an application used to run a website that encompasses banking, online shopping, and the management of a Customer Loyalty Currency called eBucks. This system demonstrates that an ODBMS can be used in a high volume web based transactional system. While the choice of this technology has many merits, there are drawbacks. These drawbacks are examined along with the solutions that have been used at eBucks to either solve or ameliorate them.
p1029
aVThis report discusses experiences applying AspectJ [1] to modularize crosscutting concerns in a middleware product line at IBM. Aspect oriented programming techniques were used to cleanly separate platform specific facilities for aspects such as error handling, performance monitoring and logging from base components, permitting those components to be reused in multiple environments. The initiative also guided the design of the AspectJ Development Tools (AJDT) for Eclipse, and influenced the technical direction of the AspectJ implementation.
p1030
aVWhen developing large software systems, it is often difficult to foresee exactly which tradeoffs are important, and which quality parameters will be of importance down the road. This paper reports experiences from a project in which a large application framework for B2B integration has been continuously developed and used over a fiveyear period. The framework has been the foundation for a variety of different concrete applications. Here we will report on our experiences from this endeavor.
p1031
aVThere are times when it is not practical to handscript automated tests for an existing system before one starts to modify it (whether to refactor it to permit automated testing or to add new functionality). In these circumstances, the use of "record & playback" testing may be a viable alternative to handwriting all the tests.This paper describes experiences using this approach and summarizes key learnings applicable to other projects.
p1032
aVThe RealTime Specification for Java (RTSJ) provides facilities for deterministic, realtime execution in a language that is otherwise subject to variable latencies in memory allocation and garbage collection. A major consequence of these facilities is that the normal Java practice of passing around references to objects in heap memory cannot be used in hard realtime activities. Instead, designers must think carefully about what type of nonheap memory to use and how to transfer data between components without violating RTSJ's memoryarea assignment rules. This report explores the issues of programming with nonheap memory from a practitioner's view in designing and programming realtime control loops using a commercially available implementation of the RTSJ.
p1033
aVThe Parks PDA is a lightweight, handheld device for theme park guests that functions as a combination guidebook, map, and digital camera. Together with a small team of artists and designers, we created a prototype Parks PDA and content for a three hour guest experience, including a camera interface, a hyperlinked guide book, three games, an animal spotters guide, a crossreferenced map, animated movies with lipsynched sound, a ride reservation system, and more. Over 800 visitors to Disney's Animal Kingdom\u2122 theme park tested the Parks PDA over a two week period.Developing the software for this test posed a number of challenges. The processor and memory of the target device were slow, the screen was small, and we had only three months of development time.We attacked these problems using Squeak, a highlyportable, open source Smalltalk implementation. We ported Squeak to the target device and used it to provide nearly bitidentical behavior across four different platforms. This supported a crossplatform development style that streamlined the production of both software and content. We created a tiny user interface and application framework for penbased devices and implemented a simple cardstack media editor and player using it. We isolated and addressed several challenging performance issues.The project was completed on time and guest response was favorable. Looking back, we can identify seven aspects of Squeak that contributed to the success of the project. In fact, we feel that Squeak was the ideal tool for this job.
p1034
aVUsing partial interfaces, i.e. interfaces that cover only a subset of the total set of published methods of a class, has several advantages, among them being an increase in understandability of the code and extended substitutability of classes in frameworks. However, analysis of large frameworks such as the Java API suggests that partial interfaces are only sparsely used. We believe that this is partly due to the fact that introducing and maintaining partial interfaces is perceived as tedious by programmers [5]. Therefore, we have created a metrics suite and tool support to assist the developer in using partial interfaces.
p1035
aVStatic analysis of objectoriented applications has become widespread over the last decade, mainly in the context of compiletime optimizations. The paper describes how static analysis of virtual method calls can be employed to provide a highlevel view of Java applications. The result is a method call graph that can be built from either source or bytecode, and a graphical browser that enables the user to analyze control flow and the coupling between classes and packages in an intuitive fashion, thereby supporting application design as well as refactoring and debugging. In order to achieve the necessary bijection between source and bytecode representations of classes, we implement a new approach based on source code preprocessing.
p1036
aVThe Model Driven Architecture (MDA) can have a greater impact by expanding its scope to Domain Specific MDA (DSMDA). DSMDA is the use of MDA for a particular domain. This helps developers to represent their systems using familiar domain concepts. For each DSMDA, a transformer is needed to convert Domain Specific Platform Independent Models (DSPIM s) to Domain Specific Platform Specific Models (DSPDMs). Such model transformers are time consuming and error prone to develop and maintain. Hence, a highlevel specification language to formally specify the behavior of model transformers is required. The language must also have an execution framework, which can be used to execute the specifications in the language. This research proposes to develop such a language and execution framework that will help to considerably speedup the development time for model transformers.
p1037
aVThe evolution of programming languages (e.g. machine languages, assembly languages and high level languages) has been the driving force for the evolution of software development from the machinecentric to the applicationcentric. The 4th generation languages (4GLs), languages defined directly by the composition of domain features, serve as the languagebased formalism for the emerging Domain Driven Development paradigm. The 4GLs are defined in TwoLevel Grammar++ and can be compiled into 3GLs using the 4GL compiler framework.
p1038
aVWe propose a framework that uses component redundancy for enabling selfadaptation, selfoptimisation and selfhealing capabilities in componentbased enterprise software systems.
p1039
aVSystematic largescale modification of source code is tedious and errorprone, because developers use authoring and editing tools poorly suited to the program maintenance task. We combine the results from psychology of programming, software visualization, program analysis, and program transformation fields to create a novel environment that lets the programmers express operations on program source code at a level above textoriented editing.
p1040
aVAs the demand for web applications grows, so does the demand for tools that support them. As a general rule, such tools extend general purpose programming languages, like Servlets/Jsp[2] does for Java [4], or define their own programming language, like PHP[3]. But there is no established engine for web applications written with C++. This work presents technical challenges that were faced when developing T++, an engine that supports web application development with C++.
p1041
aVGenerics offer significant software engineering benefits since they provide code reuse without compromising type safety. Thus generics will be added to the Java language in the next release. While this extension to Java will help programmers when they are writing new code, it will not help legacy code unless it is rewritten to use generics. In our experience, manually modifying existing programs to use generics is complex and can be error prone and labor intensive. We describe a system, Ilwith, that (i) converts nongeneric classes to generic classes and (ii) rewrites their clients to use the newly generified classes. Our experiments with a number of Java container classes show that our system is effective in modifying legacy code to use generics.
p1042
aVThis paper proposes and implements a rigorous method for studying the dynamic behaviour of AspectJ programs. As part of this methodology several new metrics specific to AspectJ programs are proposed and tools for collecting the relevant metrics are presented. The major tools consist of: (1) a modified version of the AspectJ compiler that tags bytecode instructions with an indication of the cause of their generation, such as a particular feature of AspectJ; and (2) a modified version of the *J dynamic metrics collection tool which is composed of a JVMPIbased trace generator and an analyzer which propagates tags and computes the proposed metrics. This dynamic propagation is essential, and thus this paper contributes not only new metrics, but also nontrivial ways of computing them. We furthermore present a set of benchmarks that exercise a wide range of AspectJ's features, and the metrics that we measured on these benchmarks. The results provide guidance to AspectJ users on how to avoid efficiency pitfalls, to AspectJ implementors on promising areas for future optimization, and to tool builders on ways to understand the runtime behaviour of AspectJ.
p1043
aVDistributed enterprise applications today are increasingly being built from services available over the web. A unit of functionality in this framework is a web service, a software application that exposes a set of "typed'' connections that can be accessed over the web using standard protocols. These units can then be composed into a <i>composite</i> web service. BPEL (Business Process Execution Language) is a highlevel distributed programming language for creating composite web services. Although a BPEL program invokes services distributed over several servers, the <i>orchestration</i> of these services is typically under centralized control. Because performance and throughput are major concerns in enterprise applications, it is important to remove the inefficiencies introduced by the centralized control. In a distributed, or decentralized orchestration, the BPEL program is partitioned into independent subprograms that interact with each other without any centralized control. Decentralization can increase parallelism and reduce the amount of network traffic required for an application. This paper presents a technique to partition a composite web service written as a single BPEL program into an equivalent set of decentralized processes. It gives a new code partitioning algorithm to partition a BPEL program represented as a program dependence graph, with the goal of minimizing communication costs and maximizing the <i>throughput</i> of multiple concurrent instances of the input program. In contrast, much of the past work on dependencebased partitioning and scheduling seeks to minimize the <i>completion time</i> of a single instance of a program running in isolation. The paper also gives a cost model to estimate the throughput of a given code partition.
p1044
aVMiddleware provides simplicity and uniformity for the development of distributed applications. However, the modularity of the architecture of middleware is starting to disintegrate and to become complicated due to the interaction of too many orthogonal concerns imposed from a wide range of application requirements. This is not due to bad design but rather due to the limitations of the conventional architectural decomposition methodologies. We introduce the principles of horizontal decomposition (HD) which addresses this problem with a mixedparadigm middleware architecture. HD provides guidance for the use of conventional decomposition methods to implement the core functionalities of middleware and the use of aspect orientation to address its orthogonal properties. Our evaluation of the horizontal decomposition principles focuses on refactoring major middleware functionalities into aspects in order to modularize and isolate them from the core architecture. New versions of the middleware platform can be created through combining the core and the flexible selection of middleware aspects such as IDL data types, the oneway invocation style, the dynamic messaging style, and additional character encoding schemes. As a result, the primary functionality of the middleware is supported with a much simpler architecture and enhanced performance. Moreover, customization and configuration of the middleware for a widerange of requirements becomes possible.
p1045
aVA <i>proxy</i> object is a surrogate or placeholder that controls access to another target object. Proxies can be used to support distributed programming, lazy or parallel evaluation, access control, and other simple forms of behavioral reflection. However, <i>wrapper proxies</i> (like <i>futures</i> or <i>suspensions</i> for yettobecomputed results) can require significant code changes to be used in staticallytyped languages, while proxies more generally can inadvertently violate assumptions of transparency, resulting in subtle bugs. To solve these problems, we have designed and implemented a simple framework for proxy programming that employs a static analysis based on qualifier inference, but with additional novelties. Code for using wrapper proxies is automatically introduced via a classfiletoclassfile transformation, and potential violations of transparency are signaled to the programmer. We have formalized our analysis and proven it sound. Our framework has a variety of applications, including support for asynchronous method calls returning futures. Experimental results demonstrate the benefits of our framework: programmers are relieved of managing and/or checking proxy usage, analysis times are reasonably fast, overheads introduced by added dynamic checks are negligible, and performance improvements can be significant. For example, changing two lines in a simple RMIbased peertopeer application and then using our framework resulted in a large performance gain.
p1046
aVWe propose a heap compaction algorithm appropriate for modern computing environments. Our algorithm is targeted at SMP platforms. It demonstrates high scalability when running in parallel but is also extremely efficient when running singlethreaded on a uniprocessor. Instead of using the standard forwarding pointer mechanism for updating pointers to moved objects, the algorithm saves information for a pack of objects. It then does a small computation to process this information and determine each object's new location. In addition, using a smart parallel moving strategy, the algorithm achieves (almost) perfect compaction in the lower addresses of the heap, whereas previous algorithms achieved parallelism by compacting within several predetermined segments. Next, we investigate a method that trades compaction quality for a further reduction in time and space overhead. Finally, we propose a modern version of the twofinger compaction algorithm. This algorithm fails, thus, revalidating traditional wisdom asserting that retaining the order of live objects significantly improves the quality of the compaction. The parallel compaction algorithm was implemented on the IBM production Java Virtual Machine. We provide measurements demonstrating high efficiency and scalability. Subsequently, this algorithm has been incorporated into the IBM production JVM.
p1047
aVThis paper introduces <i>dynamic</i> object colocation, an optimization to reduce copying costs in generational and other incremental garbage collectors by allocating connected objects together in the same space. Previous work indicates that connected objects belong together because they often have similar lifetimes. Generational collectors, however, allocate all new objects in a <i>nursery</i> space. If these objects are connected to data structures residing in the <i>mature</i> space, the collector must copy them. Our solution is a cooperative optimization that exploits compiler analysis to make runtime allocation decisions. The compiler analysis discovers potential object connectivity for newly allocated objects. It then replaces these allocations with calls to <i>coalloc</i>, which takes an extra parameter called the <i>colocator</i> object. At runtime, coalloc determines the location of the colocator and allocates the new object together with it in either the nursery or mature space. Unlike pretenuring, colocation makes precise perobject allocation decisions and does not require lifetime analysis or allocation site homogeneity. Experimental results for SPEC Java benchmarks using Jikes RVM show colocation can reduce garbage collection time by 50% to 75%, and total performance by up to 1%.
p1048
aVObjectoriented programming languages provide a rich set of features that provide significant software engineering benefits. The increased productivity provided by these features comes at a justifiable cost in a more sophisticated runtime system whose responsibility is to implement these features efficiently. However, the virtualization introduced by this sophistication provides a significant challenge to understanding complete system performance, not found in traditionally compiled languages, such as C or C++. Thus, understanding system performance of such a system requires profiling that spans all levels of the execution stack, such as the hardware, operating system, virtual machine, and application. In this work, we suggest an approach, called <i>vertical profiling</i>, that enables this level of understanding. We illustrate the efficacy of this approach by providing deep understandings of performance problems of Java applications run on a VM with vertical profiling support. By incorporating vertical profiling into a programming environment, the programmer will be able to understand how their program interacts with the underlying abstraction levels, such as application server, VM, operating system, and hardware.
p1049
aVJava workloads are becoming more and more prominent on various computing devices. Understanding the behavior of a Java workload which includes the interaction between the application and the virtual machine (VM), is thus of primary importance during performance analysis and optimization. Moreover, as contemporary software projects are increasing in complexity, automatic performance analysis techniques are indispensable. This paper proposes an offline methodlevel phase analysis approach for Java workloads that consists of three steps. In the first step, the execution time is computed for each method invocation. Using an offline tool, we subsequently analyze the dynamic call graph (that is annotated with the method invocations' execution times) to identify methodlevel phases. Finally, we measure performance characteristics for each of the selected phases. This is done using hardware performance monitors. As such, our approach allows for linking microprocessorlevel information at the individual methods in the Java application's source code. This is extremely interesting information during performance analysis and optimization as programmers can use this information to optimize their code. We evaluate our approach in the Jikes RVM on an IA32 platform using the SPECjvm98 and SPECjbb2000 benchmarks. This is done according to a number of important criteria: the overhead during profiling, the variability within and between the phases, its applicability in Java workload characterization (measuring performance characteristics of the various VM components) and application bottleneck identification.
p1050
aVCode instrumentation is widely used for a range of purposes that include profiling, debugging, visualization, logging, and distributed computing. Due to their special status within the language infrastructure, the <i>standard class libraries</i>, also known as <i>system classes</i> provided by most contemporary objectoriented languages are difficult and sometimes impossible to instrument. If instrumented, the use of their rewritten versions within the instrumentation code is usually unavoidable. However, this is equivalent to `instrumenting the instrumentation', and thus may lead to erroneous results. Consequently, most systems avoid rewriting system classes. We present a novel instrumentation strategy that alleviates the above problems by renaming the instrumented classes. The proposed approach does not require any modifications to the language, compiler or runtime. It allows system classes to be instrumented both statically and dynamically. In fact, this is the first technique that enables dynamic instrumentation of Java system classes without modification of any runtime components. We demonstrate our approach by implementing two instrumentationbased systems: a memory profiler and a distributed runtime for Java.
p1051
aVA discontinuity exists between objectoriented modeling and programming languages. This discontinuity arises from ambiguous concepts in modeling languages and a lack of corresponding concepts in programming languages. It is particularly acute for binary class relationships association, aggregation, and composition. It hinders the traceability between software implementation and design, thus hampering software analysis. We propose consensual definitions of the binary class relationships with four minimal properties exclusivity, invocation site, lifetime, and multiplicity. We describe algorithms to detect automatically these properties in source code and apply these on several frameworks. Thus, we bridge the gap between implementation and design for the binary class relationships, easing software analysis.
p1052
aVJava 1.5 will include a type system (called JSR14) that supports <i>parametric polymorphism</i>, or <i>generic</i> classes. This will bring many benefits to Java programmers, not least because current Java practice makes heavy use of logicallygeneric classes, including container classes. Translation of Java source code into semantically equivalent JSR14 source code requires two steps: parameterization (adding type parameters to class definitions) and instantiation (adding the type arguments at each use of a parameterized class). Parameterization need be done only once for a class, whereas instantiation must be performed for each client, of which there are potentially many more. Therefore, this work focuses on the instantiation problem. We present a technique to determine sound and precise JSR14 types at each use of a class for which a generic type specification is available. Our approach uses a precise and contextsensitive pointer analysis to determine possible types at allocation sites, and a setconstraintbased analysis (that incorporates guarded, or conditional, constraints) to choose consistent types for both allocation and declaration sites. The technique handles all features of the JSR14 type system, notably the raw types that provide backward compatibility. We have implemented our analysis in a tool that automatically inserts type parameters into Java code, and we report its performance when applied to a number of realworld Java programs.
p1053
aVKABA is an innovative system for refactoring Java class hierarchies. It uses the Snelting/Tip algorithm [13] in order to determine a behaviorpreserving refactoring which is optimal with respect to a given set of client programs. KABA can be based on dynamic as well as static program analysis. The static variant will preserve program behavior for all possible input values; the dynamic version guarantees preservation of behavior for all runs in a given test suite. KABA offers automatic refactoring as well as manual refactoring using a dedicated editor. In this contribution, we recapitulate the Snelting/Tip algorithm, present the new dynamic version, and explain new extensions which allow to handle full Java. We then present five case studies which discuss the KABA refactoring proposals for programs such as javac and antlr. KABA proved that javac does not need refactoring, but generated semanticsbased refactoring proposals for antlr.
p1054
aVWe identify three design principles for reflection and metaprogramming facilities in object oriented programming languages. <i>Encapsulation</i>: metalevel facilities must encapsulate their implementation. <i>Stratification</i>: metalevel facilities must be separated from baselevel functionality. <i>Ontological correspondence</i>: the ontology of metalevel facilities should correspond to the ontology of the language they manipulate. Traditional/mainstream reflective architectures do not follow these precepts. In contrast, reflective APIs built around the concept of <i>mirrors</i> are characterized by adherence to these three principles. Consequently, mirrorbased architectures have significant advantages with respect to distribution, deployment and general purpose metaprogramming.
p1055
aV<i>Predicate dispatch</i> is an objectoriented (OO) language mechanism for determining the method implementation to be invoked upon a message send. With predicate dispatch, each method implementation includes a predicate guard specifying the conditions under which the method should be invoked, and logical implication of predicates determines the method overriding relation. Predicate dispatch naturally unifies and generalizes several common forms of dynamic dispatch, including traditional OO dispatch, multimethod dispatch, and functionalstyle pattern matching. Unfortunately, prior languages supporting predicate dispatch have had several deficiencies that limit its utility in practice. We introduce JPred, a backwardcompatible extension to Java supporting predicate dispatch. While prior languages with predicate dispatch have been extensions to toy or nonmainstream languages, we show how predicate dispatch can be naturally added to a traditional OO language. While prior languages with predicate dispatch have required the whole program to be available for typechecking and compilation, JPred retains Java's modular typechecking and compilation strategies. While prior languages with predicate dispatch have included specialpurpose algorithms for reasoning about predicates, JPred employs generalpurpose, offtheshelf decision procedures. As a result, JPred's type system is more flexible, allowing several useful programming idioms that are spuriously rejected by those other languages. After describing the JPred language and type system, we present a case study illustrating the utility of JPred in a realworld application, including its use in the detection of several errors.
p1056
aVApplication programmer's interfaces give access to domain knowledge encapsulated in class libraries without providing the appropriate notation for expressing domain composition. Since objectoriented languages are designed for extensibility and reuse, the language constructs are often sufficient for expressing domain abstractions at the semantic level. However, they do not provide the right abstractions at the syntactic level. In this paper we describe MetaBorg, a method for providing <i>concrete syntax</i> for domain abstractions to application programmers. The method consists of <i>embedding</i> domainspecific languages in a general purpose host language and <i>assimilating</i> the embedded domain code into the surrounding host code. Instead of extending the implementation of the host language, the assimilation phase implements domain abstractions in terms of existing APIs leaving the host language undisturbed. Indeed, MetaBorg can be considered a method for promoting APIs to the language level. The method is supported by proven and available technology, i.e. the syntax definition formalism SDF and the program transformation language and toolset Stratego/XT. We illustrate the method with applications in three domains: code generation, XML generation, and userinterface construction.
p1057
aVPrograms that manipulate physical quantities typically represent these quantities as raw numbers corresponding to the quantities' measurements in particular units (e.g., a length represented as a number of meters). This approach eliminates the possibility of catching errors resulting from adding or comparing quantities expressed in different units (as in the Mars Climate Orbiter error [11]), and does not support the safe comparison and addition of quantities of the same dimension. We show how to formulate dimensions and units as classes in a nominally typed objectoriented language through the use of statically typed metaclasses. Our formulation allows both parametric and inheritance polymorphism with respect to both dimension and unit types. It also allows for integration of encapsulated measurement systems, dynamic conversion factors, declarations of scales (including nonlinear scales) with defined zeros, and nonconstant exponents on dimension types. We also show how to encapsulate most of the "magic machinery" that handles the algebraic nature of dimensions and units in a single metaclass that allows us to treat select static types as generators of a free abelian group.
p1058
aVThrough the design and implementation of a JVM that supports Pluggable Verification Modules (PVMs), the idea of an extensible protection mechanism is entertained. Linktime bytecode verification becomes a pluggable service that can be readily replaced, reconfigured and augmented. Applicationspecific verification services can be safely introduced into the dynamic linking process of the JVM. This feature is enabled by the adoption of a previously proposed modular verification architecture, Proof Linking [23, 24], which decouples bytecode verification from the dynamic linking process, rendering the verifier a replaceable module. The PVM mechanism has been implemented in an open source JVM, the Aegis VM [21]. To evaluate the software engineering and security engineering benefits of this extensible protection mechanism, an augmented type system JAC (Java Access Control) [37] has been successfully implemented as a PVM.
p1059
aVIt is difficult to write programs that behave correctly in the presence of runtime errors. Existing programming language features often provide poor support for executing cleanup code and for restoring invariants in such exceptional situations. We present a dataflow analysis for finding a certain class of errorhandling mistakes: those that arise from a failure to release resources or to clean up properly along all paths. Many realworld programs violate such resource safety policies because of incorrect error handling. Our flowsensitive analysis keeps track of outstanding obligations along program paths and does a precise modeling of control flow in the presence of exceptions. Using it, we have found over 800 error handling mistakes almost 4 million lines of Java code. The analysis is unsound and produces false positives, but a few simple filtering rules suffice to remove them in practice. The remaining mistakes were manually verified. These mistakes cause sockets, files and database handles to be leaked along some paths. We present a characterization of the most common causes of those errors and discuss the limitations of exception handling, finalizers and destructors in addressing them. Based on those errors, we propose a programming language feature that keeps track of obligations at run time and ensures that they are discharged. Finally, we present case studies to demonstrate that this feature is natural, efficient, and can improve reliability; for example, retrofitting a 34kLOC program with it resulted in a 0.5% code size decrease, a surprising 17% speed increase (from correctly deallocating resources in the presence of exceptions), and more consistent behavior.
p1060
aVThis paper describes a type system that is capable of expressing and enforcing immutability constraints. The specific constraint expressed is that the abstract state of the object to which an immutable reference refers cannot be modified using that reference. The abstract state is (part of) the transitively reachable state: that is, the state of the object and all state reachable from it by following references. The type system permits explicitly excluding fields or objects from the abstract state of an object. For a statically typesafe language, the type system guarantees reference immutability. If the language is extended with immutability downcasts, then runtime checks enforce the reference immutability constraints. In order to better understand the usability and efficacy of the type system, we have implemented an extension to Java, called Javari, that includes all the features of our type system. Javari is interoperable with Java and existing JVMs. It can be viewed as a proposal for the semantics of the Java const keyword, though Javari's syntax uses readonly instead. This paper describes the design and implementation of Javari, including the typechecking rules for the language. This paper also discusses experience with 160,000 lines of Javari code. Javari was easy to use and provided a number of benefits, including detecting errors in welltested code.
p1061
aVTracing and reference counting are uniformly viewed as being fundamentally different approaches to garbage collection that possess very distinct performance properties. We have implemented highperformance collectors of both types, and in the process observed that the more we optimized them, the more similarly they behaved  that they seem to share some deep structure.  We present a formulation of the two algorithms that shows that they are in fact duals of each other. Intuitively, the difference is that tracing operates on live objects, or "matter", while reference counting operates on dead objects, or "antimatter". For every operation performed by the tracing collector, there is a precisely corresponding antioperation performed by the reference counting collector. Using this framework, we show that all highperformance collectors (for example, deferred reference counting and generational collection) are in fact hybrids of tracing and reference counting. We develop a uniform costmodel for the collectors to quantify the tradeoffs that result from choosing different hybridizations of tracing and reference counting. This allows the correct scheme to be selected based on system performance requirements and the expected properties of the target application.
p1062
aVAs improvements in processor speed continue to outpace improvements in cache and memory speed, poor locality increasingly degrades performance. Because copying garbage collectors move objects, they have an opportunity to improve locality. However, no static copying order is guaranteed to match program traversal orders. This paper introduces <i>online object reordering</i> (OOR) which includes a new dynamic, online class analysis for Java that detects program traversal patterns and exploits them in a copying collector. OOR uses runtime method sampling that drives justintime (JIT) compilation. For each <i>hot</i> (frequently executed) method, OOR analysis identifies the hot field accesses. At garbage collection time, the OOR collector then copies referents of hot fields together with their parent. Enhancements include static analysis to exclude accesses in cold basic blocks, heuristics that decay heat to respond to phase changes, and a separate space for hot objects. The overhead of OOR is on average negligible and always less than 2% on Java benchmarks in Jikes RVM with MMTk. We compare program performance of OOR to static classoblivious copying orders (e.g., breadth and depth first). Performance variation due to static orders is often low, but can be up to 25%. In contrast, OOR matches or improves upon the best static order since its historybased copying tunes memory layout to program traversal.
p1063
aVJava is becoming an important platform for memoryconstrained consumer devices such as PDAs and cellular phones, because it provides safety and portability. Since Java uses garbage collection, efficient garbage collectors that run in constrained memory are essential. Typical collection techniques used on these devices are marksweep and markcompact. Marksweep collectors can provide good throughput and pause times but suffer from fragmentation. Markcompact collectors prevent fragmentation, have low space overheads, and provide good throughput. However, they can suffer from long pause times. Copying collectors can provide higher throughput than either of these techniques, but because of their high space overhead, they previously were unsuitable for memoryconstrained devices. This paper presents MC<sup>2</sup> (MemoryConstrained Copying), a copying generational garbage collector that meets the needs of memoryconstrained devices with soft realtime requirements. MC<sup>2</sup> has low space overhead and tight space bounds, prevents fragmentation, provides good throughput, and yields short pause times. These qualities make MC<sup>2</sup> attractive for other environments, including desktops and servers.
p1064
aVInheritance is a useful mechanism for factoring and reusing code. However, it has limitations for building extensible systems. We describe <i>nested inheritance</i>, a mechanism that addresses some of the limitations of ordinary inheritance and other code reuse mechanisms. Using our experience with an extensible compiler framework, we show how nested inheritance can be used to construct highly extensible software frameworks. The essential aspects of nested inheritance are formalized in a simple objectoriented language with an operational semantics and type system. The type system of this language is sound, so no runtime type checking is required to implement it and no runtime type errors can occur. We describe our implementation of nested inheritance as an unobtrusive extension of the Java language, called Jx. Our prototype implementation translates Jx code to ordinary Java code, without duplicating inherited code.
p1065
aVIn an objectoriented language, a derived class may declare a method with the same signature as a method in the base class. The meaning of the redeclaration depends on the language. Most commonly, the new declaration overrides the base declaration, perhaps completely replacing it, or perhaps using <b>super</b> to invoke the old implementation. Another possibility is that the base class always controls the method implementation, and the new declaration merely augments the method in the case that the base method calls <b>inner</b>. Each possibility has advantages and disadvantages. In this paper, we explain why programmers need both kinds of method redeclaration, and we present a language that integrates them. We also present a formal semantics for the new language, and we describe an implementation for MzScheme.
p1066
aVEncapsulation in objectoriented languages has traditionally been based on static type systems. As a consequence, dynamicallytyped languages have only limited support for encapsulation. This is surprising, considering that encapsulation is one of the most fundamental and important concepts behind objectoriented programming and that it is essential for writing programs that are maintainable and reliable, and that remain robust as they evolve. In this paper we describe the problems that are caused by insufficient encapsulation mechanisms and then present objectoriented encapsulation, a simple and uniform approach that solves these problems by bringing state of the art encapsulation features to dynamically typed languages. We provide a detailed discussion of our design rationales and compare them and their consequences to the encapsulation approaches used for statically typed languages. We also describe an implementation of objectoriented encapsulation in Smalltalk. Benchmarks of this implementation show that extensive use of objectoriented encapsulation results in a slowdown of less than 15 percent.
p1067
aVIn his role as United States Poet Laureate, Robert Hass spent two years battling American illiteracy, armed with the mantra, "imagination makes communities." He crisscrossed the country speaking at Rotary Club meetings, raising money to organize conferences such as "Watershed," which brought together noted novelists, poets, and storytellers to talk about writing, nature, and community. For Hass, everything is connected. When he works to heighten literacy, he is also working to promote awareness about the environment. Hass believes that natural beauty must be tended to and that caring for a place means knowing it intimately. Poets, especially, need to pay constant attention to the interaction of mind and environment. And when he is talking about poetry itself, whether Matsuo Basho's or Elizabeth Bishop's, Hass is both spontaneous and original, offering poetic insights that cannot be found in any textbook.Robert Hass has published many books of poetry including Field Guide, Praise, Human Wishes, and Sun Under Wood, as well as a book of essays on poetry, Twentieth Century Pleasures. Hass translated many of the works of Nobel Prizewinning Polish poet, Czeslaw Milosz, and he edited Selected Poems: 1954 1986 by Thomas Transtromer, The Essential Haiku: Versions of Basho, Buson,<and Issa, and , Poet's Choice: Poems for Everyday Life., Robert Hass's deep commitment to environmental issues led him to found River of Words (ROW), an organization that promotes environmental and arts education in affiliation with the Library of Congress Center for the Book. Hass is chairman of ROW's board of directors, and judges their annual international environmental poetry and art contest for youth. He is also a board member of International Rivers Network. Robert Hass was chosen as Educator of the Year by the North American Association on Environmental Education. Robert Hass was the guest editor of the 2001 edition of Best American Poetry.Awarded a MacArthur Fellowship, twice the National Book Critics Circle Award (in 1984 and 1997), and the Yale Series of Younger Poets in 1973, Robert Hass is a professor of English at UC Berkeley.
p1068
aVA new field in distributed computing, called Ambient Intelligence, has emerged as a consequence of the increasing availability of wireless devices and the mobile networks they induce. Developing software for such mobile networks is extremely hard in conventional programming languages because the network is dynamically defined. This hardware phenomenon leads us to postulate a suite of characteristics of future AmbientOriented Programming languages. A simple re ective programming language kernel, called AmbientTalk, that meets these characteristics is subsequently presented. The power of the re ective kernel is illustrated by using it to conceive a collection of high level tentative ambientoriented programming language features.
p1069
aVWhile the iterative development approaches found in Agile Software Development fulfill the promise of working software each iteration, the task of choosing which software to build first can be formidable.This experience report discusses my team's experience working with a large healthcare company writing software for use in their hospital's newborn intensive care unit (NICU). The very large scope of this project and the urgent need for delivery made project release planning difficult. Focusing on capturing feature details in XP style user stories led to confusion about priorities and release strategy. Making good use of User Centered Design user role models and task models gave us the big picture we needed to unstick the release planning process and effectively choose the bit of project scope we needed to focus on for our first and subsequent releases.
p1070
aVOO Programs are built by first defining User Types within the language environment and then realizing program requirements by using the behavior defined by these Types.We argue against defining types to deal with every scenario.OO Programs within an enterprise have to deal with the nonOO world that includes RDBMS, other Applications, and humans etc. On these EDGES that OO programs interact with the nonOO world we have observed that there is little respect for Types and behavior while the requirement and expectation is most often data. If this is true, then OO application developers will repeatedly have to extract data to and from objects defined by Types to support the pure data interface that the non OO world supports.In this paper we attempt to highlight some benefits we have realized by equipping an OO Application (Banking Middleware in Java) with a core representation for data which we refer to as the 'data fabric'. We further show that seemingly unrelated problems in database persistence, databinding, message data transformation and metadata management that appear within an Enterprise context can now be addressed well within the data fabric. The Data fabric is itself realized using reflective uniform data API defined using a few key data abstractions.This then reopens the case for OO developers to enhance their effectiveness with generic Data Models and principles of Data Driven Programming.
p1071
aVCode duplication is a serious problem with no easy solution, even in industrialstrength code. Single inheritance cannot provide for effective code reuse in all situations, and sometimes programmers are driven to duplicate code using copy and paste. A language feature called traits enables code to be shared across the inheritance hierarchy, and claims to permit the removal of most duplication. We attempted to validate this claim in a case study of the java.io library. Detecting duplication was more complex than we had imagined, but traits were indeed able to remove all that we found.
p1072
aVIn physics, like in other sciences, formulas are specified using explicit measurements, that is, a number with its unit. The first step to determine the validity of a physics formula's evaluation is to verify that the unit of the result corresponds with the prospective unit. In software development, physics, financial and other sciences formulas are programmed using mathematical expressions based only on numbers, being the units of these numbers implicitly given by the semantics of the program or assumed by the programmer's knowledge. Consequently, it is common that errors result from operating with values expressed in different units, e.g., dividing a quantity of years by a quantity of months, without obtaining any type of indication or objection to this error from the system. In this report, we discuss our experience designing and implementing a model that solves this problem reifying the concept of measurement, unit and their arithmetic. Our model relieves the programmer from the arduous task of verifying the validity of the arithmetic expressions regarding units, delegating that responsibility to the system, thereby, diminishing the errors introduced by the incorrect use of values expressed in different units. We also show that having implemented this model with a dynamically typed language simplified its programming and increased its reusability.
p1073
aVEffective and affordable businesstobusiness process integration is a key success factor in the telecommunications industry. A large telecommunication wholesaler, supplying its services to more than 150 different service retailers, enhanced the process integration capabilities of its core order management system through widespread use of SOA, business process choreography and Web services concepts. This core order management system processes 120 different complex order types.On this project, challenging requirements such as complexity of business process models and multichannel accessibility turned out to be true proof points for the applied SOA concepts, tools, and runtime environments. To implement an automated and secured businesstobusiness Web services channel and to introduce a process choreography layer into a large existing application were two of the key requirements that had to be addressed. The solution complies with the Web Services Interoperability Basic Profile 1.0 and makes use of executable business process models defined in the Business Process Execution Language (BPEL).This paper discusses the rationale behind the decision for SOA, process choreography, and Web services, and gives an overview of the BPELcentric process choreography architecture. Furthermore, it features lessons learned and best practices identified during design, implementation, and rollout of the solution.
p1074
aVIt is not uncommon for good technical solutions to fail in the marketplace. Equally true, great business opportunities are not always met with appropriate technical solutions. While there can be many causes to such failures, one common problem is the gap between expectations and implementation. Extreme Programming is an excellent delivery methodology for bridging this gap. This paper presents lessons learned from applying Extreme Programming in a startup environment. In particular, the challenges of meeting and adapting to evolving requirements are presented.
p1075
aVPredicate fields allow an object's structure to vary at runtime based on the object's state: a predicate field is present or not, depending on the values of other fields. Predicate fields and related concepts have not previously been evaluated outside a research environment. We present a case study of two industrial applications with similar requirements, one of which uses predicate fields and one of which does not. The use of predicate fields was motivated by requirements for high flexibility, by unavailability of many requirements, and by high user interface development costs. Despite an implementation of predicate fields as a library (rather than as a language extension), developers found them natural to use, and in many cases they significantly reduced development effort.
p1076
aVTeachlets are a new method originally developed to teach design patterns. Based on executable code, a problem is set that is to be solved collaboratively and interactively by all the participants of a teaching unit. A moderator is on hand to operate the computer, the development environment and video projector when called upon to do so. While deployable in themselves as an innovative teaching method, teachlets can also be used as a design object in seminarlike workshop events. In the course on advanced concepts of objectoriented programming described here, the participants themselves developed and used teachlets in a socalled teachlet laboratory.
p1077
aVAfter we adopted an objectsfirst approach in CS1, we had to redesign our CS2 and data structures courses. This paper reports on our efforts to develop a projectdriven CS2 course that expands on the objectoriented methodology introduced in the CS1 course. We focused on using collections and base classes in meaningful, largescale projects helping students understand why these classes are important before concentrating on implementation in the subsequent data structures and algorithms course. We also introduce the concepts of design patterns and frameworks. This paper focuses on the OO methodology developed in the course; a companion paper [6] deals with pedagogical issues in using our approach.
p1078
aVThis paper presents a method of teaching OO design based on modeling the software design process. Design is essentially about making choices as an initial idea is extended to construct an executable system. The paper first defines the essence of a solution, the plan structure. It then shows how a plan is built from small pieces, and how an abstract plan is translated into code. Many choices are made when translating a plan into an executable OO system; where there is a choice, a design rule is used to make that choice. The model of teaching identifies the variations and choices that arise during the process of design, and shows how an explicit set of design rules can evaluate the choices and make the best decision.
p1079
aVOrganic programming (OP) is our proposed and already emerging programming model which overcomes some of the limitations of current practice in software development in general and of objectoriented programming (OOP) in particular. Ercatons provide an implementation of the model. In some respects, OP is less than a (new) programming language, in others, it is more. An "ercato machine" implements the ideas discussed and has been used to validate the concepts described here.Organic programming is centered around the concept of a true "Thing". A thing in an executing software system is bound to behave the way an autonomous object does in our real world, or like a cell does in an organism. Software objects do not. Therefore, traditional software systems must be planned ahead like in a centrally planned economy while with OP, software systems grow. This fact is traced back to be the root why current software development often fails to meet our expectations when it comes to largescale projects. OP should then be able to provide the means to make software development achieve what other engineering disciplines have achieved a long time ago: that project effort scales sublinearly with size.With OP we introduce a new term because we hope that the approach we are pursuing is radical enough to justify this.
p1080
aVRoles can be assigned to occurrences of variables in programs according to a small number of stereotypical patterns of use. Studies on explicitly teaching roles to novices learning procedural programming have shown that roles are an excellent pedagogical tool for clarifying the structure and meaning of programs and that their use improves students' programming skills. This paper describes the results of an investigation designed to test the understandability and acceptability of the role concept and of the individual roles in novicelevel objectoriented programming.The role set used in procedural programming was found to be suitable for describing variable and attribute behavior in objectoriented programming but the need for some supplementary roles due to the early introduction of linked structures in objectoriented programming was also identified. CS educators had little problems with identifying roles in typical uses of variables. Every role was identified in typical uses of variables by 70 100% accuracy. Subjects' comments on the role concept were mostly positive.
p1081
aVCRCcards are a lightweight approach to collaborative objectoriented modeling. They have been adopted by many educators and trainers to teach early objectoriented design. Reports in the literature are generally positive. So is our own experience. However, over the years, we have noticed many subtle problems and issues that have largely gone unnoticed in the literature.In this paper, we discuss the problems and issues we experienced when teaching CRCcards to novices. Two major sources of problems can be traced back to the CRCcard roleplay. One is the usage of CRCcards as substitutes for actual objects during the scenario roleplay and the other the difficulty to document or trace the scenario roleplay ``on the fly". We propose a new type of diagram to support the roleplay activities and to overcome these problems. Our experience so far is quite positive. Novices have fewer problems with roleplay activities when using these diagrams. Teaching and learning the new type of diagram adds only little overhead to the overall CRCapproach.We also provide general guidelines for CRCcard usage. Although our improvements are aimed at novices, we believe that the proposed diagram is useful even for professional software development.
p1082
aVThis report describes the practice of using executable acceptance testing for specifying programming assignments in software engineering courses. We summarize experiences from two courses introduced in two academic institutions over four semesters   both from students' and instructors' perspectives. Examples of projects and the discussion of the assignment flows are given. The paper highlights testing as an allencompassing activity in software development projects. It also contains recommendations for academics thinking of incorporating executable acceptance testing into their courses.
p1083
aVWe describe an experimental degree program for teaching software development and computer science based on an apprenticeship model with generous borrowings from agile software development methodologies.
p1084
aVAspectoriented software development (AOSD) has a lot of interest in the research community. It has also found early adopters in application development and middleware. This panel discusses the potential expansion and use of AOP into mainstream software development.This question is not just directed to the aspect research community, but also to practicing software development teams and organizations. The panel will explore the appropriate position and awareness of aspectorientation amidst other advances in software engineering; how to prepare organizations for adoption; and what additional research and development is necessary. The panel discussion will help the OO community to understand the appropriate use of aspectorientation. It will also highlight areas where additional efforts by AOSD researchers and professionals are required.
p1085
aVSoftware can kill. What are you doing to stay alive? Our world faces an increasingly hostile environment with challenges in complexity, technology, social engineering and clashing cultures. Failure to achieve sufficient software robustness can lead to customer dissatisfaction, financial loss, or in extreme cases   loss of life. This panel brings together differing contexts and solution approaches.
p1086
aVAny ordinary panel member on an ordinary panel can sit up front and spout the same old stuff. The panelists for this event must be agile in the strictest sense of the word. Why? Because they will be asked to take a random position in response to questions from both the moderators and the audience. They must present their arguments in a timed twominute speech or present a rebuttal of the initial points in the same restricted twominute interval. The assignment to the position will be made by the moderators. Don't miss this one.
p1087
aVStructured Design has been described as a "traditional approach" and an "alternative technology" by the SEI's (Software Engineering Institute) software technology roadmap on their website [July 2005]. While website visitors are cautioned that structured design does not lend itself to object orientation the method has clearly influenced the evolution of objectoriented design practices. This panel brings together software design visionaries to discuss and debate "echoes" in software design "practice".
p1088
aVAs the volume of legacy software grows, how have we grown in our ability to leverage this legacy   or, for that matter, is it worth the effort? Is legacy software a hoard of useful information and behavior   or is it a ball and chain, something you should cut loose if you want to make progress? Legacy constraints often seem immense and burdensome   but, do they always need to be? Is objectoriented legacy software spaghetti code   or is it more like ravioli? Do agile methods embrace or reject the use of the legacy.
p1089
aVComplex structures cannot be built; they must grow. Natural living systems grow and are both the products and creators of the environment; nothing exists in isolation. To create software that can approach the complexity of natural forms, we will have to figure out how to grow our software as well. This begins with changing the way we represent it so that changes are graceful and fundamentally stable. One way to do it, which I am developing, is to represent software as a connected web of relationships among the parts. If this representation can be integrated with its environment  that is, all the information that surrounds the creation of the software, all the more likely the system as a whole will reflect reality and pave the way for automated processes to work together with humans in the same environment in the software garden.
p1090
aVOver the past decade the sheer size and complexity of traditional operating systems have prompted a wave of new approaches to help alleviate the services provided by these operating systems. The emergence of microkernels and a plethora of nontraditional operating system models, both geared toward reducing the role of the OS, attest to the promise of practical alternatives. The problem with these methods is that the threetiered system of software, operating system, and hardware is still preserved. Even though the operating system might find some reprieve by having to handle less work there is a nascent notion being triggered by these alternative approaches that the operating system as an abstract entity is no longer a necessity. We propose a radical method of computing where we take this notion to the extreme and push the operating system into the software and hardware levels. By doing so, we create a decentralized operating system environment known as Dispersed Operating System Computing (DOSC). We outline how the Dispersed Operating System paradigm works, its benefits, and immediate practical applications in today's world.
p1091
aVWe describe (and demonstrate) the execution model of a computing platform where computation is both incremental and datadriven. We call such an approach deltadriven. The platform is intended as a delivery vehicle for semantically integrated software, and thus lends itself to the semantic web, domaindriven development, and nextgeneration software development environments. Execution is transparent, versioned, and persistent. This technology  still at an early stage  is called domain/object.
p1092
aVThis film tells the story of the introduction of furniture on wheels into a very traditional corporate culture. We've all heard of innovative office environments used by design firms, startups, and skunk works, but what about the rest of us? At a 90yearold $80Billion financial institution we attempted to implement and measure the effects of an office environment that would be a logical extension of our existing culture, and would also better support collaborative work and the use of Agile Software Development methodologies. A crossfunctional project team received new office furniture on wheels which allowed more team interaction and fast layout reconfiguration. The economics and corporate culture effects of this move were recorded. The team was surveyed on ergonomics and ability to collaborate six months before and six months after the change, and the results compared with a team that did not receive the change over the same period. While the sample size was too small to imply universal results, it did anecdotally indicate the benefits of continuing and expanding the implementation.
p1093
aVWhile the relations between classes in OO analysis and design have so far been restricted to two general types: aggregation and specialization, this often falls short of correctly describing the real world. The film seeks to redress this shortcoming by postulating the existence of a third kind of relation, called parental inheritance, between classes in strongly typed languages.
p1094
aVLinguistic support for modern programming paradigms has not been welcomed into most of today's mainstream operating systems. Linus Torvalds has decreed that Linux will never again entertain C++, saying "In fact, in Linux we did try C++ once already, back in 1992. It sucks. Trust me, writing kernel code in C++ is a bloody stupid idea" Similarly, Pantelis Antoniou, an embedded PowerPC kernel developer, has captured popular systemssentiment about aspectorientation, "People like to live in denial; thinking that programming shouldn't be this hard right? There must be an easier way, if only those pesky developers followed fashionable_methodology_of_the_day" As a consequence, though a number of systems have been progressively restructuring services to leverage higherlevel paradigms, it is intentionally done without language support. This decoupling of paradigms and language mechanisms appears to suggest that conventional wisdom in the systems community prejudices modern programming methodologies because they may unnecessarily heavyweight manifestations of paradigms, and pollute otherwise elegant and optimized handcrafted C code. Simply put, if it ain't broke, don't fix it. We believe there is a growing body of evidence to suggest that, if the systems community continues to refuse support for a paradigm shift, system evolution will slow down to an unacceptable level. Already, valuable code is not being integrated into systems in a timely fashion because the tools meant to facilitate this can, and often do, impede the process. Simply put, it is broke and we believe we know how to fix it.
p1095
aVOver the past 20 years, user interface designers and usability engineers have studied and refined humancomputer interaction techniques with the goal of improving people's productivity and experience. But the target of these efforts, "the enduser," is fast becoming a thing of the past. Many people now construct software on their own, building artifacts that range from email filters to spreadsheet simulations to interactive web applications. These individuals are usedevelopers: they build ad hoc solutions to everyday computing needs.Will usedevelopers help to resolve the software crisis? Given the right tools, people and groups may be able to rapidly develop custom solutions to many contextspecific computing requirements, eliminating the wait for IT professionals to analyze and engineer a solution. Or are these individuals a danger to society? Usedevelopers are informal programmers with no training in software construction methods or computing paradigms. They have little intrinsic motivation to test their products for even basic concerns like correctness or safety. In this talk I argue that the transformation of enduser to usedeveloper is well underway and discuss the prospects for maximizing the benefits to society while addressing the risks.
p1096
aVAspectoriented programming (AOP) is gaining popularity as a new way of modularising crosscutting concerns. The aspectbench compiler (abc) is a new workbench for AOP research which provides an extensible research framework for both new language features and new compiler optimisations. This poster presents the abc architecture, as well as example uses.
p1097
aVThis poster tells the story of the introduction of furniture on wheels into a very traditional corporate culture. We've all heard of innovative office environments used by design firms, startups, and skunk works, but what about the rest of us? At a 90yearold $80Billion financial institution we attempted to implement and measure the effects of an office environment that would be a logical extension of our existing culture, and would also better support collaborative work and the use of Agile Software Development methodologies. A crossfunctional project team received new office furniture on wheels which allowed more team interaction and fast layout reconfiguration. The economics and corporate culture effects of this move were recorded. The team was surveyed on ergonomics and ability to collaborate six months before and six months after the change, and the results compared with a team that did not receive the change over the same period. While the sample size was too small to imply universal results, it did anecdotally indicate the benefits of continuing and expanding the implementation.
p1098
aVA new field in distributed computing, called Ambient Intelligence, has emerged as a consequence of the increasing availability of wireless devices and the mobile networks they induce. Developing software for such mobile networks is extremely hard in conventional programming languages because of new distribution issues related to volatile network connections, dynamic network topologies and partial failures.
p1099
aVAspectOriented (AO) frameworks improve a frameworkcentered development process by providing appropriate means for handling crosscutting concerns. However, the instantiation process of AO frameworks remains complex and errorprone. We propose a modeling and transformation approach with tool support to assist the instantiation of AO frameworks.
p1100
aVdomain/object is a new software environment in the tradition of dynamic languages like Smalltalk, Lisp and Self. Like its predecessors, domain/object blurs the usual distinctions between tools, languages, operating systems, applications and databases. domain/object also adds some interesting twists to the familiar dynamic paradigm, including spreadsheetstyle "liveness", versioned execution, transactions, full incrementality and transparency.domain/object is intended as a delivery platform for software that requires tight semantic integration between components, such as development tools and nextgeneration applications for the semantic web. Incrementality and liveness obviate the need for standard notification schemes such as Observer, ensuring that data and programs are synchronised automatically. Transactions and versioning allow the granularity and frequency of synchronisation to be adjusted to suit the particular application or user. Transparency means that the full structure of the executing program is available for queries, suggesting a considerably more dynamic realisation of aspectoriented programming.
p1101
aVDirect redeployment of an application from one scenario to another through straightforward refactoring is difficult. Application objects need to be in a form amenable to partitioning. We propose Breakable Objects  BoBs, as a solution. We show how BoBs may be used(BoB Driven Architecture) in an application and how BoBs are favorable to splitting and redeployment.
p1102
aVInfopipes are building blocks for creating data streaming applications, and it is claimed that Infopipes facilitate code reuse. To evaluate this claim, we have built a significant traffic application in Smalltalk using Infopipes. In this paper and accompanying poster, we present a short introduction to Infopipes, a traffic problem and algorithmic solution, and the types of reuse Infopipes facilitate in our implementation of said traffic algorithm.
p1103
aVWe describe CodeQuest, a system for querying source code. It combines two previous proposals, namely the use of logic programming and database system. Experiments (on projects ranging from 3KSLOC to 1300KSLOC) confirm that for this application, a query language based on DataLog strikes the right balance between expressiveness and scalability.
p1104
aVAs models are elevated to firstclass artifacts within the software development lifecycle, the task of constructing and evolving largescale system models becomes a manually intensive effort that can be very time consuming and error prone. To address these problems, this research poster presents a comprehensive approach to model transformation. Within this approach, a highlevel aspectual model transformation language is designed to specify tasks of model construction and evolution, and then a model transformation engine is used to execute transformation specifications in an automated manner. Testing and debugging tools at the modeling level are provided to assist in detecting errors in the model transformation.
p1105
aVWe propose a new approach for developing Business Object Model (BOMs). The approach uses ontologies to unify the representation and integration of knowledge from analysis patterns with different structures.
p1106
aVMartin Fowler was best described by Brian Foote as "an intellectual jackal with good taste in carrion." He's not come up with great languages or tools, built major companies or found academic success. He's an author who has struggled with understanding what good design might be and how to communicate it. His books on patterns, refactoring, UML, and agile development reflect this question and his struggles to find an answer. He hasn't succeeded yet, but is happy to share his current position, lost in a maze of twisty objects, all alike.
p1107
aVThis poster presents a customizable roundtripping UML class diagram plugin for Eclipse called Green. While this tool was developed primarily with students and instructors in introductory OO computer science courses in mind, its extensible architecture makes it potentially useful to others.Green is a flexible tool: each binary class relationship is implemented as a separate plugin to the basic tool. The set of relationships supported is therefore easily tailored to the needs of the user. More importantly, the semantics of the relationships can be defined to suit the user. The tool can therefore adapt to the needs of its users.
p1108
aVWe have implemented an interpreter (InvTS) for a declarative rule language (InvTL) supporting invariantdriven transformations of objectoriented programs. Using a library of rules, it can perform incrementalization across object abstractions, allowing the programmer to write clear, straightforward code while relying on InvTS to generate sophisticated and efficient implementations.
p1109
aVThere is a critical need for approaches to support software testing. Our research exploits the information described at Architectural Patterns to drive the definition of tests. As a result, we intend to assist developers in finding relatively shorter and cheaper paths to high dependable software.
p1110
aVAs software systems grow in size and use more thirdparty libraries and frameworks, the need for developers to understand unfamiliar large codebases is rapidly increasing. In this poster, we present a tool, Relo that supports users' understanding by allowing interactive exploration of code. As the developer explores relationships found in the code, Relo builds and automatically manages a visualization mirroring the developer's mental model, allowing them to group viewed artifacts or use the viewed items to ask the system for further exploration suggestions.
p1111
aVThe objective of this research project is to improve the reusability of objectoriented software. We have introduced anchored exception declarations to allow checked exceptions to be used conveniently in reusable software elements such as design patterns. We are now investigating an innovative inheritance mechanism based on existing inheritance mechanisms like that of Eiffel and on traits. The resulting mechanism should allow a programmer to construct a class from existing components with little effort.
p1112
aVMeta: a collection of metalanguages that augment/unify languagefamilies, an advanced programming environment, a language interoperability framework, an infrastructure for reflection over source documents, and a generalization of XML. XML provides an extensible mechanism for defining syntax, while Meta provides an extensible mechanism for defining syntax and semantics.
p1113
aVFrameworks are important in software development. There are problematic aspects of framework development. When frameworks are extended with functionality implemented by other frameworks, developers face a difficult task solving static and (specially) dynamic mismatches. The dynamic aspect is less visible to developers thus it is usually the cause of failure, specially in frameworks using multi threading programming.This paper shows how to verify the soundness of framework compositions at dynamic level using using temporal logic and tools provided by Full Maude.
p1114
aVApplying refactorings to objectoriented systems usually affects source code and its associated models, involving complex maintenance efforts to keep those artifacts up to date. Most projects abandon design information in the form of models early in the life cycle, as their sustentation becomes extremely expensive. We propose a formal approach to consistently refactor systems in a modeldriven manner. The refactoring applied to the model is linked to a sequence of behaviorpreserving transformations that automatically refactor the underlying source code, based on structural properties from the model that must be implemented by the program. As a consequence, sound program refactoring can be accomplished without developer intervention, based only on the applied model transformations. Also, the refactored source code is consistent with the refactored model. Model information can be additionally used to improve refactoring automation, as more powerful transformations can be mechanized.
p1115
aVModeldriven software product lines combine the abstraction capability of Model Driven Software Development (MDSD) and the variability management capability of Software Product Line Engineering (SPLE). This short contribution motivates the idea of modeldriven software product lines and briefly explains the concepts underlying featurebased model templates, which is a particular technique for modeling software product lines.
p1116
aVJimmy "Jimbo" Wales is the founder of Wikipedia.org, the free encyclopedia project, and Wikicities.com, which extends the social concepts of Wikipedia into new areas. Jimmy was formerly a futures and options trader in Chicago, and currently travels the world evangelizing the success of Wikipedia and the importance of free culture. When not traveling, Jimmy lives in Florida with his wife and daughter.
p1117
aVThis paper describes a new modeldriven development framework, called Modeling Turnpike (or mTurnpike). It allows developers to model and program domainspecific concepts, and to gradually transform them to the final (compilable) source code. By leveraging UML metamodeling and attributeoriented programming, mTurnpike provides an abstraction to represent domainspecific concepts at the modeling and programming layers simultaneously. This paper overviews the design, implementation and performance implications of mTurnpike.
p1118
aVThough software projects can benefit from XP practices, not all projects can directly adopt them. Some practices have to be tailored to contexts specific to the projects. This paper describes the road followed when tailoring XP to R&D projects, in a time scale of 2 years. We describe our major challenges and the way we have solved them.
p1119
aVWe describe a PatternDriven Analysis and Design (PDA) method for developing software systems. PDA promotes the use of patterns throughout the different phases of software development from analysis to implementation.
p1120
aVThe Predictable Assembly from Certifiable Components (PACC) team at the Software Engineering Institute has developed a vehicle to create componentbased systems that have predictable behavior prior to implementation. The project has been used to successfully predict performance and safety properties of realtime systems, but the concepts and technology can be applied to predict other properties (e.g., reliability, security).
p1121
aVAspectOriented Programming (AOP) has been proposed as a complementary modularization technique to the existing objectoriented techniques. AOP encourages the modular development of complex software by providing support for cleanly separating the basic system functionality from its crosscutting concerns. Since, AOP aims to improve the maintainability and reusability of OO software systems, it is necessary to develop case studies that enable us to evaluate: (i) how it can be used in conjunction with existing OO techniques; and (ii) how it can be used to modularize specific software features better than OO techniques. In this work, we present the refactoring of the JUnit objectoriented framework using the AspectJ programming language. The modularization of specific features of the JUnit framework using AspectJ has brought benefits to the understanding and maintainability of the framework, as well as to implement more flexible extensions.
p1122
aVIn MRI research labs, algorithms are typically implemented in MATLAB or IDL. If performance is an issue they are ported to C and integrated with interpreted systems, not fully utilizing objectoriented software development. This paper presents Scopira, an open source C++ framework suitable for MRI data analysis and visualization.
p1123
aVCode clones in software increase maintenance cost and lower software quality. We have devised a new algorithm to detect duplicated parts of source code in large software. Our algorithm is adequate for large systems and detecting not only the exact but also similar parts of source code. Our simulation of this new algorithm, namely SDD (Similar Data Detection), indicates that it can detect duplicated parts of source code in huge software with high performance.
p1124
aVIn this paper, we describe the evolution of architecture of Mail Transfer Agents(MTA). We consider the design of MTA as a sequence of design decisions [10]. Many of the design decisions are examples of security patterns. Thus, this paper is another example of how patterns generate architecture [5].
p1125
aVThis paper introduces an approach to knowledge representation and processing based on representing information that changes during program execution using metamodel instances. In this way both runtime data and program code can be dynamically adapted to the problem at hand (in traditional software only data is designed to change at runtime). The details of the proposed approach and some prototype systems are presented and discussed.
p1126
aVI have stolen my title from the title of a paper given by Marvin Minsky in the 1960s, because it most effectively expresses what I will try to convey in this talk.We have been programming universal computers for about 50 years. Programming provides us with new tools to express ourselves. We now have intellectual tools to describe "how to" as well as "what is." This is a profound transformation: it is a revolution in the way we think and in the way we express what we think.For example, one often hears a student or teacher complain that the student knows the "theory" of some subject but cannot effectively solve problems. We should not be surprised: the student has no formal way to learn technique. We expect the student to learn to solve problems by an inefficient process: the student watches the teacher solve a few problems, hoping to abstract the general procedures from the teacher's behavior on particular examples. The student is never given any instructions on how to abstract from examples, nor is the student given any language for expressing what has been learned. It is hard to learn what one cannot express. But now we can express it!Expressing methodology in a computer language forces it to be unambiguous and computationally effective. The task of formulating a method as a computerexecutable program and debugging that program is a powerful exercise in the learning process. The programmer expresses his/her poorly understood or sloppily formulated idea in a precise way, so that it becomes clear what is poorly understood or sloppily formulated. Also, once formalized procedurally, a mathematical idea becomes a tool that can be used directly to compute results.I will defend this viewpoint with examples and demonstrations from electrical engineering and from classical mechanics.
p1127
aVAlthough orchestration represents a key component of Service Oriented Architectures (SOAs), few adopters of service orientation actually use it. In spite of available standards and products, aggressive marketing has forced many users to jump on the SOA bandwagon illprepared to use the technology. We are developing a pattern language to bring the state of the practice closer to the state of the art. These patterns will help SOA designers and product evaluators make informed decisions when building SOAs or evaluating orchestration middleware.
p1128
aVThe Squawk virtual machine is a small Java(TM) VM written in Java that runs without an OS on small devices. Squawk implements an isolate mechanism allowing applications to be reified. Multiple isolates can run in the one VM, and isolates can be migrated between different instances of the VM.
p1129
aVThis research focuses on using frameworks, modeldriven development, and aspectoriented software development techniques to address key configuration and deployment concerns of componentbased distributed realtime and embedded (DRE) systems. System designers and deployers can use these techniques to configure quality of service (QoS) aspects of their systems and finetune their systems during the design and runtime phases to ensure their systems meet endtoend performance requirements.
p1130
aVMulticodes have been demonstrated to provide performance gains of up to 25 percent. Present multicode identification techniques rely on frequency of occurrence and sequence length alone. This research extends previous work by presenting a multicode identification algorithm based on the frequency of occurrence and the optimization potential of the sequence.
p1131
aVWe describe the results of visualizing object oriented programs errors, and utilizing these results in the design of a set of visual queries on the runtime execution history of a program. We bring together under one coherent framework different approaches such as error classification, bug patterns in object oriented programs, and visual queries. Our work is founded on a novel interactive visualization system for Java called JIVE, developed at Buffalo.
p1132
aVRuby  an ObjectOriented scripting language  is used worldwide because of its ease of use. However, the current interpreter is slow. To solve this problem, some virtual machines were developed, but none with adequate performance or functionality. To fill this gap, I have developed a Ruby interpreter called YARV (Yet Another Ruby VM). YARV is based on a stack machine architecture and features optimizations for high speed execution of Ruby programs. In this poster, I introduce the Ruby programming language, discuss certain characteristics of Ruby from the aspect of a Ruby interpreter implementer, and explain methods of implementation and optimization. Benchmark results are given at the end.
p1133
aVTwo experimental tools to support usagecentered design using essential use cases and canonical abstract prototypes are described. The models and methods of usagecentered design are outlined and the new tools are described briefly.
p1134
aVThis demonstration will present a new approach, based on the Dependency Structure Matrix (DSM), which uses intermodule dependencies to specify and manage the architecture of software systems. The system is decomposed into a hierarchy of subsystems with the dependencies between the subsystems presented in the form of an adjacency matrix. The matrix representation is concise, intuitive and appears to overcome scaling problems that are commonly associated with directed graph representations. It also permits succinct definition of design rules to specify allowable dependencies.A tool, Lattix LDM, will be used to demonstrate this approach by loading actual open source Java applications to create DSMs that can represent systems with thousands of classes. We will show how algorithms can be applied to organize the matrix in a form that reflects the architecture and highlights problematic dependencies.We will demonstrate how design rules can be used to specify and enforce architectural patterns such as layering and componentization. We will examine the evolution of architecture by creating dependency models for successive generations of Ant, a popular Java utility. Finally, we will explore the application of this approach to the reengineering of Haystack, an information retrieval system.
p1135
aVIo is small prototypebased programming language. The ideas in Io are mostly inspired by Smalltalk[1] (all values are objects), Self[2] (prototypebased), NewtonScript[3] (differential inheritance), Act1[4] (actors and futures for concurrency), LISP[5] (code is a runtime inspectable / modifiable tree) and Lua[6] (small, embeddable).Io offers a more flexible language with more scalable concurrency in a smaller, simpler package than traditional languages and is well suited for use as both scripting and embedding within larger projects. Io is implemented in C and it's actor based concurrency model is built on coroutines and asynchronous i/o. It supports exceptions, incremental garbage collection and weak links. Io has bindings for many multiplatform libraries including Sockets, OpenGL, FreeType, PortAudio and others as well as some modules for transparent distributed objects and a user interface toolkit written in Io. This presentation will include an overview of the language and demos of some multiplatform desktop applications written with it.
p1136
aVThis demonstration presents an overview of a customizable roundtripping UML class diagram plugin for Eclipse called Green. While this tool was developed primarily with students and instructors in introductory OO computer science courses in mind, its extensible architecture makes it potentially useful to others.Green is a flexible tool: each binary class relationship is implemented as a separate plugin to the basic tool. The set of relationships supported is therefore easily tailored to the needs of the user. More importantly, the semantics of the relationships can be defined to suit the user. The tool can therefore grow with a student/developer as their needs change.
p1137
aVUnderlying Croquet is an objectoriented semantics based on active objects that have the capability of temporal reflection. That is, each object is aware and in direct control of its behavior in time. Croquet also directly supports replication of computation, allowing computation to be moved close to the point of interaction on demand, while maintaining a consistent view of behaviors that can scale to include thousands of nodes. In the talk, we'll highlight the main concepts of TeaTime, which provides that semantic model, and also talk about some of the interesting implementation issues involved in realizing the TeaTime semantics.
p1138
aVWe present refactorings that automate the process of migrating pregenerics Java programs to use generics. The task is divided in two parts: introduction of formal type parameters (parameterization) and inference of actual type parameters (instantiation). We developed efficient techniques and tools to assist developers in both parts. We will present them during the demonstration.
p1139
aVDesigning componentbased application that meets performance requirements remains a challenging problem, and usually requires a prototype to be constructed to benchmark performance. Building a custom benchmark suite is however costly and tedious. This demonstration illustrates an approach for generating customized componentbased benchmark applications using a Model Driven Architecture (MDA) approach. All the platform related plumbing and basic performance testing routines are encapsulated in MDA generation "cartridges" along with default implementations of testing logic. We will show how to use a tailored version of the UML 2.0 Testing Profile to model a customized load testing client. The performance configuration (such as transaction mix and spiking simulations) can also be modeled using the UML model. Executing the generated deployable code will collect the performance testing data automatically. The tool implementation is based on a widely used open source MDA framework AndroMDA. We extended it by providing a cartridge for a performance testing tailored version of the UML 2.0 Testing Profile. Essentially, we use OObased metamodeling in designing and implementing a lightweight performance testing domain specific language with supporting infrastructure on top of the existing UML testing standard.
p1140
aVDomainspecific languages (DSLs) use idioms that are closer to the abstractions found in a specific problem domain. Tool support for testing and debugging DSLs is lacking when compared to the capabilities provided for standard general purpose languages (GPLs). In fact, support for debugging and testing a program written in a DSL is often nonexistent. A common approach for implementing DSLs is to create a preprocessor that translates the DSL source into an objectoriented GPL, such as Java or C++. A DSL grammar serves as the primary artifact for defining DSLs from a higher level of abstraction. This demonstration is focused on a grammardriven technique to build a testing tool from existing DSL grammars. The DSL grammars are used to generate the hooks needed to interface with a supporting infrastructure written for Eclipse that assists in testing and debugging a program written in a DSL.This demonstration will introduce a framework to automate the generation of DSL testing tools (e.g., debugger and unit test engine) for imperative and declarative DSLs. Our framework provides Eclipse plugins for defining DSLs, along with a translator and interface generator that maps the DSL debugging/testing perspective to the underlying GPL debugging/testing services. This demonstration will provide evidence to support the feasibility and applicability of using the information derived from DSL grammars and existing software components to support enduser debugging and testing in a domain friendly programming environment.
p1141
aVWe perform knowledge discovery in software archives in order to detect refactorings on the level of classes and methods. Our REFVIS prototype finds these refactorings in CVS repositories and relates them to transactions. Additionally, REFVIS relates movements of methods to the class inheritance hierarchy of the analyzed project. REFVIS creates visualizations that show these refactorings in two different layouts and uses colorcoding to distinguish between different kinds of refactorings. Moreover, our visualizations are interactive as they can be zoomed, scrolled and filtered; mouseovertooltips allow to examine details of the particular refactoring on demand.
p1142
aVThe use of design patterns in mainstream have been suffered from the lack of an open and flexible supporting tool. To alleviate the problem, we have developed a tool called the PatternStudio. It is entirely based on open technologies, uses a standard serialization format, and can be extended to make further enhancement.
p1143
aVOur goal is to enable rapid production of static and dynamic object models from natural language description of problems. Rapid modeling is achieved through automation of analysis tasks. This automation captures the cognitive schemes analysts use to build their models of the world through the use of a precise methodology. The methodology is based on the use of proposed technique called role posets, and a seminatural language (called 4W). First versions of this tool were used as prototypes to produce early design artifacts for very small (toy) problems. Current version has been successfully used as an educational tool in object oriented software engineering courses. We present the tool with its new complete features and results of its application in learning process. Original problem statements are automatically translated to 4W language. The produced sentences then, are analyzed with role posets to produce static model views. Finally the 4W sentences are used to generate dynamic views of the problem. This set of methods maximizes analysis process agility, promotes reusability and constitutes a valuable tool in the learning process of object thinking. The prototype tool: GOOAL (Graphic Object Oriented Analysis Laboratory) receives a natural language (NL) description of a problem and produces the object models taking decisions sentence by sentence. The user realizes the consequences of the analysis of every sentence in real time.
p1144
aVIn this paper we describe AutAT, an open source Eclipse plugin to better enable test driven development of web applications. AutAT lets nontechnical people write acceptance tests (or functional tests) using a userfriendly graphical editor, and convert this visual representation of the tests into executable tests.
p1145
aVYour program fails. What is the cause of this failure? In this demo, we present two delta debugging plugins for the Eclipse environment which isolate failure causes in the program history and in the program's execution.
p1146
aVThe eROSE plugin for ECLIPSE uses version archives to guide programmers: You changed fKeys[] and eROSE suggests you to change initDefaults(), because both items always have been changed together in the past. In addition, eROSE points out item coupling that is undetectable by program analysis, for instance between fKeys[] and a properties file. All eROSE needs is a CVS repository; we designed eROSE to be as efficient and unobtrusive as possible.
p1147
aVIt is a sign of maturity for any given engineering discipline when we can name, study, and apply the patterns relevant to that domain. In civil engineering, chemical engineering, mechanical engineering, electrical engineering, and now even genomic engineering, there exist libraries of common patterns that have proven themselves useful in practice. Unfortunately, no such architectural reference yet exists for softwareintensive systems. Although the patterns community has pioneered the vocabulary of design patterns through the work of the Hillside Group and the Gang of Four, our industry has no parallel to the architecture handbooks found in more mature design disciplines.Following the work of Bruce Anderson, who over a decade ago conducted a series of workshops at OOPSLA, I've begun an effort to fill this void in software engineering by codifying a the architecture of a large collection of interesting softwareintensive systems, presenting them in a manner that exposes their essential patterns and that permits comparison across domains and architectural styles.In this presentation, we'll examine the nature of architectural patterns and the process of conducting architectural digs to harvest them, and then examine a few of the systems studied thus far.
p1148
aVThis paper, along with the accompanying demonstration, describes an objectoriented framework for highperformance computing. The original system [1] was developed with Common Object Request Broker Architecture (Corba) [2] objects to implement the framework interfaces to codedependent methods. To add new codes to the initial system required the implementation of new Corba interface objects. Globus Grid objects [3] have extended that original system, and have resulted in added flexibility in the design for the addition of new codes. Highperformance computing agent services were also implemented as Globus Grid objects, resulting in enhanced communications between the framework user interface services and the agents responsible for executing the codes. The management functions for remote agents have also been improved.
p1149
aVWe introduce and demonstrate "TableCode," a new programming environment based on an extended metamodeling approach to define an objectoriented program in databases. TableCode defines the object in databases, then extends this definition (1) "vertically" by relating the object to the application model and the domain model that the object is a part of; and (2) "horizontally" by adding more definition and description to the object model (as well as to the application and the domain model) in order to fully define the object and the environment which surrounds it. This will enable a unified approach to apply AspectOriented techniques and allow domain experts to change their software in accordance with their dynamics needs.
p1150
aVWe have proposed and implemented, and will demonstrate, the language CoJava, which offers both the advantages of simulationlike process modeling, and the capabilities of true decision optimization.By design, the syntax of CoJava is identical to the programming language Java, extended with special constructs to (1) make a nondeterministic choice of a numeric value, (2) assert a constraint, and (3) designate a program variable as the objective to be optmized.A CoJava program thus defines a set of nondeterministic execution paths, each being a program run with specific selection of values in the choice statements. The semantics of CoJava interprets a program as an optimal nondeterministic execution path, namely, a path that (1) satisfies the range conditions in the choice statements, (2) satisfies the assertconstraint statements, and (3) produces the optimal value in a designated program variable, among all execution paths that satisfy (1) and (2). Thus, to run a CoJava program amounts to first finding an optimal execution path, and then procedurally executing it.To find an optimal nondeterministic execution path, we have developed a reduction to a standard constraint optimization formulation. Constraint variables represent values in program variables that can be created at any state of a nondeterministic execution. Constraints encode transitions, triggered by CoJava statements, from one program state to the next.Based on the reduction, we have developed a CoJava constraint compiler. The compiler operates by first translating the Java program into a similar Java program in which the primitive numeric operators and data types are replaced by symbolic constraint operators and data types. This intermediate java program functions as a constraint generator. This program is compiled and executed to produce a symbolic decision problem. The decision problem is then submitted to an external optimization solver.
p1151
aVA new field in distributed computing, called Ambient Intelligence, has emerged as a consequence of the increasing availability of wireless devices and the mobile networks they induce. Developing software for such mobile networks is extremely hard in conventional programming languages because the network is dynamically defined. We demonstrate a new distributed programming language named AmbientTalk, a research vehicle to experiment with expressive highlevel language constructs for AmbientOriented Programming.
p1152
aVAs software systems grow in size and use more thirdparty libraries and frameworks, the need for developers to understand unfamiliar large codebases is rapidly increasing. In this demonstration, we present a tool, Relo that supports users' understanding by allowing interactive exploration of code. As the developer explores relationships found in the code, Relo builds and automatically manages a visualization mirroring the developer's mental model, allowing them to group viewed artifacts or use the viewed items to ask the system for further exploration suggestions.
p1153
aVFeaturebased model templates have been proposed as a technique for modeling software product lines. We describe a set of tools supporting the technique, namely a feature model editor and feature configurator, and a modeltemplate editor, processor, and verifier.
p1154
aVWe demonstrate the execution model of a computing platform where computation is both incremental and datadriven. We call such an approach deltadriven. The platform is intended as a delivery vehicle for semantically integrated software, and thus lends itself to the semantic web, domaindriven development, and nextgeneration software development environments. Execution is transparent, versioned, and persistent. This technology  still at an early stage  is called domain/object.
p1155
aVWe intend to investigate the problem of parameter optimisation for application servers, to evaluate the application of evolutionary algorithms to the problem domain and finally to allow for autonomous optimisation of applications server configuration under varying performance contexts.
p1156
aVBoth model and program refactorings are usually proposed in an ad hoc way because it is difficult to prove that they are sound with respect to a formal semantics. Even developers using refactoring tools have to rely on compilation and tests to guarantee the semantics preservation, which may not be satisfactory to critical software development. This work proposes a set of primitive structural semanticspreserving transformations for Alloy, a formal objectoriented modeling language. We use the Prototype Verification System (PVS), which contains a formal specification language and a theorem prover, to specify and prove the soundness of the transformations. Although our transformations are primitive, we can compose them in order to derive coarse grained transformations, such as a model refactoring to introduce a design pattern into a model. Moreover, proposing refactorings in this way can not only facilitate design, but also improve the quality of model refactoring tools.
p1157
aVCan virtual machine developers benefit from religiously observing the principles more often embraced for exploratory programming? To find out, we are concurrently constructing two artifacts a Self VM entirely in Self (the Klein VM), and a specialized development environment with strict adherence to pure objectorientation, metacircularity, heavy code reuse, reactivity, and mirrorbased reflection. Although neither artifact is yet complete, the environment supports many remote debugging and incremental update operations, and the exported virtual machine has successfully run the whole compiler.As a result of our adherence to these principles, there have been both positive and negative consequences. We have been able to find and exploit many opportunities for parsimony. For example, the very same code creates objects in the bootstrap image, builds objects in the running VM, and implements a remote debugger. On the other hand, we have been forced to expend effort to optimize the performance of the environment. Overall, this approach trades off the performance of the environment against the architectural simplicity and ease of development of the resulting VM artifact. As computers continue to improve in performance, we believe that this approach will increase in value.
p1158
aVDomainspecific languages (DSLs) assist an enduser in writing a software program using idioms that are closer to the abstractions found in a specific problem domain. Language tool support for DSLs is lacking when compared to the capabilities provided for standard objectoriented general purpose languages (GPLs). For example, support for debugging and testing a program written in a DSL is often nonexistent. This poster abstract introduces an investigation into a grammardriven technique to build a framework to generate DSL testing tools (e.g., debugger and test engine). This research demonstrates the feasibility and applicability of using information derived from DSL grammars and existing software components to support enduser debugging and testing in a domain friendly environment.
p1159
aVWe propose a new application area for grammar inference which intends to make domainspecific language development easier and finds a second application in renovation tools for legacy systems. We use the genetic programming approach for grammatical inference and propose the use of frequent sequences, syntax graphs and incremental construction of grammars in order to be able to infer a more comprehensive set of contextfree grammars.
p1160
aVDomainspecific modeling (DSM) raises the level of abstraction by specifying a metamodel that is aligned to a particular problem domain. A key feature of DSM is the construction of model interpreters that synthesize the domain models into software artifacts. In the presence of new stakeholder requirements, it is possible that a metamodel undergoes numerous changes during periods of evolution (a new instance of the classic schema evolution problem). Consequently, there is a fundamental problem in keeping the model interpreters up to date with metamodel changes. This research abstract outlines the technical challenges in formalizing model interpreter implementation with the intent to facilitate interpreter evolution in terms of metamodel migration.
p1161
aVIt is possible that the performance of Composite Web Services can be improved if it is split into its elemental parts. This paper proposes that an agent will be involved to act for the user in the splitting of the CWS. This paper also presents an overview of how the performance improvement will be measured and how the service will be split.
p1162
aVDesign patterns are applied in software development to decouple individual concerns, so that a change in a design decision is isolated to one location of the code base. However, multidimensional concerns exist in software development and some concerns are even mutually exclusive. Therefore, no single design pattern offers a panacea toward addressing problems of change evolution. By analyzing the matrix of concerns during the software development process, this abstract describes a paradigm for twodimensional separation of concerns based on pattern transformation. In particular, it shows an example to transform code back and forth between an objectoriented implementation of the Inheritance pattern and an aspectoriented implementation of the Visitor pattern. The approach allows the same software to be evolved along different dimensions, enabling developers to choose the most appropriate dimension for a given task.
p1163
aVThis research focuses on developing a systematic modularization approach to decompose systems into "stable" modules that can evolve over time while preserving their structure. We highlight the underlying concept of the approach, and provide a brief overview of its main activities.
p1164
aVThis research involves empirical software engineering studies applied in academic and professional settings to assess the influence of testdriven development on software quality. Particular focus is given to internal software design quality. Pedagogical implications are also examined. Initial results and the study protocol and plans will be presented.
p1165
aVThis paper presents a separation of concerns approach to solve the tangling problem of functional and Quality of Service (QoS) concerns in traditional Componentbased Software Engineering (CBSE) and Software Product Line (SPL) technologies applied to Distributed Realtime and Embedded (DRE) systems. This problem originates from the interchangeability for fulfilling functional and QoS concerns during composition. The approach utilizes the perspective of QoS to design and analyze a set of software systems represented by a collection of QoS systemic paths, which determine how well functional tasks perform in terms of flows of applicationspecific and functionalitydetermined information between components. Our approach not only reserves the virtues of reusability, changeability, productivity and expeditiousness that traditional CBSE and SPL technologies possess, but also dedicates the contributions in terms of separation of concerns, design space exploration, finegrained commonality and reusability evaluation and less subjective feasibility analyses for a componentbased SPL.
p1166
aVWhile distributed code writing is becoming widespread, objectoriented software design still requires facetoface interaction, curbing the potential and quality of global software development. Most designers reject general purpose conferencing tools for not meeting their needs, and featurerich distributed CASE tools for being too formal. Our longterm goal is to develop effective tools for distributed software design that preserve natural working styles.A necessary first step is to identify the unique lowlevel characteristics of design meetings which must be mimicked in the virtual world. Our work embarks on this path with a detailed ethnographic study of two collocated design meetings. We present several observations and their implications for the design of collaboration tools.
p1167
aVFrameworks and libraries change their APIs during evolution. Migrating an application to the new API is tedious and disrupts the development process. Although some tools and techniques have been proposed to solve the evolution of APIs, most updates are done manually. Our goal is to reduce the burden of reuse on maintenance by reducing the cost of adapting to change. We studied the API changes of three frameworks and one library and discovered that over 80% of the changes the break existing applications are refactorings. This suggests that refactoringbased migration tools should be used to effectively update applications. We propose a methodology to automatically and safely update componentbased applications with no overhead on the component producers.
p1168
aVAn acceptability envelope is a region of imperfect but acceptable software systems surrounding a given perfect system. Explicitly targeting the acceptability envelope during development (rather than attempting to minimize the number of errors, as is the current practice) has several potential benefits. Specifically, leaving acceptable errors in the system eliminates the risks and costs associated with attempting to repair the errors; investing fewer resources in less critical regions of the program and more resources in more critical regions may increase acceptability and reduce the overall investment of development resources.To realize these benefits, the acceptability envelope must be both sizable and accessible. We present several case studies that explore the acceptability envelopes of the Pine email client and the SurePlayer MPEG decoder. These studies show that both Pine and SurePlayer can tolerate the addition of many offbyone errors without producing unacceptable behavior. This result suggests that current systems may be overengineered in the sense that they can tolerate many more errors than they currently contain.Our SurePlayer case study also shows that SurePlayer has unforgiving regions of code that must be close to perfect for the system to function at all. To effectively exploit the acceptability envelope, developers must be able to distinguish forgiving and unforgiving regions so that they can appropriately prioritize their development effort. In SurePlayer, the unforgiving regions occur in code that uses metadata to parse the input stream; the forgiving regions tend to access the data within each image. This result suggests that developers may be able to use relatively simple indicators to effectively prioritize their development effort.
p1169
aVInteroperability and loose coupling requirements are pushing next generation of distributed applications towards more decentralized and more dynamic interaction schemes, which the classic request/response communication paradigm can hardly accommodate. Hence, sound foundations and mechanisms for the establishment of unanticipated peertopeer interactions across organizational boundaries are of significant importance to upcoming middleware platforms. The Executable Choreography Framework (ECF) is a middlewarelevel framework that targets dynamic and decentralized service compositions. The ECF combines transparent context propagation with aspectoriented software composition techniques to dynamically refine the default control and data flow of service invocations. The framework provides a ground for experimentation with dynamic and distributed workflows, and a base to assess their safety and applicability when deployed across organizational boundaries.
p1170
aVFrameworks and libraries change their APIs during evolution. Migrating an application to the new API is tedious and disrupts the development process. Although some tools and techniques have been proposed to solve the evolution of APIs, most updates are done manually. Our goal is to reduce the burden of reuse on maintenance by reducing the cost of adapting to change. We studied the API changes of three frameworks and one library and discovered that over 80% of the changes the break existing applications are refactorings. This suggests that refactoringbased migration tools should be used to effectively update applications. We propose a methodology to automatically and safely update componentbased applications with no overhead on the component producers.
p1171
aVThis research involves empirical software engineering studies applied in academic and professional settings to assess the influence of testdriven development on software quality. Particular focus is given to internal software design quality. Pedagogical implications are also examined. Initial results and the study protocol and plans will be presented.
p1172
aVWe propose a new application area for grammar inference which intends to make domainspecific language development easier and finds a second application in renovation tools for legacy systems. We use the genetic programming approach for grammatical inference and propose the use of frequent sequences, syntax graphs and incremental construction of grammars in order to be able to infer a more comprehensive set of contextfree grammars.
p1173
aVPerformance of modern computers is tied closely to the effective use of cache because of the continually increasing speed discrepancy between processors and main memory. Optimum system performance is achieved when software and hardware work symbiotically to increase performance. This work focuses on identifying locality in objectoriented systems and developing techniques for approaching optimum performance with respect to the memory hierarchy.
p1174
aVCurrently, the most adopted criterion to invoke garbage collection is heap space exhaustion. In other words, garbage collection is invoked when the heap space (either the entire space or generational space) runs out. A possible alternative but much more difficult approach is to invoke garbage collection when good garbage collection efficiencies can be obtained. To do so, we need to know number of garbage objects that can be collected before the collection actually takes place.In this short abstract, we introduce a predictive approach that can provide an accurate estimation of the number of dead objects at any specific point of execution. The proposed estimation model relies on the information obtained from partial reference counting. Our plan is to use this information as a criterion to invoke garbage collection. We have conducted a preliminary study to determine the feasibility of this idea and found that the model is sufficiently accurate in three SPECjvm98 benchmark applications.
p1175
aVExorcism is mainly thought of as the rite of driving out the Devil and his demons from possessed persons. This text is about the same process except here the target is a legacy software system. The target system was a major component based system having been developed over 7 years by 30 to 60 people continuously under a classic plan driven approach. The Pareto Principle, or 80/20 rule as it is often called, is used as the framework to prioritize activities in a major reengineering initiative on the system from limited resources. The initiative's main focus was to increase the developer productivity in the maintenance project in the system by 25 percent. Typical agile practices were the inspiration for many of the changes implemented through the project.A measurement program is presented for validating success, and the XRadar open source tool is used for measuring the program. In one year, the productivity increase was above 30 percent. There seems to be a high correlation between productivity and the implementation of the agile practices such as short iterations, daily standupmeetings and pair programming as substitutes with the practice of a formal QA regime. During the same period the error proneness of the system decreased with several magnitudes and our definition of the internal software quality increased by 22 percent. Hence, based on our measurements, the increased productivity was not substituted by lower quality in the system  on the contrary.
p1176
aVIt is well documented that software product cost estimates are notoriously inaccurate across the software industry. Creating accurate cost estimates for software product development projects early in the product development lifecycle has always been a challenge for the industry. This article describes how a large multiteam software engineering organization (over 450 engineers) estimates project cost accurately and early in the software development lifecycle using Use Case Points, and the process of evaluating metrics to ensure the accuracy of the model.The engineering teams of Agilis Solutions in partnership with FPT Software, provide our customers with accurate estimates for software product projects early in the product lifecycle. The bases for these estimates are initial definitions of Use Cases, given point factors and modified for technical and environmental factors according to the Use Case Point method defined within the Rational Unified Process. After applying the process across hundreds of sizable (60 manmonths average) software projects, we have demonstrated metrics that prove an estimating accuracy of less than 9% deviation from actual to estimated cost on 95% of our projects. Our process and this success factor is documented over a period of five years, and across more than 200 projects.
p1177
aVAround 350 BCE, Aristotle set down in Poetics an understanding of narrative forms, based upon notions of the nature and intricate relations of various elements of dramatic structure and causation. Drama relied upon human performers to represent action. With computers, interactive forms and simulation inherit much of its dramatic structure from traditional narrative forms, but authorship is more explicitly shared among designers, engineers, and interactors. I and others have proposed extensions of the fundamental elements of Aristotle's Poetics to understand these new narrative forms. Ubiquitous computing is a horse of a different color. Today's new blends of sensors, networks, computation, and space create contexts for novel interactive narrative forms. When we embed what Rob Tow calls "perceptionrepresentationaction loops" in objects and spaces, we enter a realm that I call designed animism. What new forms of narrative and experience may emerge from such systems? How do we understand them in terms of structure, causality, narrative, and experience? What are the poetics of this newly animistic world? And, does it have a soul.
p1178
aVThe development of reliable software is a challenging task, especially in a business environment that forces developers to focus on meeting tight deadlines instead of producing quality software. Researchers and practitioners are exploring various approaches for addressing this problem, such as autonomic computing and conscientious autopoietic software. These approaches describe software systems that are capable of managing and preserving themselves. In this paper, we propose a new, concrete selfmanaging software architecture based on the biological concept of commensalistic symbiosis and the notion of autopoietic software. We present a detailed description of our architecture, and a working prototype of a minimal commensalistic system. In addition, we specify a new programming language, examine usage scenarios and discuss implementation issues for realizing a working commensalistic system on a larger scale.
p1179
aVAdding to and restructuring the design of a large existing code base is known to be a difficult and time consuming task. There are methods for assisting these challenges, such as composition refactoring, reengineering, and antipatterns detection methods. Most of these methods concentrate on specific perspectives of software engineering.This practical paper presents the Composition Refactoring Triangle (CRT) unified approach for handling multiple changes across complex environments. The CRT is a combination of: the process (CRP), the management tool (CRMT), and the external and internal refactoring elements. The CRT was constructed during ongoing need to implement major changes within a living product. This paper contains the "Via Delarosa"  The "Path of Suffering" which describes the road of how the CRT was created, and why other methods failed.Practical evaluation was conducted using the CRT, demonstrating its capabilities. The unified perspectives of the CRT enable improved risk analysis and technical control over multiple architectural evolution changes and their relative dependencies. Its implementation encourages quick testing procedure, code correctness, and short timetomarket response of the development team.
p1180
aVEngenio made the transition to software product line practice in order to keep pace with growing business demand for its products. By using an incremental transition strategy, Engenio avoided the typical upfront adoption barrier  the equivalent development effort of 2 to 3 standalone products  which in their case was projected to be 900 to 1350 developermonths. Engenio discovered that by making an upfront investment of only 4 developermonths, they were able to start a chain reaction in which the tactical and strategic incremental returns quickly outpaced the incremental investments, making the transition pay for itself.
p1181
aVIn this paper, we present an approach that we have used to address security when running projects according to agile principles. Misuse stories have been added to user stories to capture malicious use of the application. Furthermore, misuse stories have been implemented as automated tests (unit tests, acceptance tests) in order to perform security regression testing. Penetration testing, system hardening and securing deployment have been started in early iterations of the project.
p1182
aVIn this paper we present the experience gained and lessons learned when the IT department at Statoil ASA, a large Oil and Gas company in Norway, extended their Enterprise Architecture with strategic level DomainDriven design techniques and used the extended Enterprise Architecture to improve the software architecture of a large enterprise system.Traditionally, Enterprise Architecture has been prescribed as the key tool to conquer complexity and align IT development with business priorities and strategies, but we found our Enterprise Architecture too coarse to be practical useful at the software level.By extending our Enterprise Architecture with context maps and the process of context mapping valuable insight was gained, insight that enabled better scoping of new projects and architectural improvement of existing software in a controlled way.In addition, use of responsibility layers combined with context maps reduces the perceived complexity of the architecture. Use of other techniques such as distillation and identification of the core domain looks promising at the tactical level of a single project, but its value is more uncertain at the strategic level.The key issue is that large enterprise systems do not have a single core. On the other hand, at the project level, there should always be a core, and the project is best of by knowing its core domain and aim its best resources to work with the core.
p1183
aVSystem builders have historically used informal software architecture models to understand options, make choices, and communicate with others. Research into software architecture over the past fifteen years has indicated that more precise architecture models may be beneficial. At a large financial firm, we applied precise software architecture techniques on four software projects and this experience has revealed a number of practical issues. We made the following observations across the projects: 1) Architecture models can be used to bridge gaps between business requirements and technology, 2) A small collection of techniques and a detail knob are practical and useful in a variety of projects, 3) Architecture modeling techniques amplify the skills of the architects, 4) A model of domain concepts and relationships is helpful when building architecture models, and 5) It is difficult to know when to stop adding detail to your architecture model. We believe that these observations motivate future research and can help practitioners make software architecture more effective in practice.
p1184
aVPurchasing a CommercialOffTheShelf (COTS) package solution can be a complex and daunting task. Selecting and evaluating the right candidate is difficult, especially when the solution aims at the heart of company business. The company's competitive edge must be maintained, while at the same time ensuring the intended goals such as reduced costs and better functional coverage. A good Enterprise Architecture should be a prime tool when evaluating several solutions against the company's needs.In this paper we will recount the experience and lessons learned when we evaluated three COTS systems to replace a set of legacy oil trading and operations systems. Based on weaknesses in our Enterprise Architecture, we applied strategic domaindriven design principles to extend our Enterprise Architecture during the evaluation. We found that these techniques enabled us to thoroughly analyse our domain with the domain experts and provide answers based on tacit domain knowledge, without going through the cost and effort of performing a fullscale architectural analysis. At the same time, the tacit domain knowledge became explicit and shared, easing the communication with various stakeholders.
p1185
aVThough ObjectOriented Analysis, Design, and languages have become the dominant practices in many, or most, domains of software engineering, concerns about complexity, size, and performance in the embedded, realtime software domain have led to a prevalent view that OO technology is not suitable for the domain. We challenge this view through a successful application of OOA, OOD, and C++ (including STL) in the embedded, realtime flight software in an Earthorbiting science instrument named Aquarius (see [1]). We've found that OOA and OOD with UML actually enhance communication with systems and hardware engineers. We also found that C++, thoughtfully used, need not lead to code bloat, and that its performance is every bit as good as that of C. We begin with an overview of the requirements and describe our overall use of UML modeling, followed by a discussion of the use of UML for ObjectOriented Analysis with use cases. Then the application of UML for highlevel and detailed design, the use of frameworks supporting a component architecture and multiplatform execution, and code generation from UML detailed design are described. We also present the use of UML for organizing and designing and documenting our verification and test environment and scenarios, and using HTML, generated by our UML tool, for all documentation and for requirement traceability. Finally, we discuss the use of C++ as the implementation language, and give an overview of status and work metrics.
p1186
aVDue to globalization and especially with China becoming the world's production center, global trade is strongly increasing. The vast majority of all goods are shipped in oceangoing containers. Secure Trade Lane (STL) is a solution for making container shipments more predictable and more secure. The solution comprises an embedded controller that is mounted on oceangoing containers, and a sophisticated backend communicating with the embedded controller and integrating all trading partners. Both components are tightly linked and couldn't exist in isolation. In this paper we outline the rationale of technologies being used in this Sensor Network solution, and describe endtoend architecture and key components. We share the key lessons learned and take a look at the future evolution of asset tracking and monitoring, in particular in transportation industries.
p1187
aVThis paper details the application of Software Product Lines (SPL)16 and ModelDriven Engineering (MDE)15 to the software defined radio domain. More specifically it is an experience report emphasizing the synergy 17 resulting from combining MDE and SPL technologies. The software defined radio domain has very unique characteristics as its systems typically are a confluence of a number of typically challenging aspects of software development. To name a few, these systems are usually described by modifiers such as, embedded, realtime, distributed, objectoriented, portable, heterogeneous, multithreaded, high performance, dynamic, resourceconstrained, safetycritical, secure, networked, component based and faulttolerant. Each one of these modifiers by themselves carries with it a set of unique challenges, but building systems characterized by all of these modifiers all at the same time makes for a daunting task in software development. In addition to all of these, it is quite common in these embedded systems for components to have multiple implementations that must run on disparate processing elements. With all of this taken into account, it stands to reason that these systems could and should benefit greatly from advances in software technology such as product line engineering, domainspecific modeling and modeldriven engineering. It is our experience that one big benefit to the software development industry is the combination of the Software Product Lines and Model Driven Engineering technologies.
p1188
aVThe NetBeans Platform is the opensource NetBeans Integrated Development Environment (IDE) less those modules that make it a development tool. The platform is a "generic application"  a runtime which can be used to develop applications.One of the key distinctions of software built upon the NetBeans Platform is modularity  reuse in the large. Such software is designed as logical sets of macrocomponents which integrate through welldefined API contracts. Writing modular applications brings some enhancements to programming in standard Java  particularly the ability to have Java classes which are only public to other classes within the archive they reside in. This has a number of beneficial effects on development  in particular, the ability to develop cleaner, simpler APIs by being able to fully conceal implementation from foreign code, while retaining typesafety.This workshop will cover developing on the NetBeans Platform from the groundup. We will start with nonGUI applications  simply making use of the module system  the core runtime of the NetBeans platform. After that, we will cover basic patterns and commonly used APIs. The afternoon will be spent on developing a real software application which consists of multiple, decoupled modules; participants are encouraged to come with specific ideas for applications they would be interested in exploring in a modular environment.
p1189
aVObjectoriented programming has worked quite well  so far. What are the objects, how do they relate to each other? Once we clarified these questions we typically feel confident to design and implement even the most complex systems. However, objects can deceive us. They can lure us into a false sense of understanding. The metaphor of objects can go too far by making us try to create objects that are too much inspired by the real world. This is a serious problem, as a resulting system may be significantly more complex than it would have to be, or worse, will not work at all. We postulate the notion of an antiobject as a kind of object that appears to essentially do the opposite of what we generally think the object should be doing. As a Gedankenexperiment antiobjects allow us to literally think outside the proverbial box or, in this case outside the object. This article discusses two examples, a Pacman game and a soccer simulation where antiobjects are employed as part of a game AI called Collaborative Diffusion. In CollaborativeDiffusion based soccer the player and grass tile agents are antiobjects. Counter to the intuition of most programmers the grass tile agents, on top of which all the players are moving, are doing the vast majority of the computation, while the soccer player agents are doing almost no computation. This article illustrates that this role reversal is not only a different way to look at objects but, for instance, in the case with Collaborative Diffusion, is simple to implement, incremental in nature and more robust than traditional approaches.
p1190
aVThis paper describes the experience of evolving a domainspecific language embedded in Java over several generations of a test framework. We describe how the framework changed from a library of classes to an embedded language. We describe the lessons we have learned from this experience for framework developers and language designers.
p1191
aVChanging the approach to dynamic memory allocation in a large legacy application is challenging. In order to improve the robustness of memory allocation, we fundamentally changed it. We replaced standard heap allocation with classspecific heaps. We were able to do it with almost no changes to the existing class code by overriding the C++ new() and delete() operators, and using templates creatively to insert the changes into class hierarchies.The results have been very positive. Misuse of dynamic memory has been detected and errors caused by memory misuse have been avoided. Performance of the new memory management code has been as good or better as the previous code. Additional capabilities such as audits have been added to further increase the robustness of dynamic memory usage.
p1192
aVJava's interface construct allows for a clear distinction between subtype polymorphism based on a shared interface and code reuse based on class extension or inheritance. Design Patterns argues that in an objectoriented setting, programming should be done to an interface, not to an implementation, that class inheritance is a mechanism for code reuse rather than for subtyping and polymorphism, and that even composition should be favored over class inheritance. We conclude from this that interface should be introduced prior to and given more focus than classes and class inheritance. We survey 27 textbooks from major publishers to show that very few available texts teach Java this way. We propose an alternative ordering of material that will promote the principles mentioned above.
p1193
aVWith the advent of the objectsfirst approach for introductory programming, instructors are challenged to think differently regarding the projects and exercises they create for their classrooms. The objectsfirst approach reduces the emphasis on syntax and encourages the student to focus upon the proper construction and use of classes. This change in emphasis means that students must understand the relationships between classes within a code solution and how such relationships affect the overall design of a system. Unfortunately, such critical thinking exercises can prove challenging to the introductory student, especially if presented in an manner. In this paper, the authors examine how fundamental principles such as inheritance, composition, and association can be conveyed to introductory programming students within a collaborative virtual environment. The examples chosen follow established guidelines for objectsfirst examples while leveraging features of an engaging, threedimensional interactive environment.
p1194
aVA grammar is suitable for topdown recursive descent parsing if it is LL(1) [1]. Deriving the algorithm to test if a grammar is LL(1) usually involves inspection of a BNF version of the grammar, representing certain adjacency relations as matrices, and finally performing some subtle computations on the matrices. It is difficult to present the problem intuitively enough so that the algorithm can be discovered by students.oops [2,3] is an LL(1) parser generator that represents an EBNFbased grammar as a tree of objects that are observed as they recognize input. The objects are also able to test if the grammar is LL(1). Distributing the algorithm over several classes makes it simple enough to be discovered during a classroom presentation.This paper discusses the new architecture of oops and the LL(1) checking and parsing algorithms and defines some extensions to EBNF which simplify some sticky language definition problems but are very simple to implement using inheritance.
p1195
aVExposing students to the process of programming is merely implied but not explicitly addressed in texts on programming which appear to deal with 'program' as a noun rather than as a verb.We present a set of principles and techniques as well as an informal but systematic process of decomposing a programming problem. Two examples are used to demonstrate the application of process and techniques.The process is a carefully downscaled version of a full and rich software engineering process particularly suited for novices learning objectoriented programming. In using it, we hope to achieve two things: to help novice programmers learn faster and better while at the same time laying the foundation for a more thorough treatment of the aspects of software engineering.
p1196
aVStudents in a masterslevel objectoriented design class participated in several exercises related to improving a notyetpublished textbook in objectoriented design. Each student was asked, sometime during the semester, to improve an explanation from the book, to make up an example of a concept described in the book, and to create an exercise for a chapter in the book. Their contributions were peerreviewed by other members of the class. A strong majority of the students (29 of 49) indicated that they learned a lot from doing the assignments, and 27 of 49 of them agreed or strongly agreed that the assignments were enjoyablea good measure of their engagement. The textbook author found the feedback very helpful in revising the textbook.
p1197
aVTesting is an important part of the software development cycle that should be covered throughout the computer science curriculum. However, for students to truly learn the value of testing, they need to benefit from writing test cases for their own software.We report on our initial experiences teaching students to write test cases and evaluating studentwritten test suites, with an emphasis on our observation that, without proper incentive to write test cases early, many students will complete the programming assignment first and then add the build of their test cases afterwards. Based on these experiences, we propose new mechanisms to provide better incentives for students to write their test cases early.We also report on some of the limitations of code coverage as a tool for evaluating test suites, and finally conclude with a survey of related work on introducing testing into the undergraduate curriculum.
p1198
aVUse cases are one of the most common mechanisms for describing and analyzing software system requirements. Due to use of natural language in use case descriptions, it is often assumed that they are easy to understand for stakeholders involved in the software development process [8]. However some authors argue [17,18] that the most common pitfalls of use cases written by professionals is that the customer does not understand them. In this paper we would like to consider whether it is appropriate to take their understandability to nontechnically minded stakeholders for granted.We have analyzed 88 use cases written by finalyear undergraduate computer science students for an assignment at the University of Hertfordshire and point out ways in which a computingbased mindset or way of thinking infiltrates the use cases, possibly making them difficult to understand for nontechnical partners and hindering or preempting design decisions. We suggest that the problems we observed among students indicate a need for adding a new rule and/or guideline when teaching students to write good quality use cases, which is to ensure as far as possible that their use cases are free from computingbased structures and vocabulary. We also suggest ways in which students studying technical aspects of computer science might be led into practicing the use of nontechnical language.
p1199
aVWhile objectoriented programming and highperformance databases are now mainstream, programmers continue to struggle with persistent storage of objects. Juggling object persistence with requirements for simplicity, flexibility, maintainability, transparency, scalability and fivenines uptime can rattle even the most hardened architect.It has been 10 years since the last panel on objects and databases at OOPSLA. Solutions are still evolving rapidly and increasing in complexity, with no end in sight. At the same time developers continue to experiment with alternatives, both old and new. This panel will discuss the state of the union between objects and databases, as seen from the trenches, with focus on current trends in objectoriented databases and objectrelational mapping.
p1200
aVThe Web is maturing as a rich application development platform, and efforts are being made to provide richer and more dynamic interactions using JavaScript. JavaScriptbased Web applications such as Google Maps have gained extra attention because they can be easily included in HTML for reuse. Unfortunately, various technical hurdles have made it difficult for JavaScript reuse to extend beyond its current state. Furthermore, JavaScript reuse is still out of reach for a large portion of the Web user base unversed in the use of programming languages.In this paper, we dive deeper into our previous work on the JavaScript Dataflow Architecture (JDA). JDA is intended for Web client applications written using HTML and JavaScript. We discuss the ways in which the architecture addresses many of the hurdles that modern Web client applications face in the realm of largescale reuse and remixing. JDA aims to provide an ecosystem comprised of black box components operating within a JavaScriptbased asynchronous messagepassing environment. The environment allows you to use simple HTML to assemble Web applications from JavaScript black boxes scattered around the World Wide Web. No programming skill is required in their assembly, and no plugins or applets are required for their execution. Furthermore, the architecture extends the black box metaphor beyond the boundaries of JavaScript and allows multiple JavaScript components contained within an HTML file to be reused as a whole.A detailed account of an early prototype is discussed, and research is being done to improve it. JDA suggests that largescale reuse and arbitrary remixing of Web applications can be realized using currently existing technologies.
p1201
aVHow can the ultra large systems (ULS) of the future be built if they will have the complexity of trillions of lines of code, maintain continuous 24x7 operations with no downtime, and live in a hostile environment with unpredictably changing requirements? This panel will discuss and debate the challenges posed by ultra large systems in terms of their design, growth, deployment and dynamics.
p1202
aVThis panel will bring together the surviving authors (Erich Gamma, Richard Helm, and Ralph Johnson) of the book Design Patterns: Elements of Reusable ObjectOriented Software (AddisonWesley) and it is dedicated to the memory of the fourth author John Vlissides. The discussion will focus on the beginnings of their collaboration that led to the book and a look forward to the future.
p1203
aVAspectOriented Programming (AOP) and AspectOriented Software Development (AOSD) endeavor to aid programmers in the separation of concerns, specifically crosscutting concerns, as an advance in modularization. AOP does so using primarily language changes, while AOSD uses a combination of language, environment, and methodology. But the concepts of obliviousnessnot universally accepted as part of AOPand parameterization appear to contradict the wellestablished principles of modularity and encapsulation that David Parnas and other greats of the past laid out and on which software engineering has depended for the last 40 years. Are we moving forward with better understandings of software engineering, modularity, and design/development principles, or are we losing our way? This debate is the postscript to Friedrich Steinmann's OOPSLA Essay, "The Paradoxical Success of AspectOriented Programming.
p1204
aVAgile software development practices including XP and Scrum have risen to prominence within the software engineering community over the past ten years. Are agile software development practices converging? Are some practices becoming more integrated and/or more widely adopted than others? In the early 90s there was a convergence of objectoriented design methodologies  is a similar pattern being repeated within the agile software development community? Several years ago conferences featured debates on the number of practices inherent to XP  or for that matter what constituted XP. Is the Agile community on the verge of converging to standardization or do individual practices retain their individually and evangelists/disciples? A somewhat related question is: Can an agile practice be applied out of the box or is some assembly required? What does it take to get agility going in an organization? Does it work as advertised? What practices work and play well with others? Hear the experiences of panelists in their attempts to actually make agile work in the real world. From Crystal, DSDM, FDD, LEAN, Scrum, to XP (and others)  participants will to share their perspectives and experiences. Be warned  this fishbowl will be stocked with piranhas.
p1205
aVOOPSLA attendees have traditionally looked to the panel program for redmeat relief from a diet of highfiber technical track material and abstruse Onward! exotica. However, panels often feature a tired parade of the usual suspects: the same faces hawking the same hackneyed twentieth century ideas over and over again. At the same time, familiar names are a predicable draw. The effect is that it may take many years for new blood to find its way into the panel program.This panel seeks to break this logjam by featuring only panelists at or under the age of 0x20. They will offer their opinions and insights as to what we have been doing wrong for the last twenty years and what they will have to do fix it. Grudging recognition of what has gone right may be offered as well.Mechanisms to guarantee the anonymity of the most brash positions will allow the participants to be provocative as well as precocious.
p1206
aVThe PyPy project seeks to prove both on a research and a practical level the feasibility of constructing a virtual machine (VM) for a dynamic language in a dynamic language  in this case, Python. The aim is to translate (i.e. compile) the VM to arbitrary target environments, ranging in level from C/Posix to Smalltalk/Squeak via Java and CLI/.NET, while still being of reasonable efficiency within these environments.A key tool to achieve this goal is the systematic reuse of the Python language as a system programming language at various levels of our architecture and translation process. For each level, we design a corresponding type system and apply a generic type inference engine  for example, the garbage collector is written in a style that manipulates simulated pointer and address objects, and when translated to C these operations become Clevel pointer and address instructions.
p1207
aVScripting languages are ubiquitous in modern software engineering and are often used as the sole language for application development. However, some applications, specifically scientific and multimedia applications, often have small sections of code that require a higher level of performance than the host language can deliver. In many cases, the algorithm being optimized is simple and has a clear mapping to hardware resources. But, without introducing an intermediate language, developers generally have no direct methods to implement an optimized solution.In this paper, we present the synthetic programming environment, a runtime system for synthesizing and executing highperformance instruction sequences directly from scripting languages. Our implementation, available for download, is implemented in Python for PowerPC processors and gives Python developers direct access to system resources for performance critical code. We discuss strategies for creating and managing synthetic programs and provide two realworld examples, an interactive particle system and a chemical fingerprint comparison tool.
p1208
aVAs scripts grow into fullfledged applications, programmers should want to port portions of their programs from scripting languages to languages with sound and rich type systems. This form of interlanguage migration ensures typesafety and provides minimal guarantees for reuse in other applications, too.In this paper, we present a framework for expressing this form of interlanguage migration. Given a program that consists of modules in the untyped lambda calculus, we prove that rewriting one of them in a simply typed lambda calculus produces an equivalent program and adds the expected amount of type safety, i.e., code in typed modules can't go wrong. To ensure these guarantees, the migration process infers constraints from the statically typed module and imposes them on the dynamically typed modules in the form of behavioral contracts.
p1209
aVOffering secure and anonymous communications in mobile ad hoc networking environments is essential to achieve confidence and privacy, thus promoting widespread adoption of this kind of networks. In addition, some minimum performance levels must be achieved for any solution to be practical and become widely adopted. In this paper, we propose and implement HOP, a novel solution based on cryptographic Host Identity Protocol (HIP) that offers security and userlevel anonymity in MANET environments while maintaining good performance levels. In particular, we introduce enhancements to the authentication process to achieve Host Identity Tag (HIT) relationship anonymity, along with source/destination HIT anonymity when combined with multihoming. Afterward we detail how we integrate our improved version of HIP with the OLSR routing protocol to achieve efficient support for pseudonyms. We implemented our proposal in an experimental testbed, and the results obtained show that performance levels achieved are quite good, and that the integration with OLSR is achieved with a low overhead.
p1210
aVA significant body of research in ubiquitous computing deals with mobile networks, i.e. networks of mobile devices interconnected by wireless communication links. Due to the very nature of such mobile networks, addressing and communicating with remote objects is significantly more difficult than in their fixed counterparts. This paper reconsiders the remote object reference concept  one of the most fundamental programming ions of distributed programming languages  in the context of mobile networks. We describe four desirable characteristics of remote References in mobile networks, show how existing remote object References fail to exhibit them, and subsequently propose ambient references: remote object references designed for mobile networks.
p1211
aVMore than five years ago, the OMG proposed the Model Driven Architecture (MDA\u2122) approach to deal with the separation of platform dependent and independent aspects in information systems. Since then, the initial idea of MDA evolved and Model Driven Engineering (MDE) is being increasingly promoted to handle separation and combination of various kinds of concerns in software or data engineering. MDE is more general than the set of standards and practices recommended by the OMG's MDA proposal. In MDE the concept of model designates not only OMG models but a lot of other artifacts like XML documents, Java programs, RDBMS data, etc. Today we observe another evolutionary step. A convergence between MDE and DSL (Domain Specific Language) engineering is rapidly appearing. In the same way as MDE is a generalization of MDA, the DSL engineering may be viewed as a generalization of MDE. One of the goals of this paper is to explore the potential of this important evolution of engineering practices. In order to anchor the discussion on practical grounds, we present a set of typical problems that could be solved by classical (objectoriented and others), MDE, or DSLbased techniques. Solutions to these problems will be based on current platforms (EMF, AMMA, GME, etc.). This paper illustrates how powerful modelbased frameworks, allowing to use and build a variety of DSLs, may help to solve complex problems in a more efficient way.
p1212
aVThe workshop on LibraryCentric Software Design (LCSD) is a forum for researchers and practitioners to share original work concerning the design, implementation, and evaluation of software libraries. We report on the LCSD'06 workshop at OOPSLA'06.
p1213
aVThe Eclipse platform (http://www.eclipse.org) is designed for building integrated development environments (IDEs) for objectoriented application development. Building on the success of the Eclipse Technology eXchange workshops at OOPSLA 2003, 2004, and 2005, we invite original papers that describe potential new uses of Eclipse and how the core Eclipse technology can be leveraged, improved and/or extended for research and teaching projects. Accepted papers will be presented at the workshop. Due to the popularity of this workshop in the past, this year's ETX will be a 1.5 day event. Workshop topics include (but are not limited to) the use of Eclipse for: IDEs, supporting the software development process, debugging or testing, design requirements/specification, modeling environments or frameworks, aspectoriented programming, program analysis and transformation, such as for refactoring, optimization, or obfuscation, computerbased learning, software engineering education, teaching foundations of objectoriented programming courseware, teaching an introductory undergraduate programming course, web service applications, rich client application.
p1214
aVThis workshop follows a history of successful workshops at OOPSLA around topics in software engineering for pervasive systems. It is known to spawn fruitful and lively discussions from a variety of participants from industry and academia.Sensor networks and Pervasive Computing solutions offer the potential to significantly change business processes, across all industries. Computing and connectivity gets woven into everyday life. Technologies in this space are constantly changing. New wireless protocols, operating systems, and new applications are emerging. Architectures are very highly distributed and often influenced by physical characteristics. The embedded functionality needs to get complemented with corresponding backend components and services. This workshop will bring together practitioners who have actively been involved in the development of sensor networks and pervasive computing solutions, researchers who have been working in this area, and professionals who have been involved in the definition of standards. The goals are (1) to identify recurring architecture themes and patterns, (2) to raise issues and identify gaps for implementation that need to be resolved; and (3) to discuss new ideas and changes to object technology to better support Sensor Networks and Pervasive Computing Topics of interest include but are not limited to the theory and practice of pervasive sensor networks, emerging standards, development methods, security and privacy, architecture, infrastructure, middleware, quality of service, energy efficiency, adaptability and learning, energy conservation, peertopeer systems, mobile agents, integration of heterogeneous components, application case studies, experience reports, adaptation and learning, alternative programming models such as Aspect and and AmbientOriented Programming, modeling, simulation, verification and testing.
p1215
aVModelbased testing of systems has long been a goal of the testing research community, and many paradigms have been proposed and developed. However, while modelbased testing approaches are successfully used in industry, they have yet to be adopted as mainstream practices. This workshop explored the impact of objectorientation on modelbased testing, as it affects both the creation of test generation tools, and the application of tools to objectoriented systems. Our primary purpose was to identify the key challenges to widespread use of modelbased testing approaches for testing object oriented software, and to determine possible ways that objectorientation (and related approaches) may be helpful in overcoming these challenges.
p1216
aVDomainSpecific Modeling raises the level of ion beyond programming by specifying the solution directly using visual models to express domain concepts. In many cases, final products can be generated automatically from these highlevel specifications. This automation is possible because both the language and generators fit the requirements of only one domain. This paper introduces DomainSpecific Modeling and describes the related workshop.
p1217
aVRecently, analysts and vendors have been promoting event driven architecture as the "next big thing". Simple eventdriven processing has been in common use for at least ten years with technology such as messageoriented middleware and, in the past few years, messagedriven Enterprise JavaBeans. However, the next big thing to which they refer is the architecture that supports processing complex business events to address realtime business needs. Recognizing the importance of this topic, a oneday workshop was held in conjunction with OOPSLA 06 to address issues related to EDA on Sunday October 22, 2006, at the Oregon Conference Center.The workshop provides a forum in which researchers and practitioners from academia and industry can discuss various aspects of eventdriven processing and its relationship to other architecture and programming paradigms, particularly with serviceoriented architecture.Topics to include (but are not limited to): definition and classification of events, event modeling, tooling for event processing, methods for developing event driven systems and applications, event processing languages and toolsEvent driven business process management, event processing for business activity management, relationship with serviceoriented architecture and other architecture, information delivery services and implementation of 'publish/subscribe' protocols, event pattern detection, implementation issues, potential inhibitors to the adoption of EDA, security and privacy issues in eventbased systems and application.The primary expected outcomes of this workshop are: 1. Crossfertilization between researchers and practitioners in all aspects of event processing; 2. A report on the discussions of the workshop which, based on the quality of the outcome, can be published as a technical paper.
p1218
aVMany systems of the future will be of ultralarge size on one or many dimensions  number of lines of code; number of people employing the system for different purposes; amount of data stored, accessed, manipulated, and refined; number of connections and interdependencies among software components; number of hardware elements to which they interface. They will be ultralargescale (ULS) systems. Is the software community ready to tackle ULS systems? Will incremental changes in our current software development and management practices be sufficient? In fact, the characteristics of ULS systems, already evident in some of today's largest systems, imply changes in the fundamental assumptions that underlie today's software engineering approaches. The gaps are strategic, not tactical. Issues that are not significant at smaller scales become significant at ultralarge scales. Our current practices and more fundamentally the way we define our discipline are unlikely to scale to the size and levels of complexity of ULS systems. A new multidisciplinary perspective and breakthrough research are needed. We must begin to take a more expansive view of software research and include its interactions with associated research in the physical and social sciences. This talk is based on the results of a yearlong study on ULS systems, documented in UltraLargeScale Systems: The Software Challenge of the Future (ISBN 0978695607).
p1219
aVThe "Killer Examples" series of workshops are highly interactive workshops whose goals are to bring together educators and developers to share their respective design pattern and objectoriented expertise, and to provide a forum for discussion of techniques for presenting these examples and the design patterns they showcase to students.These workshops have been an annual occurrence at OOPSLA since 2002, in Seattle. The theme of the workshop is "killer examples" for design patterns; to earn the title "killer" an example must provide clear and overwhelmingly compelling motivation for design pattern use.While there is a formal application procedure to guarantee admission to the workshop, we do accept walkins if space permits and the walkins are determined to have adequate interest and background in the workshop theme to be able to contribute positively to the discussions.
p1220
aVWhat are some of the common practices for taking new ideas and converting them into products? What are the key obstacles that cause the failure of research prototypes turned into commercial products? This workshop examines the practices that have worked well and the approaches that can help developers and managers avoid customer problems.
p1221
aVInformative Workspace is one of the new practices launched in the second edition of "Extreme Programming Explained" [1]. To follow this practice, an agile team builds feedback mechanisms around them that support them in their daily work. These feedback mechanisms can take the form of visual displays, Information Radiators [2], which are manually updated by the team, or electronic eXtreme Feedback Devices [3] (XFD), such as lava lamps or audio signals linked to automated processes. It is vital to ensure that feedback mechanisms are easy to interpret, low maintenance and adapted to local working practices. This workshop will gather and share experiences on how to develop and maintain an Informative Workspace.
p1222
aVThis workshop is a continuation of a highly successful series of workshops at OOPSLA05, OOPSLA04 and OOPSLA03. The main goal is to identify, discuss and promote best practices to properly engineer SOA's and webservices.
p1223
aVThe NetBeans Platform is the opensource NetBeans Integrated Development Environment (IDE) less those modules that make it a development tool. The platform is a "generic application"  a runtime which can be used to develop applications.One of the key distinctions of software built upon the NetBeans Platform is modularity  reuse in the large. Such software is designed as logical sets of macrocomponents which integrate through welldefined API contracts. Writing modular applications brings some enhancements to programming in standard Java  particularly the ability to have Java classes which are only public to other classes within the archive they reside in. This has a number of beneficial effects on development  in particular, the ability to develop cleaner, simpler APIs by being able to fully conceal implementation from foreign code, while retaining typesafety.This workshop will cover developing on the NetBeans Platform from the groundup. We will start with nonGUI applications  simply making use of the module system  the core runtime of the NetBeans platform. After that, we will cover basic patterns and commonly used APIs. The afternoon will be spent on developing a real software application which consists of multiple, decoupled modules; participants are encouraged to come with specific ideas for applications they would be interested in exploring in a modular environment.
p1224
aVScale changes everything. The trend in the design and development of softwareintensive systems today is toward scale that increases in every measurable way. Lines of code, complexity, dependency, communication, bandwidth, memory, datasets, and many other measures for our systems continue to reach and exceed the limits of our ability to produce highquality systems for all purposes.These systems will be unbounded, integrating internetscale resources. They will serve diverse stakeholders with competing objectives and at the same time be constrained by policy, regulation, and the behaviors of their users. The lines between development, acquisition, and operations will blur: ULS systems will not die; they will be too large to be replaced and will be inextricably connected to the daytoday mission. Rather, they will continue to evolve over time with behavior often more emergent than planned. Because complete specifications will not be achievable, sufficient assurance will have to do. ULS systems present "wicked problems," ones for which each attempt to create a solution changes the problem. Some of these characteristics appear in conventional systems, but in ULS systems they will dominate.
p1225
aVDomainspecific Modeling raises the level of abstraction beyond programming by specifying the solution directly using visual models to express domain concepts. In many cases, final products can be generated automatically from these highlevel specifications. This automation is possible because both the language and generators fit the requirements of only one domain. This paper introduces DomainSpecific Modeling and describes the related workshop.
p1226
aVThe "Killer Examples" series of workshops are highly interactive workshops whose goals are to bring together educators and developers to share their respective design pattern and objectoriented expertise, and to provide a forum for discussion of techniques for presenting these examples and the design patterns they showcase to students.These workshops have been an annual occurrence at OOPSLA since 2002, in Seattle. The theme of the workshop is "killer examples" for design patterns; to earn the title "killer" an example must provide clear and overwhelmingly compelling motivation for design pattern use.This poster will describe the results of this year's workshop. See "Killer Examples" for Design Patterns: The fifth annual OOPSLA "Killer Examples" workshop, elsewhere in the Conference Companion, for more information.
p1227
aVAs data persistence is very poorly supported by current programming systems, we have initiated a research project to improve this situation. The result is the new programming language Persistent Active Oberon, which directly institutionalizes persistence as a fundamental concept and liberates the programmer from writing complicated code for database interactions.
p1228
aVA common practice for rapid prototyping of an objectoriented program analysis is to define a lightweight fragment of Java, that is sufficiently small to facilitate a rigorous analysis of key properties. Such a lightweight fragment lacks important Java features, thus the experimental evaluation on realworld code is not easy. The solution is either to extend the prototype to the whole Java or to rewrite the realworld code in the lightweight language. We propose an intermediate solution through CoreJava, an expressionoriented core calculus of Java and a comprehensive set of translation rules from Java to CoreJava. The translation can be guided by the specific requirements of each program analysis. We have built an implementation of our framework and have used it for two different analyses on Java programs.
p1229
aVI gave a talk at the 1998 OOPSLA called "Growing a Language" (sometimes remembered as "the wordsofonesyllable talk") in which I suggested that programming languages have become such complex artifacts that they cannot be designed all at once; rather, they must grow over time. Therefore, programming language designers should plan for such growth, along with the growth of a user community. Moreover, language growth may be more effective if the growing user community can participate.This raises an interesting technical question: how might the explicit goal of planning for growth over time affect the design of a programming language?.The Fortress programming language project at Sun Microsystems Laboratories has three principal design goals: to promote the expression and use of multithreaded parallel algorithms; to support a large variety of syntactic notations, including (as far as possible) standard mathematical notation as used to specify scientific computations; and to allow the language to grow and to be extended by the user community. (This work has been funded in part by DARPA through their program for High Productivity Computing Systems.The Fortress design team has followed a key strategic design principle: whenever we consider adding a feature to the language, we ask whether it might better be provided by a library routine, written in Fortress source code, so that it can be modified, extended, or replaced by others. To this end, we have incorporated rather elaborate mechanisms for encapsulation and ion, including composable components and a parameterized polymorphic objectoriented type system. The result is that we have a fairly complicated language for library writers that enables them to write libraries that present a relatively simple set of interfaces to the application programmer. Thus Fortress is as much a framework for language developers as it is a language for coding scientific applications. We find ourselves using objects and traits extensively within the libraries in order to present to the programmer a scientific application language that is formula and arrayoriented.
p1230
aVThe Eclipse plugin HAM identifies potential aspects in large programs. It analyzes the program's history and obtains sets of function calls that are likely to be crosscutting. Later during programming, HAM informs the programmer when she is about to extend or change such a problematic concern.
p1231
aVIn this poster, we address the fact that development involves both the desire to achieve a shared vision and a willingness to deal with the parameters that constrain improvement. autochthony is an evolving empirical framework for assessing organizations and building patternsbased roadmaps toward their vision of the future.
p1232
aVThe cornerstone of objectoriented programming is the representation of data as a set of objects. In all of the widelyadopted languages that claim to support objectoriented programming, however, the lifetime of an object is bound by the lifetime of the process that instantiated it. In real applications, the lifetime of data is almost never related to the lifetime of the process that created it. This impedance mismatch necessitates a great deal of repetitive, errorprone labor. A true objectoriented design language must be a persistent language; in other words, the lifetime of an object must be independent of the lifetime of the process.Many persistent languages have been developed in research settings. Most of these languages, however, have attempted to maintain backwards compatibility with some previous, nonpersistent language, such as Modula3 or Java. Glib, on the other hand, is a programming language designed from the outset to support object persistence. I propose that Glib's constructs are simpler and more powerful than those of its predecessors, and now that I have an OOPLSA poster displaying those constructs, you can judge for yourself.
p1233
aVIn this paper, we present a sizing algorithm using resonant model based on a proposed novel heap structure  multiple spaces in heap. Experiments using the algorithm and selected GC method show that, in average, the performance overhead from managing multispaces in heap can be reduced from 8.38% to 4.25% when CPU utilization of server is 40% and from 42.42% to 3.52% when CPU utilization of the server is 70%.
p1234
aVIn systemfamily approaches, product configuration is the activity of selecting the features desired for a software product. Although this process is typically collaborative this aspect has long been neglected. On the other hand, enabling collaborative product configuration brings new and challenging problems such as the proper coordination of configuration decisions. This paper introduces a framework for collaborative configuration that addresses the major issues that arise in this context. Some aspects of the framework can be customized to accommodate specific collaboration goals.
p1235
aVWe describe JunGL, a language to script refactoring transformations. It manipulates a graph representation of the program, including extensible semantic information such as variable binding and dataflow. JunGL enables the full automation of complex refactorings: finding program elements of interest, checking preconditions and performing the transformation itself.
p1236
aVJavana is a tool for creating customized Java program analysis tools. It comes with an easytouse instrumentation framework that enables programmers to develop profiling tools that crosscut the Java application, the Java Virtual Machine (JVM) and the native execution layers. The goal of this poster is to demonstrate the power of Javana, using object lifetime computation as an example.Object lifetime has proven to be useful for analyzing and optimizing the behavior of Java applications. Computing object lifetime is conceptually simple, however, in practice it is often challenging. The JVM needs to be adjusted in numerous ways in order to track all possible accesses to all objects, including accesses that occur through the Java Native Interface (JNI), the standard class libraries, and the JVM implementation itself. Capturing all object accesses through manual instrumentation requires an indepth understanding of the JVM and its libraries. We show that using Javana is both easier and more accurate than manual instrumentation.
p1237
aVWe are interested in finding new ways to visualize our software execution traces. An issue in visualizing our execution traces is deploying and integrating them into users' environments. We have a tool called VET3D that transforms execution traces into visualizations over the web. Our tool will help developers to understand the structure and behaviour of software.
p1238
aVDynamic invariant detection, the automatic recovery of partial program specifications by inferring likely constraints from program executions, has been successful in the context of procedural programs. The implementation for dynamic invariant detection examines only the declared type of a variable, lacking many details in the context of objectoriented programs. This paper shows how this technique can be extended to detect invariants of objectoriented programs in the presence of polymorphism by examining the runtime type of a polymorphic variable, which may have different declared and runtime types. We demonstrate the improved accuracy of the dynamically detected specification on two realworld examples: the Money example from the JUnit testing framework tutorial, and a database query engine model example, which we adopted from a commercial database application. Polymorphic constraints in both cases are shown to reveal the specification of the runtime behavior of the systems.
p1239
aVOOPS (ObjectOriented Parallel System) is a framework designed to support programming of concurrent scientific applications for parallel execution. The high level abstractions provided by OOPS free the programmer from dealing with many parallel implementation details, such as the ones found in handcoded MPI programs. However, for performance reasons, parallelism is not completely hidden. The use of the classes supplied by OOPS simplifies the implementation of parallel applications, without introducing significant overhead.
p1240
aVIn lieu of a traditional , I've tried to distill the essence of the talk into a collection of maxims:All programmers are API designers. Good programs are modular, and intermodular boundaries define APIs. Good modules get reused.APIs can be among your greatest assets or liabilities. Good APIs create longterm customers; bad ones create longterm support nightmares.Public APIs, like diamonds, are forever. You have one chance to get it right so give it your best.APIs should be easy to use and hard to misuse. It should be easy to do simple things; possible to do complex things; and impossible, or at least difficult, to do wrong things.APIs should be selfdocumenting: It should rarely require documentation to read code written to a good API. In fact, it should rarely require documentation to write it.When designing an API, first gather requirements  with a healthy degree of skepticism. People often provide solutions; it's your job to ferret out the underlying problems and find the best solutions.Structure requirements as usecases: they are the yardstick against which you'll measure your API.Early drafts of APIs should be short, typically one page with class and method signatures and oneline descriptions. This makes it easy to restructure the API when you don't get it right the first time.Code the usecases against your API before you implement it, even before you specify it properly. This will save you from implementing, or even specifying, a fundamentally broken API.Maintain the code for usescases as the API evolves. Not only will this protect you from rude surprises, but the resulting code will become the examples for the API, the basis for tutorials and tests.Example code should be exemplary. If an API is used widely, its examples will be the archetypes for thousands of programs. Any mistakes will come back to haunt you a thousand fold.You can't please everyone so aim to displease everyone equally. Most APIs are overconstrained.Expect APIdesign mistakes due to failures of imagination. You can't reasonably hope to imagine everything that everyone will do with an API, or how it will interact with every other part of a system.API design is not a solitary activity. Show your design to as many people as you can, and take their feedback seriously. Possibilities that elude your imagination may be clear to others.Avoid fixed limits on input sizes. They limit usefulness and hasten obsolescence.If it's hard to find good names, go back to the drawing board. Don't be afraid to split or merge an API, or embed it in a more general setting. If names start falling into place, you're on the right track.Names matter. Strive for intelligibility, consistency, and symmetry. Every API is a little language, and people must learn to read and write it. If you get an API right, code will read like prose.When in doubt, leave it out. If there is a fundamental theorem of API design, this is it. It applies equally to functionality, classes, methods, and parameters. Every facet of an API should be as small as possible, but no smaller. You can always add things later, but you can't take them away.Minimizing conceptual weight is more important than class or methodcount.Keep APIs free of implementations details. They confuse users and inhibit the flexibility to evolve. It isn't always obvious what's an implementation detail: Be wary of overspecification.Minimize mutability. Immutable objects are simple, threadsafe, and freely sharable.Documentation matters. No matter how good an API, it won't get used without good documentation. Document every exported API element: every class, method, field, and parameter.Consider the performance consequences of API design decisions, but don't warp an API to achieve performance gains. Luckily, good APIs typically lend themselves to fast implementations.APIs must coexist peacefully with the platform, so do what is customary. It is almost always wrong to "transliterate" an API from one platform to another.Minimize accessibility; when in doubt, make it private. This simplifies APIs and reduces coupling.Subclass only if you can say with a straight face that every instance of the subclass is an instance of the superclass. Exposed classes should never subclass just to reuse implementation code.Design and document for inheritance or else prohibit it. This documentation takes the form of selfuse patterns: how methods in a class use one another. Without it, safe subclassing is impossible.Don't make the client do anything the library could do. Violating this rule leads to boilerplate code in the client, which is annoying and errorprone.Obey the principle of least astonishment. Every method should do the least surprising thing it could, given its name. If a method doesn't do what users think it will, bugs will result.Fail fast. The sooner you report a bug, the less damage it will do. Compiletime is best. If you must fail at runtime, do it as soon as possible.Provide programmatic access to all data available in string form. Otherwise, programmers will be forced to parse strings, which is painful. Worse, the string forms will turn into de facto APIs.Overload with care. If the behaviors of two methods differ, it's better to give them different names.Use the right data type for the job. For example, don't use string if there is a more appropriate type.Use consistent parameter ordering across methods. Otherwise, programmers will get it backwards.Avoid long parameter lists, especially those with multiple consecutive parameters of the same type.Avoid return values that demand exceptional processing. Clients will forget to write the specialcase code, leading to bugs. For example, return zerolength arrays or collections rather than nulls.Throw exceptions only to indicate exceptional conditions. Otherwise, clients will be forced to use exceptions for normal flow control, leading to programs that are hard to read, buggy, or slow.Throw unchecked exceptions unless clients can realistically recover from the failure.API design is an art, not a science. Strive for beauty, and trust your gut. Do not adhere slavishly to the above heuristics, but violate them only infrequently and with good reason..
p1241
aVGarbage Collection (GC) has critical impact on the performance and robustness of Java Application Servers. We proposed a ServiceOriented Garbage Collector (SOGC), which exploits the fact that remote objects live longer than local objects. SOGC allocates remote and local objects in different spaces in the young generation. The experimental results showed that SOGC increases the performance and robustness of Java Application Servers compared to traditional generational approaches.
p1242
aVA pervasive computing system requires the integration of computation and communication with physical objects in the environment. The Ambiance project is building a platform for Macroprogramming pervasive systems based on an Adaptive ObjectModel; the platform provides a metalevel architecture to analyze the execution context and customize the services it provides.
p1243
aVAspectoriented programming has introduced new possibilities for developing frameworks, but it also brought complexity to aspectoriented framework reuse. Therefore, if an aspectoriented framework does not have well documented extension points and reuse steps, it becomes very difficult for the application developer to reuse it. This work presents AFR (Aspectoriented Framework Reuse), an approach to facilitate the aspectoriented framework reuse. The AFR assists application developer when placing application specific needs into frameworks extension points.
p1244
aVAspectJ's pointcut language is complex, yet often not expressive enough to directly capture a desired property. Prolog has been suggested as an alternative, but Prolog queries may not terminate, and they tend to be verbose. We solve expressiveness, termination and verbosity by using Datalog plus rewrite rules.
p1245
aVMarmoset is a framework for storing and testing student submissions to programming assignments. It gives students a limited ability to release test their code using an instructor's private suite of test cases. This encourages them to start early and implement their own test cases. It also provides facilities for instructors to manage the grading process and gives researchers access to finegrained snapshots of the student development process.
p1246
aVSpecialized Java bytecodes provide functionality that is easily replicated using other Java bytecodes. This study uses profiling to explore how the set of specialized bytecodes currently implemented by the Java Virtual Machine is utilized by comparing it to the other specialized bytecodes which could have been implemented.
p1247
aVFindBugs looks for bugs in Java programs. It is based on the concept of bug patterns. A bug pattern is a code idiom that is often an error. Bug patterns arise for a variety of reasons, such as difficult language features, misunderstood API semantics, misunderstood invariants when code is modified during maintenance, garden variety mistakes: typos, use of the wrong boolean operator and simple mistakes such as typos.FindBugs uses static analysis to inspect Java bytecode for occurrences of bug patterns. We have found that FindBugs finds real errors in most Java software. Because its analysis is sometimes imprecise, FindBugs can report false warnings, which are warnings that do not indicate true errors. In practice, the rate of false warnings reported by FindBugs is generally lower than 50%, often much lower.
p1248
aVFrameworks and libraries change their APIs. Migrating an application to the new API is tedious and disrupts the development process. Although some tools and ideas have been proposed to solve the evolution of APIs, most updates are done manually. Our study of the API changes in five components revealed that over 80% of the changes that break existing applications are caused by refactorings. This suggests that refactoringbased migration tools should be used to effectively upgrade applications. We propose an approach that is both automated and safe, without any overhead on the component producers. First, component refactorings are automatically detected (either inferred or recorded), then they are incorporated into applications by replaying.
p1249
aVThe Availability Manager design pattern is presented for monitoring the state of an application component, likely a faade to an external system. Failure and recovery of the component are handled gracefully. A reference implementation is provided, which adds builtin reporting on the availability of all such components in the system.
p1250
aVWriting patterns is a very important task for leveraging knowledge within an organization or in the software engineering community as a whole. Patterns are more than text, diagrams or source code. Patterns are knowledge that comes from experience. Sharing patterns is sharing knowledge. The creation of a language for pattern metaspecification and a catalog of patterns from different pattern languages described using this language is a clear step towards managing software engineering knowledge. The creation of a webbased visualization tool for the catalog makes this knowledge available to the world, allowing using, searching, linking, and discussing the patterns in the catalog.
p1251
aVFaith and evolution provide complementary  and sometimes conflicting  models of the world, and they also can model the adoption of programming languages. Adherents of competing paradigms, such as functional and objectoriented programming, often appear motivated by faith. Families of related languages, such as C, C++, Java, and C#, may arise from pressures of evolution. As designers of languages, adoption rates provide us with scientific data, but the belief that elegant designs are better is a matter of faith.This talk traces one concept, secondorder quantification, from its inception in the symbolic logic of Frege through to the generic features introduced in Java 5, touching on features of faith and evolution. The remarkable correspondence between natural deduction and functional programming informed the design of type classes in Haskell. Generics in Java evolved directly from Haskell type classes, and are designed to support evolution from legacy code to generic code. Links, a successor to Haskell aimed at AJAXstyle threetier web applications, aims to reconcile some of the conflict between dynamic and static approaches to typing.
p1252
aVWe present Sourcerer, a search engine for opensource code. Sourcerer extracts finegrained structural information from the code and stores it in a relational model. This information is used to implement a basic notion of CodeRank and to enable search forms that go beyond conventional keywordbased searches.
p1253
aVSmalltalk Card Game (SCG) is an activity using index cards for learning objectoriented thinking. In its name, 'Smalltalk', does not represent a kind of computer language but the literal meaning of the word  polite conversation about ordinary subject, especially at social occasions. It is because the game is based on 'rules of conversation' that we use in our daily lives. The SCG was designed to teach ObjectOriented (OO) concepts in a bottomup approach. Participants can experience and understand OO concepts in an evolutionary way by extending basic rules of game without using terminologies or previous knowledge on objectoriented programming.
p1254
aVA wealth of recent research involves generating program monitors from declarative specifications. Doing this efficiently has proved challenging, and available implementations often produce infeasibly slow monitors. We demonstrate how to dramatically improve performance   typically reducing overheads to within an order of magnitude of the program's normal runtime.
p1255
aVThis paper shows how to substantially increase the performance of Smalltalk programs by creating more classes to take advantage of polymorphism. An improved implementation of the wellknown message match:, using this and other techniques, can run up to twice as fast as the current inlined implementation VisualWorks Smalltalk includes. In this particular case, creating more classes is shown to be so powerful as to become preferable to heavy use of identity checks on immediate objects by a margin of up to 20% on average. In addition, noninlined implementations compare quite well to the existing inlined implementation of match:. While they can run faster in some cases, their overall performance falls behind by no more than a factor of 2.This is a quick summary of chapter 3 from the book currently being written by the author. It is due to be published in 2007.
p1256
aVDomainSpecific Modeling (DSM) raises the level of abstraction beyond programming by specifying the solution directly using domain concepts. In many cases, the final products can be generated from these highlevel specifications. This automation is possible because both the language and generators need fit the requirements of only one company and domain.This demonstration illustrates DSM by showing real world cases from various fields of software development. These cases describe how DSM, giving first class support for modeling, can prevent incorrect or unwanted designs at the early stages of development, and how full code can be generated from the modeler's point of view. Second part of the demonstration will show in an interactive manner both the design side and the use side of DSM languages and generators. Using MetaEdit+ tool for metamodeling, we define a DSM for a given domain and apply it to generate full code from highlevel models.
p1257
aVIn this paper we describe WebTest, an Open Source tool for automated testing of web applications. In particular we will show how to quickly create tests that shine with excellent maintainability and runtime performance as well as perfect integration in the application development cycle.
p1258
aVAlthough refactoring tools have been integrated into numerous development environments over the past ten years, we have seen little variation in the human interfaces to refactoring tools   when a refactoring fails, most tools present a textual error message. Existing interfaces can cause programmers to refactor slowly, conservatively, and in an errorprone manner. In this demonstration, we will show how our tools can help programmers overcome barriers to a successful refactoring.
p1259
aVI will present a prototype tool that enables Java annotations to serve as an extension point for making programs more visually expressive. Thus, programmers can view and edit code in a way that more closely resembles the intention of the code, rather than the raw text. Examples that we have applied the tool to are JDBC 4.0, getters, setters and constraints, JSR 181Webservices, AspectJ, and state charts.
p1260
aVThe software engineering community has taken a great interest in using domainspecific languages (DSLs) [1] to improve the productivity of software development. We demonstrate the design of a DSL as a variant of objectoriented development by applying UML [2] via the Eclipse Modeling Framework (EMF) [3] [4], exposing significant software functionality to the nonprogrammer domain experts.
p1261
aVThe DigitalAssets Manager1 is an integrated asset repository that promotes software components reuse through sophisticated yet easy to use mechanisms for searching, sharing, collaborating, managing and measuring results. It implements a peertopeer (P2P) network that allows affiliated companies, partners or different organizations to share software assets in a federated architecture.
p1262
aVIt has been suggested that fareastern (predominantly Chinese and Japanese) and western reasoning styles differ greatly: Westerners focus on objects, whereas Easterners focus on fields of interaction. 2.Traditional forms of objectorientation seem to follow the western style of thought, in which individual entities are captured cleanly, while interactions between them are not: It has been widely noted that many software systems that involve dynamically interacting components can be complex to design and implement using a strictly objectoriented approach.Eastern reasoning style lends itself better to description of interactions between entities than does the Western style. Hence, we posit that programmers designing systems that involve complex interactions might benefit from a more eastern approach for their design.However, there are currently no easternstyle programming languages of which we are aware. So, we begin our exploration with the work presented in this paper, in which we interview Easterners about how they would describe a typical objectoriented scene, and then attempt to capture and distill their descriptions into the guidelines for a programming paradigm.
p1263
aVAliasJava is a type annotation system that extends Java to express how data is confined within, passed among, or shared between objects in a software system. We present an implementation of the AliasJava system as Java 1.5 annotations and an analysis using the Eclipse infrastructure.
p1264
aVDuring the presentation we will demonstrate a new modeling tool, implemented as an AddIn for Microsoft Visual Studio. Our tool provides complete roundtrip capabilities enabling application manipulation using either the model or the source code. The tool manipulates all three application layers of conventional enterprise applications, currently with support for SQL, C# and XAML.
p1265
aVWe describe the Sun1 Small Programmable Object Technology, or Sun SPOT. The Sun SPOT is a small wireless computing platform that runs Java1 directly, with no operating system. The system comes with an onboard set of sensors, and I/O pins for easy connection to external devices, and supporting software.
p1266
aVWe wish to make framebased computing easer to deal with by providing a visualization interface. This interface will both show the contents of the knowledge base and show the code execution of a framebased program. We have implemented FRL, a framebased language, and extended it with the visualization interface.
p1267
aVLanguage Integrated Query (LINQ) is a framework that is rooted in the theoretical ideas of monads and monad comprehensions to allow seamless querying over objects, XML, and relational data. Instead of blindly gazing at the perceived impedance mismatch between the structure of these various data models, LINQ leverages the commonalities between the operations on these data models to achieve deep semantic integration.
p1268
aVAspectoriented programming (AOP) allows programmers to modularize the implementation of crosscutting concerns. AspectJ and related languages achieve this with a linguistic approach, which enables different modules of the program to have a crosscutting structural relationship.In fluid AOP the development environment temporarily shifts a program to an alternative crosscutting module structure to enable specific editing or reasoning tasks. The program text can appear to have different crosscutting modularities simultaneously, as opposed to just having modules that crosscut each other.In this demonstration we show three possible fluid AOP designs, and compare their look and feel using common examples.
p1269
aVThe Eclipse plugin HAM identifies potential aspects in large programs. It analyzes the program's history and obtains sets of function calls that are likely to be crosscutting. Later during programming, HAM informs the programmer when she is about to extend or change such a problematic concern.
p1270
aVThe recently introduced EPmodel citeeptr proposes a declarative executable model for engineering objectbased systems which achieves executability through a hybrid approach that annotates model elements with Java code snippets. Current modeling tools are not appropriate for this hybrid approach which requires graphical model editing, code generation and tight IDE integration to provide an effective modeling environment. DEMOS citedemostool is an Eclipsebased tool which supports editing and executing EP models with rulebased background code generation, and which provides immediate feedback on the syntactic validity of both model elements and usersupplied code snippets. The tool, which features an AOMbased architecture that renders it adaptable to different metamodels, enables EPmodels to be used as firstclass artifacts in the software engineering process.
p1271
aVFramework interfaces are complex, so programmers often copy repeating patterns, either their own or from others, to interact with them. Design Fragments allow framework interactions to be explicitly defined, describing knowngood uses of the framework. We demonstrate a tool that provides continual feedback on framework conformance as a program evolves.
p1272
aVIn the context of Model Driven Engineering (MDE), models are the main development artifacts and model transformations are among the most important operations applied to models. A number of specialized languages have been proposed in order to specify model transformations. The OMG has, for instance, adopted the QVT specification. Apart from the software engineering properties of transformation languages, the availability of high quality tool support is also of major importance for the industrial adoption and ultimate success of MDE. In this paper, we present ATL: a QVTlike model transformation language and its execution environment based on the Eclipse framework.
p1273
aVAs agent technology practitioners, some time ago we determined to develop an extension to UML 2.0 that addressed our specific needs, such as modeling autonomicity, proactivity and rolebased behavior. We called this extension the Agent Modeling Language (AML) and have recently published the metamodel and specification for public use. In a recent project, we realized that AML could also be applied to the domain of autonomic computing and so decided to publish some of our findings in this paper. AML can be directly used by designers of autonomous and autonomic computing systems to visually model their architectures and behaviors. Herein we provide an overview of the scope, approach taken, the specific language structure and optional extensibility. The core modeling constructs of AML are explained using a series of didactic examples describing the IBM Unity architecture, an wellgrounded exemplar of an autonomic system. We thus focus on the features of AML that differentiate it from UML 2.0 with a specific focus on those aspects that support the autonomic principles of selfhealing and survivability.
p1274
aVThe system demonstrated is an advanced open source issue tracker built using Smalltalk, the continuation based web framework Seaside and an OODB called Magma.These unorthodox components enabled an uncompromised object oriented implementation and very fast development. The demonstration presents the system, its design, project experiences and the development environment used.
p1275
aVGetting a new programming language into the hands of users is still a huge undertaking. SAFARI is an Eclipsebased metatooling framework for generating languagespecific IDEs that greatly accelerates that process. It exploits common themes and structures that recur in many languages and language tools. It supports the generation of languagedependent IDE services, while allowing developers to focus on the languagespecific aspects of their environments rather than the surrounding IDE framework. SAFARI has been used to generate IDEs for several languages. These IDEs include such features as parser generation; editors with keyword highlighting, text folding, text completion, hyperlinking and so on; outline views, project building; and more.
p1276
aVRealtime audio and music processing frequently requires actions to be taken relative to a number of different control rates such as audio sample rate or sensor inputs. These processing systems frequently use schedulers to manage this complexity. The design of a flexible scheduling system must avoid limiting the types of actions or the range of control rates available. Abstracting time and events from the underlying scheduler provides a high degree of flexibility for designers of processing networks. Marsyas[4] is an open source software framework for analysis, retrieval, and synthesis of audio signals with specific emphasis to Music Information Retrieval applications. The Marsyas scheduler uses these abstractions to provide a highly extensible scheduler.
p1277
aVIntegrated Solution Engineering helps developers manage software complexity by offering semiautomated support for capturing and mining relationships among artifacts and/or developer tasks at different stages of the software lifecycle, and by aiding developers in the use and management of the information contained in these relationships. The use of these relationships can facilitate traceability, propagation of change, change impact analysis, evolution, and comprehension.
p1278
aVThe Java Language Extender is a compilergenerator tool that allows programmers to create new domainadapted languages by importing a set of domainspecific language extensions into an extensible specification of Java 1.4. Language extensions define the syntax, semantic analysis, and optimizations of new language constructs. Java and the language extensions are specified as attribute grammar fragments written in Silver, an attribute grammar language supporting forwarding and higherorder attributes. Programmers need no implementationlevel knowledge of the language extensions and the Silver tools automatically compose the programmerselected extensions and the Java host language specification. We demonstrate several language extensions. One embeds the SQL database query language into Java and statically checks for syntax and type errors in SQL queries. Other extensions for the domain of computational geometry provide transformations that simplify the writing of efficient and robust geometric algorithms. General purpose extensions include Java 1.5 features such as the foreach loop and autoboxing and unboxing and features from Pizza such as pattern matching.
p1279
aVToday's large software systems and libraries are geared towards a broad range of platforms and environments, often relying on conditional compilation through preprocessor directives to generate specific builds for a given set of configuration flags. In spite of the welldocumented benefits of using preprocessor directives for conditional compilation, heavy preprocessor presence can hinder code readability and affect maintenance and debugging. Although various existing preprocessor tools hide unwanted preprocessor conditionals, we present a more portable Eclipsebased solution called CViMe (Conditionalcompilation Viewer and Miner) that relies on objectmodeling to not only fold noncompiled code, but refactor conditionally compiled blocks into reusable units like macros, significantly reducing the presence of preprocessor directives at the editor level.
p1280
aVRefactoring tools allow programmers to change their source code quicker than before. However, the complexity of these changes cause versioning tools that operate at a file level to lose the history of entities and be unable to merge refactored entities. This problem can be solved by semantic, operationbased SCM with persistent IDs. We propose that versioning tools be aware of program entities and refactoring operations. We present MolhadoRef, our prototype, which uses these techniques to ensure that it never loses history. MolhadoRef can successfully merge edit and refactoring operations which were performed on different development branches.
p1281
aVWe demonstrate a static analysis for extracting instancebased hierarchical views showing the runtime object graph for objectoriented programs. The code is annotated with ownership domain annotations and with additional annotations to make the output more visually appealing.
p1282
aV.NET Language Integrated Query (LINQ) is based on the philosophy that querying should be native to your objectoriented programming language. LINQ allows you to write queries in a uniform way in your programming language itself, taking full advantage of strong typing and tool support. Any data source provider can plug into the LINQ framework, allowing it to be queried from within any .NET programming language in a strongly typed, uniform way. This demo focuses on querying in C#. We show the strongly typed querying experience across inmemory collections, relational databases and XML documents.
p1283
aVA FrameworkSpecific Modeling Language (FSML) is a kind of DomainSpecific Modeling Language that is used for modeling frameworkbased software. FSMLs enable automated roundtrip engineering over nontrivial modeltocode mappings and thereby simplify the task of creating and evolving frameworkbased applications. In this demonstration, we present a prototype implementation of Eclipse Workbench Part Interaction, a FSML capturing an aspect of Eclipse plugin development. We walk through an example Eclipse plugin development scenario and demonstrate the roundtrip engineering capabilities of the prototype.
p1284
aVOne crosscutting requirement (also called aspect) affects several parts of a software system. Handling aspects is well understood at sourcecode level or at runtime. However, only a few aspectoriented approaches handle other software artefact types, like UML models, configuration files, or database schema definitions. Instead of rewriting the same aspect newly for each artefact type, this paper suggests to write down an aspect once independent of artefact types.But, wait a minute: Which places does such an aspect affect? Where do we weave in an aspect if its pointcut doesn't refer to artefact details? This paper suggests expressing aspects via ContextBased Constraints (CoCons). They select their constrained system elements according to the element's context. For instance, CoCons affect all system elements used in a certain department, workflow, or location. CoCons are easy to grasp for users and customers because they express business requirements without referring to technical details. This paper focuses on how to express and monitor crosscutting requirements in UML models via CoCons. Moreover, it reveals that CoCons are a new notion of constrains by comparing CoCons to the Object Constraint Language OCL.
p1285
aVAspectC++ is a general purpose aspectoriented language extension to C++. It is aimed to bring fullyfledged aspectoriented programming (AOP) support in areas with strong demands on runtime efficiency and code density. This makes it possible to exploit the power of AOP for the domain of (deeply) embedded systems, where computation speed and available memory resources are strictly limited. AOP concepts are particularly useful for the development of scalable embedded system product lines. This will be demonstrated by a real world example: a small embedded device equipped with meteorological sensors and an 8bit microcontroller running AspectC++ code. By covering the complete build cycle of configuration, compilation and installation, participants will understand, how easy it is to integrate AspectC++ with an existing tool chain. A presentation of the AspectC++ tools for Eclipse and the pure::variants variantmanagement system rounds up the demonstration.
p1286
aVAs models are elevated to firstclass artifacts within the software development lifecycle, new approaches are needed to address the accidental complexities associated with current modeling practice (e.g., manually evolving the deep hierarchical structures of large system models can be error prone and labor intensive). This research abstract presents a model transformation approach to automate model evolution and testing tools to improve the quality of model transformation.
p1287
aVWhile the integration of refactoring tools into many development environments has increased, the usability of these tools has remained stagnant. Specifically, when refactorings fail, tools communicate the failure to the programmer poorly, causing the programmer to restructure slowly, conservatively, and without preserving behavior. In the accompanying poster, I show how refactoring correctness, speed, and user satisfaction can be measurably increased by improving the usability of refactoring tools. In the long run, better usability will aid in the adoption and utilization of refactoring tools.
p1288
aVMany tools for objectoriented software design focus on assisting individuals in creating UML models for documentation and implementation purposes. Since software design is a highly collaborative activity, one must ask whether the requirements for facilitating collaborative design are similar.We report on a study of design teams, focusing on their use of notations and artifacts. Our findings highlight the unique characteristic of the use of UML in these settings and emphasize the importance of context and relations between artifacts over the details of specific artifacts.
p1289
aVOur research proposes a novel framework to automatically infer systemspecific interface properties from program source code using static modelchecking traces.
p1290
aVAccess control policies are increasingly written in specification languages such as XACML. To increase confidence in the correctness of specified policies, policy developers can conduct policy testing to probe the Policy Decision Point (PDP) with some typical test inputs (in the form of requests) and check test outputs (in the form of responses) against expected ones. Unfortunately, manual test generation is tedious and manually generated tests are often not sufficient to exercise various policy behaviors. In this paper we present an efficient test generation approach and its supporting tool called Targen. We further reduce the number of generated requests based on structural coverage information to facilitate manual inspection. If a rule is unreachable due to an unsatisfiable set of constraints, it is redundant. We also present an approach for redundantrule detection based on changeimpact analysis and its supporting tool call Cirg. We have evaluated Targen on policies collected from various sources, some of which are complex policies being used in real systems. Our results show that Targen can effectively generate tests to achieve high structural coverage of policies and outperforms the existing random test generation in terms of structural coverage and faultdetection capability. Cirg can identify a large number of redundant rules among rules defined in a complex, real policy.
p1291
aVThe poster describes the design and implementation of the optimizing JITcompilation subsystem for SSCLI (Rotor) 2.0 virtual machine. This presentation covers overall design of the subsystem, integration issues, and a fast algorithm for the 1st level compilation.
p1292
aVGoaldirected programming languages present many unique challenges for compiler developers. The objective of this project was to design and implement an optimizing compiler for the goaldirected, objectoriented Unicon programming language. The resulting compiler (Uniconc) has a unique topology that is designed to facilitate code transformation experiments. Uniconc produces intermediate code at four points in the compilation process, and code transformations in Uniconc can span one or more forms of intermediate code. Code transformation experiments conducted with Uniconc have produced optimizations that increase the speed of method invocations and field references, decrease the size of class instances, and improve the quality of type inferencing information. Measurements indicate that native Unicon targets generated by Uniconc are at least four times faster than VMhosted Unicon targets.
p1293
aVAspectOriented Programming (AOP) is a methodology that provides new modularization of software systems by dealing explicitly with separation of concerns in software development. AspectJ, a language designed to support AOP uses abstractions like pointcuts, advice, and aspects to achieve AOP's primary functionality. Pointcuts are constructs modeled using expressions that identify events during the execution of a program called join points. It is likely that developers tend to write expressions with incorrect strength thereby selecting additional events than intended to or leaving out necessary events. This causes aspects, the set crosscutting concerns, to fail. Our framework automatically tests pointcuts to identify their strength and determines variants of the expression with different strengths which the developer can inspect to choose an expression with the correct strength. The framework selects and ranks the list of variants based on a similarity measure. The framework eases the task for developers to inspect variants that resemble closely to the original expression.
p1294
aVThis paper presents a mechanism for the definition and execution of viewpoints in workflow processes. The strategy proposed in this work is based on the definition of a viewpoint metamodel to express crosscutting concerns in workflow processes. This proposal includes a weaving mechanism to integrate viewpoint models and process models using aspect oriented modeling techniques. Some preliminary results and the future work of this research are also presented.
p1295
aVIn this article a new programming paradigm is discussed: naturalistic programming. Naturalistic Programming means writing computer programs with the help of natural language.The authors are convinced that contemporary programming techniques have reached a level where only a fundamental change of paradigm can develop them any further.Introducing, philosophical and epistemological issues related to programming and human thinking are discussed.After that, the programming language Pegasus is presented. It has been developed as a scientific prototype of a naturalistic programming language at the Darmstadt University of Technology.Pegasus can read natural language and create executable program files from that. In addition to that, Pegasus can automatically translate programs between different natural languages, by the time of writing: German and English.Subsequently, an overview on related work is given, followed by remarks on the advantages and disadvantages of naturalistic programming in general and Pegasus in particular.Finally, an outlook on future research concludes.
p1296
aVObject oriented frameworks impose new burdens on programmers that libraries did not, such as requiring the programmer to understand the method callback sequence, respecting behavior constraints within these methods, and devising solutions within a constrained solution space. To overcome these burdens, we express the repeated patterns of engagement with the framework as a design fragment. Design fragments give programmers immediate benefit through toolbased conformance assurance and longterm benefit through expression of design intent.
p1297
aVWhile architecture is widely viewed as important, the mechanisms by which understanding architecture helps developers better change code are less clear. We are conducting a laboratory study to gather requirements for the design of future tools and techniques for making architecture more explicit while interacting with code.
p1298
aVAs models are elevated to firstclass artifacts within the software development lifecycle, new approaches are needed to address the accidental complexities associated with current modeling practice (e.g., manually evolving the deep hierarchical structures of large system models can be error prone and labor intensive). This research abstract presents a model transformation approach to automate model evolution and testing tools to improve the quality of model transformation.
p1299
aVDomainspecific languages (DSLs) assist an enduser programmer in writing programs using idioms that are closer to the abstractions found in a specific problem domain. Language testing tool support for DSLs is lacking when compared to the capabilities provided in standard general purpose languages (e.g., Java and C++). For example, support for debugging a program written in a DSL is often nonexistent. This research abstract describes a grammardriven technique to build a testing tool generation framework through automated transformation of existing DSL grammars. The modified grammars generate the hooks needed to interface with a supporting infrastructure written for an Integrated Development Environment that assists in debugging, testing, and profiling a DSL program.
p1300
aVIn systemfamily approaches, product configuration is the activity of selecting the features desired for a software product. Although this process is typically collaborative this aspect has long been neglected. On the other hand, enabling collaborative product configuration brings new and challenging problems such as the proper coordination of configuration decisions. This paper introduces a framework for collaborative configuration that addresses the major issues that arise in this context. Some aspects of the framework can be customized to accommodate specific collaboration goals.
p1301
aVThe area of clone detection (i.e., searching for duplicate fragments of source code) has received wide interest recently as indicated by numerous efforts in clone detection tool development. Additionally, some work has been done in the area of selecting and refactoring the detected clones. However, the actual refactoring of clones is typically separated from the initial clone detection and subsequent selection of clones, because the task is delegated to a refactoring tool. This research abstract describes an investigation that will bridge the gap between clone detection and refactoring by researching and developing a comprehensive clone detection and refactoring process that begins with the detection of clones and ends with their refactoring.
p1302
aVAn important question in the context of software product line development is how to improve the modularization and composition of crosscutting features. However, little attention has been paid to the closely related issue of testing the crosscutting features. There is still a lack of techniques to help developers test such features. Usually, developers test the crosscutting features in combination with the affected feature, which impairs the fault diagnosis. This work proposes a verification approach for crosscutting features implemented by means of aspects and crosscutting interfaces (XPIs).
p1303
aVReaders turn to narrative for certain familiar pleasures; and yet, reading the opening sentences, they hope to find themselves in unknown territory. They want to be lost in a book, transported through a shared act of imagination. If what they read seems too strange, though, if they start to feel truly lost, they're likely to feel anxious, frustrated, even angry. The challenge for the writer, then the challenge for every discoverer and creator is to communicate with the past, while guiding the reader (or follower, or user) someplace new. Using examples from writing and cartography, this talk will explore the challenges of discovery, the challenges of presenting those discoveries, and how the presentation itself is often the key to discovery (think Impressionism). It will also consider the tension between intention and inspiration, or good luck. Columbus was headed for India, James Cook mapped the Pacific only because he couldn't find Terra Australis, and both Mark Twain (soon after publishing The Adventures of Huckleberry Finn) and F. Scott Fitzgerald (in the weeks following the publication of The Great Gatsby) expressed despair over their failure to write the books they thought they meant to write. The thing they had discovered the thing they had createdtranscended their own conception of a "good book. Before we can lead anyone anywhere, we need to look clearly at where we are, and to prepare ourselves to see like never before.
p1304
aVEveryone talks about software bloat, feature creep and the everincreasing complexity of software. Each new version of a software package adds in new features. Very rarely, features are removed. But what really happens as software evolves? This animated film illustrates the evolution of PowerPointover seven versions from 19872001. With each version the user has faced increasing application complexity. Knowing how software evolves is of increasing importance as we move to building ultralarge scale software and developing software in the context of software ecologies. This film uses abstract graphical representations of the application features and relationships between features. Timelapse animation of these abstract representations are used to convey an understanding of how this application has evolved. This animation is based on data from a project that is mapping the user interface and application functionality available in every release of Microsoft PowerPoint for the Macintosh. To date this study spans 7 releases of this application.
p1305
aVIn runtime monitoring, a programmer specifies a piece of code to execute when a trace of events occurs during program execution. Previous and related work has shown that runtime monitoring techniques can beuseful in order to validate or guarantee the safety and security of running programs. Yet, those techniques have not yet been able to make the transition to everyday use in regular software development processes. This is due to two reasons. Firstly, many of the existing runtime monitoring tools cause a significant runtime overhead, lengthening test runs unduly. This is particularly true for tools that allow reasoning about single objects, opposed to classes. Secondly, the kind of specifications that can be verified by such tools often follow a quite cumbersome notation. This leads to the fact that only verification experts, not programmers, can at all understand what a given specification means and in particular, whether it is correct. We propose a methodology to overcome both problems by providing a design and efficient implementation of expressive formal monitoring techniques with programmerfriendly notations.
p1306
aVThere have been various proposals for the formalization ofappropriate viewpointbased frameworks. The model that we intend to devise attempts to provide a basis for conceptual model integration particularly with the existence of partial ignorance and uncertainty. The model will try to formalize the degree of uncertainty present in experts' expressions, and proposes tools for conceptual model integration and formal consensus building between the involved viewpoints.
p1307
aVThe use of bad names   names that are wrong, inconsistent or inconcise   hinder program comprehension. The root of the problem is that there is no mechanism for aligning the name and implementation of a method. We believe that judgment on the suitability of a chosen name can be passed automatically by comparing the actual implementation of a method to what we would expect from the name itself. The gist of our approach is to understand how programmers use language in their programs, by analysing a large corpus of Java applications.
p1308
aVThe advent of domainspecific modeling in enterprise systems development has given rise to new tool requirements. Existing tools do not offer sufficient modeling guidance or inconsistency management for the multitude of new metamodels and models. Specifically, there is a need to offer guidance on 1) valid editing operations, 2) ensuring consistency among models, 3) bridging the gap between models and custom code, and 4) managing the evolution of domainspecific languages. Based on two empirical casestudies, we propose a new unirepresentational modeling tool  SmartEMF  which provides guidance and inconsistency management when developing enterprise systems with multipledomainspecific languages.
p1309
aVThe purpose of the JVM is to abstract the Java language from the hardware and software platforms it runs on. For this reason, the JVM uses services offered by the host operating system in order to implement identical services for the Java language. The obvious duplication of effort in service provision and resource management between the JVM and the operating system has a measurable cost on the performance of Java programs. In my PhD research, I try to find ways of minimizing the cost of sharing resources between the OS and the JVM, by identifying and removing unnecessary software layers.
p1310
aVDesign defects come from poor design choices and have the effect of degrading the quality of objectoriented designs.Therefore, they present opportunities for improvements.However, design defects have not been precisely specified and there are few appropriate tools that allow their detection as well as their correction. Our goal is to provide a systematic method to specify systematically design defects precisely and to generate automatically detection and correction algorithms from their specifications. The detection algorithms are based not only on metrics but also on semantical and structural properties whereas the correction algorithms are based on refactorings. We apply and validate these algorithms on opensource objectoriented programs to show that our method allows a systematic specification, a precise detection, and a suitable correction of design defects.
p1311
aVByname subtyping (or userdefined subtyping) and structural subtyping each have their own strengths and weaknesses. Byname subtyping allows programmers to explicitly express design intent, and, when types are associated with run time tags, enables runtime "type" tests and external/multimethod dispatch. On the other hand, structural subtyping is flexible and compositional, allowing unanticipated reuse. To date, nearly all objectoriented languages fully support only one subtyping paradigm or the other. I propose a language that combines the key aspects of byname and structural subtyping in a unified framework. The goal is to provide the flexibility of structural subtyping while still allowing static typechecking of external methods. The work offers a clean foundation for the designof future languages that enjoy the benefits of both byname and structural subtyping. I also propose a language extension to Java that combines byname and structural subtyping, and includes parametric polymorphism. The practical utility of this language will be illustrated through case studies that show that code written in the new language is more flexible and easier to modify than the original code.
p1312
aVThe process of design construction and design review is notorious in its endless debates and discussions. Sometimes, these "gurus' style" debates are triggered by simple mistakes that could have been easily avoided. Precious time is lost, both because of the designer's lack of experience and the reviewer's overconfidence. If both sides were effectively prepared and armed with the same ammunition of knowledge, the "balance of terror" would reduce the will to argue and focus would be maintained. Reinforcing the abilities to face the intimidating guru's knowledge can be by formal encapsulation of that knowledge and systematic gurus' guidance to the design steps. This paper presents a methodology that captures the recommended selection of entities' relationships using UML notation. The UML arrow methodology is comprised of a checklist, which provides immediate yes/no feedback to simple guiding questions and a governing iterative process. The consolidated questions encompass best practice design methodologies, composed, edited and corroborated by the company's experienced designers. It contains a common process based on capturing the gurus' recommendations according to the company's specific needs. The methodology was evaluated in practitioners' workshops as well as practical design sessions, based on data received through questionnaires. The results obtained from 62 participants reveal that (1) usage of this methodology led to effective identification of inappropriate entities' relationships; (2) shortened the design review duration; (3) removed redundant technical remarks; (4) improved the preparation stages as well as brainstorming sessions, and (4) overall maintained a focused discussions regarding the problem.
p1313
aVIt is difficult to determine the cost effectiveness of program analysis tools because we cannot evaluate them in the same environment where we will be using the tool. Tool evaluations are usually run on mature, stable code after it has passed developer testing. However, program analysis tools are usually run on unstable code, and some tools are meant to run right after compilation. Naturally, the results of the evaluation are not comparable to the true contribution of the tool. This leaves program analysis tool evaluations being very subjective and usually dependent on the evaluators intuition. While we could not solve this problem, we suggest techniques to make the evaluations more objective. We started by making enforcementbased customizations of the tool to be evaluated. When we evaluate a tool, we used a comparative evaluation technique to make the ROI analysis more objective. We also show how to use coverage models to select several tools when they each find different kinds of issues. Finally, we suggest that the tool vendors include features that assist us with a continuous evaluation of the tool as it runs in our software process.
p1314
aVSomething extraordinarily strange is going on in the computer industry. Despite the widespread availability of jobs, the challenging and exciting nature of the work, and the impressive earning potential of workers, the (industry is experiencing a massive decline in the number of new recruits. Universities and companies alike are starting to wonder: where did all the warm bodies go? Numbers from the US are showing declines upwards of 20%, and across North America it is not uncommon to find decreases in students applying to computer science postsecondary programmes that double that. It appears that a dearth of highlytrained professionals is looming, which is bound to negatively impact future advances in computer technology and ultimately threaten major economies. Given the current crisis, many groups are addressing the issue, with varying approaches and perspectives. The film "The Elephant in the Room: Who Will Take Care of the Code?", exposes the symptoms of this problem and examines some of the current solutions. In particular, this film profiles efforts at the University of Victoria in the CScIDE Computer Science Initiatives for Diversity and Equity research group and their experiements introducing programming concepts to children in grades two through seven.
p1315
aVIn Custom House's financial system, money, rate, currency and markup are among the key concepts to describe the domain. Without explicit modeling, those concepts appear in the code, but only in variable names of primitive types and sometime in method names for operations whose arguments and return types are all primitives. In this paper we present the experience gained when Custom House development team applies DomainDriven Design to explicitly model fundamental domain concepts and abstract them into value objects. Using code snippets, we show how those concepts are discovered, how we refactor the existing code base to apply the value objects, what difference the value objects make and how they evolve over time. We also report the pitfalls and stumbles during the process, which are not uncommon for projects that undergo ongoing development work. We show that, with all the incompleteness and imperfection, DomainDriven Design is gradually accepted into real development process, making a continuous and crucial transformation.
p1316
aVCustom House's new currency exchange system is integrated with a legacy system. After a few years of growth, the two systems were so intricately tangled that even small changes made in the integration layer would have unpredictable side effects. Refactoring on the integration layer was risky and time consuming. The situation called for a revolutionary redesign. The solution was to introduce an anticorruption layer to isolate the two systems. This layer encapsulated the translation of conceptual objects and actions between the two systems, insulating the domain layer from knowing the existence of the other system. By freeing the domain layer from performing tasks that were only relevant to the other system, the anticorruption layer allowed additional external systems to be integrated without requiring any changes to the domain layer itself. Full implementation of an anticorruption layer reduced overhead of legacy integration from 30% of total development to 10%. The biggest challenge in implementing the anticorruption layer is to control the complexity of translation work. This was managed in an innovative way: by building an object model reflecting the implicit model of the legacy system. Our experiences show that an external system need not be objectoriented for its model to be adequately abstracted, and this has proven to be the key to a clean and extensible translation.
p1317
aVIn this paper we present the experience gained and lessons learned when the IT department at Statoil ASA, a large Oil and Gas company in Norway, applied DomainDriven design techniques in combination with agile software development practices to assess the software architecture of our next generation oil trading and supply chain application. Our hypothesis was that the use of object oriented techniques, domain driven design and a proper objectrelational mapping tool would significantly improve the performance and reduce the code base compared with current legacy systems. The legacy system is based on several Oracle databases serving a variety of clients written in Java, Gupta Centura Team Developer and HTML. The databases have a layer of business logic written in PL/SQL offering various system services to the clients. To validate our new objectoriented software architecture, we reimplemented one of the most computationally heavy and data intensive services using Test First and DomainDriven design techniques. The resulting software was then tested on a set of servers with a representative subset of data from the production environment. We found that using these techniques improved our software architecture with respect to performance as well as code quality when running on top of our Oracle databases. We also tested the switch to an object database from Versant and achieved additional performance gains.
p1318
aVMany Software Product Line case studies focus on the fact that an ROI can be achieved in 35 projects. This paper asks the question "what has to be done differently to be able to generate 10,000 custom applications a year?". As wholesalers of custom web applications for Small to Medium Sized Businesses, we have to create highly customizable web applications in minutes   not months. After 18 months of research and experimentation we have developed a layered system that focuses on the reuse of declarative executable specifications rather than the reuse of imperative code, allowing us to blend speed of development with flexibility of the generated solutions. The system uses a feature modeler to select common functionality and a decision support system to deskill the customization process. It has a collection of Domain Specific Languages for describing the vast majority of custom functionality required by our clients and an extensible framework allowing any system functionality to be overloaded/extended using custom code if necessary. In this paper we provide an introduction to the key theoretical concepts required to understand the system. We then introduce our domain specific languages for describing web applications. We then look at the process of building applications using SystemsForge and then we highlight our conclusions to date and document some of the outstanding issues that we are still investigating relating to managing Domain Specific Language evolution and interactions.
p1319
aVVarious issues make framework development harder than regular development. Building product lines and frameworks requires increased coordination and communication between stakeholders and across the organization. The difficulty of building the right abstractions ranges from understanding the domain models, selecting and evaluating the framework architecture, to designing the right interfaces, and adds to the complexity of a framework project.
p1320
aVSimula 67 (SIMple Universal LAnguage 67) is considered by many as one of the earliest  if not the first  objectoriented language. Simula 67 was developed by OleJohan Dahl and Kristen Nygaard in Oslo, Norway and has greatly influenced objectoriented language development over the past 40 years. This panel brings together leading programming language innovators to discuss and debate past, present, and future language evolutions.
p1321
aVEvery few years the computing industry sees the emergence of another "silver bullet". Our "trade" press follows the siren call, vendors become optimistic (at least the marketing departments), managers and developers think that all will be solved, and the customer is skeptical as usual. Is our industry fated to go through hype cycles, and what is the underlying reality? This panel will explore this by looking at what may be a current "silver bullet"  Domain Specific Languages (DSLs). DSLs have become a focus of current interest with the increasing popularity of model driven development and dynamically typed languages. By allowing software engineers to focus on a specific domain it can be argued that DSLs bridge the gap between the business and implementation. Thus allowing us to produce better systems. Are DSLs really a "silver bullet", and what do we mean when we say that. The panel will take the form of a nontraditional debate (modeled on a public radio talk show "Left, Right, and Centre"). We have three practitioners with differing experiences using DSLs taking three differing positions: 'for', 'against', and somewhere in the 'centre'. Unlike the radio show there will also be audience participation.
p1322
aVTwenty years after the paper No Silver Bullet: Essence and Accidents of Software Engineering by Frederick P. Brooks first appeared in IEEE Computer in April 1987 (following its 1986 publication in Information Processing, ISBN 044470773) does the premise hold that the complexity of software is not accidental? How have the "hopes for silver" which included highlevel language advances, objectoriented programming, artificial intelligence, expert systems, great designers, etc.  evolved? Panelists will discuss what has changed and/or stayed the same in the past twenty years  and the paper's influence on the community.
p1323
aVEvery halfdecade or so, the computing world is infected by a meme that energizes IT and stimulates architectural thinking but also distorts discussion and clouds judgment. A few generations ago it was objects; now it's services. As every developer has noticed, SOA is everywhere even, perhaps, in some places it shouldn't be. What has this focus on services done to the role of objects? Some of the more extreme SOA proponents maintain that servicereuse replaces objectreuse across the board. More mainstream architects view these two models as complementary reuse strategies at different levels of scale. There are even object diehards who think that services can't come close to the flexibility and durability of objects. This panel will represent the full range of opinions. Specifically, panelists and attendees will be challenged to explore such topics as: Where is the scalability boundary between object responsibilities and service responsibilities? Do we have to conceptualize, design, or implement differently at different levels of scale? Where and how do binary components such as RMI, EJB, or CORBA fit in alongside more looselycoupled interfaces such as web services? Panelists will also be invited to comment on whether some of the new concepts defined into SOA orchestration and discovery, for example can be profitably fed back into the object world.
p1324
aV3D web software visualization has always been expensive, special purpose, and hard to program. Most of the technologies used require large amounts of scripting, are not reliable on all platforms, are binary formats, or no longer maintained. We can make 3D software visualization of objectoriented programs cheap, portable, and easy by using X3D, which is a new open standard for web 3D graphics. In this film we show our X3D web software visualizations in action.
p1325
aVThe "Killer Examples" series of workshops are highly interactive workshops which have been an annual occurrence at OOPSLA since 2002. The goals of the workshops are to bring together educators and developers to share their objectoriented expertise, and to provide a forum for discussion of teaching techniques and pedagogical goals. The theme of last year's workshop was design patterns; the the theme of this year's workshop is process: for teaching, learning and programming. While there is a formal application procedure to guarantee admission to the workshop, we accept walkins if space permits and the walkins have adequate interest and background to be able to contribute positively to the discussions.
p1326
aVProjects, once stranded, suffocate from their own weight. The weight of the project is the complexity that it has created, or that has been burdened upon it: complexity of the problem, of the organization, of the chosen solution, of the environment and of the team dynamics. While complexity cannot be avoided, it can be influenced and managed. This workshop explores how complexity arrives at a project, how it can be measured, what heuristics indicate risk, and how complexity can be managed and overcome.
p1327
aVImproving the software engineering development process requires collection of data, but collection of data interferes with how developers work. At present, most of the software engineering tools, data collection, and analysis techniques available use manual data collection, despite known problems with reliability, correctness, and timeliness of the data. To overcome such limitations and reduce interference with the development process, software engineering researchers must develop tools and data analysis techniques that collect data without human interactions. Such tools produce very detailed and extensive data, but lack the filtering and classification that humans perform on manually collected data. This unfiltered data requires the development of new analysis techniques and new prediction models to use it effectively. This workshop focuses on defining the research challenges created by in process software measurement and analysis of the software development process using tools that do not affect or modify the process but extract data automatically from it.
p1328
aVA pattern language consists of a cascade or hierarchy of parts, linked closely together by patterns, which solve generically recurring problems that are associated with the parts. Each pattern has a title, and collectively the titles form a language for design [1] Pattern Languages are in life, simply a collection of interrelated patterns [2]. These interrelated patterns are combined in any way and combination to create new environments, where practitioners can solve contextspecific problems. Precisely, the concept of pattern languages has invaded over into the software engineering field, to describe prior experiences and the processes that stem from them, in a very simple language, where patterns are tactfully woven as a whole, and can be combined in any manner to solve a particular and complex problem. Yet, this process is still done in an adhoc manner and is not straightforward enough, to ease and speed up the software development process. Thus, this workshop is driven forward by three main questions. First, how can we classify, develop, and utilize analysis and design patterns together towards the path of a problem resolution? Second, what is thebehindthelanguage that guides the sewing of patterns together as a whole? And third, how can we overcome and face challenges, other than patterns composition problems (patterns traceability, etc.) that can hinder the development of a system of patterns? The inherent inability to answer these questions detrimentally impacts the understanding of how to put patterns in real practice, and will therefore make software patterns' use more complex than it should.
p1329
aVIn this 5th International Workshop on SOA and Web Services, we will explore bestpractices in the adoption, design, implementation, management and monitoring of SOA and Web servicesrelated methods, tools and technologies. This workshop is a continuation of a highly successful series of workshops at OOPSLA 2006, OOPSLA 2005, OOPSLA 2004, and OOPSLA 2003.
p1330
aVDomainSpecific Modeling raises the level of abstraction beyond programming by specifying the solution directly using visual models to express domain concepts. In many cases, final products can be generated automatically from these highlevel specifications. This automation is possible because both the language and generators fit the requirements of only one domain. This paper introduces DomainSpecific Modeling and describes the related 2day workshop (21st and 22nd October).
p1331
aVThe Eclipse platform is designed for building integrated development environments for objectoriented applications. The goal of the ETX workshop is to bring together researchers and practitioners to exchange ideas about potential new uses of Eclipse and how the core Eclipse technology can be leveraged, improved and/or extended for research and teaching projects.
p1332
aVSecond Life is a large, online virtual world where avatars dance, fly, buy virtual clothing, play games, have meetings...and program. About 256k residents of Second Life write code that runs 24/7 in over 2M simulated objects in a continuous 3D landscape twice the size of Montral. This giant, collaborative development environment is run on a large grid of over 12k CPUs in a grid of "simulators" that run the land of Second Life. The simulators have an integral virtual machine for the scripting language people use. Despite the inherit difficulties, the system demonstrably does enough right to enable development of a huge amount of content in Second Life. As the virtual world grows, we have been evolving its infrastructure for programming in several ways. Integration of the Mono virtual machine presented a huge set of challenges but offers major advantages as Second Life grows. We have also had to architect and extend in light of the fact that Second Life is a continuously running system on which over a million people rely. Finally, apart from the language and runtime environment, Second Life also presents a social environment in which to program collaboratively. Within Linden Lab, we have pioneered the use of Second Life as an integral part of our development methodology even when working on the underlying code of Second Life itself. These experiences point toward a reimagining of programming as a globally immersive collaborative experience.
p1333
aVDeveloping large software systems has largely become an exercise in integration. The main effort is writing "glue" that holds externally developed components together. We will explore the shift away from commercial offtheshelf (COTS) to free/open source software (F/OSS) components, and identify the opportunities and issues introduced by it.
p1334
aVDuring the core development phases, managers and software developers rightfully focus on the activities needed to complete the software's first release. They strive to minimize the project risk and to keep the schedule. The management of future releases, version identification, compatibility checks, migration and update strategies are often treated as an afterthought. In many systems this amount of naivety is what brings a project forward: solving the problems at their time. However, insufficiencies or inconsistencies here have all the potential to cause trouble once the software has met the customer.
p1335
aVA historical cycle has been observed where the use of graphical tools becomes critical to software development but these tools eventually fall from use as the underlying cause of complexity is removed through a new programming paradigm. This workshop attempts to take a unified view of UMLrelated ideas which span from high level software design (UML and MDD) to technical details of implementing reusable associations. It also identifies gaps in the existing programming languages addressed by UML class diagrams. The discussion is not language specific and applies to both C++ and Java. For additional details see www.codefarms.com/OOPSLA07/workshop.
p1336
aVNo "Silver Bullet" is a classic software engineering paper that deserves revisiting. What if we had a chance to rewrite Brooks' article today? What have we learned about effective software development techniques over the last 20 years? Do we have some experiences that reinforce or contradict Brooks' thesis?
p1337
aVSoftware systems are intrinsically complex from a number of perspectives. The level of complexity is increasing due to the growing need to integrate different and diverse systems in order to achieve organizational goals (evolving toward ecosystems as an ideal). Effective integration may be argued as important as, from a knowledgebased perspective of organizations, the effectiveness of services rendered by resources depends upon how they are combined and applied. Consequently, systems development and reengineering must focus on ways of dealing with the complexity of modern software systems in an effective manner. Since software systems, in essence, model real world phenomena, it is necessary to adopt modeling and development techniques founded on semantics. Broadly speaking, semantics enable the precise mapping between complex real world phenomena and their modeled counterparts and/or enable the (dynamic) mapping/integration between different representations (and understandings) of real world phenomena. In recent years, ontologies have emerged as the prime focus of semantic modeling, with the main focus on the development of representation languages and the resulting ontologies. Limited work has been carried out within the software engineering community in relation to the development of semanticbased systems   though research into the modeling, alignment and evolution of ontologies has progressed significantly among the Semantic Web community. It would seem plausible that languages, tools and techniques developed to achieve the objectives of the Semantic Web could be integrated within the software development process as a means to produce more flexible and adaptive systems. Numerous challenges exist however, as the development of semanticbased systems will have to manage things that exist (ontology), specific organizational knowledge of what exists (epistemology) and the required organizational action (pragmatics). With that in mind, this workshop aims to bring together researchers and practitioners with diverse cultural and professional backgrounds in order to discuss and analyze the different perspectives, issues and challenges of SemanticBased Systems Development. Researchers and practitioners are invited to provide contributions in the form of research, case study or position papers related to the workshop theme. Topics include, but are not limited to the following: Ontological modeling paradigms Development approaches for semanticbased systems (e.g., methods, process and tool support) Resolution of semantic mismatches Persisting semantic models Automated ontology generation and management Ontology languages (e.g., RDF, OWL and UML) Semantic Web services Mapping of semantic models with system models Extracting (business) semantics from legacy systems.. During the workshop the discussion will be particularly aimed at:Identifying key obstacles in relation to SemanticBased Software Development; Contributing toward the improvement of the StateoftheArt in SemanticBased Software Development; Instigating collaborative research efforts among the participants..
p1338
aVBuilding a Unified Data Mining Engine (UDME) is not an easy exercise, specifically, when several factors can undermine their quality success, such as cost, time, and lack of systematic approaches. We would like to architect and develop a UDME, that has the some or all of the following properties: 1. Ease of use, 2. No Need of Expert to run the tool 3. Easy to add new functionality 4. Easy to interface 6. Multiple algorithms 7. Fewer resources 8. Stable 9. Isolation of Application logic 10. Minimum Maintenance Cost The workshop will address the unified data mining engine' challenges, and also debate several issues that are related to the architecture and development of the UDME.
p1339
aVThere has been recognition that the writers' workshop process is beneficial for the production of high quality papers ever since the first patterns conference or PLoP was held in 1994. In this OOPSLA workshop, attendees will participate in a writers' workshop of their paper as well as writers' workshops for papers of other attendees in their group. Participants who do not have a paper to workshop are also invited. Everyone will be expected to read all the papers in their groups in advance. The organizers of this workshop were all involved in PLoP '06 and would like to continue a close association between this important patterns activity and OOPSLA. Our hope is that the relationship will benefit both OOPSLA, by attracting authors of papers that are worksinprogress, and PLoP, by allowing authors who might not be able to attend PLoP to participate in the writers' workshop process.
p1340
aVWhen agile methods arose, they were tried in small, collocated teams developing medium critical client or server software mostly with internal teams or effortbased contracts. Even today many proponents and skeptics of agile development believe that these parameters limit its use. Others have tried to cross these boundaries and have applied the agile value system to large teams, distributed development, highly critical or embedded systems or in an environment that doesn't support decisionmaking. Some of them have succeeded, others have failed. Most of them had to adapt agile techniques to their specific context. This workshop tries to deepen the understanding of agile values, principles and techniques by exploring these experiences. We invite practitioners and academics who have experience with testing the boundaries of agility or are interested in their findings.
p1341
aVPredictive models can be used to discover potentially problematic components. Source code metrics can be used as input features to predictive models, however, there are many structural and design measures that capture related metrics of coupling, cohesion, inheritance, complexity and size. Feature selection is the process of identifying a subset of attributes that improves the performance of a predictive model. This paper presents a prototype that implements a parallel genetic algorithm as a searchbased feature selection method that enhances a predictive model's ability to identify cognitively complex components in a Java application.
p1342
aVLanguages what's to learn from them? Relics of the past; we know how to design them/to use them. Types/messages /invocation/loops/numbers/methods/big ol' libraries/lots of = signs. Heh, but... What is programming, and what role do programming languages play in that process? We have learned a lot over the last five decades: organizing principles, established conventions, theory, fashions, and fads. "Those who cannot remember the past are condemned to repeat it." In this talk we survey what we think are the most important lessons of the past that future programmers and future programming language designers ought not forget. We illustrate each lesson by discussing specific programming languages of the past, and endeavor to shine what light we can on the future.
p1343
aVWe present an extension to Java, dubbed OOMatch. It allows method parameters to be specified as patterns, which are matched against the arguments to the method call. When matches occur, the method applies; if multiple methods apply, the method with the more specific pattern overrides the others.
p1344
aVJastAddJ is a Java compiler that is easy to extend with new analyses and language constructs. The main systems contains a Java 1.4 compiler with modular Java5 extensions. It compares favourably in quality and size compared to other Java compilers, and runs within a factor of three compared to javac.
p1345
aVScenarios are vital for the specification of software systems. We are developing an open framework for the specification, execution, and conformance evaluation of scenarios. The scenarios define a contract which is bound to an implementation under test. The scenarios are executed by our framework to ensure conformance against the contract.
p1346
aVSpecification mining is a dynamic analysis process aimed at automatically inferring suggested specifications of a program from its execution traces. We describe a method, a framework, and a tool, for mining interobject scenariobased specifications in the form of a UML2compliant variant of Damm and Harel's Live Sequence Charts (LSC), which extends the classical partial order semantics of sequence diagrams with temporal liveness and symbolic class level lifelines, in order to generate compact and expressive specifications. Moreover, we use previous research work and tools developed for LSC to visualize, analyze, manipulate, test, and thus evaluate the scenariobased specifications we mine. Our mining framework is supported by statistically sound metrics. Its effectiveness and the usefulness of the mined scenarios are further improved by an array of extensions to the basic mining algorithm, which include various userguided filters and abstraction mechanisms. We demonstrate and evaluate our work using a case study.
p1347
aVThis article presents our refactoring plugin for the Eclipse Ruby Development Tools IDE. Refactoring is a very important technique for every software engineer to ensure the healthiness of his code and a cornerstone of agile software development. We have implemented sixteen automated refactorings and three code generators, for example Rename Variable and Extract Method.
p1348
aVThis article reveals our work on refactoring plugins for Eclipse's C++ Development Tooling (CDT). With CDT a reliable open source IDE exists for C/C++ developers. Unfortunately it has been lacking of overarching refactoring support. There used to be just one single refactoring  Rename. But our plugin provides several new refactorings which support a C++ developer in his everyday work.
p1349
aVThis article describes the design and use of the CUTE C++ testing framework and its integration into the Eclipse C++ Development Tooling. Unit testing supports code quality and is a corner stone of agile software development. CUTE and its Eclipse plugin are an easy to use C++ testing framework.
p1350
aVOne of the major concerns in the processes which involve human analysts is the existence of uncertainty/inconsistency. In this paper, we propose a model based on belief theory that attempts to capture the degree of analysts' uncertainty towards their expressed specifications and employs these information to create an integrated unique model.
p1351
aVAn Adaptive ObjectModel is a system that represents classes, attributes, relationships, and behavior as metadata. Consequently, the object model is adaptable; when the descriptive information is modified, the system immediately reflects those changes. This architectural style makes a heavy use of patterns and all the attempts to document it have been done using patterns. Nevertheless, the patterns used to document AOMs are written using different templates and styles. Also many of the patterns have not been written yet or are incomplete. This poster session will present a more comprehensive and homogeneous pattern language for describing this kind of architectural style with the ultimate goal being to facilitate the creation of these types of systems.
p1352
aVThis paper presents the design of our language, Unity. Unity has a novel subtyping system that combines both byname and structural subtyping. With this combination, Unity provides the flexiblity of structural subtyping while still allowing static typechecking of external methods.
p1353
aVA new characteristic of design in the 20th century is the dominant use of teams to do design. We design with teams both because we are in a hurry and because our creations require more skills than one mind can master. Yet we want our designs to have excellence, and that requires conceptual integrity. Achieving conceptual integrity in team design is then a formidable challenge. Telecollaboration is now, in the 21st century, not only possible but even fashionable. The mantra of "telecollaboration" assumes implicitly that collaboration is a good thing per se. The more one collaborates, the better. This is far from selfevident; it probably is not true. Nevertheless, there are parts of the design process where collaboration not only shares out the work, but also produces a better design. Here telecollaboration can be most fruitful. Analysis of these aspects of design inevitably generates opinions on how design should be done and taught.
p1354
aVThis poster presents continuing work on Green UML. Green is a UML class diagram plugin for the Eclipse IDE developed originally for educational purposes. Due to this nature of the tool, its prominent features include live roundtripping and a customizable set of relationships. As a plugin to Eclipse, Green is able to utilize the development environment and maintain a realtime synchronization between its class diagrams and the Java source code. The extensible style of Eclipse plugins also allows Green to have its own plugins, which in turn are the relationship semantics. By allowing end users to create, add, and remove relationships which are recognized by Green, the too becomes highly flexible and easily tailors to a user's specific needs.
p1355
aVJava performance is far from trivial to benchmark because it is affected by various factors such as the Java application, its input, the virtual machine, the garbage collector, the heap size, etc. In addition, nondeterminism due to JustinTime compilation/optimization, thread scheduling, etc., causes the execution time of a Java program to differ from run to run. This poster advocates statistically rigorous data analysis when reporting Java performance. We advise to model nondeterminism by computing confidence intervals. In addition, we show that prevalent data analysis approaches may lead to misleading or even incorrect conclusions. Although we focus on Java performance, the techniques can be readily applied to any managed runtime system.
p1356
aVSoftware frameworks contain many internal constraints which plugins cannot break. These constraints are relative to the context of the plugin, and they can involve multiple framework objects. This work creates a lightweight mechanism specifying semantic constraints on the framework and checking plugins against this specification.
p1357
aVDistributed programming has shifted from private networks to the Internet using heterogeneous Web APIs. This enables the creation of situational applications of composed services exposing user interfaces, i.e., mashups. However, this programmable Web lacks unified models that can facilitate mashup creation, reuse, and deployments. This poster demonstrates a platform to facilitate Web 2.0 mashups.
p1358
aVJava Specification Request 305 defines a set of annotations that can understood by multiple static analysis tools. Rather than push the bleeding edge of static analysis, this JSR represents an attempt to satisfy different static analysis tool vendors and address the engineering issues required to make these annotations widely useful.
p1359
aVLive Sequence Charts (LSCs) is a scenariobased language for modeling objectbased reactive systems with liveness properties. A tool called the PlayEngine allows users to create LSC requirements using a pointandclick interface and generate executable traces using features called playout and smart playout. Finite executable trace fragments called supersteps are generated by smart playout in response to user inputs. Each superstep is guaranteed not to violate the LSC requirements, provided one exists. However, nonviolation is not guaranteed beyond each individual superstep. In this work, we demonstrate a powerful extension to smart playout which produces only traces that are guaranteed not to violate the LSC requirements, provided the requirements are realizable. Using this method, we may synthesize correct executable programs directly from LSC requirements.
p1360
aVGenerative Programming advocates developing a family of systems rather than a set of single systems. Feature modeling can assist in supporting the development of such software product lines through software reuse. To our knowledge, CASEFX is the first implementation of statelevel feature modeling support within a CASE tool.
p1361
aVThis poster will present our experiences using FindBugs in production software development environments, including both open source efforts and Google's internal code base. We summarize the defects found, describe the issue of real but trivial defects, and discuss the integration of FindBugs into Google's Mondrian code review system.
p1362
aVRequirements volatility is an issue in software development life cycle which often originated from our incomplete knowledge about the domain of interest. In this paper, we propose an agentbased approach to manage evolving requirements in biomedical software applications using an integrated ontologydriven framework.
p1363
aVWe have created a system that enables programmers to add custom type qualifiers to the Java language in a backwardcompatible way. The system allows programmers to write type qualifiers in their programs and to create compiler plugins that enforce the semantics of these qualifiers at compile time. The system builds on existing Java tools and APIs, and on JSR 308. As an example, we introduce a plugin to Sun's Java compiler that uses our system to typecheck the NonNull qualifier. Programmers can use the @NonNull annotation to prohibit an object reference from being null; then, by invoking a Java compiler with the NonNull plugin, they can check for NonNull errors at compile time and rid their programs of nullpointer exceptions.
p1364
aVElephant 2000 is a proposed programming language good for writing and verifying programs that interact with people (e.g., transaction processing) or interact with programs belonging to other organizations (e.g., electronic data interchange). Communication inputs and outputs are in an I/O language whose sentences are meaningful speech acts identified in the language as questions, answers, offers, acceptances, declinations, requests, permissions, and promises. The correctness of programs is partly defined in terms of proper performance of the speech acts. Answers should be truthful and responsive, and promises should be kept. Sentences of logic expressing these forms of correctness can be generated automatically from the form of the program. Elephant source programs may not need data structures, because they can refer directly to the past. Thus a program can say that an airline passenger has a reservation if he has made one and hasn't cancelled it. Elephant programs themselves can be represented as sentences of logic. Their extensional properties follow from this representation without an intervening theory of programming or anything like Hoare axioms. Elephant programs that interact nontrivially with the outside world can have both inputoutput specifications, relating the programs inputs and outputs, and accomplishment specifications concerning what the program accomplishes in the world. These concepts are respectively generalizations of the philosophers' illocutionary and perlocutionary speech acts. Programs that engage in commercial transactions assume obligations on behalf of their owners in exchange for obligations assumed by other entities. It may be part of the specifications of an Elephant 2000 program that these obligations are exchanged as intended, and this too can be expressed by a logical sentence. Human speech acts involve intelligence. Elephant 2000 is on the borderline of AI, but the talk emphasizes the Elephant usages that do not require AI.
p1365
aVUnit testing frameworks like JUnit are a popular and effective way to prevent developer bugs. We are investigating two ways of building on these frameworks to prevent more bugs with less effort. First, theories are developerwritten statements of correct behavior over a large set of inputs, which can be automatically verified. Second, characterization tools summarize observations over a large number of directed executions, which can be checked by developers, and added to the test suite if they specify intended behavior. We outline a toolset that gives developers the freedom to use either or both of these techniques, and frame further research into their usefulness.
p1366
aVPointers or references can be identified as the root cause of many fundamental problems in current programming languages, typically resulting in unspecified object dependencies and missing hierarchical encapsulation. We therefore propose to abandon references from the language and to use expressive program relations instead. For this purpose, we have developed a programming language which is only based on hierarchical composition and interface connections.
p1367
aVR<scp>ANDOOP</scp> for Java generates unit tests for Java code using feedbackdirected random test generation. Below we describe R<scp>ANDOOP</scp>'s input, output, and test generation algorithm. We also give an overview of RANDOOP's annotationbased interface for specifying configuration parameters that affect R<scp>ANDOOP</scp>'s behavior and output.
p1368
aVWe propose a portable JavaScript thread library to facilitate the development of Ajax applications. It enables programmers to write asynchronous programs using threads. Since the proposed library is implemented without modifying any existing systems, it is portable among popular Web browsers and has high affinity with the existing event system.
p1369
aVApplication developers often use example applications as a guide to learn how to implement a frameworkprovided concept. To ease applying this technique, we present a novel framework comprehension technique called FUDA. FUDA integrates a new dynamic slicing approach with clustering and data mining techniques to generate the implementation recipes of a desired concept.
p1370
aVA variety of different designs and optimisation strategies for trace monitoring have been proposed recently. Here, we examine tradeoffs in simplicity of implementation and expressiveness of supported patterns, briefly discuss the underlying data structures of two mainstream implementations, and provide a short evaluation of the effectiveness ofmemory optimisations.
p1371
aVWeb applications exemplify the need for generative programming techniques in part due to the many languages, artifacts, and groups of developers involved. Some problems remain, including those that that arise from the interplay with versioning. This paper proposes addressing these problems with structured program transformations, and explores a framework for the coevolution of platform artifacts and the models that generate them.
p1372
aVAdaptive Programming allows developers to write structureshy programs. However, in Adaptive Programming, recursive computations are known to require a good deal of boiler plate code to express. This paper describes perobject visitors; a programming construct that allows developers to write recursive adaptive computations at a higher level of abstraction. This paper also describes a prototype implementation for perobject visitors.
p1373
aVRewriting logic semantics provides an environment for defining new and existing languages. These language definitions are formal and executable, providing language interpreters almost for free while also providing a framework for building analysis tools, such as type checkers, model checkers, and abstract interpreters. Large subsets of several existing objectoriented languages have been defined, while a new research language, KOOL, has been created as a platform for experimenting with language features and type systems. At the same time, new tools and formalisms aimed specifically at programming languages are being developed.
p1374
aVOur research involves improving performance of programs written in the Java programming language. By selective specialization of generic types, we enable the compiler to eliminate typecasting, and provide type information to remove dynamic method lookup at runtime. An example of this specialization using Quicksort showed performance improvement of about 25%.
p1375
aVComputer Scientists have been talking about the use of of objectorientation (under a variety of rubrics) to achieve "separation of concerns" for more than 40 years. In all that time, it has been taken for granted that it was the structure of the program text itself that mattered. Whenever it was felt that additional information was needed it was assumed that this would be closely associated with the program text either as simple comments, as inline assertions, or "woven" in with the program text using elaborate tools. This talk takes a different position. It argues that for true separation of concerns we need an integrated set of separate documents, some of which are to be read by people who will never read the code, some that describe the structure of the code, and some that describe the behaviour of individual components. It describes a collection of mathematical ideas and notations that make it possible to produce documentation that is both precise and readable. We then describe the use of these documents in testing and inspection. Finally, it discusses the way that other programming paradigms, particularly functional programming, can be used to make these documents more useful.
p1376
aVWe have created a system that enables programmers to add custom type qualifiers to the Java language in a backwardcompatible way. The system allows programmers to write type qualifiers in their programs and to create compiler plugins that enforce the semantics of these qualifiers at compile time. The system builds on existing Java tools and APIs, and on JSR 308. As an example, we introduce a plugin to Sun's Java compiler that uses our system to typecheck the NonNull qualifier. Programmers can use the @NonNull annotation to prohibit an object reference from being null; then, by invoking a Java compiler with the NonNull plugin, they can check for NonNull errors at compile time and rid their programs of nullpointer exceptions.
p1377
aVCruiseControl.rb is reincarnation of CruiseControl, an open source continuous integration tool, written in Ruby. Its basic purpose in life is to alert members of a software project when one of them checks something into source control that breaks the build. CruiseControl.rb was created by ThoughtWorks to meet the needs of a growing number of Ruby projects in the company. It is small and simple, takes about 10 minutes to install, has a streamlined web interface, and can be easily modified. This application takes the spirit and values of Ruby on Rails web development framework to the field of continuous integration.
p1378
aVGreen is a live round tripping UML class diagram editor plugin for Eclipse, originally designed with the intention of focusing CS1/CS2 students on modeling and design. Green's ease of use and flexible features has allowed it to grow into a robust tool providing end users with an easy to use application satisfying their individual class diagramming needs. Green's live roundtripping capability allows users to generate (Java) code from UML class diagrams and generate diagrams from (Java) code and have them both update each other as any changes are made. This demo will demonstrate the main features of Green, including forward and reverse engineering, live roundtripping, incremental exploration, and the various aspects of relationship semantics. Since Green is an ongoing project, demonstrations of additional features not in this list may also be included.
p1379
aVAs emerging Software Product Line (SPL) technologies have evolved, ModelDriven Development (MDD) has remained an underserved part of the SPL portfolio development lifecycle, making it difficult to simultaneously leverage the benefits of both practices. The Telelogic Rhapsody/BigLever Gears\u2122 Bridge is the industry's first solution to provide fully integrated MDD and SPL technologies. With the Bridge's innovative capabilities, you can achieve new levels of efficiency by utilizing: (1) Rhapsody MDD models, rather than working with conventional source code, and (2) Gears' SPL consolidation, firstclass model variation points, and automated production capabilities   rather than creating "cloneandown" copies of MDD models for each product or building "onesizefitsall" models for all products. This increased efficiency enables you to deliver more new products and features faster, while reducing the development effort and optimizing product quality.
p1380
aVAs software complexity increases, the process of software development is shifting from being codecentric to modelcentric. For this purpose, UML augments the objectoriented paradigm with powerful and flexible behavioral modeling capabilities. It allows the developer to describe the system's behavior in a higher level of abstraction by using state machines, activities, and interactions. To facilitate such modeldriven development, we present a plugin for IBM Rational's modeling tools, which enables the execution, debugging and testing of UML models. The presentation will show how to use our tools to discover defects early in the development cycle, thus preventing costly rework at later stages. We will highlight the innovative features of our tools, such as modellevel debug control, interactive dynamic debugging, and the extensibility that allows developing support for UML profiles.
p1381
aVWe describe Quality of service pICKER (QUICKER), a modeldriven QoS mapping toolchain for supporting the QoS design and evolution of systems software. QUICKER automates the mapping of QoS requirements onto middlewarespecific QoS configuration options by (1) choosing appropriate subset of QoS options for given QoS policies and (2) assigning values to each of these selected QoS options. QUICKER also provides support for validating the generated QoS configurations and resolving any dependencies between them using model checking.
p1382
aVEnergy consumption is a major cost of operating IT equipment in organizations and data centers. Hardware and software manufacturers are beginning to include power saving options in a range of products from processors to operating systems (OS). Previous work on power saving has focused mainly on OS control of the operating modes of laptop computers and mobile devices to maximize battery life. To take full advantage of these newly available energy saving features, we have developed a power management agent with a set of power control and monitoring interfaces which allows power saving features to be implemented on individual applications. We call these power saving capable applications Green Applications. In this demonstration, we will discuss and show a web based Green Application which implements these newly introduced power saving features. This application runs on an IBM HS20 Blade Server with Linux OS and equipped with a processor (Intel Xeon 3 GHz) capable of power management. When the application runs, it continuously adjusts the power level (range from standby to maximum) of the server processor in accordance with the state and performance requirements of the application (expressed as policies or static userdefined control parameters) via the power management agent. The power management agent can manage multiple Green Applications and can set the power level of the processor according to the aggregate power requirement of the applications. Using an advanced monitoring tool, the IBM Tivoli Monitoring (ITM) System [1], we will show graphically the dynamic interactions of the state and performance of the application, including processor temperature, CPU utilization, power cap, and power usage in a single integrated view, and present the amount of energy used in comparison with running the same application with no power saving features included.
p1383
aVBigLever Software Gears is a software product line development tool that allows you to engineer your product line portfolio as though it is a single system. Gears is designed to support and enable all three tiers in the new generation 3Tiered Software Product Line (SPL) Methodology, across the full SPL engineering lifecycle. Gears and the 3Tiered SPL Methodology have played an instrumental role in some of the industry's most notable realworld success stories including Salion, 2004 Software Product line Hall of Fame Inductee, and Engenio/LSI Logic, 2006 Software Product Line Hall of Fame inductee.
p1384
aVWriting developer tests as software is built can provide peace of mind. As the software grows, running the tests can prove that everything still works as the developer envisioned it. But what about the behavior the developer failed to envision? Although verifying a few wellpicked scenarios is often enough, experienced developers know bugs can often lurk even in welltested code, when correct but untested inputs provoke obviously wrong responses. This leads to worry. We suggest writing Theories alongside developer tests, to specify desired universal behaviors. We will demonstrate how writing theories affects testdriven development, how new features in JUnit can verify theories against handpicked inputs, and how a new tool, Theory Explorer, can search for new inputs, leading to a new, less worrysome approach to development.
p1385
aVIn the past years, there is significant growth in the number of new technologies for Java developers. More recently, Web Services and Service Oriented Architectures (SOA) are becoming popular. Projects are now more complex and are becoming more challenging to complete on time. Sybase WorkSpace is a serviceoriented unified design and development environment that includes the power of enterprise modeling with comprehensive tooling capabilities. This demonstration shows how enterprise modeling, database development, Web application development, servicesoriented development and orchestration, and mobile development all come together to build SOA applications quickly. It demonstrates how Sybase WorkSpace integrates the most important design and development tools in an easytouse opensource framework.
p1386
aVContext plays a large role in our perspective on the world around us people see things differently depending on background, role, task at hand, and many other variables. How do different contexts affect developer perspectives on software? What different ways do developers want to see a program? What different ways do they want to work with a program? How does a program mean different things to different people? How does context influence perspective? How do different contexts and perspectives interact? Can these interactions be reified, controlled, and parameterized?. A broad range of work has explored these questions, but many issues remain open. We lack a general understanding of the concepts and mechanisms that can support the changes in perspective we need. We lack the ability to handle context and perspective systematically, easily, and reliably throughout software development. Work is needed in a number of areas, from conceptual foundations to theory, languages, tools, and methods. A truly satisfying handle on these issues may even require a material expansion of the foundations of computation or at the very least the foundations of programming languages.
p1387
aVThis demonstration shows a lightweight and fast method for creating a tested and working domain specific language. The method is demonstrated using the ngrease metalanguage. The creation of a new language is started by writing a representative example of the final product with a test that tests the transformation from a stub source to the result. The test is made to pass by writing a constant transformer that unconditionally outputs the result. At each step the language is extended by refactoring: Some part of the transformer template is converted from a constant subtree to a reference to data read from the source tree, thus driving additions to the new language. Optionally, each refactoring step can be driven by a new test that demonstrates the lack of parameterization of some part of the final product.
p1388
aVLanguage Integrated Query (LINQ) is part of the upcoming version 3.5 of the .NET Framework. As a combination of APIs and enhancements to the .NET programming languages, LINQ provides a uniform approach to querying of data across any data source. LINQ pulls the querying experience into the programming language space, providing full static typing and tool support. LINQ is built to be pluggable, allowing data source providers to insert their own query engines. Using the new C# 3.0 this demonstration peels apart the layers of LINQ to show how a smooth user experience on the surface emerges from new language features, naming conventions and metaprogramming facilities.
p1389
aVDebugging the timing behavior of realtime systems is notoriously difficult, and with a new generation of complex systems consisting of tens of millions of lines of code, the difficulty is increasing enormously. We have developed TuningFork, a tool especially designed for visualization and analysis of largescale realtime systems. TuningFork is capable of recording highfrequency events at submicrosecond resolution with minimal perturbation. Users can visualize system activity online in realtime and interactively explore the data. Data can be gathered from multiple layers and/or components and synthesized into visualizations that illuminate whole system interactions. Interactive exploration of hypothesis is naturally supported by direct manipulation to quickly build up complex visualizations.
p1390
aVThis will be a live demonstration of FindBugs, a static analysis bug finding tool, on the current development version of Eclipse 3.4. FindBugs reports issues such as null pointer dereferences, comparing incompatible types with equals, invalid method calls, infinite recursive loops, bad integer operations, and more. FindBugs reports more than 400 such issues in Eclipse 3.3. During this demonstration, we'll give a quick overview of the FindBugs GUI andwalk through 1020 bug warnings, categorize each warning as to whether or not fixing the issue is important, and enter comments about the bug. We'll be able to browse warnings by date of introduction, so we can see if the issues introduced in the past month are more or less serious than the issues that have been in the code base since Eclipse 3.3, 3.2 or earlier. Vocal audience participation is encouraged, and participants with laptops can follow along and enter their own categorization and comments either during the demonstration or afterwards. Audience members with commit privileges to the Eclipse project will get free FindBugs Tshirts. We'll also briefly demonstrate how to set up FindBugs as part of a production development environment.
p1391
aVProgramming distributed dataintensive web and mobile applications is gratuitously hard. As the world is moving more and more toward the software as services model, we have to come up with practical solutions to build distributed systems that are approachable for normal programmers. Just like Visual Basic democratized programming Windows by removing much of the boilerplate, such as message pumps and window handles, that contributed more to the problem than to the solution, we propose a toolkit of language extensions, APIs, and tools that do the same for web programming. As a result, ordinary programmers can concentrate on the essential aspects of building distributed and mobile applications such as partitioning and flowing code and data across tiers, deployment, security, etc. without getting bogged down in low level details.
p1392
aVWhen people hear "Visual Basic", they remember QuickBasic from the DOS days. As a result, their kneejerk reaction is often that Visual Basic is not really a serious language, and they do not pay it the attention it actually deserves. In reality Visual Basic is a fullfledged modern objectoriented language with many unique features, such as static typing where possible but dynamic typing where necessary, declarative event handling, deep XML integration with optional layered XSD types, highly expressive query comprehension syntax, type inference, etc. etc. This makes Visual Basic actually more interesting to researchers and practitioners than the "popular" static languages such as Java, C# and dynamic languages such as Ruby or JavaScript.
p1393
aVSelenium is a tool for creating and running automated web tests and is a good fit for agile projects where it can be used for creating acceptance tests corresponding to the web application's user stories. This demonstration will show how Selenium additionally can be leveraged to create security tests. First, we model security threats as misuse stories, similar to user stories except that we focus on illegal or nonnormative use of the application. Subsequently, we create security tests in Selenium that manifest the misuse stories by exploiting vulnerabilities in the application. This approach can be seen as a contribution to strengthening the security focus in agile projects by trying to apply familiar agile concepts, methods, and tools to the security aspects of the application. We have found that several of the most common security vulnerabilities in web applications can be addressed with this approach, such as cross site scripting (XSS), broken authentication and access management, information leakage, and improper error handling. This demonstration will show examples of such vulnerabilities and corresponding tests, in addition to discussing the cases where there are shortcomings.
p1394
aVIn this demonstration we show draganddrop distribution of centralized, modular Java applications. Our system is based on OSGi, an industry standard for building Java applications out of modular units loosely connected through services. Since OSGi is a centralized system, we have elaborated a solution to seamlessly distribute OSGi applications along the boundaries of services and thereby turning arbitrary OSGi applications into distributed applications. In this demonstration, we present an Eclipse based tool that takes the source code of an OSGi application as input, produces a graph of its modules and module dependencies, and allows the user to deploy the application across a distributed system by dragginganddropping its constituent modules on different machines. By defining constraints on the distribution, the tool can also support advanced features like loadbalancing or redundancy of modules.
p1395
aVAccidental mutation is a major source of difficulttodetect errors in objectoriented programs. We have built tools that detect and prevent such errors. The tools include a javac plugin that enforces the Javari type system, and a type inference tool. The system is fully compatible with existing Java programs.
p1396
aVThe PTIDEJ project started in 2001 to study code generation from and identification of patterns. Since then, it has evolved into a complete reverseengineering tool suite that includes several identification algorithms. It is a flexible tool suite that attempts to ease as much as possible the development of new identification and analysis algorithms. Recently, the module D<scp>ECOR</scp> has been added to P<scp>TIDEJ</scp> and allows the detection of design defects, which are recurring design problems. In this demonstration, we particularly focus on the creation and use of identification algorithms for design patterns and defects.
p1397
aVThis talk is informed by the concepts of computational reflection including reflective architectures, causal connections, and the definitions of these concepts in an objectoriented setting, and by an effort to radically rethink the humanmachine interactive experience. The latter effort endeavors to design interfaces that are more immersive, more intelligent, and more interactive, and by doing so to change the humanmachine relationship and to create systems that are more responsive to people's needs and actions and that become true "accessories" for expanding our minds.
p1398
aVThe main goal of modeldriven architecture is the generation of the full implementation of a systembased on a precise description of a platformindependent model and a platform model. Such a description must accurately specify the static structure as well as the dynamic behavior of the system. We present a tool   called DEMOCLES   that realizes a hybrid approach to platformindependent modeling. It describes the static structure using a modified UML class diagram that separates query operations from modifier operations. The former are defined in the class diagram via OCL constraints, while the latter are defined using a MOFbased metamodel that contains modifier operations and properties as firstclass entities and augments them with associations and OCL expressions. The tool is an Eclipseplugin that offers overlay views of the structure and behavior with visual editing capabilities and permits execution of a platformindependent system.
p1399
aVDigitalAssets Discoverer is a tool that implements a group of indicators for automatic identification of software components that can be reused in the development of new applications and Web Services. This tool brings into light the J2EE applications portfolio developed inhouse, increasing productivity and anticipating the ROI in companies. The process of components harvesting and analysis uses an interactive user graphical interface that enables the tuning of selected indicators, visualization of the results and publishing the identified components into a reusable software development assets repository.
p1400
aVThe PACC Starter Kit is an eclipsebased development environment that combines a modeldriven development approach with reasoning frameworks that apply performance, safety, and security analyses. These analyses predict runtime behavior based on specifications of component behavior and are accompanied by some measure of confidence.
p1401
aVApplication developers often apply the Monkey See/Monkey Do rule for frameworkbased application development, i.e., they use existing applications as a guide to understand how to implement a desired frameworkprovided concept (e.g., a context menu in an Eclipse view). However, the code that implements the concept of interest might be scattered across and tangled with code implementing other concepts. To address this issue, we introduce a novel framework comprehension technique called FUDA (<u>F</u>ramework API <u>U</u>nderstanding through <u>D</u>ynamic <u>A</u>nalysis). The main idea of this technique is to extract the implementation recipes of a given frameworkprovided concept from dynamic traces with the help of a dynamic slicing approach integrated with clustering and data mining techniques. In this demonstration, we present the prototype implementation of FUDA as two Eclipse plugins, and use them to generate the implementation recipes for a number of concepts in Eclipse views and GEF editors by using only a few example applications.
p1402
aVOne recent change in software development is developers starting to take responsibility for the quality of their work by writing and executing automated tests. As with any new activity, there is a wide range of ways to perform this task. DevCreek collects, aggregates, and displays testing activity for individuals, teams, and the developer community as a whole. The goal is to provide individuals with global norms with which they can compare their testing. This demonstration will show the data collection, realtime feedback, and aggregate reporting facilities within DevCreek. We will exercise our Eclipse plugin during a short, live development session.
p1403
aVNavigate code, find bugs, compute metrics, check style rules, and enforce coding conventions in Eclipse with SemmleCode. SemmleCode is a new free Eclipse plugin that allows you to phrase these tasks as queries over the codebase  it thus takes the search facilities in Eclipse to a whole new level. A large library of queries for common operations is provided, including metrics and Java EE style rules. Query results can be displayed as a tree view, a table view, in the problem view, as charts or graphs, all with links to the source code.
p1404
aVEstimating the real effort spent to implement the requirements of a software system, without superimposing any overhead on the development team, represents a paramount opportunity to keep a software project under control. Lagrein is a software system that tries to address this problem by supporting managers and developers in exploring how a software system has been developed. It supports the visualization of multiple metrics (polymetric views), it links individual requirements to the portions of the source code expected to implement them, it couples the source code with the effort spent in producing it. With a certain level of approximation, Lagrein makes it possible to estimate the effort required to implement each single requirement.
p1405
aVThe JastAdd Extensible Java Compiler is a high quality Java compiler that is easy to extend with new analyses as well as new language constructs. In this demonstration we show how the existing framework for name analysis and type checking can be extended when adding new language constructs to Java. The same techniques have been used to implement all language features of Java 5 as modular extensions to a Java 1.4 compiler.
p1406
aVRecursion is an important concept in computer science and one that possesses beauty and simplicity, yet many educators describe challenges in teaching the topic. Kim Bruce champions the early use of structural recursion in an objectoriented introductory programming course as a more intuitive concept than traditional (functional) recursion. He uses many graphical examples for motivation (e.g., nested boxes, a ringed bullseye, fractals), providing concreteness to the recursive concept. Internally, most of those examples are disguised forms of a basic recursive list pattern. Recursive lists are important in and of themselves and a mainstay within the functional programming paradigm. However, further challenges exist in providing a tangible presentation for pure lists when disassociated from a graphical structure. Recursion is an important concept in computer science and one that possesses beauty and simplicity, yet many educators describe challenges in teaching the topic. Kim Bruce champions the early use of structural recursion in an objectoriented introductory programming course as a more intuitive concept than traditional (functional) recursion. He uses many graphical examples for motivation (e.g., nested boxes, a ringed bullseye, fractals), providing concreteness to the recursive concept. Internally, most of those examples are disguised forms of a basic recursive list pattern. Recursive lists are important in and of themselves and a mainstay within the functional programming paradigm. However, further challenges exist in providing a tangible presentation for pure lists when disassociated from a graphical structure.
p1407
aVTo prevent skilled professionals from being phased out or forced into professions for which they are not talented, organized forms of lifelong learning are needed. Continuing professional development is an approach supporting lifelong learning. This approach is however criticized for being expensive and not providing the necessary knowledge. In response to this, we have executed a study in order to understand how universities can effectively support continuous professional development. By involving industry professionals as participants in university courses using problem based learning, we have designed what we call Practitioner Integrated Learning (PIL). This learning approach has shown positive effects in terms of level of learning, realism, knowledge diffusion, study load and costs. We present a 15months action research project integrating 16 industry managers and 16 university students in a continuing professional development effort. Based on this study, we argue that PIL is a learning approach that effectively supports continuing professional development.
p1408
aVOne recent change in software development is developers starting to take responsibility for the quality of their work by writing and executing automated tests. As with any new activity, there is a wide range of ways to perform this task. DevCreek helps illuminate the testing process and present a visual representation of the underlying rhythms. This film presents nine months of testing and development activity on the DevCreek tool itself in an animated form. We attempt to reveal, warts and all, the effort expended running tests, the test methods exercised, and the added test methods.
p1409
aVIntroductory programming course have two very specific difficulties for novice students. First is the lack of real world examples in the sessions. It is very difficult to find areas of application where all the students are familiar enough and that offers challenging and engaging examples. Second it is the lack of palpable results of the job done. Introductory courses in other fields generate products that the students can show to others, and feel proud about it. In CS1, for example, explaining loops by printing a series of numbers on the screen doesn't yield the same sense of accomplishment as drawing a basic perspective in an architecture course. We propose using Project Hoshimi [1], a Microsoft Platform, as a base for introducing computer programming to CS1 students. Through the paper we discuss how it could be implemented in the classroom, the main advantages and disadvantages in our experience of using Project Hoshimi, comparing its use against other more traditional approaches, as well as against other graphic programming methods such as Alice or videogame based learning.
p1410
aVWe present CodeGenie, a tool that implements a testdriven approach to search and reuse of code available on largescale code repositories. With CodeGenie, developers designtest cases for a desired feature first, similar to Testdriven Development (TDD). However, instead of implementing the feature from scratch, CodeGenie automatically searches foran existing implementation based on information available in the tests. To check the suitability of the candidate results in the local context, each result is automatically woven into the developer's project and tested using the original tests. The developer can then reuse the most suitable result. Later, reused code can also be unwoven from the project as wished. For the code searching and wrapping facilities, CodeGenie relies on Sourcerer, an Internetscale source code infrastructure that we have developed.
p1411
aVOwnership domain annotations express and enforce design intent related to object encapsulation and communication directly in real objectoriented code. First, this work will make the ownership domains type system more expressive. Second, ownership domain annotations enable obtaining, at compile time, the execution structure of an annotated program. The execution structure is sound, hierarchical and scales to large programs. It also conveys more design intent that existing compiletime approaches that do not rely on ownership annotations. Finally, tools will infer these annotations semiautomatically at compile time, once a developer provides the design intent.
p1412
aVCode Search Engines (CSE) can serve as powerful resources of open source code, as they can search in billions of lines of open source code available on the web. The strength of CSEs can be used for several tasks like searching relevant code samples, identifying hotspots, and finding bugs. However, the major limitations in using CSEs for these tasks are that the returned samples are too many and they are often partial. Our framework addresses the preceding limitations and thereby helps in using CSEs for these tasks. We showed the effectiveness of our framework with two tools developedbased on our framework.
p1413
aVAn appealing strategy for supporting specialization of an objectoriented framework is to adopt a domainspecific modeling approach, where a domain metamodel and a code generator are manually developed to support modeldriven framework specialization. Our research advocates that this support can be automated by having an additional specialization layer in the framework.
p1414
aVRefactoring tools promise to increase the speed at which programmers write code, but programmers report that contemporary tools sometimes slow them down. Some of that slowdown can be attributed to the time it takes to activate refactoring tools, typically with a combination of code selection, hotkeys, linear menus, and wizard interfaces. In this paper, I present pie menus and refactoring cues, two new mechanisms for activating refactoring tools. These mechanisms were designed to accommodate how programmers want to refactor and to make good on the promise of refactoring tools to help programmers write code faster.
p1415
aVThis research explores the synergies between objectoriented application frameworks and modeldriven engineering. We propose FrameworkSpecific Modeling Languages (FSMLs) which are domainspecific modeling languages designed for areas of concern to objectoriented frameworks. A frameworkspecific model expressed using an FSML describes how an application built on top of a framework is using the framework. The semantics of FSMLs can be precisely defined based on frameworkcompletion knowledge: the prescribed steps and rules of writing the frameworkcompletion code for the given framework. The mapping between the abstract syntax of an FSML and its base framework's API enables automatic forward, reverse, and roundtrip engineering of thecompletion code.
p1416
aVWe apply speculative multithreading to sequential Java programs in software to achieve speedup on existing multiprocessors. A common speculation library supports both Java bytecode interpreter and JIT compiler implementations. Initial profiling results indicate three main optimizations: adaptive return value prediction, online fork heuristics, and inorder nested method level speculation.
p1417
aVCopyandpaste is a common practice in industrial software development and maintenance, which results in code clones. Prior research has focused on automatically detecting and analyzing code clones from legacy systems and on eliminating clones. We believe that it is equally important to provide automated support in an integrated development environment (IDE) for the copyandpaste practice when programs are being written. By instrumenting an IDE, the cloning relation among multiple copyandpasted code fragments will be tracked, thus obtaining a clone group. The commonality among members of a clone group will be extracted and represented as rules that capture code intent. We envision uses of the extracted rules for better software quality. Our CnP tool is currently targeted at Java and integrated into Eclipse. Empirical evaluation in terms of false positives, usefulness, and usability will be performed.
p1418
aVProviding Quality of Experience (QoE) for Web Services (WS) with minimal performance penalties is a problem that still confounds many eminent researchers in the field. The current solutions offered ignore many factors which effect QoS and the subjective nature of client requirements thereby reducing the client's QoE. The objective of this work is the design of a framework to improve QoE for the WS client. This approach is based on the application of history, environment and rating information to the selection of decomposed web services.
p1419
aVMany contemporary objectoriented programming languages support firstclass queries or comprehensions. These language extensions make it easier for programmers to write queries, but are generally implemented no more efficiently than the code using collections, iterators, and loops that they replace. Crucially, whenever a query is reexecuted, it is recomputed from scratch. We describe a general approach to optimising queries over mutable objects: query results are cached, and those caches are incrementally maintained whenever the collections and objects underlying those queries are updated. We hope that the performance benefits of our optimisations may encourage more general adoption of firstclass queries by objectoriented programmers.
p1420
aVIn multithreaded programming, locks are frequently used as a mechanism for synchronization. Because today's operating systems do not consider lock usage as a scheduling criterion, scheduling decisions can be unfavorable to multithreaded applications, leading to performance issues such as convoying and heavy lock contention in systems with multiple processors. Previous efforts to address these issues (e.g., transactional memory, lockfree data structure) often treat scheduling decisions as "a fact of life," and therefore these solutions try to cope with the consequences of undesirable scheduling instead of dealing with the problem directly. In this paper, we introduce ContentionAware Scheduler (CAScheduler), which is designed to support efficient execution of large multithreaded Java applications in multiprocessor systems. Our proposed scheduler employs a scheduling policy that reduces lock contention. As will be shown in this paper, our prototype implementation of the CAScheduler in Linux and Sun HotSpot virtual machine only incurs 3.5% runtime overhead, while the overall performance differences, when compared with a system with no contention awareness, range from a degradation of 3% in a small multithreaded benchmark to an improvement of 15% in a large Java application server benchmark.
p1421
aVTransactional memory (TM) is a promising concurrency control alternative to locks. Recent work has highlighted important memory model issues regarding TM semantics and exposed problems in existing TM implementations. For safe, managed languages such as Java, there is a growing consensus towards strong atomicity semantics as a sound, scalable solution. Strong atomicity has presented a challenge to implement efficiently because it requires instrumentation of nontransactional memory accesses, incurring significant overhead even when a program makes minimal or no use of transactions. To minimize overhead, existing solutions require either a sophisticated type system, specialized hardware, or static wholeprogram analysis. These techniques do not translate easily into a production setting on existing hardware. In this paper, we present novel dynamic optimizations that significantly reduce strong atomicity overheads and make strong atomicity practical for dynamic language environments. We introduce analyses that optimistically track which nontransactional memory accesses can avoid strong atomicity instrumentation, and we describe a lightweight speculation and recovery mechanism that applies these analyses to generate speculativelyoptimized but safe code for strong atomicity in a dynamicallyloaded environment. We show how to implement these mechanisms efficiently by leveraging existing dynamic optimization infrastructure in a Java system. Measurements on a set of transactional and nontransactional Java workloads demonstrate that our techniques substantially reduce the overhead of strong atomicity from a factor of 5x down to 10% or less over an efficient weak atomicity baseline.
p1422
aVThis paper presents a software transactional memory system that introduces firstclass C++ language constructs for transactional programming. We describe new C++ language extensions, a productionquality optimizing C++ compiler that translates and optimizes these extensions, and a highperformance STM runtime library. The transactional language constructs support C++ language features including classes, inheritance, virtual functions, exception handling, and templates. The compiler automatically instruments the program for transactional execution and optimizes TM overheads. The runtime library implements multiple execution modes and implements a novel STM algorithm that supports both optimistic and pessimistic concurrency control. The runtime switches a transaction's execution mode dynamically to improve performance and to handle calls to precompiled functions and I/O libraries. We present experimental results on 8 cores (two quadcore CPUs) running a set of 20 nontrivial parallel programs. Our measurements show that our system scales well as the numbers of cores increases and that our compiler and runtime optimizations improve scalability.
p1423
aVIn this paper we introduce a novel methodology for verifying a large set of Java programs which builds on recent theoretical developments in program verification: it combines the idea of abstract predicate families and the idea of symbolic execution and abstraction using separation logic. The proposed technology has been implemented in a new automatic verification system, called jStar, which combines theorem proving and abstract interpretation techniques. We demonstrate the effectiveness of our methodology by using jStar to verify example programs implementing four popular design patterns (subject/observer, visitor, factory, and pooling). Although these patterns are extensively used by objectoriented developers in realworld applications, so far they have been highly challenging for existing objectoriented verification techniques.
p1424
aVThe atomic block, a synchronization primitive provided to programmers in transactional memory systems, has the potential to greatly ease the development of concurrent software. However, atomic blocks can still be used incorrectly, and race conditions can still occur at the level of application logic. In this paper, we present a intraprocedural static analysis, formalized as a type system and proven sound, that helps programmers use atomic blocks correctly. Using access permissions, which describe how objects are aliased and modified, our system statically prevents race conditions and enforces typestate properties in concurrent programs. We have implemented a prototype static analysis for the Java language based on our system and have used it to verify several realistic examples.
p1425
aVIn this paper, we consider object protocols that constrain interactions between objects in a program. Several such protocols have been proposed in the literature. For many APIs (such as JDOM, JDBC), API designers constrain how API clients interact with API objects. In practice, API clients violate such constraints, as evidenced by postings in discussion forums for these APIs. Thus, it is important that API designers specify constraints using appropriate object protocols and enforce them. The goal of an object protocol is expressed as a protocol invariant. Fundamental properties such as ownership can be expressed as protocol invariants. We present a language, PROLANG, to specify object protocols along with their protocol invariants, and a tool, INVCOP++, to check if a program satisfies a protocol invariant. INVCOP++ separates the problem of checking if a protocol satisfies its protocol invariant (called protocol correctness), from the problem of checking if a program conforms to a protocol (called program conformance). The former is solved using static analysis, and the latter using runtime analysis. Due to this separation (1) errors made in protocol design are detected at a higher level of abstraction, independent of the program's source code, and (2) performance of conformance checking is improved as protocol correctness has been verified statically. We present theoretical guarantees about the way we combine static and runtime analysis, and empirical evidence that our tool INVCOP++ finds usage errors in widely used APIs. We also show that statically checking protocol correctness greatly optimizes the overhead of checking program conformance, thus enabling API clients to test whether their programs use the API as intended by the API designer.
p1426
aVWe explore the concept of staticdynamic coupling the degree to which changes in a program's static modular structure imply changes to its dynamic structure. This paper investigates the impact of staticdynamic coupling in a programming language on the effort required to evolve the coarse modular structure of programs written in that language. We performed a series of remodularization case studies in both Java and SubjectJ. SubjectJ is designed to be similar to Java, but have strictly less staticdynamic coupling. Our results include quantitative measurestime taken and number of bugs introduced as well as a more subjective qualitative analysis of the remodularization process. All results point in the same direction and suggest that staticdynamic coupling causes substantial accidental complexity for the remodularization of Java programs.
p1427
aVDescriptive names are crucial to understand code. However, good names are notoriously hard to choose and manually changing a globally visible name can be a maintenance nightmare. Hence, tool support for automated renaming is an essential aid for developers and widely supported by popular development environments. This work improves on two limitations in current refactoring tools: too weak preconditions that lead to unsoundness where names do not bind to the correct declarations after renaming, and too strong preconditions that prevent renaming of certain programs. We identify two main reasons for unsoundness: complex name lookup rules make it hard to define sufficient preconditions, and new language features require additional preconditions. We alleviate both problems by presenting a novel extensible technique for creating symbolic names that are guaranteed to bind to a desired entity in a particular context by inverting lookup functions. The inverted lookup functions can then be tailored to create qualified names where otherwise a conflict would occur, allowing the refactoring to proceed and improve on the problem with too strong preconditions. We have implemented renaming for Java as an extension to the JastAdd Extensible Java Compiler and integrated it in Eclipse. We show examples for which other refactoring engines have too weak preconditions, as well as examples where our approach succeeds in renaming entities by inserting qualifications. To validate the extensibility of the approach we have implemented renaming support for Java 5 and AspectJ like intertype declarations as modular extensions to the initial Java 1.4 refactoring engine. The renaming engine is only a few thousand lines of code including extensions and performance is on par with industrial strength refactoring tools.
p1428
aVSince annotations were added to the Java language, many frameworks have moved to using annotated Plain Old Java Objects (POJOs) in their newest releases. Legacy applications are thus forced to undergo extensive restructuring in order to migrate from old framework versions to new versions based on annotations (Version Lockin). Additionally, because annotations are embedded in the application code, changing between framework vendors may also entail largescale manual changes (Vendor Lockin). This paper presents a novel refactoring approach that effectively solves these two problems. Our approach infers a concise set of semanticspreserving transformation rules from two versions of a single class. Unlike prior approaches that detect only simple structural refactorings, our algorithm can infer general composite refactorings and is more than 97% accurate on average. We demonstrate the effectiveness of our approach by automatically upgrading more than 80K lines of the unit testing code of four opensource Java applications to use the latest version of the popular JUnit testing framework.
p1429
aVSoftware engineering tools often deal with the source code of programs retrieved from the web or source code repositories. Typically, these tools only have access to a subset of a program's source code (one file or a subset of files) which makes it difficult to build a complete and typed intermediate representation (IR). Indeed, for incomplete objectoriented programs, it is not always possible to completely disambiguate the syntactic constructs and to recover the declared type of certain expressions because the declaration of many types and class members are not accessible. We present a framework that performs partial type inference and uses heuristics to recover the declared type of expressions and resolve ambiguities in partial Java programs. Our framework produces a complete and typed IR suitable for further static analysis. We have implemented this framework and used it in an empirical study on four large open source systems which shows that our system recovers most declared types with a low error rate, even when only one class is accessible.
p1430
aVTransparent persistence promises to integrate programming languages and databases by allowing programs to access persistent data with the same ease as nonpersistent data. In this work we demonstrate the feasibility of optimizing transparently persistent programs by extracting queries to efficiently prefetch required data. A static analysis derives query structure and conditions across methods that access persistent data. Using the static analysis, our system transforms the program to execute explicit queries. The transformed program composes queries across methods to handle method calls that return persistent data. We extend an existing Java compiler to implement the static analysis and program transformation, handling recursion and parameterized queries. We evaluate the effectiveness of query extraction on the OO7 and TORPEDO benchmarks. This work is focused on programs written in the current version of Java, without languages changes. However, the techniques developed here may also be of value in conjunction with objectoriented languages extended with highlevel query syntax.
p1431
aVThe .NET intermediate language (MSIL) allows expressing both statically verifiable memory and type safe code (typically called managed), as well as unsafe code using direct pointer manipulations. Unsafe code can be expressed in C# by marking regions of code as unsafe. Writing unsafe code can be useful where the rules of managed code are too strict. The obvious drawback of unsafe code is that it opens the door to programming errors typical of C and C++, namely memory access errors such as buffer overruns. Worse, a single piece of unsafe code may corrupt memory and destabilize the entire runtime or allow attackers to compromise the security of the platform. We present a new static analysis based on abstract interpretation to check memory safety for unsafe code in the .NET framework. The core of the analysis is a new numerical abstract domain, Strp, which is used to efficiently compute memory invariants. Strp is combined with lightweight abstract domains to raise the precision, yet achieving scalability. We implemented this analysis in Clousot, a generic static analyzer for .NET. In combination with contracts expressed in FoxTrot, an MSIL based annotation language for .NET, our analysis provides static safety guarantees on memory accesses in unsafe code. We tested it on all the assemblies of the .NET framework. We compare our results with those obtained using existing domains, showing how they are either too imprecise (e.g., Intervals or Octagons) or too expensive (Polyhedra) to be used in practice.
p1432
aVThis paper presents a static analysis of typestatelike temporal specifications of groups of interacting objects, which are expressed using tracematches. Whereas typestate expresses a temporal specification of one object, a tracematch state may change due to operations on any of a set of related objects bound by the tracematch. The paper proposes a latticebased operational semantics equivalent to the original tracematch semantics but better suited to static analysis. The paper defines a static analysis that computes precise local pointsto sets and tracks the flow of individual objects, thereby enabling strong updates of the tracematch state. The analysis has been proved sound with respect to the semantics. A contextsensitive version of the analysis has been implemented as instances of the IFDS and IDE algorithms. The analysis was evaluated on tracematches used in earlier work and found to be very precise. Remaining imprecisions could be eliminated with more precise modeling of references from the heap and of exceptional control flow.
p1433
aVA managed runtime environment, such as the Java virtual machine, is nontrivial to benchmark. Java performance is affected in various complex ways by the application and its input, as well as by the virtual machine (JIT optimizer, garbage collector, thread scheduler, etc.). In addition, nondeterminism due to timerbased sampling for JIT optimization, thread scheduling, and various system effects further complicate the Java performance benchmarking process. Replay compilation is a recently introduced Java performance analysis methodology that aims at controlling nondeterminism to improve experimental repeatability. The key idea of replay compilation is to control the compilation load during experimentation by inducing a prerecorded compilation plan at replay time. Replay compilation also enables teasing apart performance effects of the application versus the virtual machine. This paper argues that in contrast to current practice which uses a single compilation plan at replay time, multiple compilation plans add statistical rigor to the replay compilation methodology. By doing so, replay compilation better accounts for the variability observed in compilation load across compilation plans. In addition, we propose matchedpair comparison for statistical data analysis. Matchedpair comparison considers the performance measurements per compilation plan before and after an innovation of interest as a pair, which enables limiting the number of compilation plans needed for accurate performance analysis compared to statistical analysis assuming unpaired measurements.
p1434
aVThis paper describes a novel approach to reduce the memory consumption of Java programs, by focusing on their "string memory inefficiencies". In recent Java applications, string data occupies a large amount of the heap area. For example, about 40% of the live heap area is used for string data when a production J2EE application server is running. By investigating the string data in the live heap, we identified two types of memory inefficiencies   "duplication" and "unused literals". In the heap, there are many string objects that have the same values. There also exist many string literals whose values are not actually used by the application. Since these inefficiencies exist as live objects, they cannot be eliminated by existing garbage collection techniques, which only remove dead objects. Quantitative analysis of Java heaps in real applications revealed that more than 50% of the string data in the live heap is wasted by these inefficiencies. To reduce the string memory inefficiencies, this paper proposes two techniques at the Java virtual machine level, "StringGC" for eliminating duplicated strings at the time of garbage collection, and "Lazy Body Creation" for delaying part of the literal instantiation until the literal's value is actually used. We also present an interesting technique at the Java program level, which we call "BundleConverter", for preventing unused message literals from being instantiated. Prototype implementations on a production Java virtual machine have achieved about 18% reduction of the live heap in the production application server. The proposed techniques could also reduce the live heap of standard Java benchmarks by 11.6% on average, without noticeable performance degradation.
p1435
aVMany popular programming languages use interpreterbased execution for portability, supporting dynamic or reflective properties, and ease of implementation. Codecopying is an optimization technique for interpreters that reduces the performance gap between interpretation and JIT compilation, offering significant speedups over directthreading interpretation. Due to varying language features and virtual machine design, however, not all languages benefit from codecopying to the same extent. We consider here properties of interpreted languages, and in particular bytecode and virtual machine construction that enhance or reduce the impact of codecopying. We implemented codecopying and compared performance with the original directthreading virtual machines for three languages, Java (SableVM), OCaml, and Ruby (Yarv), examining performance on three different architectures, ia32 (Pentium 4), x86_64 (AMD64) and PowerPC (G5). Best speedups are achieved on ia32 by OCaml (maximum 4.88 times, 2.81 times on average), where a small and simple bytecode design facilitates improvements to branch prediction brought by codecopying. Yarv only slightly improves over directthreading; large working sizes of bytecodes, and a relatively small fraction of time spent in the actual interpreter loop both limit the application of codecopying and its overall net effect. We are able to show that simple ahead of time analysis of VM and execution properties can help determine the suitability of codecopying for a particular VM before an implementation of codecopying is even attempted.
p1436
aVWith Java 5 and C# 2.0, firstorder parametric polymorphism was introduced in mainstream objectoriented programming languages under the name of generics. Although the firstorder variant of generics is very useful, it also imposes some restrictions: it is possible to abstract over a type, but the resulting type constructor cannot be abstracted over. This can lead to code duplication. We removed this restriction in Scala, by allowing type constructors as type parameters and abstract type members. This paper presents the design and implementation of the resulting type constructor polymorphism. Furthermore, we study how this feature interacts with existing objectoriented constructs, and show how it makes the language more expressive.
p1437
aVThe VISITOR design pattern shows how to separate the structure of an object hierarchy from the behaviour of traversals over that hierarchy. The pattern is very flexible; this very flexibility makes it difficult to capture the pattern as anything more formal than prose, pictures and prototypes. We show how to capture the essence of the VISITOR pattern as a reusable software library, by using advanced type system features appearing in modern objectoriented languages such as Scala. We preserve typesafety statically and modularly: no reflection or similar mechanisms are used and modules can be independently compiled. The library is generic, in two senses: not only is it parametrised by both the return type and the shape of the object hierarchy, but also it allows a number of implementation choices (internal versus external control, imperative versus functional behaviour, orthogonal aspects such as tracing and memoisation) to be specified by parameters rather than fixed in early design decisions. Finally, we propose a generalised datatypelike notation,on top of our visitor library: this provides a convenient functional decomposition style in objectoriented languages.
p1438
aVX10 is a modern objectoriented language designed for productivity and performance in concurrent and distributed systems. In this setting, dependent types offer significant opportunities for detecting design errors statically, documenting design decisions, eliminating costly runtime checks (e.g., for array bounds, null values), and improving the quality of generated code. We present the design and implementation of constrained types, a natural, simple, clean, and expressive extension to objectoriented programming: A type C{c} names a class or interface C and a constraint c on the immutable state of C and inscope final variables. Constraints may also be associated with class definitions (representing class invariants) and with method and constructor definitions (representing preconditions). Dynamic casting is permitted. The system is parametric on the underlying constraint system: the compiler supports a simple equalitybased constraint system but, in addition, supports extension with new constraint systems using compiler plugins.
p1439
aVInference of static types for local variables in Java bytecode is the first step of any serious tool that manipulates bytecode, be it for decompilation, transformation or analysis. It is important, therefore, to perform that step as accurately and efficiently as possible. Previous work has sought to give solutions with good worstcase complexity. We present a novel algorithm, which is optimised for the common case rather than worstcase performance. It works by first finding a set of minimal typings that are valid for all assignments, and then checking whether these minimal typings satisfy all uses. Unlike previous algorithms, it does not explicitly build a data structure of type constraints, and it is easy to implement efficiently. We prove that the algorithm produces a typing that is both sound (obeying the rules of the language) and as tight as possible. We then go on to present extensive experiments, comparing the results of the new algorithm against the previously best known method. The experiments include bytecode that is generated in other ways than compilation of Java source. The new algorithm is always faster, typically by a factor 6, but on some real benchmarks the gain is as high as a factor of 92. Furthermore, whereas that previous method is sometimes suboptimal, our algorithm always returns a tightest possible type. We also discuss in detail how we handle primitive types, which is a difficult issue due to the discrepancy in their treatment between Java bytecode and Java source. For the application to decompilation, however, it is very important to handle this correctly.
p1440
aVThis paper presents novel techniques for checking the soundness of a type system automatically using a software model checker. Our idea is to systematically generate every type correct intermediate program state (within some finite bounds), execute the program one step forward if possible using its small step operational semantics, and then check that the resulting intermediate program state is also type correct but do so efficiently by detecting similarities in this search space and pruning away large portions of the search space. Thus, given only a specification of type correctness and the small step operational semantics for a language, our system automatically checks type soundness by checking that the progress and preservation theorems hold for the language (albeit for program states of at most some finite size). Our preliminary experimental results on several languages including a language of integer and boolean expressions, a simple imperative programming language, an objectoriented language which is a subset of Java, and a language with ownership types indicate that our approach is feasible and that our search space pruning techniques do indeed significantly reduce what is otherwise an extremely large search space. Our paper thus makes contributions both in the area of checking soundness of type systems, and in the area of reducing the state space of a software model checker.
p1441
aVLarge software systems are typically composed of multiple layers, written in different languages and loosely coupled using a stringbased interface. For example, in modern webapplications, a server written in Java communicates with a database backend by passing in query strings. This widely prevalent approach is unsafe as the analyses developed for the individual layers are oblivious to the semantics of the dynamically constructed strings, making it impossible to statically reason about the correctness of the interaction. Further, even simple refactoring in such systems is daunting and error prone as the changes must also be applied to isolated string fragments scattered across the code base. We present techniques for deep typechecking and refactoring for systems that combine Java code with a database backend using the Java Persistence API [10]. Deep typechecking ensures that the queries that are constructed dynamically are type safe and that the values returned from the queries are used safely by the program. Deep refactoring builds upon typechecking to allow programmers to safely and automatically propagate code refactorings through the query string fragments. Our algorithms are implemented in a tool called QUAIL. We present experiments evaluating the effectiveness of QUAIL on several benchmarks ranging from 3,369 to 82,907 lines of code.We show that QUAIL is able to verify that 84% of query strings in our benchmarks are type safe. Finally, we show that QUAIL reduces the number of places in the code that a programmer must look at in order to perform a refactoring by several orders of magnitude.
p1442
aVJava 5, the most recent major update to the Java Programming Language, introduced a number of sophisticated features, including a major extension to the type system. While the technical details of these new features are complex, much of this complexity is hidden from the typical Java developer by an ambitious type inference mechanism. Unfortunately, the extensions to the Java 5 type system were so novel that their technical details had not yet been thoroughly investigated in the research literature. As a result, the Java 5 compiler includes a pragmatic but flawed type inference algorithm that is, by design, neither sound nor locally complete. The language specification points out that neither of these failures is catastrophic: the correctness of potentiallyunsound results must be verified during type checking; and incompleteness can usually be worked around by manually providing the method type parameter bindings for a given call site. This paper dissects the type inference algorithm of Java 5 and proposes a signficant revision that is sound and able to calculate correct results where the Java 5 algorithm fails. The new algorithm is locally complete with the exception of a difficult corner case. Moreover, the new algorithm demonstrates that several arbitrary restrictions in the Java type system most notably the ban on lowerbounded type parameter declarations and the limited expressibility of intersection types are unnecessary. We hope that this work will spur the evolution of a more coherent, more comprehensive generic type system for Java.
p1443
aVWe describe semantic mappings of four highlevel programming languages to our delegationbased machine model for aspectoriented programming. One of the languages is a classbased objectoriented one. The other three represent extensions thereof that support various approaches to modularizing crosscutting concerns. We explain informally that an operational semantics expressed in terms of the model's concepts preserves the behavior of a program written in one of the highlevel languages. We hence argue our model to be semantically sound in that sense, as well as sufficiently expressive in order to correctly support features such as classbased objectoriented programming, the openclasses and pointcutandadvice flavors of aspectoriented programming, and dynamic layers. For the latter, being a core feature of contextoriented programming, we also provide a formal semantics.
p1444
aVAfter more than 10 years, AspectOriented Programming (AOP) is still a controversial idea. While the concept of aspects appeals to everyone's intuitions, concrete AOP solutions often fail to convince researchers and practitioners alike. This discrepancy results in part from a lack of an adequate theory of aspects, which in turn leads to the development of AOP solutions that are useful in limited situations. We propose a new theory of aspects that can be summarized as follows: concerns are latent topics that can be automatically extracted using statistical topic modeling techniques adapted to software. Software scattering and tangling can be measured precisely by the entropies of the underlying topicoverfiles and filesovertopics distributions. Aspects are latent topics with high scattering entropy. The theory is validated empirically on both the large scale, with a study of 4,632 Java projects, and the small scale, with a study of 5 individual projects. From these analyses, we identify two dozen topics that emerge as generalpurpose aspects across multiple projects, as well as projectspecific topics/concerns. The approach is also shown to produce results that are compatible with previous methods for identifying aspects, and also extends them. Our work provides not only a concrete approach for identifying aspects at several scales in an unsupervised manner but, more importantly, a formulation of AOP grounded in information theory. The understanding of aspects under this new perspective makes additional progress toward the design of models and tools that facilitate software development.
p1445
aVMultiple dispatch uses the run time types of more than one argument to a method call to determine which method body to run. While several languages over the last 20 years have provided multiple dispatch, most objectoriented languages still support only single dispatch forcing programmers to implement multiple dispatch manually when required. This paper presents an empirical study of the use of multiple dispatch in practice, considering six languages that support multiple dispatch, and also investigating the potential for multiple dispatch in Java programs. We hope that this study will help programmers understand the uses and abuses of multiple dispatch; virtual machine implementors optimise multiple dispatch; and language designers to evaluate the choice of providing multiple dispatch in new programming languages.
p1446
aVIn the realm of componentbased software systems, pursuers of the holy grail of automated application composition face many significant challenges. In this paper we argue that, while the general problem of automated composition in response to highlevel goal statements is indeed very difficult to solve, we can realize composition in a restricted context, supporting varying degrees of manual to automated assembly for specific types of applications. We propose a novel paradigm for composition in flowbased information processing systems, where application design and component development are facilitated by the pervasive use of faceted, tagbased descriptions of processing goals, of component capabilities, and of structural patterns of families of application. The facets and tags represent different dimensions of both data and processing, where each facet is modeled as a finite set of tags that are defined in a controlled folksonomy. All data flowing through the system, as well as the functional capabilities of components are described using tags. A customized AI planner is used to automatically build an application, in the form of a flow of components, given a highlevel goal specification in the form of a set of tags. Endusers use an automatically populated faceted search and navigation mechanism to construct these highlevel goals. We also propose a novel software engineering methodology to design and develop a set of reusable, welldescribed components that can be assembled into a variety of applications. With examples from a case study in the Financial Services domain, we demonstrate that composition using a faceted, tagbased application design is not only possible, but also extremely useful in helping endusers create situational applications from a wide variety of available components.
p1447
aVCurrent programming languages and software engineering paradigms are proving insufficient for building intelligent multiagent systems such as interactive games and narratives where developers are called upon to write increasingly complex behavior for agents in dynamic environments. A promising solution is to build adaptive systems; that is, to develop software written specifically to adapt to its environment by changing its behavior in response to what it observes in the world. In this paper we describe a new programming language, An Adaptive Behavior Language (A2BL), that implements adaptive programming primitives to support partial programming, a paradigm in which a programmer need only specify the details of behavior known at codewriting time, leaving the runtime system to learn the rest. Partial programming enables programmers to more easily encode software agents that are difficult to write in existing languages that do not offer languagelevel support for adaptivity. We motivate the use of partial programming with an example agent coded in a cuttingedge, but nonadaptive agent programming language (ABL), and show how A2BL can encode the same agent much more naturally.
p1448
aVWe describe an extension of Visual Basic 9.0 with asynchronous concurrency constructs  join patterns  based on the join calculus. Our design of Concurrent Basic (CB) builds on earlier work on Polyphonic C# and Comega. Since that work, the need for languageintegrated concurrency has only grown, both due to the arrival of commodity, multicore hardware, and the trend for Rich Internet Applications that rely on asynchronous clientserver communication to hide latency. Unlike its predecessors, CB adopts an eventlike syntax that should be familiar to existing VB programmers. Coupled with Generics, CB allows one to declare reuseable concurrency abstractions that were clumsy to express previously. CB removes its ancestors' inconvenient inheritance restriction, while providing new extensibility points useful in practical applications that must coexist with or want to exploit alternative threading models available on the platform. CB is implemented as an extension of the production VB 9.0 compiler.
p1449
aVThis paper presents WHITEOAK: a JAVA extension that introduces structural type equivalence and subtyping into the language. We argue that structural subtyping addresses common software design problems, and promotes the development of loosely coupled modules without compromising type safety. We discuss language design issues, including subtyping in face of selfreferencing structural types, compiletime operators for computing the new types from existing ones, and the semantics of constructors and nonabstract methods in structural types. We describe implementation techniques, including the compiletime and runtime challenges that we faced (in particular, preserving the identity of objects). Measurement indicate that the performance of our implementation of structural dispatching is comparable to that of the JVM's standard invocation mechanisms.
p1450
aVLanguage extensions increase programmer productivity by providing concise, often domainspecific syntax, and support for static verification of correctness, security, and style constraints. Language extensions can often be realized through translation to the base language, supported by preprocessors and extensible compilers. However, various kinds of extensions require further adaptation of a base compiler's internal stages and components, for example to support separate compilation or to make use of lowlevel primitives of the platform (e.g., jump instructions or unbalanced synchronization). To allow for a more loosely coupled approach, we propose an open compiler model based on normalization steps from a highlevel language to a subset of it, the core language. We developed such a compiler for a mixed Java and (core) bytecode language, and evaluate its effectiveness for composition mechanisms such as traits, as well as statementlevel and expressionlevel language extensions.
p1451
aVType safety and garbage collection in managed languages eliminate memory errors such as dangling pointers, double frees, and leaks of unreachable objects. Unfortunately, a program still leaks memory if it maintains references to objects it will never use again. Leaked objects decrease program locality and increase garbage collection frequency and workload. A growing leak will eventually exhaust memory and crash the program. This paper introduces a leak tolerance approach called Melt that safely eliminates performance degradations and crashes due to leaks of dead but reachable objects in managed languages, given sufficient disk space to hold leaking objects. Melt (1) identifies stale objects that the program is not accessing; (2) segregates inuse and stale objects by storing stale objects to disk; and (3) preserves safety by activating stale objects if the program subsequently accesses them. We design and build a prototype implementation of Melt in a Java VM and show it adds overhead low enough for production systems. Whereas existing VMs grind to a halt and then crash on programs with leaks, Melt keeps many of these programs running much longer without significantly degrading performance. Melt provides users the illusion of a fixed leak and gives developers more time to fix leaky programs.
p1452
aVIt has been observed that componentbased applications exhibit object churn, the excessive creation of shortlived objects, often caused by trading performance for modularity. Because churned objects are shortlived, they appear to be good candidates for stack allocation. Unfortunately, most churned objects escape their allocating function, making escape analysis ineffective. We reduce object churn with three contributions. First, we formalize two measures of churn, capture and control (15). Second, we develop lightweight dynamic analyses for measuring both capture and control. Third, we develop an algorithm that uses capture and control to inline portions of the call graph to make churned objects nonescaping, enabling churn optimization via escape analysis. JOLT is a lightweight dynamic churn optimizer that uses our algorithms. We embedded JOLT in the JIT compiler of the IBM J9 commercial JVM, and evaluated JOLT on large application frameworks, including Eclipse and JBoss. We found that JOLT eliminates over 4 times as many allocations as a stateoftheart escape analysis alone.
p1453
aVCoping with software defects that occur in the postdeployment stage is a challenging problem: bugs may occur only when the system uses a specific configuration and only under certain usage scenarios. Nevertheless, halting production systems until the bug is tracked and fixed is often impossible. Thus, developers have to try to reproduce the bug in laboratory conditions. Often the reproduction of the bug consists of the lion share of the debugging effort. In this paper we suggest an approach to address the aforementioned problem by using a specialized runtime environment (QVM, for Quality Virtual Machine). QVM efficiently detects defects by continuously monitoring the execution of the application in a production setting. QVM enables the efficient checking of violations of userspecified correctness properties, e.g., typestate safety properties, Java assertions, and heap properties pertaining to ownership. QVM is markedly different from existing techniques for continuous monitoring by using a novel overhead manager which enforces a userspecified overhead budget for quality checks. Existing tools for error detection in the field usually disrupt the operation of the deployed system. QVM, on the other hand, provides a balanced trade off between the cost of the monitoring process and the maintenance of sufficient accuracy for detecting defects. Specifically, the overhead cost of using QVM instead of a standard JVM, is low enough to be acceptable in production environments. We implemented QVM on top of IBM's J9 Java Virtual Machine and used it to detect and fix various errors in realworld applications.
p1454
aVThis paper presents Flapjax, a language designed for contemporary Web applications. These applications communicate with servers and have rich, interactive interfaces. Flapjax provides two key features that simplify writing these applications. First, it provides event streams, a uniform abstraction for communication within a program as well as with external Web services. Second, the language itself is reactive: it automatically tracks data dependencies and propagates updates along those dataflows. This allows developers to write reactive interfaces in a declarative and compositional style. Flapjax is built on top of JavaScript. It runs on unmodified browsers and readily interoperates with existing JavaScript code. It is usable as either a programming language (that is compiled to JavaScript) or as a JavaScript library, and is designed for both uses. This paper presents the language, its design decisions, and illustrative examples drawn from several working Flapjax applications.
p1455
aVPrior work has found call path profiles to be useful for optimizers and programmerproductivity tools. Unfortunately, previous approaches for collecting path profiles are expensive: they need to either execute additional instructions (to track calls and returns) or they need to walk the stack. The stateoftheart techniques for call path profiling slow down the program by 7% (for C programs) and 20% (for Java programs). This paper describes an innovative technique that collects minimal information from the running program and later (offline) infers the full call paths from this information. The key insight behind our approach is that readily available information during program execution  the height of the call stack and the identity of the current executing function  are good indicators of calling context. We call this pair a context identifier. Because more than one call path may have the same context identifier, we show how to disambiguate context identifiers by changing the sizes of function activation records. This disambiguation has no overhead in terms of executed instructions. We evaluate our approach on the SPEC CPU 2006 C++ and C benchmarks. We show that collecting context identifiers slows down programs by 0.17% (geometric mean). We can map these context identifiers to the correct unique call path 80% of the time for C++ programs and 95% of the time for C programs.
p1456
aVFullfeatured integrated development environments have become critical to the adoption of new programming languages. Key to the success of these IDEs is the provision of services tailored to the languages. However, modern IDEs are large and complex, and the cost of constructing one from scratch can be prohibitive. Generators that work from language specifications reduce costs but produce environments that do not fully reflect distinctive language characteristics. We believe that there is a practical middle ground between these extremes that can be effectively addressed by an open, semiautomated strategy to IDE development. This strategy is to reduce the burden of IDE development as much as possible, especially for internal IDE details, while opening opportunities for significant customizations to IDE services. To reduce the effort needed for customization we provide a combination of frameworks, templates, and generators. We demonstrate an extensible IDE architecture that embodies this strategy, and we show that this architecture can be used to produce customized IDEs, with a moderate amount of effort, for a variety of interesting languages.
p1457
aVProgrammers build largescale systems with multiple languages to reuse legacy code and leverage languages best suited to their problems. For instance, the same program may use Java for easeofprogramming and C to interface with the operating system. These programs pose significant debugging challenges, because programmers need to understand and control code across languages, which may execute in different environments. Unfortunately, traditional multilingual debuggers require a single execution environment. This paper presents a novel composition approach to building portable mixedenvironment debuggers, in which an intermediate agent interposes on language transitions, controlling and reusing singleenvironment debuggers. We implement debugger composition in Blink, a debugger for Java, C, and the Jeannie programming language. We show that Blink is (1) relatively simple: it requires modest amounts of new code; (2) portable: it supports multiple Java Virtual Machines, C compilers, operating systems, and component debuggers; and (3) powerful: composition eases debugging, while supporting new mixedlanguage expression evaluation and Java Native Interface (JNI) bug diagnostics. In realworld case studies, we show that languageinterface errors require singleenvironment debuggers to restart execution multiple times, whereas Blink directly diagnoses them with one execution. We also describe extensions for other mixedenvironments to show debugger composition will generalize.
p1458
aVThe Task Parallel Library (TPL) is a library for .NET that makes it easy to take advantage of potential parallelism in a program. The library relies heavily on generics and delegate expressions to provide custom control structures expressing structured parallelism such as mapreduce in user programs. The library implementation is built around the notion of a task as a finite CPUbound computation. To capture the ubiquitous applytoall pattern the library also introduces the novel concept of a replicable task. Tasks and replicable tasks are assigned to threads using work stealing techniques, but unlike traditional implementations based on the THE protocol, the library uses a novel data structure called a 'duplicating queue'. A surprising feature of duplicating queues is that they have sequentially inconsistent behavior on architectures with weak memory models, but capture this nondeterminism in a benign way by sometimes duplicating elements. TPL ships as part of the Microsoft Parallel Extensions for the .NET framework 4.0, and forms the foundation of Parallel LINQ queries (however, note that the productized TPL library may differ in significant ways from the basic design described in this article).
p1459
aVWe present the DOOP framework for pointsto analysis of Java programs. DOOP builds on the idea of specifying pointer analysis algorithms declaratively, using Datalog: a logicbased language for defining (recursive) relations. We carry the declarative approach further than past work by describing the full endtoend analysis in Datalog and optimizing aggressively using a novel technique specifically targeting highly recursive Datalog programs. As a result, DOOP achieves several benefits, including full orderofmagnitude improvements in runtime. We compare DOOP with Lhotak and Hendren's PADDLE, which defines the state of the art for contextsensitive analyses. For the exact same logical pointsto definitions (and, consequently, identical precision) DOOP is more than 15x faster than PADDLE for a 1callsite sensitive analysis of the DaCapo benchmarks, with lower but still substantial speedups for other important analyses. Additionally, DOOP scales to very precise analyses that are impossible with PADDLE and Whaley et al.'s bddbddb, directly addressing open problems in past literature. Finally, our implementation is modular and can be easily configured to analyses with a wide range of characteristics, largely due to its declarativeness.
p1460
aVBruce and Foster proposed the language LOOJ, an extension of Java with the notion of MyType, which represents the type of a self reference and changes its meaning along with inheritance. MyType is useful to write extensible yet typesafe classes for objects with recursive interfaces, that is, ones with methods that take or return objects of the same type as the receiver. Although LOOJ has also generics, MyType has been introduced as a feature rather orthogonal to generics. As a result, LOOJ cannot express an interface that refers to the same generic class recursively but with different type arguments. This is a significant limitation because such an interface naturally arises in practice, for example, in a generic collection class with method map(), which converts a collection to the same kind of collection of a different element type. Altherr and Cremet and Moors, Piessens, and Odersky gave solutions to this problem but they used a highly sophisticated combination of advanced mechanisms such as abstract type members, higherorder type constructors, and Fbounded polymorphism. In this paper, we give another solution by introducing self type constructors, which integrate MyType and generics so that MyType can take type arguments in a generic class. Self type constructors are tailored to writing recursive interfaces more concicely than previous solutions. We demonstrate the expressive power of self type constructors by means of examples, formalize a core language with self type constructors, and prove its type safety.
p1461
aVMany popular scripting languages such as Ruby, Python, and Perl include highly dynamic language constructs, such as an eval method that evaluates a string as program text. While these constructs allow terse and expressive code, they have traditionally obstructed static analysis. In this paper we present PRuby, an extension to Diamondback Ruby (DRuby), a static type inference system for Ruby. PRuby augments DRuby with a novel dynamic analysis and transformation that allows us to precisely type uses of highly dynamic constructs. PRuby's analysis proceeds in three steps. First, we use runtime instrumentation to gather perapplication profiles of dynamic feature usage. Next, we replace dynamic features with statically analyzable alternatives based on the profile. We also add instrumentation to safely handle cases when subsequent runs do not match the profile. Finally, we run DRuby's static type inference on the transformed code to enforce type safety. We used PRuby to gather profiles for a benchmark suite of sample Ruby programs. We found that dynamic features are pervasive throughout the benchmarks and the libraries they include, but that most uses of these features are highly constrained and hence can be effectively profiled. Using the profiles to guide type inference, we found that DRuby can generally statically type our benchmarks modulo some refactoring, and we discovered several previously unknown type errors. These results suggest that profiling and transformation is a lightweight but highly effective approach to bring static typing to highly dynamic languages.
p1462
aVPrograms written in managed languages are compiled to a platformindependent intermediate representation, such as Java bytecode. The relative high level of Java bytecode has engendered a widespread practice of changing the bytecode directly, without modifying the maintained version of the source code. This practice, called bytecode engineering or enhancement, has become indispensable in introducing various concerns, including persistence, distribution, and security, transparently. For example, transparent persistence architectures help avoid the entanglement of business and persistence logic in the source code by changing the bytecode directly to synchronize objects with stable storage. With functionality added directly at the bytecode level, the source code reflects only partial semantics of the program. Specifically, the programmer can neither ascertain the program's runtime behavior by browsing its source code, nor map the runtime behavior back to the original source code. This paper presents an approach that improves the utility of sourcelevel programming tools by providing enhancement specifications written in a domainspecific language. By interpreting the specifications, a sourcelevel programming tool can gain an awareness of the bytecode enhancements and improve its precision and usability. We demonstrate the applicability of our approach by making a source code editor and a symbolic debugger enhancementsaware.
p1463
aVAn object diagram makes explicit the object structures that are only implicit in a class diagram. An object diagram may be missing and must extracted from the code. Alternatively, an existing diagram may be inconsistent with the code, and must be analyzed for conformance with the implementation. One can generalize the global object diagram of a system into a runtime architecture which abstracts objects into components, represents how those components interact, and can decompose a component into a nested subarchitecture. A static object diagram represents all objects and interobject relations possibly created, and is recovered by static analysis of a program. Existing analyses extract static object diagrams that are nonhierarchical, do not scale, and do not provide meaningful architectural abstraction. Indeed, architectural hierarchy is not readily observable in arbitrary code. Previous approaches used breaking language extensions to specify hierarchy and instances in code, or used dynamic analyses to extract dynamic object diagrams that show objects and relations for a few program runs. Typecheckable ownership domain annotations use existing language support for annotations and specify in code object encapsulation, logical containment and architectural tiers. These annotations enable a pointsto static analysis to extract a sound global object graph that provides architectural abstraction by ownership hierarchy and by types, where architecturally significant objects appear near the top of the hierarchy and data structures are further down. Another analysis can abstract an object graph into a built runtime architecture. Then, a third analysis can compare the built architecture to a target, analyze and measure their structural conformance, establish traceability between the two and identify interesting differences.
p1464
aVModeldriven development (MDD) is widely used to develop modern business applications. MDD involves creating models at different levels of abstractions. Starting with models of domain concepts, these abstractions are successively refined, using transforms, to designlevel models and, eventually, codelevel artifacts. Although many tools exist that support transform creation and verification, tools that help users in understanding and using transforms are rare. In this paper, we present an approach for assisting users in understanding model transformations and debugging their input models. We use automated programanalysis techniques to analyze the transform code and compute constraints under which a transformation may fail or be incomplete. These codelevel constraints are mapped to the input model elements to generate modellevel rules. The rules can be used to validate whether an input model violates transform constraints, and to support general user queries about a transformation. We have implemented the analysis in a tool called XYLEM. We present empirical results, which indicate that (1) our approach can be effective in inferring useful rules, and (2) the rules let users efficiently diagnose a failing transformation without examining the transform source code.
p1465
aVMultiple inheritance has long been plagued with the "diamond" inheritance problem, leading to solutions that restrict expressiveness, such as mixins and traits. Instead, we address the diamond problem directly, considering two difficulties it causes: ensuring a correct semantics for object initializers, and typechecking multiple dispatch in a modular fashionthe latter problem arising even with multiple interface inheritance. We show that previous solutions to these problems are either unsatisfactory or cumbersome, and suggest a novel approach: supporting multiple inheritance but forbidding diamond inheritance. Expressiveness is retained through two features: a "requires" construct that provides a form of subtyping without inheritance (inspired by Scala), and a dynamicallydispatched "super" call similar to that found in traits. Through examples, we illustrate that inheritance diamonds can be eliminated via a combination of "requires" and ordinary inheritance. We provide a sound formal model for our language and demonstrate its modularity and expressiveness.
p1466
aVMulticore processors are widely used in computer systems. As the performance of microprocessors greatly exceeds that of memory, the memory wall becomes a limiting factor. It is important to understand how the large disparity of speed between processor and memory influences the performance and scalability of Java applications on emerging multicore platforms. In this paper, we studied two popular Java benchmarks, SPECjbb2005 and SPECjvm2008, on multicore platforms including Intel Clovertown and AMD Phenom. We focus on the "partially scalable" benchmark programs. With smaller number of CPU cores these programs scale perfectly, but when more cores and software threads are used, the slope of the scalability curve degrades dramatically. We identified a strong correlation between scalability, object allocation rate and memory bus write traffic in our experiments with our partially scalable programs. We find that these applications allocate large amounts of memory and consume almost all the memory write bandwidth in our hardware platforms. Because the write bandwidth is so limited, we propose the following hypothesis: the scalability and performance is limited by the object allocation on emerging multicore platforms for those objectsallocation intensive Java applications, as if these applications are running into an "allocation wall". In order to verify this hypothesis, several experiments are performed, including measuring key architecture level metrics, composing a microbenchmark program, and studying the effect of modifying some of the "partially scalable" programs. All the experiments strongly suggest the existence of the allocation wall.
p1467
aVWe propose a novel online method of identifying the preferred NUMA nodes for objects with negligible overhead during the garbage collection time as well as object allocation time. Since the number of CPUs (or NUMA nodes) is increasing recently, it is critical for the memory manager of the runtime environment of an objectoriented language to exploit the low latency of local memory for high performance. To locate the CPU of a thread that frequently accesses an object, prior research uses the runtime information about memory accesses as sampled by the hardware. However, the overhead of this approach is high for a garbage collector. Our approach uses the information about which thread can exclusively access an object, or the Dominant Thread (DoT). The dominant thread of an object is the thread that often most accesses an object so that we do not require memory access samples. Our NUMAaware GC performs DoT based object copying, which copies each live object to the CPU where the dominant thread was last dispatched before GC. The dominant thread information is known from the thread stack and from objects that are locked or reserved by threads and is propagated in the object reference graph. We demonstrate that our approach can improve the performance of benchmark programs such as SPECpower ssj2008, SPECjbb2005, and SPECjvm2008.We prototyped a NUMAaware memory manager on a modified version of IBM Java VM and tested it on a ccNUMA POWER6 machine with eight NUMA nodes. Our NUMAaware GC achieved performance improvements up to 14.3% and 2.0% on average over a JVM that only used the NUMAaware allocator. The total improvement using both the NUMAaware allocator and GC is up to 53.1% and 10.8% on average.
p1468
aVObject versioning refers to how an application can have access to previous states of its objects. Implementing this mechanism is hard because it needs to be efficient in space and time, and well integrated with the programming language. This paper presents HistOOry, an object versioning system that uses an efficient data structure to store and retrieve past states. It needs only three primitives, and existing code does not need to be modified to be versioned. It provides finegrained control over what parts of objects are versioned and when. It stores all states, past and present, in memory. Code can be executed in the past of the system and will see the complete system at that point in time. We have implemented our model in Smalltalk and used it for three applications that need versioning: checked postconditions, stateful execution tracing and a planar point location implementation. Benchmarks are provided to asses the practical complexity of our implementation.
p1469
aVModern objectoriented languages have complex features that cause programmers to overspecify their programs. This overspecification hinders automatic optimizers, since they must preserve the overspecified semantics. If an optimizer knew which semantics the programmer intended, it could do a better job. Making a programmer clarify his intentions by placing assumptions into the program is rarely practical. This is because the programmer does not know which parts of the programs' overspecified semantics hinder the optimizer. Therefore, the programmer has to guess which assumption to add. Since the programmer can add many different assumptions to a large program, he will need to place many such assumptions before he guesses right and helps the optimizer. We present IOpt, a practical optimizer that uses a specification of the programmers' intended semantics to enable additional optimizations. That way, our optimizer can significantly improve the performance of a program. We present case studies in which we use IOpt to speed up two programs by a factor of 2. To make specifying the intended semantics practical, IOpt communicates with the programmer. IOpt identifies which assumptions the programmer should place, and where he should place them. IOpt ranks each assumption by (i) the likelyhood that the assumption conforms to the programmers' intended semantics and (ii) how much the assumption will help IOpt improve the programs' performance. IOpt proposes ranked assumptions to the programmer, who just picks those that conform to his intended semantics.With this approach, IOpt keeps the programmers' specification burden low. In our case studies, programmers had to add just a few assumptions to realize significant performance speedups.
p1470
aVGeneric classes can be used to improve performance by allowing compiletime polymorphism. But the applicability of compiletime polymorphism is narrower than that of runtime polymorphism, and it might bloat the object code. We advocate a programming principle whereby a generic class should be implemented in a way that minimizes the dependencies between its members (nested types, methods) and its generic type parameters. Conforming to this principle (1) reduces the bloat and (2) gives rise to a previously unconceived manner of using the language that expands the applicability of compiletime polymorphism to a wider range of problems. Our contribution is thus a programming technique that generates faster and smaller programs. We apply our ideas to GCC's STL containers and iterators, and we demonstrate notable speedups and reduction in object code size (real application runs 1.2x to 2.1x faster and STL code is 1x to 25x smaller). We conclude that standard generic APIs (like STL) should be amended to reflect the proposed principle in the interest of efficiency and compactness. Such modifications will not break old code, simply increase flexibility. Our findings apply to languages like C++, C#, and D, which realize generic programming through multiple instantiations.
p1471
aVIntegrated development environments (IDEs) increase programmer productivity, providing rapid, interactive feedback based on the syntax and semantics of a language. A heavy burden lies on developers of new languages to provide adequate IDE support. Code generation techniques provide a viable, efficient approach to semiautomatically produce IDE plugins. Key components for the realization of plugins are the language's grammar and parser. For embedded languages and language extensions, constituent IDE plugin modules and their grammars can be combined. Unlike conventional parsing algorithms, scannerless generalizedLR parsing supports the full set of contextfree grammars, which is closed under composition, and hence can parse language embeddings and extensions composed from separate grammar modules. To apply this algorithm in an interactive environment, this paper introduces a novel error recovery mechanism, which allows it to be used with files with syntax errors   common in interactive editing. Error recovery is vital for providing rapid feedback in case of syntax errors, as most IDE services depend on the parser from syntax highlighting to semantic analysis and crossreferencing. We base our approach on the principles of island grammars, and derive permissive grammars with error recovery productions from normal SDF grammars. To cope with the added complexity of these grammars, we adapt the parser to support backtracking. We evaluate the recovery quality and performance of our approach using a set of composed languages, based on Java and Stratego.
p1472
aVDesign pattern density is a metric that measures how much of an objectoriented design can be understood and represented as instances of design patterns. Expert developers have long believed that a high design pattern density implies a high maturity of the design under inspection. This paper presents a quantifiable and observable definition of this metric. The metric is illustrated and qualitatively validated using four realworld case studies. We present several hypotheses of the metric's meaning and their implications, including the one about design maturity. We propose that the design pattern density of a maturing framework has a fixed point and we show that if software design patterns make learning frameworks easier, a framework's design pattern density is a measure of how much easier it will become.
p1473
aVThis paper demonstrates how the cognitive model of the mind can explain the core fundamentals behind widely accepted design principles. The conclusion is that software design is largely a task of chunking analogies and presents a theory that is detailed enough to be accessible to even the most inexperienced programmer. The corollary of which is a pedagogical approach to understanding design principles rather than the necessity of years of software development experience.
p1474
aVSuccessful programming languages change as they age. They tend to become more complex, and eventually some features become outdated or are rarely used. Programming tools for these languages become more complex as well, since they have to support archaic features. Old programs are hard to maintain, since these archaic features are unfamiliar to modern programmers. These problems can be solved by refactoring tools that can transform programs to use the modern form. We show that refactoring tools can ease the cost of program evolution by examining the evolution of two languages, Fortran and Java, and showing that each change corresponds to an automatable refactoring.
p1475
aVObjectoriented languages involve a threefold tradeoff between runtime efficiency, expressiveness (multiple inheritance), and modularity, i.e. openworld assumption (OWA). Runtime efficiency is conditioned by both the implementation technique and compilation scheme. The former specifies the data structures that support method invocation, attribute access and subtype testing. The latter consists of the production line of an executable from the source code. Many implementation techniques have been proposed and several compilation schemes can be considered from fully global compilation under the closedworld assumption (CWA) to separate compilation with dynamic loading under the OWA, with midway solutions. This article reviews a significant subset of possible combinations and presents a systematic, empirical comparison of their respective efficiencies with all other things being equal. The testbed consists of the Prm compiler that has been designed for this purpose. The considered techniques include C++ subobjects, coloring, perfect hashing, binary tree dispatch and caching. A variety of processors were considered. Qualitatively, these first results confirm the intuitive or theoretical abstract assessments of the tested approaches. As expected, efficiency increases as CWA strengthens. From a quantitative standpoint, the results are the first to precisely compare the efficiency of techniques that are closely associated with specific languages like C++ and Eiffel. They also confirm that perfect hashing should be considered for implementing Java and .Net interfaces.
p1476
aVCurrent programming languages and techniques realize many features which allow their users to extend these languages on a semantic basis: classes, functions, interfaces, aspects and other entities can be defined. However, there is a lack of modern programming languages which are both semantically and syntactically extensible from within the language itself, i.e., with no additional tool or metalanguage. In this paper we present \u03c0 as an approach that aims to overcome this lack. \u03c0 provides an abstraction mechanism based on parameterized symbols which is capable of semantically and syntactically unifying programming concepts like variables, controlstructures, procedures and functions into one concept: the pattern. We have evaluated the abstraction potential and the syntactic extensibility of \u03c0 by successfully creating patterns for the aforementioned programming concepts. \u03c0 could serve as a tool for designing new experimental languages and might generally influence the view we have on current programming concepts.
p1477
aVSoftware systems evolve over time. Currently we do not do a good job of documenting this evolution. This essay discusses the need to better document software evolution and introduces the Moving Picture Metaphor. Source Control Management systems are more like collections of still photographs than moving pictures. Still photography is not ideal when trying to capture evolutional changes. Moving pictures do a much better job. A storyteller can use moving pictures to tell compelling stories that are easier to digest than traditional documentation. We can learn a great deal from watching stories that document a system's evolution.
p1478
aVDuring its formative decades the software community looked twice to the theories of Christopher Alexander for inspiration, both times failing to completely master the architect's most useful insights. Now a third opportunity presents itself with Alexander's recent publication, The Nature of Order. Serious apprenticeship, however, imposes a prerequisite of sober selfreflection and evaluation. What, really, is the nature of the developer's tasks? Under what philosophical umbrella has the software community matured until now? Do other philosophical traditions offer alternative and perhaps more pertinent epistemologies? What voices, besides Alexander's, might contribute to the community's evolution? We address these questions along with theory building, ethnography, weak links, design heuristics, agility, and complex systems, all of which combine with Alexander's new theories to suggest different ways of doing what we do, better.
p1479
aVIn this paper we explore the idea that the code that constitutes a program actually forms a higherlevel, program specific language. The symbols of the language are the abstractions of the program, and the grammar of the language is the set of (generally unwritten) rules about the allowable combinations of those abstractions. As such, a program is both a language definition, and the only use of that language. This specificity means that reading a neverbefore encountered program involves learning a new natural language, and that porting code from one program to another requires translation from one natural language into another. We suggest that the complexity and depth of the program language is affected by the gap between the program semantics (what the program is meant to do) and the code semantics (the way in which the machine runs). We believe that in seeing that programs are languages, we gain new insight into our own experience as programmers, and are able to gain new perspective on the intense complexity of code and its creation.
p1480
aVWe present Chorus, a highlevel parallel programming model suitable for irregular, heapmanipulating applications like mesh refinement and epidemic simulations, and JChorus, an implementation of the model on top of Java. One goal of Chorus is to express the dynamic and instancedependent patterns of memory access that are common in typical irregular applications. Its other focus is locality of effects: the property that in many of the same applications, typical imperative commands only affect small, local regions in the shared heap. Chorus addresses dynamism and locality through the unifying abstraction of an object assembly: a local region in a shared data structure equipped with a shortlived, speculative thread of control. The thread of control in an assembly can only access objects within the assembly. While objects can migrate from assembly to assembly, such migration is local i.e., objects only move from one assembly to a neighboring one and does not lead to aliasing. Programming primitives include a merge operation, by which an assembly merges with an adjacent assembly, and a split operation, which splits an assembly into smaller ones. Our abstractions are race and deadlockfree, and inherently datacentric. We demonstrate that Chorus and JChorus allow natural programming of several important applications exhibiting irregular dataparallelism. We also present an implementation of JChorus based on a manytoone mapping of assemblies to lowerlevel threads, and report on preliminary performance numbers.
p1481
aVThe shift from single to multiple core architectures means that programmers must write concurrent, multithreaded programs in order to increase application performance. Unfortunately, multithreaded applications are susceptible to numerous errors, including deadlocks, race conditions, atomicity violations, and order violations. These errors are notoriously difficult for programmers to debug. This paper presents Grace, a softwareonly runtime system that eliminates concurrency errors for a class of multithreaded programs: those based on forkjoin parallelism. By turning threads into processes, leveraging virtual memory protection, and imposing a sequential commit protocol, Grace provides programmers with the appearance of deterministic, sequential execution, while taking advantage of available processing cores to run code concurrently and efficiently. Experimental results demonstrate Grace's effectiveness: with modest code changes across a suite of computationallyintensive benchmarks (116 lines), Grace can achieve high scalability and performance while preventing concurrency errors.
p1482
aVToday's sharedmemory parallel programming models are complex and errorprone.While many parallel programs are intended to be deterministic, unanticipated thread interleavings can lead to subtle bugs and nondeterministic semantics. In this paper, we demonstrate that a practical type and effect system can simplify parallel programming by guaranteeing deterministic semantics with modular, compiletime type checking even in a rich, concurrent objectoriented language such as Java. We describe an objectoriented type and effect system that provides several new capabilities over previous systems for expressing deterministic parallel algorithms.We also describe a language called Deterministic Parallel Java (DPJ) that incorporates the new type system features, and we show that a core subset of DPJ is sound. We describe an experimental validation showing thatDPJ can express a wide range of realistic parallel programs; that the new type system features are useful for such programs; and that the parallel programs exhibit good performance gains (coming close to or beating equivalent, nondeterministic multithreaded programs where those are available).
p1483
aVScripting languages enjoy great popularity due to their support for rapid and exploratory development. They typically have lightweight syntax, weak data privacy, dynamic typing, powerful aggregate data types, and allow execution of the completed parts of incomplete programs. The price of these features comes later in the software life cycle. Scripts are hard to evolve and compose, and often slow. An additional weakness of most scripting languages is lack of support for concurrency  though concurrency is required for scalability and interacting with remote services. This paper reports on the design and implementation of Thorn, a novel programming language targeting the JVM. Our principal contributions are a careful selection of features that support the evolution of scripts into industrial grade programs  e.g., an expressive module system, an optional type annotation facility for declarations, and support for concurrency based on message passing between lightweight, isolated processes. On the implementation side, Thorn has been designed to accommodate the evolution of the language itself through a compiler plugin mechanism and target the Java virtual machine.
p1484
aVThis paper describes our samplingbased profiler that exploits a processor's HPM (Hardware Performance Monitor) to collect information on running Java applications for use by the Java VM. Our profiler provides two novel features: Javalevel event profiling and lightweight contextsensitive event profiling. For Java events, we propose new techniques to leverage the sampling facility of the HPM to generate object creation profiles and lock activity profiles. The HPM sampling is the key to achieve a smaller overhead compared to profilers that do not rely on hardware helps. To sample the object creations with the HPM, which can only sample hardware events such as executed instructions or cache misses, we correlate the object creations with the store instructions for Java object headers. For the lock activity profile, we introduce an instrumentationbased technique, called ProbeNOP, which uses a special NOP instruction whose executions are counted by the HPM. For the contextsensitive event profiling, we propose a new technique called CallerChaining, which detects the calling context of HPM events based on the call stack depth (the value of the stack frame pointer). We show that it can detect the calling contexts in many programs including a large commercial application. Our proposed techniques enable both programmers and runtime systems to get more valuable information from the HPM to understand and optimize the programs without adding significant runtime overhead.
p1485
aVSoftware has spent the bounty of Moore's law by solving harder problems and exploiting abstractions, such as highlevel languages, virtual machine technology, binary rewriting, and dynamic analysis. Abstractions make programmers more productive and programs more portable, but usually slow them down. Since Moore's law is now delivering multiple cores instead of faster processors, future systems must either bear a relatively higher cost for abstractions or use some cores to help tolerate abstraction costs. This paper presents the design, implementation, and evaluation of a novel concurrent, configurable dynamic analysis framework that efficiently utilizes multicore cache architectures. It introduces Cachefriendly Asymmetric Buffering (CAB), a lockfree ringbuffer that implements efficient communication between application and analysis threads. We guide the design and implementation of our framework with a model of dynamic analysis overheads. The framework implements exhaustive and sampling event processing and is analysisneutral. We evaluate the framework with five popular and diverse analyses, and show performance improvements even for lightweight, lowoverhead analyses. Efficient intercore communication is central to high performance parallel systems and we believe the CAB design gives insight into the subtleties and difficulties of attaining it for dynamic analysis and other parallel software.
p1486
aVAs programmers, we like to think of software as the product of our intelligent design, carefully crafted to meet wellspecified goals. In reality, software evolves inadvertently through the actions of many individual programmers, often leading to unanticipated consequences. Large complex software systems are subject to constraints similar to those faced by evolving biological systems, and we have much to gain by viewing software through the lens of evolutionary biology. The talk will highlight recent research that applies the mechanisms of evolution quite directly to the problem of repairing software bugs.
p1487
aVEvangelists for Agile methods strongly encourage all projects to follow every practice of their chosen method. Based on a Grounded Theory study involving 40 participants at 16 organizations, and corroborated by 4 independent case studies, we argue that development methods and practices must be adapted to fit their contexts. Understanding Agility in context will help development teams, their managers, and Agile coaches to adapt development processes to fit their projects' contexts.
p1488
aVThe halt in clock frequency scaling has forced architects and language designers to look elsewhere for continued improvements in performance. We believe that extracting maximum performance will require compilation to highly heterogeneous architectures that include reconfigurable hardware. We present a new language, Lime, which is designed to be executable across a broad range of architectures, from FPGAs to conventional CPUs. We present the language as a whole, focusing on its novel features for limiting sideeffects and integration of the streaming paradigm into an object oriented language. We conclude with some initial results demonstrating applications running either on a CPU or co executing on a CPU and an FPGA.
p1489
aVConsumer electronics today such as cell phones often have one or more lowpower FPGAs to assist with energyintensive operations in order to reduce overall energy consumption and increase battery life. However, current techniques for programming FPGAs require people to be specially trained to do so. Ideally, software engineers can more readily take advantage of the benefits FPGAs offer by being able to program them using their existing skills, a common one being objectoriented programming. However, traditional techniques for compiling objectoriented languages are at odds with today's FPGA tools, which support neither pointers nor complex data structures. Open until now is the problem of compiling an objectoriented language to an FPGA in a way that harnesses this potential for huge energy savings. In this paper, we present a new compilation technique that feeds into an existing FPGA tool chain and produces FPGAs with up to almost an order of magnitude in energy savings compared to a lowpower microprocessor while still retaining comparable performance and area usage.
p1490
aVAccurately predicting program behaviors (e.g., locality, dependency, method calling frequency) is fundamental for program optimizations and runtime adaptations. Despite decades of remarkable progress, prior studies have not systematically exploited program inputs, a deciding factor for program behaviors. Triggered by the strong and predictive correlations between program inputs and behaviors that recent studies have uncovered, this work proposes to include program inputs into the focus of program behavior analysis, cultivating a new paradigm named inputcentric program behavior analysis. This new approach consists of three components, forming a threelayer pyramid. At the base is program input characterization, a component for resolving the complexity in program raw inputs and the extraction of important features. In the middle is inputbehavior modeling, a component for recognizing and modeling the correlations between characterized input features and program behaviors. These two components constitute inputcentric program behavior analysis, which (ideally) is able to predict the largescope behaviors of a program's execution as soon as the execution starts. The top layer of the pyramid is inputcentric adaptation, which capitalizes on the novel opportunities that the first two components create to facilitate proactive adaptation for program optimizations. By centering on program inputs, the new approach resolves a proactivityadaptivity dilemma inherent in previous techniques. Its benefits are demonstrated through proactive dynamic optimizations and version selection, yielding significant performance improvement on a set of Java and C programs.
p1491
aVIn this paper we propose a communicationcentric approach to specifying and checking how multithreaded programs use shared memory to perform interthread communication. Our approach complements past efforts for improving the safety of multithreaded programs such as race detection and atomicity checking. Unlike prior work, we focus on what pieces of code are allowed to communicate with one another, as opposed to declaring what data items are shared or what code blocks should be atomic. We develop a language that supports composable specifications at multiple levels of abstraction and that allows libraries to specify whether or not sharedmemory communication is exposed to clients. The precise meaning of a specification is given with a formal semantics we present. We have developed a dynamicanalysis tool for Java that observes program execution to see if it obeys a specification. We report results for using the tool on several benchmark programs to which we added specifications, concluding that our approach matches the modular structure of multithreaded applications and that our tool is performant enough for use in development and testing.
p1492
aVSoftware bugs, such as concurrency, memory and semantic bugs, can significantly affect system reliability. Although much effort has been made to address this problem, there are still many bugs that cannot be detected, especially concurrency bugs due to the complexity of concurrent programs. Effective approaches for detecting these common bugs are therefore highly desired. This paper presents an invariantbased bug detection tool, DefUse, which can detect not only concurrency bugs (including the previously understudied order violation bugs), but also memory and semantic bugs. Based on the observation that many bugs appear as violations to programmers' data flow intentions, we introduce three different types of definitionuse invariants that commonly exist in both sequential and concurrent programs. We also design an algorithm to automatically extract such invariants from programs, which are then used to detect bugs. Moreover, DefUse uses various techniques to prune false positives and rank error reports. We evaluated DefUse using sixteen realworld applications with twenty realworld concurrency and sequential bugs. Our results show that DefUse can effectively detect 19 of these bugs, including 2 new bugs that were never reported before, with only a few false positives. Our training sensitivity results show that, with the benefit of the pruning and ranking algorithms, DefUse is accurate even with insufficient training.
p1493
aVSoftware developers often duplicate source code to replicate functionality. This practice can hinder the maintenance of a software project: bugs may arise when two identical code segments are edited inconsistently. This paper presents DejaVu, a highly scalable system for detecting these general syntactic inconsistency bugs. DejaVu operates in two phases. Given a target code base, a parallel /inconsistent clone analysis/ first enumerates all groups of source code fragments that are similar but not identical. Next, an extensible /buggy change analysis/ framework refines these results, separating each group of inconsistent fragments into a finegrained set of inconsistent changes and classifying each as benign or buggy. On a 75+ million line preproduction commercial code base, DejaVu executed in under five hours and produced a report of over 8,000 potential bugs. Our analysis of a sizable random sample suggests with high likelihood that at this report contains at least 2,000 true bugs and 1,000 code smells. These bugs draw from a diverse class of software defects and are often simple to correct: syntactic inconsistencies both indicate problems and suggest solutions.
p1494
aVA Java application sometimes raises an outofmemory exception. This is usually because it has exhausted the Java heap. However, a Java application can raise an outofmemory exception when it exhausts the memory used by Java that is not in the Java heap. We call this area nonJava memory. For example, an outofmemory exception in the nonJava memory can happen when the JVM attempts to load too many classes. Although it is relatively rare to exhaust the nonJava memory compared to exhausting the Java heap, a Java application can consume a considerable amount of nonJava memory. This paper presents a quantitative analysis of nonJava memory. To the best of our knowledge, this is the first indepth analysis of the nonJava memory. To do this we created a tool called Memory Analyzer for Redundant, Unused, and String Areas (MARUSA), which gathers memory statistics from both the OS and the Java virtual machine, breaking down and visualizing the nonJava memory usage. We studied the use of nonJava memory for a wide range of Java applications, including the DaCapo benchmarks and Apache DayTrader. Our study is based on the IBM J9 Java Virtual Machine for Linux. Although some of our results may be specific to this combination, we believe that most of our observations are applicable to other platforms as well.
p1495
aVHeterogeneous multicore processors, such as the IBM Cell processor, can deliver high performance. However, these processors are notoriously difficult to program: different cores support different instruction set architectures, and the processor as a whole does not provide coherence between the different cores' local memories. We present HeraJVM, an implementation of the Java Virtual Machine which operates over the Cell processor, thereby making this platforms more readily accessible to mainstream developers. HeraJVM supports the full Java language; threads from an unmodified Java application can be simultaneously executed on both the main PowerPCbased core and on the additional SPE accelerator cores. Migration of threads between these cores is transparent from the point of view of the application, requiring no modification to Java source code or bytecode. HeraJVM supports the existing Java Memory Model, even though the underlying hardware does not provide cache coherence between the different core types. We examine HeraJVM's performance under a series of realworld Java benchmarks from the SpecJVM, Java Grande and Dacapo benchmark suites. These benchmarks show a wide variation in relative performance on the different core types of the Cell processor, depending upon the nature of their workload. Execution of these benchmarks on HeraJVM can achieve speedups of up to 2.25x by using one of the Cell processor's SPE accelerator cores, compared to execution on the main PowerPCbased core. When all six SPE cores are exploited, parallel workloads can achieve speedups of up to 13x compared to execution on the single PowerPC core.
p1496
aVAs software becomes increasingly complex and difficult to analyze, it is more and more common for developers to use highlevel, typesafe, objectoriented (OO) programming languages and to architect systems that comprise multiple components. Different components are often implemented in different programming languages. In stateoftheart multicomponent, multilanguage systems, crosscomponent communication relies on remote procedure calls (RPC) and message passing. As components are increasingly colocated on the same physical machine to ensure high utilization of multicore systems, there is a growing potential for using shared memory for crosslanguage crossruntime communication. We present the design and implementation of CoLocated Runtime Sharing (CoLoRS), a system that enables crosslanguage, crossruntime typesafe, transparent shared memory. CoLoRS provides object sharing for colocated OO runtimes for both static and dynamic languages. CoLoRS defines a languageneutral object/classmodel,which is a staticdynamic hybrid and enables class evolution while maintaining the space/time efficiency of a static model. CoLoRS uses type mapping and class versioning to transparently map shared types to private types. CoLoRS also contributes a synchronization mechanism and a parallel, concurrent, onthefly GC algorithm, both designed to facilitate crosslanguage crossruntime object sharing. We implement CoLoRS in opensource, productionquality runtimes for Python and Java. Our empirical evaluation shows that CoLoRS extensions impose low overhead. We also investigate RPC over CoLoRS and find that using shared memory to implement colocated RPC significantly improves both communication throughput and latency by avoiding data structure serialization.
p1497
aVScientists and artists share a conviction that key elements of their workcreativity, craft, analysis, intuition are somehow deeply similar. Most Onward! participants will have a clear sense of the scientific variants of these qualities, and of what it takes to manifest them in our daytoday work as researchers and software designers. This talk looks at how ideas are developed and refined in the making of visual art, how the same qualities especially creativity, the most ineffable come into play, and how artists and scientists grapple with the various fears that beset creative work.
p1498
aVFixing concurrency bugs (or "crugs") is critical in modern software systems. Static analyses to find crugs such as data races and atomicity violations scale poorly, while dynamic approaches incur high runtime overheads. Crugs manifest only under specific execution interleavings that may not arise during inhouse testing, thereby demanding a lightweight program monitoring technique that can be used postdeployment. We present Cooperative Crug Isolation (CCI), a lowoverhead instrumentation framework to diagnose productionrun failures caused by crugs. CCI tracks specific thread interleavings at runtime, and uses statistical models to identify strong failure predictors among these. We offer a varied suite of predicates that represent different tradeoffs between complexity and fault isolation capability. We also develop variant random sampling strategies that suit different types of predicates and help keep the runtime overhead low. Experiments with 9 realworld bugs in 6 nontrivial C applications show that these schemes span a wide spectrum of performance and diagnosis capabilities, each suitable for different usage scenarios.
p1499
aVWe present the DeAL language for heap assertions that are efficiently evaluated during garbage collection time. DeAL is a rich, declarative, logicbased language whose programs are guaranteed to be executable with good wholeheap locality, i.e., within a single traversal over every live object on the heap and a finite neighborhood around each object. As a result, evaluating DeAL programs incurs negligible cost: for simple assertion checking at each garbage collection, the endtoend execution slowdown is below 2%. DeAL is integrated into Java as a VM extension and we demonstrate its efficiency and expressiveness with several applications and properties from the past literature. Compared to past systems for heap assertions, DeAL is distinguished by its very attractive expressiveness/efficiency tradeoff: it o ers a significantly richer class of assertions than what past systems could check with a single traversal. Conversely, past systems that can express the same (or more) complex assertions as DeAL do so only by su ering ordersofmagnitude higher costs.
p1500
aVThere has been significant interest in equipping programs with runtime checks aimed at detecting errors to improve fault detection during testing and in the field. Recent work in this area has studied methods for efficiently monitoring a program execution's conformance to path property specifications, e.g., such as those captured by a finite state automaton. These techniques show great promise, but their broad applicability is hampered by the fact that for certain combinations of programs and properties the overhead of checking can slow the program down by up to 3500%. We have observed that, in many cases, the overhead of runtime monitoring is due to the behavior of program loops. We present a general framework for optimizing the monitoring of loops relative to a property. This framework allows monitors to process a loop in constanttime rather than time that is proportional to the number of loop iterations. We present the results of an empirical study that demonstrates that significant overhead reduction that can be achieved by applying the framework to monitor properties of several large Java programs.
p1501
aVModern IDEs for objectoriented languages like Java provide support for a basic set of simple automated refactorings whose behaviour is easy to describe intuitively. It is, however, surprisingly difficult to specify their behaviour in detail. In particular, the popular preconditionbased approach tends to produce somewhat unwieldy descriptions if advanced features of the object language are taken into account. This has resulted in refactoring implementations that are complex, hard to understand, and even harder to maintain, yet these implementations themselves are the only precise "specification" of many refactorings. We have in past work advocated a different approach based on several complementary notions of dependencies that guide the implementation, and on the concept of microrefactorings that structure it. We show in this work that these concepts are powerful enough to provide highlevel specifications of many of the refactorings implemented in Eclipse. These specifications are precise enough to serve as the basis of a cleanroom reimplementation of these refactorings that is very compact, yet matches Eclipse's for features and outperforms it in terms of correctness.
p1502
aVReusing existing library components is essential for reducing the cost of software development and maintenance. When library components evolve to accommodate new feature requests, to fix bugs, or to meet new standards, the clients of software libraries often need to make corresponding changes to correctly use the updated libraries. Existing API usage adaptation techniques support simple adaptation such as replacing the target of calls to a deprecated API, however, cannot handle complex adaptations such as creating a new object to be passed to a different API method, or adding an exception handling logic that surrounds the updated API method calls. This paper presents LIBSYNC that guides developers in adapting API usage code by learning complex API usage adaptation patterns from other clients that already migrated to a new library version (and also from the API usages within the library's test code). LIBSYNC uses several graphbased techniques (1) to identify changes to API declarations by comparing two library versions, (2) to extract associated API usage skeletons before and after library migration, and (3) to compare the extracted API usage skeletons to recover API usage adaptation patterns. Using the learned adaptation patterns, LIBSYNC recommends the locations and edit operations for adapting API usages. The evaluation of LIBSYNC on realworld software systems shows that it is highly correct and useful with a precision of 100% and a recall of 91%.
p1503
aVSoftware's expense owes partly to frequent reimplementation of similar functionality and partly to maintenance of patches, ports or components targeting evolving interfaces. More modular noninvasive approaches are unpopular because they entail laborious wrapper code. We propose Cake, a rulebased language describing compositions using interface relations. To evaluate it, we compare several existing wrappers with reimplemented Cake versions, finding the latter to be simpler and better modularised.
p1504
aVType classes were originally developed in Haskell as a disciplined alternative to adhoc polymorphism. Type classes have been shown to provide a typesafe solution to important challenges in software engineering and programming languages such as, for example, retroactive extension of programs. They are also recognized as a good mechanism for conceptbased generic programming and, more recently, have evolved into a mechanism for typelevel computation. This paper presents a lightweight approach to type classes in objectoriented (OO) languages with generics using the CONCEPT pattern and implicits (a typedirected implicit parameter passing mechanism). This paper also shows how Scala's type system conspires with implicits to enable, and even surpass, many common extensions of the Haskell type class system, making Scala ideally suited for generic programming in the large.
p1505
aVWeb sites and web browsers have recently evolved into platforms on top of which entire applications are delivered dynamically, mostly as JavaScript source code. This delivery format has sparked extremely enthusiastic efforts to customize both individual web sites and entire browsers in ways the original authors never expected or accommodated. Such customizations take the form of yet more script dynamically injected into the application, and the current idioms to do so exploit arcane JavaScript features and are extremely brittle. In this work, we accept the popularity of extensions and seek better linguistic mechanisms to support them. We suggest adding to JavaScript aspectoriented features that allow straightforward and declarative ways for customization code to modify the targeted application. Compared to most prior aspectrelated research, our work has a different motivation and a different target programming environment, both of which lead to novel design and implementation techniques. Our aspect weaving is entirely integrated into a new dynamic JIT compiler, which lets us properly handle advice to firstclass functions in the presence of arbitrary aliasing, without resorting to wholeprogram code transformations. Our prototype demonstrates that an aspectoriented approach to webapplication customization is often more efficient than current idioms while simplifying the entire process.
p1506
aVWhile most approaches to automatic parallelization focus on compilation approaches for parallelizing loop iterations, we advocate the need for new virtual machines that can parallelize the execution of recursive programs. In this paper, we show that recursive programs can be effectively parallelized when arguments to procedures are evaluated concurrently and branches of conditional statements are speculatively executed in parallel. We introduce the continuator concept, a runtime structure that tracks and manages the control dependences between such concurrently spawned tasks, ensuring adherence to the sequential semantics of the parallelized program. As a proof of concept, we discuss the details of a parallel interpreter for Scheme (implemented in Common Lisp) based on these ideas, and show the results from executing the Clinger benchmark suite for Scheme.
p1507
aVMany relational static analysis techniques for precise reasoning about heap contents perform an explicit case analysis of all possible heaps that can arise. We argue that such precise relational reasoning can be obtained in a more scalable and economical way by enforcing the memory invariant that every concrete memory location stores one unique value directly on the heap abstraction. Our technique combines the strengths of analyses for precise reasoning about heap contents with approaches that prioritize axiomatization of memory invariants, such as the theory of arrays. Furthermore, by avoiding an explicit case analysis, our technique is scalable and powerful enough to analyze realworld programs with intricate use of arrays and pointers; in particular, we verify the absence of buffer overruns, incorrect casts, and null pointer dereferences in OpenSSH (over 26,000 lines of code) after fixing 4 previously undiscovered bugs found by our system. Our experiments also show that the combination of reasoning about heap contents and enforcing existence and uniqueness invariants is crucial for this level of precision.
p1508
aVSomeday computer scientists may build systems of astronomical complexity that provide profound benefit to humanity. However, the question today is how such feats will ultimately be achieved. As is common with technology, present limitations to modern approaches challenge our imagination to search for new paradigms and organizing principles. To examine the prerequisites to achieving our most ambitious objectives, this talk will contemplate the implications of recent counterintuitive results from experiments with evolutionary algorithms that suggest that search (which is a metaphor for innovation in general) is sometimes most effective when it is not explicitly seeking an objective. In particular, through several experiments in interactive evolution and with an algorithm called "novelty search," a picture of innovation is emerging in which objectives can help to guide us one steppingstone away from our present understanding, yet ultimately become handcuffs that also blind us to essential orthogonal discoveries on the road to longterm innovation. While the implications of these insights for reaching our highest goals are in part sobering, the silver lining is that much can be gained by liberating ourselves from the temptation to frame all our projects in terms of what they ultimately aim to achieve. Instead, with evidence in hand, we can exploit the structure of the unknown by orienting ourselves towards discovery and away from the shackles of mandated outcomes.
p1509
aVThe quality of a static analysis of heapmanipulating programs is largely determined by its heap abstraction. Object allocation sites are a commonlyused abstraction, but are too coarse for some clients. The goal of this paper is to investigate how various refinements of allocation sites can improve precision. In particular, we consider abstractions that use call stack, object recency, and heap connectivity information. We measure the precision of these abstractions dynamically for four different clients motivated by concurrency and on nine Java programs chosen from the DaCapo benchmark suite. Our dynamic results shed new light on aspects of heap abstractions that matter for precision, which allows us to more effectively navigate the large space of possible heap abstractions
p1510
aVInclusionbased pointsto analysis provides a good tradeoff between precision of results and speed of analysis, and it has been incorporated into several production compilers including gcc. There is an extensive literature on how to speed up this algorithm using heuristics such as detecting and collapsing cycles of pointerequivalent variables. This paper describes a complementary approach based on exploiting parallelism. Our implementation exploits two key insights. First, we show that inclusionbased pointsto analysis can be formulated entirely in terms of graphs and graph rewrite rules. This exposes the amorphous dataparallelism in this algorithm and makes it easier to develop a parallel implementation. Second, we show that this graphtheoretic formulation reveals certain key properties of the algorithm that can be exploited to obtain an efficient parallel implementation. Our parallel implementation achieves a scaling of up to 3x on a 8core machine for a suite of ten large C programs. For all but the smallest benchmarks, the parallel analysis outperforms a stateoftheart, highly optimized, serial implementation of the same algorithm. To the best of our knowledge, this is the first parallel implementation of a pointsto analysis.
p1511
aVSpoofax is a language workbench for efficient, agile development of textual domainspecific languages with stateoftheart IDE support. Spoofax integrates language processing techniques for parser generation, metaprogramming, and IDE development into a single environment. It uses concise, declarative specifications for languages and IDE services. In this paper we describe the architecture of Spoofax and introduce idioms for highlevel specifications of language semantics using rewrite rules, showing how analyses can be reused for transformations, code generation, and editor services such as error marking, reference resolving, and content completion. The implementation of these services is supported by languageparametric editor service classes that can be dynamically loaded by the Eclipse IDE, allowing new languages to be developed and used sidebyside in the same Eclipse environment.
p1512
aVWe propose a Javalike language where class definitions are first class values and new classes can be derived from existing ones by exploiting the full power of the language itself, used on top of a small set of primitive composition operators, instead of using a fixed mechanism like inheritance. Hence, compilation requires to perform (meta)reduction steps, by a process that we call compiletime execution. This approach differs from metaprogramming techniques available in mainstream languages since it is metacircular, hence programmers are not required to learn new syntax and idioms. Compiletime execution is guaranteed to be sound (not to get stuck) by a lightweight technique, where class composition errors are detected dynamically, and conventional typing errors are detected by interleaving typechecking with metareduction steps. This allows for a modular approach, that is, compiletime execution is defined, and can be implemented, on top of typechecking and execution of the underlying language. Moreover, programmers can handle errors due to composition operators. Besides soundness, our technique ensures an additional important property called metalevel soundness, that is, typing errors never originate from (meta)code in already compiled programs.
p1513
aVIn logic metaprogramming, programs are not stored as plain textfiles but rather derived from a deductive database. While the benefits of this approach for metaprogramming are obvious, its incompatibility with separate checking limits its applicability to largescale projects. We analyze the problems inhibiting separate checking and propose a class of logics that reconcile logic metaprogramming and separate checking. We have formalized the resulting module system and have proven the soundness of separate checking. We validate its feasibility by presenting the design and implementation of a specific logic that is able to express many metaprogramming examples from the literature.
p1514
aVEncapsulated abstractions are fundamental in objectoriented programming. A single class may employ multiple abstractions to achieve its purpose. Such abstractions are often related and combined in disciplined ways. This paper explores ways to express, verify and rely on logical relationships between abstractions. It introduces two general specification mechanisms: export clauses for relating abstractions in individual classes, and axiom clauses for relating abstractions in a class and all its descendants. MultiStar, an automatic verification tool based on separation logic and abstract predicate families, implements these mechanisms in a multiple inheritance setting. Several verified examples illustrate MultiStar's underlying logic. To demonstrate the flexibility of our approach, we also used MultiStar to verify the core iterator hierarchy of a popular data structure library.
p1515
aVRecent work has introduced class sharing as a mechanism for adapting a family of related classes with new functionality. This paper introduces homogeneous family sharing, implemented in the J&h language, in which the sharing mechanism is lifted from classlevel sharing to true familylevel sharing. Compared to the original (heterogeneous) class sharing mechanism, homogeneous family sharing provides useful new functionality and substantially reduces the annotation burden on programmers by eliminating the need for masked types and sharing declarations. This is achieved through a new mechanism, shadow classes, which permit homogeneous sharing of all related classes in shared families. The new sharing mechanism has a straightforward semantics, which is formalized in the J&h calculus. The soundness of the J&h type system is proved. The J&h language is implemented as an extension to the J& language. To demonstrate the effectiveness of family sharing, the Polyglot compiler framework is ported to J&h.
p1516
aVThe modularity of aspectoriented programming (AOP) has been a controversial issue. To investigate this issue compared with objectoriented programming (OOP), we propose a simple language providing AOP mechanisms, which are enhanced traditional OOP mechanisms. We also present its formal system and then show that programs in this language can be only mostly modularly (i.e. separately) typechecked and compiled.We mention a source of this unmodularity and discuss whether or not it is appropriate to claim that AOP breaks modularity compared with OOP.
p1517
aVTesting is among the most effective tools available for finding bugs. Still, we know of no automatic technique for generating test cases that expose bugs involving a combination of mutable state and callbacks, even though objects and method overriding set up exactly that combination. For such cases, a test generator must create callbacks or subclasses that aggressively exercise sideeffecting operations using combinations of generated objects. This paper presents a new algorithm for randomly testing programs that use state and callbacks. Our algorithm exploits a combination of contracts and environment bindings to guide the testcase generator toward interesting inputs. Our prototype implementation for Racket (formerly PLT Scheme)  which has a Javalike class system, but with firstclass classes as well as gbetalike augmentable methods  uncovered dozens of bugs in a welltested and widely used texteditor library. We describe our approach in a precise, formal notation, borrowing the techniques used to describe operational semantics and type systems. The formalism enables us to provide a compact and selfcontained explanation of the core of our technique without the ambiguity usually present in pseudocode descriptions.
p1518
aVContinuationbased Web servers provide advantages over traditional Web application development through the increase of expressive power they allow. This leads to fewer errors and more productivity for the programmers that adopt them. Unfortunately, existing implementation techniques force a hard choice between scalability and expressiveness. Our technique allows a smoother path to scalable, continuationbased Web programs. We present a modular program transformation that allows scalable Web applications to use thirdparty, higherorder libraries with higherorder arguments that cause Web interaction. Consequently, our system provides existing Web applications with more scalability through significantly less memory use than the traditional technique.
p1519
aVMany language implementations, particularly for highlevel and scripting languages, are based on carefully honed runtime systems that have an internally sequential execution model. Adding support for parallelism in the usual form   as threads that run arbitrary code in parallel   would require a major revision or even a rewrite to add safe and efficient locking and communication. We describe an alternative approach to incremental parallelization of runtime systems. This approach can be applied inexpensively to many sequential runtime systems, and we demonstrate its effectiveness in the Racket runtime system and Parrot virtual machine. Our evaluation assesses both the performance benefits and the developer effort needed to implement our approach. We find that incremental parallelization can provide useful, scalable parallelism on commodity multicore processors at a fraction of the effort required to implement conventional parallel threads.
p1520
aVThe Java language lacks the important notions of ownership (an object owns its representation to prevent unwanted aliasing) and immutability (the division into mutable, immutable, and readonly data and references). Programmers are prone to design errors, such as representation exposure or violation of immutability contracts. This paper presents Ownership Immutability Generic Java (OIGJ), a backwardcompatible purelystatic language extension supporting ownership and immutability. We formally defined a core calculus for OIGJ, based on Featherweight Java, and proved it sound. We also implemented OIGJ and performed case studies on 33,000 lines of code. Creation of immutable cyclic structures requires a "cooking phase" in which the structure is mutated but the outside world cannot observe this mutation. OIGJ uses ownership information to facilitate creation of immutable cyclic structures, by safely prolonging the cooking phase even after the constructor finishes. OIGJ is easy for a programmer to use, and it is easy to implement (flowinsensitive, adding only 14 rules to those of Java). Yet, OIGJ is more expressive than previous ownership languages, in the sense that it can typecheck more good code. OIGJ can express the factory and visitor patterns, and OIGJ can typecheck Sun's java.util collections (except for the clone method) without refactoring and with only a small number of annotations. Previous work required major refactoring of existing code in order to fit its ownership restrictions. Forcing refactoring of welldesigned code is undesirable because it costs programmer effort, degrades the design, and hinders adoption in the mainstream community.
p1521
aVTribal Ownership unifies class nesting and object ownership. Tribal Ownership is based on Tribe, a language with nested classes and object families. In Tribal Ownership, a program's runtime object ownership structure is characterised by the lexical nesting structure of its classes. We build on a variant of Tribe to present a descriptive ownership system, using object nesting to describe heap partitions, but without imposing any restrictions on programming disciplines. We then demonstrate how a range of different prescriptive ownership policies can be supported on top of the descriptive Tribal Ownership mechanism; including a novel ownersaslocaldominators policy. We formalise our type system and prove soundness and several ownership invariants. The resulting system requires strikingly few annotations, and uses wellunderstood encapsulation techniques to create ownership systems that should be intuitive for programmers.
p1522
aVWe introduce a type system based on intervals, objects representing the time in which a block of code will execute. The type system can verify timebased properties such as when a field will be accessed or a method will be invoked. One concrete application of our type system is datarace protection: For fields which are initialized during one phase of the program and constant thereafter, users can designate the interval during which the field is mutable. Code which happens after this initialization interval can safely read the field in parallel. We also support fields guarded by a lock and even the use of dynamic race detectors. Another use for intervals is to designate different phases in the object's lifetime, such as a constructor phase. The type system then ensures that only appropriate methods are invoked in each phase.
p1523
aVThis paper presents TransFinder, a compiletime tool that automatically determines which statements of an unsynchronized multithreaded program must be enclosed in atomic regions to enforce conflictserializability. Unlike previous tools, TransFinder requires no programmer input (beyond the program) and is more efficient in both time and space. Our implementation shows that the generated atomic regions range from being identical to, or smaller than, the programmerspecified transactions in the three Java Grande benchmarks considered, and in five of the eight STAMP benchmarks considered, while still providing identical synchronization semantics and results. The generated atomic regions are between 5 and 38 lines larger in the three remaining STAMP benchmarks. In the most conservative case, TransFinder can, based on the program structure, successfully identify and suggest an alternative that conforms exactly to the programmerspecified atomic regions. By generating small, highlytargeted, conflictserializable atomic regions, TransFinder allows the programmer to focus further tuning efforts on only a small portion of the code (when further tuning is needed).
p1524
aVAtomic regions are an important concept in correct concurrent programming: since atomic regions can be viewed as having executed in a single step, atomicity greatly reduces the number of possible interleavings the programmer needs to consider. This paper describes a method for building atomicity into a programming language in an organic fashion. We take the view that atomicity holds for whole threads by default, and a division into smaller atomic regions occurs only at points where an explicit need for sharing is needed and declared. A corollary of this view is every line of code is part of some atomic region. We define a polymorphic type system, Task Types, to enforce most of the desired atomicity properties statically. We show the reasonableness of our type system by proving that type soundness, isolation invariance, and atomicity enforcement properties hold at run time. We also present initial results of a Task Types implementation built on Java
p1525
aVBuilding applications that are responsive and can exploit parallel hardware while remaining simple to write, understand, test, and maintain, poses an important challenge for developers. In particular, it is often desirable to enable various tasks to read or modify shared data concurrently without requiring complicated locking schemes that may throttle concurrency and introduce bugs. We introduce a mechanism that simplifies the parallel execution of different application tasks. Programmers declare what data they wish to share between tasks by using isolation types, and execute tasks concurrently by forking and joining revisions. These revisions are isolated: they read and modify their own private copy of the shared data only. A runtime creates and merges copies automatically, and resolves conflicts deterministically, in a manner declared by the chosen isolation type. To demonstrate the practical viability of our approach, we developed an efficient algorithm and an implementation in the form of a C# library, and used it to parallelize an interactive game application. Our results show that the parallelized game, while simple and very similar to the original sequential game, achieves satisfactory speedups on a multicore processor.
p1526
aVTracing justintime compilers (TJITs) determine frequently executed traces (hot paths and loops) in running programs and focus their optimization effort by emitting optimized machine code specialized to these traces. Prior work has established this strategy to be especially beneficial for dynamic languages such as JavaScript, where the TJIT interfaces with the interpreter and produces machine code from the JavaScript trace. This direct coupling with a JavaScript interpreter makes it difficult to harness the power of a TJIT for other components that are not written in JavaScript, e.g., the DOM implementation or the layout engine inside a browser. Furthermore, if a TJIT is tied to a particular highlevel language interpreter, it is difficult to reuse it for other input languages as the optimizations are likely targeted at specific idioms of the source language. To address these issues, we designed and implemented a TJIT for Microsoft's Common Intermediate Language CIL (the target language of C#, VisualBasic, F#, and many other languages). Working on CIL enables TJIT optimizations for any program compiled to this platform. In addition, to validate that the performance gains of a TJIT for JavaScript do not depend on specific idioms of JavaScript that are lost in the translation to CIL, we provide a performance evaluation of our JavaScript runtime which translates JavaScript to CIL and then runs on top of our CIL TJIT.
p1527
aVAutomated refactoring is a key feature of modern IDEs. Existing refactorings rely on the transformation of source code declarations, in which references may also be transformed as a side effect. However, there exist situations in which a declaration is not available for refactoring or would be inappropriate to transform, for example, in the presence of dangling references or where a set of references should be retargeted to a different declaration. We investigate the problem of dangling references through a detailed study of three open source libraries. We find that the introduction of dangling references during library migration is a significant real problem, and characterize the specific issues that arise. Based on these findings we provide and test a prototype tool, called Trident, that allows programmers to refactor references. Our results suggest that supporting the direct refactoring of references is a significant improvement over the stateoftheart.
p1528
aVThis paper presents an approach for performance analysis of modern enterpriseclass server applications. In our experience, performance bottlenecks in these applications differ qualitatively from bottlenecks in smaller, standalone systems. Small applications and benchmarks often suffer from CPUintensive hot spots. In contrast, enterpriseclass multitier applications often suffer from problems that manifest not as hot spots, but as idle time, indicating a lack of forward motion. Many factors can contribute to undesirable idle time, including locking problems, excessive systemlevel activities like garbage collection, various resource constraints, and problems driving load. We present the design and methodology for WAIT, a tool to diagnosis the root cause of idle time in server applications. Given lightweight samples of Java activity on a single tier, the tool can often pinpoint the primary bottleneck on a multitier system. The methodology centers on an informative abstraction of the states of idleness observed in a running program. This abstraction allows the tool to distinguish, for example, between holdups on a database machine, insufficient load, lock contention in application code, and a conventional bottleneck due to a hot method. To compute the abstraction, we present a simple expert system based on an extensible set of declarative rules. WAIT can be deployed on the fly, without modifying or even restarting the application. Many groups in IBM have applied the tool to diagnosis performance problems in commercial systems, and we present a number of examples as case studies.
p1529
aVGlass box software model checking incorporates novel techniques to identify similarities in the state space of a model checker and safely prune large numbers of redundant states without explicitly checking them. It is significantly more efficient than other software model checking approaches for checking certain kinds of programs and program properties. This paper presents Pipal, a system for modular glass box software model checking. Extending glass box software model checking to perform modular checking is important to further improve its scalability. It is nontrivial because unlike traditional software model checkers such as Java PathFinder (JPF) and CMC, a glass box software model checker does not check every state separately instead, it checks a large set of states together in each step. We present a solution and demonstrate Pipal's effectiveness on a variety of programs.
p1530
aVProgramming language innovation has been hindered by the difficulty of making changes to existing languages. A key source of difficulty is the tyrannical nature of existing approaches to realizing languages   adding a new language construct means that any tool, document or programmer that works with the language must be prepared to deal with that construct. A registrationbased approach makes it possible to define language constructs that are not tyrannical. They are instead transient   the program appears to be written using the constructs only so long as a given programmer wants to see it that way. This approach may have the potential to greatly facilitate programming language innovation.
p1531
aVTo support development tools like debuggers, runtime systems need to provide a metaprogramming interface to alter their semantics and access internal data. Reflective capabilities are typically fixed by the Virtual Machine (VM). Unanticipated reflective features must either be simulated by complex program transformations, or they require the development of a specially tailored VM. We propose a novel approach to behavioral reflection that eliminates the barrier between applications and the VM by manipulating an explicit tower of firstclass interpreters. Pinocchio is a proofofconcept implementation of our approach which enables radical changes to the interpretation of programs by explicitly instantiating subclasses of the base interpreter. We illustrate the design of Pinocchio through nontrivial examples that extend runtime semantics to support debugging, parallel debugging, and backintime objectflow debugging. Although performance is not yet addressed, we also discuss numerous opportunities for optimization, which we believe will lead to a practical approach to behavioral reflection.
p1532
aVGeneral purpose objectoriented programs typically aren't embarrassingly parallel. For these applications, finding enough concurrency remains a challenge in program design. To address this challenge, in the Panini project we are looking at reconciling concurrent program design goals with modular program design goals. The main idea is that if programmers improve the modularity of their programs they should get concurrency for free. In this work we describe one of our directions to reconcile these two goals by enhancing GangofFour (GOF) objectoriented design patterns. GOF patterns are commonly used to improve the modularity of objectoriented software. These patterns describe strategies to decouple components in design space and specify how these components should interact. Our hypothesis is that if these patterns are enhanced to also decouple components in execution space applying them will concomitantly improve the design and potentially available concurrency in software systems. To evaluate our hypothesis we have studied all 23 GOF patterns. For 18 patterns out of 23, our hypothesis has held true. Another interesting preliminary result reported here is that for 17 out of these 18 studied patterns, concurrency and synchronization concerns were completely encapsulated in our concurrent design pattern framework.
p1533
aVWe present several general, broadly applicable mechanisms that enable computations to execute with reduced resources, typically at the cost of some loss in the accuracy of the result they produce.We identify several general computational patterns that interact well with these resource reduction mechanisms, present a concrete manifestation of these patterns in the form of simple model programs, perform simulationbased explorations of the quantitative consequences of applying these mechanisms to our model programs, and relate the model computations (and their interaction with the resource reduction mechanisms) to more complex benchmark applications drawn from a variety of fields.
p1534
aVThe act of computer programming is generally considered to be temporally removed from a computer program's execution. In this paper we discuss the idea of programming as an activity that takes place within the temporal bounds of a realtime computational process and its interactions with the physical world. We ground these ideas within the con text of livecoding   a live audiovisual performance practice. We then describe how the development of the programming environment "Impromptu" has addressed our ideas of programming with time and the notion of the programmer as an agent in a cyberphysical system.
p1535
aVAs heterogeneous parallel systems become dominant, application developers are being forced to turn to an incompatiblemix of low level programming models (e.g. OpenMP, MPI, CUDA, OpenCL). However, these models do little to shield developers from the difficult problems of parallelization, data decomposition and machinespecific details. Most programmersare having a difficult time using these programming models effectively. To provide a programming modelthat addresses the productivity and performance requirements for the average programmer, we explore a domainspecificapproach to heterogeneous parallel programming. We propose language virtualization as a new principle that enables the construction of highly efficient parallel domain specific languages that are embedded in a common host language. We define criteria for language virtualization and present techniques to achieve them.We present two concrete case studies of domainspecific languages that are implemented using our virtualization approach.
p1536
aVA serious tool gap exists at the start of the software lifecycle, before requirements formulation. Prerequirements analysts gather information, organize it to gain insight, envision possible futures, and present insights and recommendations to stakeholders. They typically use office tools, which give great freedom, but no help with consistency management, change propagation, or information migration to downstream tools. Despite these downsides, office tools are still favored over modeling tools, which are constraining and difficult to use. We introduce the notion of flexible modeling tools, which blend the advantages of office and modeling tools. We propose a conceptual architecture for such tools, and outline research challenges to be met in realizing them. We briefly describe the Business Insight Toolkit, a prototype tool embodying this architecture.
p1537
aVOnline software upgrades are often plagued by runtime behaviors that are poorly understood and difficult to ascertain. For example, the interactions among multiple versions of the software expose the system to race conditions that can introduce latent errors or data corruption. Moreover, industry trends suggest that online upgrades are currently needed in largescale enterprise systems, which often span multiple administrative domains (e.g., Web 2.0 applications that rely on AJAX clientside code or systems that lease cloudcomputing resources). In such systems, the enterprise does not control all the tiers of the system and cannot coordinate the upgrade process, making existing techniques inadequate to prevent mixedversion races. In this paper, we present an analytical framework for impact assessment, which allows system administrators to directly compare the risk of following an onlineupgrade plan with the risk of delaying or canceling the upgrade. We also describe an executable model that implements our formal impact assessment and enables a systematic approach for deciding whether an online upgrade is appropriate. Our model provides a method of last resort for avoiding undesirable program behaviors, in situations where mixedversion races cannot be avoided through other technical means.
p1538
aVWe propose a new way to raise the level of discourse in the programming process: permit ambiguity, but manage it by linking it to unambiguous examples. This allows programming environments to work with informal descriptions that lack precise semantics, such as natural language descriptions or conceptual diagrams, without requiring programmers to formulate their ideas in a formal language first. As an example of this idea, we present Zones, a code search and reuse interface that connects code with ambiguous natural language statements about its purpose. The backend, called ProcedureSpace, relates purpose statements, static code analysis features, and natural language background knowledge. ProcedureSpace can search for code given statements of purpose or vice versa, and can find code that was never annotated or commented. Since completed Zones searches become annotations, system coverage grows with user interaction. Users in a preliminary study found that reasoning jointly over natural language and programming language helped them reuse code.
p1539
aVHow do artists and scientists work? The same.
p1540
aVAlthough static type systems are an essential part in teaching and research in software engineering and computer science, there is hardly any knowledge about what the impact of static type systems on the development time or the resulting quality for a piece of software is. On the one hand there are authors that state that static type systems decrease an application's complexity and hence its development time (which means that the quality must be improved since developers have more time left in their projects). On the other hand there are authors that argue that static type systems increase development time (and hence decrease the code quality) since they restrict developers to express themselves in a desired way. This paper presents an empirical study with 49 subjects that studies the impact of a static type system for the development of a parser over 27 hours working time. In the experiments the existence of the static type system has neither a positive nor a negative impact on an application's development time (under the conditions of the experiment).
p1541
aVFinegrain case studies of scientific inquiry, lessons from linguistics on metaphoric thinking, the epistemology of Charles Sanders Peirce, recent work on architectural imageschemata, along with the computer world's own theorist, Peter Naur, all suggest that software developers (frequently dulled and desiccated from overdosing on 'Cartesian' methodologies) could benefit from imbibing a little 'mysticism' not the waveyourhands woowoo kind but the more ineffable hunch and gut side of human cognition. Scholarly publications in their final polished forms rarely admit that stories, jokes, eroticism, and dreams were the fertile seeds that germinated into 'serious' results. This essay looks to these 'closet' sources, nonreductionist, nonself conscious, metaphorical, aformal modes of thought as the salvation of a profession gone awry. It is notably protoscientific imageschemata that retain our attention as a pragmatic tool for improving the fecundity of Agile methodology, at its roots, so to speak. The necessary context is provided by Peter Naur's fundamental insights about software development as 'theory building' coupled with an elaboration of the Agile concept of storytelling.
p1542
aVSyntax definitions are pervasive in modern software systems, and serve as the basis for language processing tools like parsers and compilers. Mainstream parser generators pose restrictions on syntax definitions that follow from their implementation algorithm. They hamper evolution, maintainability, and compositionality of syntax definitions. The pureness and declarativity of syntax definitions is lost. We analyze how these problems arise for different aspects of syntax definitions, discuss their consequences for language engineers, and show how the pure and declarative nature of syntax definitions can be regained.
p1543
aVResearch in the area of programming languages has different facets   from formal reasoning about new programming language constructs (such as type soundness proofs for new type systems) over inventions of new abstractions, up to performance measurements of virtual machines. A closer look into the underlying research methods reveals a distressing characteristic of programming language research: developers, which are the main audience for new language constructs, are hardly considered in the research process. As a consequence, it is simply not possible to state whether a new construct that requires some kind of interaction with the developer has any positive impact on the construction of software. This paper argues for appropriate research methods in programming language research that rely on studies of developers   and argues that the introduction of corresponding empirical methods not only requires a new understanding of research but also a different view on how to teach software science to students.
p1544
aVGiven a highlevel specification and a lowlevel programming language, our goal is to automatically synthesize an efficient program that meets the specification. In this paper, we present a new algorithmic methodology for inductive synthesis that allows us to do this. We use Second Order logic as our generic high level specification logic. For our lowlevel languages we choose small applicationspecific logics that can be immediately translated into code that runs in expected linear time in the worst case. We explain our methodology and provide examples of the synthesis of several graph classifiers, e.g, lineartime tests of whether the input graph is connected, acyclic, etc. In another set of applications we automatically derive many finite differencing expressions equivalent to ones that Paige built by hand in his thesis [Pai81]. Finally we describe directions for automatically combining such automatically generated building blocks to synthesize efficient code implementing more complicated specifications. The methods in this paper have been implemented in Python using the SMT solver Z3 [dMB].
p1545
aVThe challenging nature of error handling constantly escalates as a growing number of environments consists of networked devices and software components. In these environments, errors cover a uniquely large spectrum of situations related to each layer ranging from hardware to distributed platforms, to software components. Handling errors becomes a daunting task for programmers, whose outcome is unpredictable. Scaling up error handling requires to raise the level of abstraction beyond the code level and the trycatch construct, approaching error handling at the software architecture level. We propose a novel approach that relies on an Architecture Description Language (ADL), which is extended with errorhandling declarations. To further raise the level of abstraction, our approach revolves around a domainspecific architectural pattern commonly used in pervasive computing. Error handling is decomposed into components dedicated to platformwide, errorrecovery strategies. At the application level, descriptions of functional components include declarations dedicated to error handling. We have implemented a compiler for an ADL extended with errorhandling declarations. It produces customized programming frameworks that drive and support the programming of error handling. Our approach has been validated with a variety of applications for building automation.
p1546
aVProgramming forums are becoming the primary tools for programmers to find answers for their programming problems. Our empirical study of popular programming forums shows that the forum users experience long waiting period for answers and a small number of experts are often overloaded with questions. To improve the usage experience, we have designed and implemented GFinder, both an algorithm and a tool that makes intelligent routing decisions as to which participant is the expert for answering a particular programming question. Our main approach is to leverage the source code information of the software systems that forums are dedicated to, and discover latent relationships between forums users. Our algorithms construct the concept networks and the user networks from the program source and the forum data.We use programming questions to dynamically integrate these two networks and present an adaptive ranking of the potential experts. Our evaluation of GFinder, using the data from three large programming forums, takes a retrospective view to check if GFinder can correctly predict the experts who provided answers to programming questions. The evaluation results show that GFinder improves the prediction precision by 25% to 74%, compared to related approaches.
p1547
aVWe are trapped in a sequential prison. We use sequential character strings to write sequential programs to control sequential computers. No wonder concurrency remains elusive. How did we come to be here? The high cost of vacuum tube logic forced sequence upon early computer builders. Sequential character strings were the economic way to describe what sequential computers should do. Sequential programs controlled the expensive part of the machine, namely logic. The lethargic pace of logic circuits masked the cost of moving data over distance, allowing programming languages to ignore the cost of communication. Today, the time delay and energy cost of communicating over distance dominate modern computers; logic is essentially free. Why then, do programming languages continue to control logic and largely ignore communication? It will take a broad effort to escape our sequential prison, requiring changes in hardware, programming notations and the ways in which they are expressed. Most importantly, it will require recognizing that we are in sequential prison, and planning for an escape.
p1548
aVThe reliability of compilers, interpreters, and development environments for programming languages is essential for effective software development and maintenance. They are often tested only as an afterthought. Languages with a smaller scope, such as domainspecific languages, often remain untested. Generalpurpose testing techniques and test case generation methods fall short in providing a lowthreshold solution for testdriven language development. In this paper we introduce the notion of a languageparametric testing language (LPTL) that provides a reusable, generic basis for declaratively specifying language definition tests. We integrate the syntax, semantics, and editor services of a language under test into the LPTL for writing test inputs. This paper describes the design of an LPTL and the tool support provided for it, shows use cases using examples, and describes our implementation in the form of the Spoofax testing language.
p1549
aVProfilers help developers to find and fix performance problems. But do they find performance bugs   performance problems that real users actually notice? In this paper we argue that   especially in the case of interactive applications   traditional profilers find irrelevant problems but fail to find relevant bugs. We then introduce lag hunting, an approach that identifies perceptible performance bugs by monitoring the behavior of applications deployed in the wild. The approach transparently produces a list of performance issues, and for each issue provides the developer with information that helps in finding the cause of the problem. We evaluate our approach with an experiment where we monitor an application used by 24 users for 1958 hours over the course of 3months. We characterize the resulting 881 issues, and we find and fix the causes of a set of representative examples.
p1550
aVAs hardware failures are no longer rare in the era of cloud computing, cloud software systems must "prevail" against multiple, diverse failures that are likely to occur. Testing software against multiple failures poses the problem of combinatorial explosion of multiple failures. To address this problem, we present PreFail, a programmable failureinjection tool that enables testers to write a wide range of policies to prune down the large space of multiple failures. We integrate PreFail to three cloud software systems (HDFS, Cassandra, and ZooKeeper), show a wide variety of useful pruning policies that we can write for them, and evaluate the speedups in testing time that we obtain by using the policies. In our experiments, our testing approach with appropriate policies found all the bugs that one can find using exhaustive testing while spending 10X 200X less time than exhaustive testing.
p1551
aVHighcoverage testing is challenging. Modern objectoriented programs present additional challenges for testing. One key difficulty is the generation of proper method sequences to construct desired objects as method parameters. In this paper, we cast the problem as an instance of program synthesis that automatically generates candidate programs to satisfy a userspecified intent. In our setting, candidate programs are method sequences, and desired object states specify an intent. Automatic generation of desired method sequences is difficult due to its large search space sequences often involve methods from multiple classes and require specific primitive values. This paper introduces a novel approach, called Seeker, to intelligently navigate the large search space. Seeker synergistically combines static and dynamic analyses: (1) dynamic analysis generates method sequences to cover branches; (2) static analysis uses dynamic analysis information for notcovered branches to generate candidate sequences; and (3) dynamic analysis explores and eliminates statically generated sequences. For evaluation, we have implemented Seeker and demonstrate its effectiveness on four subject applications totalling 28K LOC. We show that Seeker achieves higher branch coverage and defuse coverage than existing stateoftheart approaches. We also show that Seeker detects 34 new defects missed by existing tools.
p1552
aVParallelization transformations are an important vehicle for improving the performance and scalability of a software system. Utilizing concurrency requires that the developer first identify a suitable parallelization scope: one that poses as a performance bottleneck, and at the same time, exhibits considerable available parallelism. However, having identified a candidate scope, the developer still needs to ensure the correctness of the transformation. This is a difficult undertaking, where a major source of complication lies in tracking down sequential dependencies that inhibit parallelization and addressing them. We report on Hawkeye, a dynamic dependenceanalysis tool that is designed to assist programmers in pinpointing such impediments to parallelization. In contrast with fieldbased dependence analyses, which track concrete memory conflicts and thus suffer from a high rate of false reports, Hawkeye tracks dependencies induced by the abstract semantics of the data type while ignoring dependencing arising solely from implementation artifacts. This enables a more concise report, where the reported dependencies are more likely to be real as well as intelligible to the programmer.
p1553
aVWe present a technique for automatically adding finegrain locking to an abstract data type that is implemented using a dynamic forest i.e., the data structures may be mutated, even to the point of violating forestness temporarily during the execution of a method of the ADT. Our automatic technique is based on Domination Locking, a novel locking protocol. Domination locking is designed specifically for software concurrency control, and in particular is designed for objectoriented software with destructive pointer updates. Domination locking is a strict generalization of existing locking protocols for dynamically changing graphs. We show our technique can successfully add finegrain locking to libraries where manually performing locking is extremely challenging. We show that automatic finegrain locking is more efficient than coarsegrain locking, and obtains similar performance to handcrafted finegrain locking.
p1554
aVSpeculative parallelization divides a sequential program into possibly parallel tasks and permits these tasks to run in parallel if and only if they show no dependences with each other. The parallelization is safe in that a speculative execution always produces the same output as the sequential execution. In this paper, we present the dependence hint, an interface for a user to specify possible dependences between possibly parallel tasks. Dependence hints may be incorrect or incomplete but they do not change the program output. The interface extends Cytron's doacross and recent OpenMP ordering primitives and makes them safe and safely composable. We use it to express conditional and partial parallelism and to parallelize largesize legacy code. The prototype system is implemented as a software library. It is used to improve performance by nearly 10 times on average on current multicore machines for 8 programs including 5 SPEC benchmarks.
p1555
aVRemote data access latency is a significant performance bottleneck in many modern programs that use remote databases and web services. We present Sprint  a runtime system for optimizing such programs by prefetching and caching data from remote sources in parallel to the execution of the original program. Sprint separates the concerns of exposing potentiallyindependent data accesses from the mechanism for executing them efficiently in parallel or in a batch. In contrast to prior work, Sprint can efficiently prefetch data in the presence of irregular or inputdependent access patterns, while preserving the semantics of the original program. We used Sprint to automatically improve the performance of several realworld Java programs that access remote databases (MySQL, DB2) and web services (Facebook, IBM's Yellow Pages). Sprint achieves speedups ranging 2.4x to 15.8x over sequential execution, which are comparable to those achieved by manually modifying the program for asynchronous and batch execution of data accesses. Sprint provides a simple interface that allows a programmer to plug in support for additional data sources without modifying the client program.
p1556
aVAssertions are a familiar and widely used bug detection technique. Traditional assertion checking, however, is performed synchronously, imposing its full cost on the runtime of the program. As a result, many useful kinds of checks, such as data structure invariants and heap analyses, are impractical because they lead to extreme slowdowns. We present a solution that decouples assertion evaluation from program execution: assertions are checked asynchronously by separate checking threads while the program continues to execute. Our technique guarantees that asynchronous evaluation always produces the same result as synchronous evaluation, even if the program concurrently modifies the program state. The checking threads evaluate each assertion on a consistent snapshot of the program state as it existed at the moment the assertion started. We implemented our technique in a system called Strobe, which supports asynchronous assertion checking in both singleand multithreaded Java applications. Strobe runs inside the Java virtual machine and uses copyonwrite to construct snapshots incrementally, onthefly. Our system includes all necessary synchronization to support multiple concurrent checking threads, and to prevent data races with the main program threads. We find that asynchronous checking significantly outperforms synchronous checking, incurring tolerable overheads   in the range of 10% to 50% over no checking at all   even for heavyweight assertions that would otherwise result in crushing slowdowns.
p1557
aVThe need for programs to execute subcomponents in isolation from each other or with lower privileges is prevalent among today's systems. We introduce ribbons: a shared memory programming model that allows for more implicit sharing of memory than processes but is more restrictive than threads. Ribbons structure the heap into protection domains. Privileges between these protection domains are carefully controlled in order to confine computation. We propose RibbonJ, a backwardscompatible extension of Java, to easily create or port programs to use the ribbons model. We study the progress and isolation properties of a subset of the language. Building on JikesRVM we implement ribbons by leveraging existing memory protection mechanisms in modern hardware and operating systems, avoiding the overhead of inline security checks and read or write barriers. We evaluate efficiency via microbenchmarks and the DaCapo suite, observing minor overhead. Additionally, we refactor Apache Tomcat to use ribbons for application isolation, discuss the refactoring's design and complexity, and evaluate performance using the SPECweb2009 benchmark.
p1558
aVFalse sharing is an insidious problem for multithreaded programs running on multicore processors, where it can silently degrade performance and scalability. Previous tools for detecting false sharing are severely limited: they cannot distinguish false sharing from true sharing, have high false positive rates, and provide limited assistance to help programmers locate and resolve false sharing. This paper presents two tools that attack the problem of false sharing: SheriffDetect and SheriffProtect. Both tools leverage a framework we introduce here called Sheriff. Sheriff breaks out threads into separate processes, and exposes an API that allows programs to perform perthread memory isolation and tracking on a perpage basis. We believe Sheriff is of independent interest. SheriffDetect finds instances of false sharing by comparing updates within the same cache lines by different threads, and uses sampling to rank them by performance impact. SheriffDetect is precise (no false positives), runs with low overhead (on average, 20%), and is accurate, pinpointing the exact objects involved in false sharing. We present a case study demonstrating SheriffDetect's effectiveness at locating false sharing in a variety of benchmarks. Rewriting a program to fix false sharing can be infeasible when source is unavailable, or undesirable when padding objects would unacceptably increase memory consumption or further worsen runtime performance. SheriffProtect mitigates false sharing by adaptively isolating shared updates from different threads into separate physical addresses, effectively eliminating most of the performance impact of false sharing. We show that SheriffProtect can improve performance for programs with catastrophic false sharing by up to 9, without programmer intervention.
p1559
aVMemory safety defends against inadvertent and malicious misuse of memory that may compromise program correctness and security. A critical element of memory safety is zero initialization. The direct cost of zero initialization is surprisingly high: up to 12.7%, with average costs ranging from 2.7 to 4.5% on a high performance virtual machine on IA32 architectures. Zero initialization also incurs indirect costs due to its memory bandwidth demands and cache displacement effects. Existing virtual machines either: a) minimize direct costs by zeroing in large blocks, or b) minimize indirect costs by zeroing in the allocation sequence, which reduces cache displacement and bandwidth. This paper evaluates the two widely used zero initialization designs, showing that they make different tradeoffs to achieve very similar performance. Our analysis inspires three better designs: (1) bulk zeroing with cachebypassing (nontemporal) instructions to reduce the direct and indirect zeroing costs simultaneously, (2) concurrent nontemporal bulk zeroing that exploits parallel hardware to move work off the application's critical path, and (3) adaptive zeroing, which dynamically chooses between (1) and (2) based on available hardware parallelism. The new software strategies offer speedups sometimes greater than the direct overhead, improving total performance by 3% on average. Our findings invite additional optimizations and microarchitectural support.
p1560
aVModern computing has adopted the floating point type as a default way to describe computations with real numbers. Thanks to dedicated hardware support, such computations are efficient on modern architectures, even in double precision. However, rigorous reasoning about the resulting programs remains difficult. This is in part due to a large gap between the finite floating point representation and the infiniteprecision realnumber semantics that serves as the developers' mental model. Because programming languages do not provide support for estimating errors, some computations in practice are performed more and some less precisely than needed. We present a library solution for rigorous arithmetic computation. Our numerical data type library tracks a (double) floating point value, but also a guaranteed upper bound on the error between this value and the ideal value that would be computed in the realvalue semantics. Our implementation involves a set of linear approximations based on an extension of affine arithmetic. The derived approximations cover most of the standard mathematical operations, including trigonometric functions, and are more comprehensive than any publicly available ones. Moreover, while interval arithmetic rapidly yields overly pessimistic estimates, our approach remains precise for several computational tasks of interest. We evaluate the library on a number of examples from numerical analysis and physical simulations. We found it to be a useful tool for gaining confidence in the correctness of the computation.
p1561
aVJava's type system enforces exceptionchecking rules that stipulate a checked exception thrown by a method must be declared in the throws clause of the method. Software written in Java often invokes native methods through the use of the Java Native Interface (JNI). Java's type system, however, cannot enforce the same exceptionchecking rules on Java exceptions raised in native methods. This gap makes Java software potentially buggy and often difficult to debug when an exception is raised in native code. In this paper, we propose a complete staticanalysis framework called JET to extend exceptionchecking rules even on native code. The framework has a twostage design where the first stage throws away a large portion of irrelevant code so that the second stage, a finegrained analysis, can concentrate on a small set of code for accurate bug finding. This design achieves both high efficiency and accuracy. We have applied JET on a set of benchmark programs with a total over 227K lines of source code and identified 12 inconsistent nativemethod exception declarations.
p1562
aVIn the current work, we investigate the benefits of immutability guarantees for allowing more flexible handling of aliasing, as well as more precise and concise specifications. Our approach supports finer levels of control that can mark data structures as being immutable through the use of immutability annotations. By using such annotations to encode immutability guarantees, we expect to obtain better specifications that can more accurately describe the intentions, as well as prohibitions, of the method. Ultimately, our goal is improving the precision of the verification process, as well as making the specifications more readable, more precise and as an enforceable program documentation. We have designed and implemented a new entailment procedure to formally and automatically reason about immutability enhanced specifications. We have also formalised the soundness for our new procedure through an operational semantics with mutability assertions on the heap. Lastly, we have carried out a set of experiments to both validate and affirm the utility of our current proposal on immutability enhanced specification mechanism.
p1563
aVHybrid partial evaluation (HPE) is a pragmatic approach to partial evaluation that borrows ideas from both online and offline partial evaluation. HPE performs offlinestyle specialization using an online approach without static binding time analysis. The goal of HPE is to provide a practical and predictable level of optimization for programmers, with an implementation strategy that fits well within existing compilers or interpreters. HPE requires the programmer to specify where partial evaluation should be applied. It provides no termination guarantee and reports errors in situations that violate simple binding time rules, or have incorrect use of side effects in compiletime code. We formalize HPE for a small imperative objectoriented language and describe Civet, a straightforward implementation of HPE as a relatively simple extension of a Java compiler. Code optimized by Civet performs as well as the output of a stateoftheart offline partial evaluator.
p1564
aVExisting approaches to extend a programming language with syntactic sugar often leave a bitter taste, because they cannot be used with the same ease as the main extension mechanism of the programming language  libraries. Sugar libraries are a novel approach for syntactically extending a programming language within the language. A sugar library is like an ordinary library, but can, in addition, export syntactic sugar for using the library. Sugar libraries maintain the composability and scoping properties of ordinary libraries and are hence particularly wellsuited for embedding a multitude of domainspecific languages into a host language. They also inherit selfapplicability from libraries, which means that sugar libraries can provide syntactic extensions for the definition of other sugar libraries. To demonstrate the expressiveness and applicability of sugar libraries, we have developed SugarJ, a language on top of Java, SDF and Stratego, which supports syntactic extensibility. SugarJ employs a novel incremental parsing technique, which allows changing the syntax within a source file. We demonstrate SugarJ by five language extensions, including embeddings of XML and closures in Java, all available as sugar libraries. We illustrate the utility of selfapplicability by embedding XML Schema, a metalanguage to define XML languages.
p1565
aVDataflow languages provide natural support for specifying constraints between objects in dynamic applications, where programs need to react efficiently to changes of their environment. Researchers have long investigated how to take advantage of dataflow constraints by embedding them into procedural languages. Previous mixed imperative/dataflow systems, however, require syntactic extensions or libraries of ad hoc data types for binding the imperative program to the dataflow solver. In this paper we propose a novel approach that smoothly combines the two paradigms without placing undue burden on the programmer. In our framework, programmers can define ordinary statements of the imperative host language that enforce constraints between objects stored in special memory locations designated as "reactive". Differently from previous approaches, reactive objects can be of any legal type in the host language, including primitive data types, pointers, arrays, and structures. Statements defining constraints are automatically reexecuted every time their input memory locations change, letting a program behave like a spreadsheet where the values of some variables depend upon the values of other variables. The constraint solving mechanism is handled transparently by altering the semantics of elementary operations of the host language for reading and modifying objects. We provide a formal semantics and describe a concrete embodiment of our technique into C/C++, showing how to implement it efficiently in conventional platforms using offtheshelf compilers. We discuss common coding idioms and relevant applications to reactive scenarios, including incremental computation, observer design pattern, and data structure repair. The performance of our implementation is compared to ad hoc problemspecific change propagation algorithms, as well as to languagecentric approaches such as selfadjusting computation and subject/observer communication mechanisms, showing that the proposed approach is efficient in practice.
p1566
aVParallel or incremental versions of an algorithm can significantly outperform their counterparts, but are often difficult to develop. Programming models that provide appropriate abstractions to decompose data and tasks can simplify parallelization. We show in this work that the same abstractions can enable both parallel and incremental execution. We present a novel algorithm for parallel selfadjusting computation. This algorithm extends a deterministic parallel programming model (concurrent revisions) with support for recording and repeating computations. On record, we construct a dynamic dependence graph of the parallel computation. On repeat, we reexecute only parts whose dependencies have changed. We implement and evaluate our idea by studying five example programs, including a realistic multipass CSS layout algorithm. We describe programming techniques that proved particularly useful to improve the performance of selfadjustment in practice. Our final results show significant speedups on all examples (up to 37x on an 8core machine). These speedups are well beyond what can be achieved by parallelization alone, while requiring a comparable effort by the programmer.
p1567
aVDynamic program optimizations are critical for the efficiency of applications in managed programming languages and scripting languages. Recent studies have shown that exploitation of program inputs may enhance the effectiveness of dynamic optimizations significantly. However, current solutions for enabling the exploitation require either programmers' annotations or intensive offline profiling, impairing the practical adoption of the techniques. This current work examines the basic feasibility of transparent integration of inputconsciousness into dynamic program optimizations, particularly in managed execution environments. It uses transparent learning across production runs as the basic vehicle, and investigates the implications of crossrun learning on each main component of inputconscious dynamic optimizations. It proposes several techniques to address some key challenges for the transparent integration, including randomized inspectioninstrumentation for crossuser data collection, a sparsitytolerant algorithm for input characterization, and selective prediction for efficiency protection. These techniques make it possible to automatically recognize the relations between the inputs to a program and the appropriate ways to optimize it. The whole process happens transparently across production runs; no need for offline profiling or programmer intervention. Experiments on a number of Java programs demonstrate the effectiveness of the techniques in enabling inputconsciousness for dynamic optimizations, revealing the feasibility and potential benefits of the new optimization paradigm in some basic settings.
p1568
aVWhile there has been decades of work on developing automatic, localityenhancing transformations for regular programs that operate over dense matrices and arrays, there has been little investigation of such transformations for irregular programs, which operate over pointerbased data structures such as graphs, trees and lists. In this paper, we argue that, for a class of irregular applications we call traversal codes, there exists substantial data reuse and hence opportunity for locality exploitation. We develop a novel optimization called point blocking, inspired by the classic tiling loop transformation, and show that it can substantially enhance temporal locality in traversal codes. We then present a transformation and optimization framework called TreeTiler that automatically detects opportunities for applying point blocking and applies the transformation. TreeTiler uses autotuning techniques to determine appropriate parameters for the transformation. For a series of traversal algorithms drawn from realworld applications, we show that TreeTiler is able to deliver performance improvements of up to 245% over an optimized (but nontransformed) parallel baseline, and in several cases, significantly better scalability.
p1569
aVConcurrency bugs are often due to inadequate synchronization that fail to prevent specific (undesirable) thread interleavings. Such errors, often referred to as Heisenbugs, are difficult to detect, prevent, and repair. In this paper, we present a new technique to increase program robustness against Heisenbugs. We profile correct executions from provided test suites to infer finegrained atomicity properties. Additional deadlockfree locking is injected into the program to guarantee these properties hold on production runs. Notably, our technique does not rely on witnessing or analyzing erroneous executions. The end result is a scheme that only permits executions which are guaranteed to preserve the atomicity properties derived from the profile. Evaluation results on large, realworld, opensource programs show that our technique can effectively suppress subtle concurrency bugs, with small runtime overheads (typically less than 15%).
p1570
aVThe flexibility of dynamically typed languages such as JavaScript, Python, Ruby, and Scheme comes at the cost of runtime type checks. Some of these checks can be eliminated via controlflow analysis. However, traditional controlflow analysis (CFA) is not ideal for this task as it ignores flowsensitive information that can be gained from dynamic type predicates, such as JavaScript's 'instanceof' and Scheme's 'pair?', and from typerestricted operators, such as Scheme's 'car'. Yet, adding flowsensitivity to a traditional CFA worsens the already significant compiletime cost of traditional CFA. This makes it unsuitable for use in justintime compilers. In response, we have developed a fast, flowsensitive typerecovery algorithm based on the lineartime, flowinsensitive sub0CFA. The algorithm has been implemented as an experimental optimization for the commercial Chez Scheme compiler, where it has proven to be effective, justifying the elimination of about 60% of runtime type checks in a large set of benchmarks. The algorithm processes on average over 100,000 lines of code per second and scales well asymptotically, running in only O(n log n) time. We achieve this compiletime performance and scalability through a novel combination of data structures and algorithms.
p1571
aVA classic problem in parallel computing is determining whether to execute a task in parallel or sequentially. If small tasks are executed in parallel, the taskcreation overheads can be overwhelming. If large tasks are executed sequentially, processors may spin idle. This granularity problem, however well known, is not well understood: broadly applicable solutions remain elusive. We propose techniques for controlling granularity in implicitly parallel programming languages. Using a cost semantics for a generalpurpose language in the style of the lambda calculus with support for parallelism, we show that taskcreation overheads can indeed slow down parallel execution by a multiplicative factor. We then propose oracle scheduling, a technique for reducing these overheads, which bases granularity decisions on estimates of taskexecution times. We prove that, for a class of computations, oracle scheduling can reduce task creation overheads to a small fraction of the work without adversely affecting available parallelism, thereby leading to efficient parallel executions. We realize oracle scheduling in practice by a combination of static and dynamic techniques. We require the programmer to provide the asymptotic complexity of every function and use runtime profiling to determine the implicit, architecturespecific constant factors. In our experiments, we were able to reduce overheads of parallelism down to between 3 and 13 percent, while achieving 6 to 10fold speedups.
p1572
aVSoftware engineers now face the difficult task of refactoring serial programs for parallel execution on multicore processors. Currently, they are offered little guidance as to how much benefit may come from this task, or how close they are to the best possible parallelization. This paper presents Kismet, a tool that creates parallel speedup estimates for unparallelized serial programs. Kismet differs from previous approaches in that it does not require any manual analysis or modification of the program. This difference allows quick analysis of many programs, avoiding wasted engineering effort on those that are fundamentally limited. To accomplish this task, Kismet builds upon the hierarchical critical path analysis (HCPA) technique, a recently developed dynamic analysis that localizes parallelism to each of the potentially nested regions in the target program. It then uses a parallel execution time model to compute an approximate upper bound for performance, modeling constraints that stem from both hardware parameters and internal program structure. Our evaluation applies Kismet to eight highparallelism NAS Parallel Benchmarks running on a 32core AMD multicore system, five lowparallelism SpecInt benchmarks, and six mediumparallelism benchmarks running on the finegrained MIT Raw processor. The results are compelling. Kismet is able to significantly improve the accuracy of parallel speedup estimates relative to prior work based on critical path analysis.
p1573
aVWith core counts on the rise, the sequential components of applications are becoming the major bottleneck in performance scaling as predicted by Amdahl's law. We are therefore faced with the simultaneous problems of occupying an increasing number of cores and speeding up sequential sections. In this work, we reconcile these two seemingly incompatible problems with a novel programming model called Nway. The core idea behind Nway is to benefit from the algorithmic diversity available to express certain key computational steps. By simultaneously launching in parallel multiple ways to solve a given computation, a runtime can justintime pick the best (for example the fastest) way and therefore achieve speedup. Previous work has demonstrated the benefits of such an approach but has not addressed its inherent waste. In this work, we focus on providing a mathematically sound learningbased statistical model that can be used by a runtime to determine the optimal balance between resources used and benefits obtainable through Nway. We further describe a dynamic culling mechanism to further reduce resource waste. We present abstractions and a runtime support to cleanly encapsulate the computationaloptions and monitor their progress. We demonstrate a lowoverhead runtime that achieves significant speedup over a range of widely used kernels. Our results demonstrate superlinear speedups in certain cases.
p1574
aVSpeculative execution at coarse granularities (e.g., codeblocks, methods, algorithms) offers a promising programming model for exploiting parallelism on modern architectures. In this paper we present Anumita, a framework that includes programming constructs and a supporting runtime system to enable the use of coarsegrain speculation to improve program performance, without burdening the programmer with the complexity of creating, managing and retiring speculations. Speculations may be composed by specifying surrogate code blocks at any arbitrary granularity, which are then executed concurrently, with a single winner ultimately modifying program state. Anumita provides expressive semantics for winner selection that go beyond time to solution to include userdefined notions of quality of solution. Anumita can be used to improve the performance of hard to parallelize algorithms whose performance is highly dependent on input data. Anumita is implemented as a userlevel runtime with programming interfaces to C, C++, Fortran and as an OpenMP extension. Performance results from several applications show the efficacy of using coarsegrain speculation to achieve (a) robustness when surrogates fail and (b) significant speedup over static algorithm choices.
p1575
aVCoordination can destroy scalability in parallel programming. A comprehensive library of scalable synchronization primitives is therefore an essential tool for exploiting parallelism. Unfortunately, such primitives do not easily combine to yield solutions to more complex problems. We demonstrate that a concurrency library based on Fournet and Gonthier's join calculus can provide declarative and scalable coordination. By declarative, we mean that the programmer needs only to write down the constraints of a coordination problem, and the library will automatically derive a correct solution. By scalable, we mean that the derived solutions deliver robust performance both as the number of processors increases, and as the complexity of the coordination problem grows. We validate our claims empirically on seven coordination problems, comparing our generic solution to specialized algorithms from the literature.
p1576
aVMechanized proof assistants are powerful verification tools, but proof development can be difficult and timeconsuming. When verifying a family of related programs, the effort can be reduced by proof reuse. In this paper, we show how to engineer product lines with theorems and proofs built from feature modules. Each module contains proof fragments which are composed together to build a complete proof of correctness for each product. We consider a product line of programming languages, where each variant includes metatheory proofs verifying the correctness of its semantic definitions. This approach has been realized in the Coq proof assistant, with the proofs of each feature independently certifiable by Coq. These proofs are composed for each language variant, with Coq mechanically verifying that the composite proofs are correct. As validation, we formalize a core calculus for Java in Coq which can be extended with any combination of casts, interfaces, or generics.
p1577
aVGradual typing is a framework to combine static and dynamic typing in a single programming language. In this paper, we develop a gradual type system for classbased objectoriented languages with generics. We introduce a special type to denote dynamically typed parts of a program; unlike dynamic types introduced to C# 4.0, however, our type system allows for more seamless integration of dynamically and statically typed code. We formalize a gradual type system for Featherweight GJ with a semantics given by a translation that inserts explicit runtime checks. The type system guarantees that statically typed parts of a program do not go wrong, even if it includes dynamically typed parts. We also describe a basic implementation scheme for Java and report preliminary performance evaluation.
p1578
aVExceptions are invaluable for structured error handling in highlevel languages, but they are at odds with linear types. More generally, control effects may delete or duplicate portions of the stack, which, if we are not careful, can invalidate all substructural usage guarantees for values on the stack. We have developed a typeandeffect system that tracks control effects and ensures that values on the stack are never wrongly duplicated or dropped. We present the system first with abstract control effects and prove its soundness. We then give examples of three instantiations with particular control effects, including exceptions and delimited continuations, and show that they meet the soundness criteria for specific control effects.
p1579
aVIn this paper, we identify trends about, benefits from, and barriers to performing user evaluations in software engineering research. From a corpus of over 3,000 papers spanning ten years, we report on various subtypes of user evaluations (e.g., coding tasks vs. questionnaires) and relate user evaluations to paper topics (e.g., debugging vs. technology transfer). We identify the external measures of impact, such as best paper awards and citation counts, that are correlated with the presence of user evaluations. We complement this with a survey of over 100 researchers from over 40 different universities and labs in which we identify a set of perceived barriers to performing user evaluations.
p1580
aVData races are subtle and difficult to detect errors that arise during concurrent program execution. Traditional testing techniques fail to find these errors, but recent research has shown that targeted dynamic analysis techniques can be developed to precisely detect races (i.e., no false race reports are generated) that occur during program execution. Unfortunately, precise race detection is still too expensive to be used in practice. Stateoftheart techniques still slow down program execution by a factor of eight or more. In this paper, we incorporate an optimization technique based on the observation that many threadshared objects are written early in their lifetimes and then become readonly for the remainder of their lifetimes; these are known as stationary objects. The main contribution of our work is the insight that once a stationary object becomes threadshared, races cannot occur. Therefore, our proposed approach does not monitor access to these objects. As such, our system only incurs an average overhead of 45% of that of an implementation of FastTrack, a lowoverhead dynamic race detector. We then compared the effectiveness of our approach to de tect races in deployed environments with that of Pacer, a sampling based race detector based on FastTrack. We found that our approach can detect over five times more races than Pacer when we budget 50% for runtime overhead.
p1581
aVOriginally conceived as the target platform for Java alone, the Java Virtual Machine (JVM) has since been targeted by other languages, one of which is Scala. This trend, however, is not yet reflected by the benchmark suites commonly used in JVM research. In this paper, we thus present the design and analysis of the first fullfledged benchmark suite for Scala. We furthermore compare the benchmarks contained therein with those from the wellknown DaCapo 9.12 benchmark suite and show where the differences are between Scala and Java code and where not.
p1582
aVJavaScript is a highly dynamic language for webbased applications. Innovative implementation techniques for improving its speed and responsiveness have been developed in recent years. Industry benchmarks such as WebKit SunSpider are often cited as a measure of the efficacy of these techniques. However, recent studies have shown that these benchmarks fail to accurately represent the dynamic nature of modern JavaScript applications, and so may be poor predictors of realworld performance. Worse, they may guide the development of optimizations which are unhelpful for real applications. Our goal is to develop a tool and techniques to automate the creation of realistic and representative benchmarks from existing web applications. We propose a recordandreplay approach to capture JavaScript sessions which has sufficient fidelity to accurately recreate key characteristics of the original application, and at the same time is sufficiently flexible that a recording produced on one platform can be replayed on a different one. We describe JSBench, a flexible tool for workload capture and benchmark generation, and demonstrate its use in creating eight benchmarks based on popular sites. Using a variety of runtime metrics collected with instrumented versions of Firefox, Internet Explorer, and Safari, we show that workloads created by JSBench match the behavior of the original web applications.
p1583
aVA new generation of mobile touch devices, such as the iPhone, iPad and Android devices, are equipped with powerful, modern browsers. However, regular websites are not optimized for the specific features and constraints of these devices, such as limited screen estate, unreliable Internet access, touchbased interaction patterns, and features such as GPS. While recent advances in web technology enable web developers to build web applications that take advantage of the unique properties of mobile devices, developing such applications exposes a number of problems, specifically: developers are required to use many loosely coupled languages with limited tool support and application code is often verbose and imperative. We introduce mobl, a new language designed to declaratively construct mobile web applications. Mobl integrates languages for user interface design, styling, data modeling, querying and application logic into a single, unified language that is flexible, expressive, enables early detection of errors, and has good IDE support.
p1584
aVObjects model the world, and state is fundamental to a faithful modeling. Engineers use state machines to understand and reason about state transitions, but programming languages provide little support for building software based on state abstractions. We propose Plaid, a language in which objects are modeled not just in terms of classes, but in terms of changing abstract states. Each state may have its own representation, as well as methods that may transition the object into a new state. A formal model precisely defines the semantics of core Plaid constructs such as state transition and traitlike state composition. We evaluate Plaid through a series of examples taken from the Plaid compiler and the standard libraries of Smalltalk and Java. These examples show how Plaid can more closely model statebased designs, enhancing understandability, enhancing dynamic error checking, and providing reuse benefits.
p1585
aVLanguage Oriented Programming (LOP) is a paradigm that puts domain specific programming languages (DSLs) at the center of the software development process. Currently, there are three main approaches to LOP: (1) the use of internal DSLs, implemented as libraries in a given host language; (2) the use of external DSLs, implemented as interpreters or compilers in an external language; and (3) the use of language workbenches, which are integrated development environments (IDEs) for defining and using external DSLs. In this paper, we contribute: (4) a novel languageoriented approach to LOP for defining and using internal DSLs. While language workbenches adapt internal DSL features to overcome some of the limitations of external DSLs, our approach adapts language workbench features to overcome some of the limitations of internal DSLs. We introduce Cedalion, an LOP host language for internal DSLs, featuring static validation and projectional editing. To validate our approach we present a case study in which Cedalion was used by biologists in designing a DNA microarray for molecular Biology research.
p1586
aVSelfadjusting computation offers a languagebased approach to writing programs that automatically respond to dynamically changing data. Recent work made significant progress in developing sound semantics and associated implementations of selfadjusting computation for highlevel, functional languages. These techniques, however, do not address issues that arise for lowlevel languages, i.e., stackbased imperative languages that lack strong type systems and automatic memory management. In this paper, we describe techniques for selfadjusting computation which are suitable for lowlevel languages. Necessarily, we take a different approach than previous work: instead of starting with a highlevel language with additional primitives to support selfadjusting computation, we start with a lowlevel intermediate language, whose semantics is given by a stackbased abstract machine. We prove that this semantics is sound: it always updates computations in a way that is consistent with full reevaluation. We give a compiler and runtime system for the intermediate language used by our abstract machine. We present an empirical evaluation that shows that our approach is efficient in practice, and performs favorably compared to prior proposals.
p1587
aVDynamic or JustinTime (JIT) compilation is crucial to achieve acceptable performance for applications (written in managed languages, such as Java and C#) distributed as intermediate language binary codes for a virtual machine (VM) architecture. Since it occurs at runtime, JIT compilation needs to carefully tune its compilation policy to make effective decisions regarding 'if' and 'when' to compile different program regions to achieve the best overall program performance. Past research has extensively tuned JIT compilation policies, but mainly for VMs with a single compiler thread and for execution on singleprocessor machines. This work is driven by the need to explore the most effective JIT compilation strategies in their modern operational environment, where (a) processors have evolved from single to multi/many cores, and (b) VMs provide support for multiple concurrent compiler threads. Our results confirm that changing 'if' and 'when' methods are compiled have significant performance impacts. We construct several novel configurations in the HotSpot JVM to facilitate this study. The new configurations are necessitated by modern Java benchmarks that impede traditional static wholeprogram discovery, analysis and annotation, and are required for simulating future manycore hardware that is not yet widely available. We study the effects on performance of increasing compiler aggressiveness for VMs with multiple compiler threads running on existing single/multicore and future manycore machines. Our results indicate that although more aggressive JIT compilation policies show no benefits on singlecore machines, these can often improve program performance for multi/manycore machines. However, accurately prioritizing JIT method compilations is crucial to realize such benefits.
p1588
aVWhen optimizing largescale applications, striking the balance between steadystate performance, startup time, and code size has always been a grand challenge. While recent advances in trace compilation have significantly improved the steadystate performance of trace JITs for largescale Java applications, the size control aspect of a trace compilation system remains largely overlooked. For instance, using the DaCapo 9.12 benchmarks, we observe that 40% of traces selected by a stateoftheart trace selection algorithm are shortlived and, on average, each selected basic block is replicated 13 times in the trace cache. This paper studies the size control problem for a class of commonly used trace selection algorithms and proposes six techniques to reduce the footprint of trace selection without incurring any performance loss. The crux of our approach is to target redundancies in trace selection in the form of either shortlived traces or unnecessary trace duplication. Using one of the best performing selection algorithms as the baseline, we demonstrate that, on the DaCapo 9.12 benchmarks and DayTrader 2.0 on WebSphere Application Server 7.0, our techniques reduce the code size and compilation time by 69% and the startup time by 43% while retaining the steadystate performance. On DayTrader 2.0, an example of largescale application, our techniques also improve the steadystate performance by 10%.
p1589
aVIn many projects, lexical preprocessors are used to manage different variants of the project (using conditional compilation) and to define compiletime code transformations (using macros). Unfortunately, while being a simple way to implement variability, conditional compilation and lexical macros hinder automatic analysis, even though such analysis is urgently needed to combat variabilityinduced complexity. To analyze code with its variability, we need to parse it without preprocessing it. However, current parsing solutions use unsound heuristics, support only a subset of the language, or suffer from exponential explosion. As part of the TypeChef project, we contribute a novel variabilityaware parser that can parse almost all unpreprocessed code without heuristics in practicable time. Beyond the obvious task of detecting syntax errors, our parser paves the road for further analysis, such as variabilityaware type checking. We implement variabilityaware parsers for Java and GNU C and demonstrate practicability by parsing the product line MobileMedia and the entire X86 architecture of the Linux kernel with 6065 variable features.
p1590
aVDynamic updates to running programs improve development productivity and reduce downtime of longrunning applications. This feature is however severely limited in current virtual machines for objectoriented languages. In particular, changes to classes often apply only to methods invoked after a class change, but not to active methods on the call stack of threads. Additionally, adding and removing methods as well as fields is often not supported. We present a novel programming model for safe and atomic code updates of Java programs that also updates methods that are currently executed. We introduce safe update regions and pause threads only there before an update. We automatically convert the stack frames to suit the new versions of the methods. Our implementation is based on a productionquality Java virtual machine. Additionally, we present SafeWeave, a dynamic aspectoriented programming system that exposes the atomic code updates through a highlevel programming model. AspectJ advice can be added to and removed from a running application.Changes are atomic and correctness is guaranteed even though weaving happens in parallel to program execution, and the system fully supports the dynamic class loading of Java. We show that the enhanced evolution features do not incur any performance penalty before and after version changes.
p1591
aVWe address the problem of testing atomicity of composed concurrent operations. Concurrent libraries help programmers exploit parallel hardware by providing scalable concurrent operations with the illusion that each operation is executed atomically. However, client code often needs to compose atomic operations in such a way that the resulting composite operation is also atomic while preserving scalability. We present a novel technique for testing the atomicity of client code composing scalable concurrent operations. The challenge in testing this kind of client code is that a bug may occur very rarely and only on a particular interleaving with a specific thread configuration. Our technique is based on modular testing of client code in the presence of an adversarial environment; we use commutativity specifications to drastically reduce the number of executions explored to detect a bug. We implemented our approach in a tool called COLT, and evaluated its effectiveness on a range of 51 realworld concurrent Java programs. Using COLT, we found 56 atomicity violations in Apache Tomcat, Cassandra, MyFaces Trinidad, and other applications.
p1592
aVIndexes are ubiquitous. Examples include associative arrays, dictionaries, maps and hashes used in applications such as databases, file systems and dynamic languages. Abstractly, a sequential index can be viewed as a partial function from keys to values. Values can be queried by their keys, and the index can be mutated by adding or removing mappings. Whilst appealingly simple, this abstract specification is insufficient for reasoning about indexes accessed concurrently. We present an abstract specification for concurrent indexes. We verify several representative concurrent client applications using our specification, demonstrating that clients can reason abstractly without having to consider specific underlying implementations. Our specification would, however, mean nothing if it were not satisfied by standard implementations of concurrent indexes. We verify that our specification is satisfied by algorithms based on linked lists, hash tables and BLink trees. The complexity of these algorithms, in particular the BLink tree algorithm, can be completely hidden from the client's view by our abstract specification.
p1593
aVIn this paper we introduce a new method for pessimistically implementing composable, nestable atomic statements. Our mechanism, called shelters, is inspired by the synchronization strategy used in the Jade programming language. Unlike previous lockbased pessimistic approaches, our mechanism does not require a wholeprogram analysis that computes a global lock order. Further, this mechanism frees us to implement several optimizations, impossible with automatically inserted locks, that are necessary for scaling on recent multicore systems. Additionally we show how our basic mechanism can be extended to support both open and closednesting of atomic statements, something that, to our knowledge, has not yet been implemented fullypessimistically in this context. Unlike optimistic, transactionalmemorybased approaches, programmers using our mechanism do not have to write compensating actions for opennesting, or worry about the possibly awkward semantics and performance impact of aborted transactions. Similar to systems using locks, our implementation requires programmers to annotate the types of objects with the shelters that protect them, and indicate the sections of code to be executed atomically with atomic statements. A static analysis then determines from which shelters protection is needed for the atomic statements to run atomically. We have implemented shelterbased atomic statements for C, and applied our implementation to 12 benchmarks totaling over 200k lines of code including the STAMP benchmark suite, and the sqlite database system. Our implementation's performance is competitive with explicit locking, Autolocker, and a mature software transactional memory implementation.
p1594
aVIsolation the property that a task can access shared data without interference from other tasks is one of the most basic concerns in parallel programming. In this paper, we present Aida, a new model of isolated execution for parallel programs that perform frequent, irregular accesses to pointerbased shared data structures. The three primary benefits of Aida are dynamism, safety and liveness guarantees, and programmability. First, Aida allows tasks to dynamically select and modify, in an isolated manner, arbitrary finegrained regions in shared data structures, all the while maintaining a high level of concurrency. Consequently, the model can achieve scalable parallelization of regular as well as irregular sharedmemory applications. Second, the model offers freedom from data races, deadlocks, and livelocks. Third, no extra burden is imposed on programmers, who access the model via a simple, declarative isolation construct that is similar to that for transactional memory. The key new insight in Aida is a notion of delegation among concurrent isolated tasks (known in Aida as assemblies). Each assembly A is equipped with a region in the shared heap that it owns the only objects accessed by A are those it owns, guaranteeing racefreedom. The region owned by A can grow or shrink flexibly however, when A needs to own a datum owned by B, A delegates itself, as well as its owned region, to B. From now on, B has the responsibility of reexecuting the task A set out to complete. Delegation as above is the only interassembly communication primitive in Aida. In addition to reducing contention in a local, datadriven manner, it guarantees freedom from deadlocks and livelocks. We offer an implementation of Aida on top of the Habanero Java parallel programming language. The implementation employs several novel ideas, including the use of a unionfind data structure to represent tasks and the regions that they own. A thorough evaluation using several irregular dataparallel benchmarks demonstrates the low overhead and excellent scalability of Aida, as well as its benefits over existing approaches to declarative isolation. Our results show that Aida performs on par with the stateoftheart customized implementations of irregular applications and much better than coarsegrained locking and transactional memory approaches.
p1595
aVThis paper introduces AC, a set of language constructs for composable asynchronous IO in native languages such as C/C++. Unlike traditional synchronous IO interfaces, AC lets a thread issue multiple IO requests so that they can be serviced concurrently, and so that longlatency operations can be overlapped with computation. Unlike traditional asynchronous IO interfaces, AC retains a sequential style of programming without requiring code to use multiple threads, and without requiring code to be "stackripped" into chains of callbacks. AC provides an "async" statement to identify opportunities for IO operations to be issued concurrently, a "do..finish" block that waits until any enclosed "async" work is complete, and a "cancel" statement that requests cancellation of unfinished IO within an enclosing "do..finish". We give an operational semantics for a core language. We describe and evaluate implementations that are integrated with message passing on the Barrelfish research OS, and integrated with asynchronous file and network IO on Microsoft Windows. We show that AC offers comparable performance to existing C/C++ interfaces for asynchronous IO, while providing a simpler programming model.
p1596
aVThis paper focuses on extensibility, the ability of a programmer using a particular language to extend the expressiveness of that language. This paper explores how to provide an interesting notion of extensibility by virtualizing the interface between code and data. A virtual value is a special value that supports behavioral intercession. When a primitive operation is applied to a virtual value, it invokes a trap on that virtual value. A virtual value contains multiple traps, each of which is a userdefined function that describes how that operation should behave on that value. This paper formalizes the semantics of virtual values, and shows how they enable the definition of a variety of language extensions, including additional numeric types; delayed evaluation; taint tracking; contracts; revokable membranes; and units of measure. We report on our experience implementing virtual values for Javascript within an extension for the Firefox browser.
p1597
aVWe propose Backstage Java (BSJ), a Java language extension which allows algorithmic, contextuallyaware generation and transformation of code. BSJ explicitly and concisely represents design patterns and other encodings by employing compiletime metaprogramming: a practice in which the programmer writes instructions which are executed over the program's AST during compilation. While compiletime metaprogramming has been successfully used in functional languages such as Template Haskell, a number of language properties (scope, syntactic structure, mutation, etc.) have thus far prevented this theory from translating to the imperative world. BSJ uses the novel approach of differencebased metaprogramming to provide an imperative programming style amenable to the Java community and to enforce that metaprograms are consistent and semantically unambiguous. To make the feasibility of BSJ metaprogramming evident, we have developed a compiler implementation and numerous working code examples.
p1598
aVProgramming idioms, design patterns and application libraries often introduce cumbersome and repetitive boilerplate code to a software system. Language extensions and external DSLs (domain specific languages) are sometimes introduced to reduce the need for boilerplate code, but they also complicate the system by introducing the need for language dialects and interlanguage mediation. To address this, we propose to extend the structural reflective model of the language with object layouts, layout scopes and slots. Based on the new reflective language model we can 1) provide behavioral hooks to object layouts that are triggered when the fields of an object are accessed and 2) simplify the implementation of staterelated language extensions such as stateful traits. By doing this we show how many idiomatic use cases that normally require boilerplate code can be more effectively supported. We present an implementation in Smalltalk, and illustrate its usage through a series of extended examples.
p1599
aVIn previous work, we presented rules for defining overloaded functions that ensure type safety under symmetric multiple dispatch in an objectoriented language with multiple inheritance, and we showed how to check these rules without requiring the entire type hierarchy to be known, thus supporting modularity and extensibility. In this work, we extend these rules to a language that supports parametric polymorphism on both classes and functions. In a multipleinheritance language in which any type may be extended by types in other modules, some overloaded functions that might seem valid are correctly rejected by our rules. We explain how these functions can be permitted in a language that additionally supports an exclusion relation among types, allowing programmers to declare "nominal exclusions" and also implicitly imposing exclusion among different instances of each polymorphic type. We give rules for computing the exclusion relation, deriving many type exclusions from declared and implicit ones. We also show how to check our rules for ensuring the safety of overloaded functions. In particular, we reduce the problem of handling parametric polymorphism to one of determining subtyping relationships among universal and existential types. Our system has been implemented as part of the opensource Fortress compiler.
p1600
aVA practical type system for MLstyle recursive modules should address at least two technical challenges. First, it needs to solve the double vision problem, which refers to an inconsistency between external and internal views of recursive modules. Second, it needs to overcome the tension between practical decidability and expressivity which arises from the potential presence of cyclic type definitions caused by recursion between modules. Although type systems in previous proposals solve the double vision problem and are also decidable, they fail to typecheck common patterns of recursive modules, such as functor fixpoints, that are essential to the expressivity of the module system and the modular development of recursive modules. This paper proposes a novel type system for recursive modules that solves the double vision problem and typechecks common patterns of recursive modules including functor fixpoints. First, we design a type system with a type equivalence based on weak bisimilarity, which does not lend itself to practical implementation in general, but accommodates a broad range of cyclic type definitions. Then, we identify a practically implementable fragment using a type equivalence based on type normalization, which is expressive enough to typecheck typical uses of recursive modules. Our approach is purely syntactic and the definition of the type system is ready for use in an actual implementation.
p1601
aVOne of the main purposes of object initialisation is to establish invariants such as a field being nonnull or an immutable data structure containing specific values. These invariants are then implicitly assumed by the rest of the implementation, for instance, to ensure that a field may be safely dereferenced or that immutable data may be accessed concurrently. Consequently, letting an object escape from its constructor is dangerous; the escaping object might not yet satisfy its invariants, leading to errors in code that relies on them. Nevertheless, preventing objects entirely from escaping from their constructors is too restrictive; it is often useful to call auxiliary methods on the object under initialisation or to pass it to another constructor to set up mutuallyrecursive structures. We present a type system that tracks which objects are fully initialised and which are still under initialisation. The system can be used to prevent objects from escaping, but also to allow safe escaping by making explicit which objects might not yet satisfy their invariants. We designed, formalised and implemented our system as an extension to a nonnull type system, but it is not limited to this application. Our system is conceptually simple and requires little annotation overhead; it is sound and sufficiently expressive for many common programming idioms. Therefore, we believe it to be the first such system suitable for mainstream use.
p1602
aVSoftware construction today often involves the use of large frameworks. The challenge in this type of programming is that objectoriented frameworks tend to grow exceedingly intricate; they spread functionality among numerous classes, and any use of the framework requires knowledge of many interacting components. We present a system named MATCHMAKER that from a simple query synthesizes code that interacts with the framework. The query consists of names of two framework classes, and our system produces code enabling interaction between them. MATCHMAKER relies on a database of dynamic program traces called DELIGHT that uses novel abstractionbased indexing techniques to answer queries about the evolution of heap connectivity in a matter of seconds. The paper evaluates the performance and effectiveness of MATCHMAKER on a number of benchmarks from the Eclipse framework. The paper also presents the results of a user study that showed a 49% average productivity improvement from the use of our tool.
p1603
aVNull dereferences are a bane of programming in languages such as Java. In this paper we propose a sound, demanddriven, interprocedurally contextsensitive dataflow analysis technique to verify a given dereference as safe or potentially unsafe. Our analysis uses an abstract lattice of formulas to find a precondition at the entry of the program such that a nulldereference can occur only if the initial state of the program satisfies this precondition. We use a simplified domain of formulas, abstracting out integer arithmetic, as well as unbounded access paths due to recursive data structures. For the sake of precision we model aliasing relationships explicitly in our abstract lattice, enable strong updates, and use a limited notion of path sensitivity. For the sake of scalability we prune formulas continually as they get propagated, reducing to true conjuncts that are less likely to be useful in validating or invalidating the formula. We have implemented our approach, and present an evaluation of it on a set of ten real Java programs. Our results show that the set of design features we have incorporated enable the analysis to (a) explore long, interprocedural paths to verify each dereference, with (b) reasonable accuracy, and (c) very quick response time per dereference, making it suitable for use in desktop development environments.
p1604
aVThis paper presents F4F (Framework For Frameworks), a system for effective taint analysis of frameworkbased web applications. Most modern web applications utilize one or more web frameworks, which provide useful abstractions for common functionality. Due to extensive use of reflective language constructs in framework implementations, existing static taint analyses are often ineffective when applied to frameworkbased applications. While previous work has included ad hoc support for certain framework constructs, adding support for a large number of frameworks in this manner does not scale from an engineering standpoint. F4F employs an initial analysis pass in which both application code and configuration files are processed to generate a specification of frameworkrelated behaviors. A taint analysis engine can leverage these specifications to perform a much deeper, more precise analysis of frameworkbased applications. Our specification language has only a small number of simple but powerful constructs, easing analysis engine integration. With this architecture, new frameworks can be handled with no changes to the core analysis engine, yielding significant engineering benefits. We implemented specification generators for several web frameworks and added F4F support to a stateoftheart taintanalysis engine. In an experimental evaluation, the taint analysis enhanced with F4F discovered 525 new issues across nine benchmarks, a harmonic mean of 2.10X more issues per benchmark. Furthermore, manual inspection of a subset of the new issues showed that many were exploitable or reflected bad security practice.
p1605
aVTo solve a problem with a dynamic programming algorithm, one must reformulate the problem such that its solution can be formed from solutions to overlapping subproblems. Because overlapping subproblems may not be apparent in the specification, it is desirable to obtain the algorithm directly from the specification. We describe a semiautomatic synthesizer of lineartime dynamic programming algorithms. The programmer supplies a declarative specification of the problem and the operators that might appear in the solution. The synthesizer obtains the algorithm by searching a space of candidate algorithms; internally, the search is implemented with constraint solving. The space of candidate algorithms is defined with a program template reusable across all lineartime dynamic programming algorithms, which we characterize as firstorder recurrences. This paper focuses on how to write the template so that the constraint solving process scales to realworld lineartime dynamic programming algorithms. We show how to reduce the space with (i)~symmetry reduction and (ii)~domain knowledge of dynamic programming algorithms. We have synthesized algorithms for variants of maximal substring matching, an assemblyline optimization, and the extended Euclid algorithm. We have also synthesized a problem outside the class of firstorder recurrences, by composing three instances of the algorithm template.
p1606
aVMATLAB is a popular dynamic programming language used for scientific and numerical programming. As a language, it has evolved from a small scripting language intended as an interactive interface to numerical libraries, to a very popular language supporting many language features and libraries. The overloaded syntax and dynamic nature of the language, plus the somewhat organic addition of language features over the years, makes static analysis of modern MATLAB quite challenging. A fundamental problem in MATLAB is determining the kind of an identifier. Does an identifier refer to a variable, a named function or a prefix? Although this is a trivial problem for most programming languages, it was not clear how to do this properly in MATLAB. Furthermore, there was no simple explanation of kind analysis suitable for MATLAB programmers, nor a publiclyavailable implementation suitable for compiler researchers. This paper explains the required background of MATLAB, clarifies the kind assignment program, and proposes some general guidelines for developing good kind analyses. Based on these foundations we present our design and implementation of a variety of kind analyses, including an approach that matches the intended behaviour of modern MATLAB 7 and two potentially better alternatives. We have implemented all the variations of the kind analysis in McLab, our extensible compiler framework, and we present an empirical evaluation of the various analyses on a large set of benchmark programs.
p1607
aVRefactoring is a popular technique for improving the structure of existing programs while maintaining their behavior. For statically typed programming languages such as Java, a wide variety of refactorings have been described, and tool support for performing refactorings and ensuring their correctness is widely available in modern IDEs. For the JavaScript programming language, however, existing refactoring tools are less mature and often unable to ensure that program behavior is preserved. Refactoring algorithms that have been developed for statically typed languages are not applicable to JavaScript because of its dynamic nature. We propose a framework for specifying and implementing JavaScript refactorings based on pointer analysis. We describe novel refactorings motivated by best practice recommendations for JavaScript programming, and demonstrate how they can be described concisely in terms of queries provided by our framework. Experiments performed with a prototype implementation on a suite of existing applications show that our approach is wellsuited for developing practical refactoring tools for JavaScript.
p1608
aVWe propose a type system to guarantee safe resource deallocation for sharedmemory concurrent programs by extending the previous type system based on fractional ownerships. Here, safe resource deallocation means that memory cells, locks, or threads are not left allocated when a program terminates. Our framework supports (1) fork/join parallelism, (2) synchronization with locks, and (3) dynamically allocated memory cells and locks. The type system is proved to be sound. We also provide a type inference algorithm for the type system and a prototype implementation of the algorithm.
p1609
aVOptimizing compilers map programs in highlevel languages to highperformance target language code. To most programmers, such a compiler constitutes an impenetrable black box whose inner workings are beyond their understanding. Since programmers often must understand the workings of their compilers to achieve their desired performance goals, they typically resort to various forms of reverse engineering, such as examining compiled code or intermediate forms. Instead, optimizing compilers should engage programmers in a dialog. This paper introduces one such possible form of dialog: optimization coaching. An optimization coach watches while a program is compiled, analyzes the results, generates suggestions for enabling further compiler optimization in the source program, and presents a suitable synthesis of its results to the programmer. We present an evaluation based on case studies, which illustrate how an optimization coach can help programmers achieve optimizations resulting in substantial performance improvements.
p1610
aVThis paper describes our multilevel compilation techniques implemented in a tracebased Java JIT compiler (traceJIT). Like existing multilevel compilation for methodbased compilers, we start JIT compilation with a small compilation scope and a low optimization level so the program can start running quickly. Then we identify hot paths with a timerbased sampling profiler, generate long traces that capture the hot paths, and recompile them with a high optimization level to improve the peak performance. A key to high performance is selecting long traces that effectively capture the entire hot paths for upgrade recompilations. To do this, we introduce a new technique to generate a directed graph representing the control flow, a TTgraph, and use the TTgraph in the trace selection engine to efficiently select long traces. We show that our multilevel compilation improves the peak performance of programs by up to 58.5% and 22.2% on average compared to compiling all of the traces only at a low optimization level. Comparing the performance with our multilevel compilation to the performance when compiling all of the traces at a high optimization level, our technique can reduce the startup times of programs by up to 61.1% and 31.3% on average without significant reduction in the peak performance. Our results show that our adaptive multilevel compilation can balance the peak performance and startup time by taking advantage of different optimization levels.
p1611
aVWhenever the need to compile a new dynamically typed language arises, an appealing option is to repurpose an existing statically typed language JustInTime (JIT) compiler (repurposed JIT compiler). Existing repurposed JIT compilers (RJIT compilers), however, have not yet delivered the hopedfor performance boosts. The performance of JVM languages, for instance, often lags behind standard interpreter implementations. Even more customized solutions that extend the internals of a JIT compiler for the target language compete poorly with those designed specifically for dynamically typed languages. Our own Fiorano JIT compiler is an example of this problem. As a stateoftheart, RJIT compiler for Python, the Fiorano JIT compiler outperforms two other RJIT compilers (Unladen Swallow and Jython), but still shows a noticeable performance gap compared to PyPy, today's best performing Python JIT compiler. In this paper, we discuss techniques that have proved effective in the Fiorano JIT compiler as well as limitations of our current implementation. More importantly, this work offers the first indepth look at benefits and limitations of the repurposed JIT compiler approach. We believe the most common pitfall of existing RJIT compilers is not focusing sufficiently on specialization, an abundant optimization opportunity unique to dynamically typed languages. Unfortunately, the lack of specialization cannot be overcome by applying traditional optimizations.
p1612
aVMethod extraction is a common refactoring feature provided by most modern IDEs. It replaces a userselected piece of code with a call to an automatically generated method. We address the problem of automatically inferring contracts (precondition, postcondition) for the extracted method. We require the inferred contract: (a) to be valid for the extracted method (validity); (b) to guard the language and programmer assertions in the body of the extracted method by an opportune precondition (safety); (c) to preserve the proof of correctness of the original code when analyzing the new method separately (completeness); and (d) to be the most general possible (generality). These requirements rule out trivial solutions (e.g., inlining, projection, etc). We propose two theoretical solutions to the problem. The first one is simple and optimal. It is valid, safe, complete and general but unfortunately not effectively computable (except for unrealistic finiteness/decidability hypotheses). The second one is based on an iterative forward/backward method. We show it to be valid, safe, and, under reasonable assumptions, complete and general. We prove that the second solution subsumes the first. All justifications are provided with respect to a new, settheoretic version of Hoare logic (hence without logic), and abstractions of Hoare logic, revisited to avoid surprisingly unsound inference rules. We have implemented the new algorithms on the top of two industrialstrength tools (CCCheck and the Microsoft Roslyn CTP). Our experience shows that the analysis is both fast enough to be used in an interactive environment and precise enough to generate good annotations.
p1613
aVComputation offloading is a promising way to improve the performance as well as reducing the battery power consumption of a smartphone application by executing some parts of the application on a remote server. Supporting such capability is not easy for smartphone application developers due to (1) correctness: some code, e.g., that for GPS, gravity, and other sensors, can run only on the smartphone so that developers have to identify which parts of the application cannot be offloaded; (2) effectiveness: the reduced execution time must be greater than the network delay caused by computation offloading so that developers need to calculate which parts are worth offloading; (3) adaptability: smartphone applications often face changes of user requirements and runtime environments so that developers need to implement the adaptation on offloading. More importantly, considering the large number of today's smartphone applications, solutions applicable for legacy applications will be much more valuable. In this paper, we present a tool, named DPartner, that automatically refactors Android applications to be the ones with computation offloading capability. For a given Android application, DPartner first analyzes its bytecode for discovering the parts worth offloading, then rewrites the bytecode to implement a special program structure supporting ondemand offloading, and finally generates two artifacts to be deployed onto an Android phone and the server, respectively. We evaluated DPartner on three realworld Android applications, demonstrating the reduction of execution time by 46%97% and battery power consumption by 27%83%.
p1614
aVDynamic software updating (DSU) systems allow programs to be updated while running, thereby permitting developers to add features and fix bugs without downtime. This paper introduces Kitsune, a new DSU system for C whose design has three notable features. First, Kitsune's updating mechanism updates the whole program, not individual functions. This mechanism is more flexible than most prior approaches and places no restrictions on data representations or allowed compiler optimizations. Second, Kitsune makes the important aspects of updating explicit in the program text, making the program's semantics easy to understand while minimizing programmer effort. Finally, the programmer can write simple specifications to direct Kitsune to generate code that traverses and transforms oldversion state for use by new code; such state transformation is often necessary, and is significantly more difficult in prior DSU systems. We have used Kitsune to update five popular, opensource, single and multithreaded programs, and find that few program changes are required to use Kitsune, and that it incurs essentially no performance overhead.
p1615
aVDynamic software updating (DSU) systems eliminate costly downtime by dynamically fixing bugs and adding features to executing programs. Given a static code patch, most DSU systems construct runtime code changes automatically. However, a dynamic update must also specify how to change the running program's execution state, e.g., the stack and heap, to make it compatible with the new code. Constructing such state transformations correctly and automatically remains an open problem. This paper presents a solution called Targeted Object Synthesis (TOS). TOS first executes the same tests on the old and new program versions separately, observing the program heap state at a few corresponding points. Given two corresponding heap states, TOS matches objects in the two versions using key fields that uniquely identify objects and correlate old and newversion objects. Given example object pairs, TOS then synthesizes the simplestpossible function that transforms an oldversion object to its newversion counterpart. We show that TOS is effective on updates to four opensource server programs for which it generates nontrivial transformation functions that use conditionals, operate on collections, and fix memory leaks. These transformations help programmers understand their changes and apply dynamic software updates.
p1616
aVWhile there have been many studies of how to schedule applications to take advantage of increasing numbers of cores in modernday multicore processors, few have focused on multithreaded managed language applications which are prevalent from the embedded to the server domain. Managed languages complicate performance studies because they have additional virtual machine threads that collect garbage and dynamically compile, closely interacting with application threads. Further complexity is introduced as modern multicore machines have multiple sockets and dynamic frequency scaling options, broadening opportunities to reduce both power and running time. In this paper, we explore the performance of Java applications, studying how best to map application and virtual machine (JVM) threads to a multicore, multisocket environment. We explore both the cost of separating JVM threads from application threads, and the opportunity to speed up or slow down the clock frequency of isolated threads. We perform experiments with the multithreaded DaCapo benchmarks and pseudojbb2005 running on the Jikes Research Virtual Machine, on a dualsocket, 8core Intel Nehalem machine to reveal several novel, and sometimes counterintuitive, findings. We believe these insights are a first but important step towards understanding and optimizing managed language performance on modern hardware.
p1617
aVWorkstealing is a promising approach for effectively exploiting software parallelism on parallel hardware. A programmer who uses workstealing explicitly identifies potential parallelism and the runtime then schedules work, keeping otherwise idle hardware busy while relieving overloaded hardware of its burden. Prior work has demonstrated that workstealing is very effective in practice. However, workstealing comes with a substantial overhead: as much as 2x to 12x slowdown over orthodox sequential code. In this paper we identify the key sources of overhead in workstealing schedulers and present two significant refinements to their implementation. We evaluate our workstealing designs using a range of benchmarks, four different workstealing implementations, including the popular forkjoin framework, and a range of architectures. On these benchmarks, compared to orthodox sequential Java, our fastest design has an overhead of just 15%. By contrast, forkjoin has a 2.3x overhead and the previous implementation of the system we use has an overhead of 4.1x. These results and our insight into the sources of overhead for workstealing implementations give further hope to an already promising technique for exploiting increasingly available hardware parallelism.
p1618
aVMolecule is a domain specific language library embedded in Scala for easing the creation of scalable and modular concurrent applications on the JVM. Concurrent applications are modeled as parallel process networks that exchange information over mobile and typesafe messaging interfaces. In this paper, we present a concurrent programming environment that combines functional and imperative programming. Using a monad, we structure the sequential or parallel coordination of userlevel threads, without JVM modifications or compiler support. Our mobile channel interfaces expose reusable and parallelizable higherorder functions, as if they were streams in a lazily evaluated functional programming language. The support for graceful termination of entire process networks is simplified by integrating channel poisoning with monadic exceptions and resource control. Our runtime and systemlevel interfaces leverage message batching and a novel flow parallel scheduler to limit expensive context switches in multicore environments. We illustrate the expressiveness and performance benefits on a 24core AMD Opteron machine with three classical examples: a thread ring, a genuine prime sieve and a chameneosredux.
p1619
aVA key challenge for concurrent programming is that sideeffects (memory operations) in one thread can affect the behavior of another thread. In this paper, we present a type system to restrict the updates to memory to prevent these unintended sideeffects. We provide a novel combination of immutable and unique (isolated) types that ensures safe parallelism (race freedom and deterministic execution). The type system includes support for polymorphism over type qualifiers, and can easily create cycles of immutable objects. Key to the system's flexibility is the ability to recover immutable or externally unique references after violating uniqueness without any explicit alias tracking. Our type system models a prototype extension to C# that is in active use by a Microsoft team. We describe their experiences building large systems with this extension. We prove the soundness of the type system by an embedding into a program logic.
p1620
aVIncreasing levels of hardware parallelism are one of the main challenges for programmers and implementers of managed runtimes. Any concurrency or scalability improvements must be evaluated experimentally. However, application benchmarks available today may not reflect the highly concurrent applications we anticipate in the future. They may also behave in ways that VM developers do not expect. We provide a set of platform independent concurrency related metrics and an indepth observational study of current state of the art benchmarks, discovering how concurrent they really are, how they scale the work and how they synchronise and communicate via shared memory.
p1621
aVGenerally applicable techniques for improving temporal locality in irregular programs, which operate over pointerbased data structures such as trees and graphs, are scarce. Focusing on a subset of irregular programs, namely, tree traversal algorithms like BarnesHut and nearest neighbor, previous work has proposed point blocking, a technique analogous to loop tiling in regular programs, to improve locality. However point blocking is highly dependent on point sorting, a technique to reorder points so that consecutive points will have similar traversals. Performing this a priori sort requires an understanding of the semantics of the algorithm and hence highly application specific techniques. In this work, we propose traversal splicing, a new, general, automatic locality optimization for irregular tree traversal codes, that is less sensitive to point order, and hence can deliver substantially better performance, even in the absence of semantic information. For six benchmark algorithms, we show that traversal splicing can deliver singlethread speedups of up to 9.147 (geometric mean: 3.095) over baseline implementations, and up to 4.752 (geometric mean: 2.079) over pointblocked implementations. Further, we show that in many cases, automatically applying traversal splicing to a baseline implementation yields performance that is better than carefully handoptimized implementations.
p1622
aVAlgorithms in new application areas like machine learning and network analysis use "irregular" data structures such as graphs, trees and sets. Writing efficient parallel code in these problem domains is very challenging because it requires the programmer to make many choices: a given problem can usually be solved by several algorithms, each algorithm may have many implementations, and the best choice of algorithm and implementation can depend not only on the characteristics of the parallel platform but also on properties of the input data such as the structure of the graph. One solution is to permit the application programmer to experiment with different algorithms and implementations without writing every variant from scratch. Autotuning to find the best variant is a more ambitious solution. These solutions require a system for automatically producing efficient parallel implementations from highlevel specifications. Elixir, the system described in this paper, is the first step towards this ambitious goal. Application programmers write specifications that consist of an operator, which describes the computations to be performed, and a schedule for performing these computations. Elixir uses sophisticated inference techniques to produce efficient parallel code from such specifications. We used Elixir to automatically generate many parallel implementations for three irregular problems: breadthfirst search, single source shortest path, and betweennesscentrality computation. Our experiments show that the best generated variants can be competitive with handwritten code for these problems from other research groups; for some inputs, they even outperform the handwritten versions.
p1623
aVThis paper describes a very highlevel language for clear description of distributed algorithms and optimizations necessary for generating efficient implementations. The language supports highlevel control flows where complex synchronization conditions can be expressed using highlevel queries, especially logic quantifications, over message history sequences. Unfortunately, the programs would be extremely inefficient, including consuming unbounded memory, if executed straightforwardly. We present new optimizations that automatically transform complex synchronization conditions into incremental updates of necessary auxiliary values as messages are sent and received. The core of the optimizations is the first general method for efficient implementation of logic quantifications. We have developed an operational semantics of the language, implemented a prototype of the compiler and the optimizations, and successfully used the language and implementation on a variety of important distributed algorithms.
p1624
aVThe desired behavior of a program can be described using an abstract model. Compiling such a model into executable code requires advanced compilation techniques known as synthesis. This paper presents an objectbased language, called Jennisys, where programming is done by introducing an abstract model, defining a concrete data representation for the model, and then being aided by automatic synthesis to produce executable code. The paper also presents a synthesis technique for the language. The technique is built on an automatic program verifier that, via an underlying SMT solver, is capable of providing concrete models to failed verifications. The technique proceeds by obtaining sample input/output values from concrete models and then extrapolating programs from the sample points. The synthesis aims to produce code with assignments, branching structure, and possibly recursive calls. It is the first to synthesize code that creates and uses objects in dynamic data structures or aggregate objects. A prototype of the language and synthesis technique has been implemented.
p1625
aVWe present Bolt, a novel system for escaping from infinite and longrunning loops. Directed by a user, Bolt can attach to a running process and determine if the program is executing an infinite loop. If so, Bolt can deploy multiple strategies to escape the loop, restore the responsiveness of the program, and enable the program to deliver useful output. Bolt operates on stripped x86 and x64 binaries, dynamically attaches and detaches to and from the program as needed, and dynamically detects loops and creates program state checkpoints to enable exploration of different escape strategies. Bolt can detect and escape from loops in offtheshelf software, without available source code, and with no overhead in standard production use.
p1626
aVDebugging concurrent programs is known to be difficult due to scheduling nondeterminism. The technique of multiprocessor deterministic replay substantially assists debugging by making the program execution reproducible. However, facing the huge replay traces and long replay time, the debugging task remains stunningly challenging for long running executions. We present a new technique, LEAN, on top of replay, that significantly reduces the complexity of the replay trace and the length of the replay time without losing the determinism in reproducing concurrency bugs. The cornerstone of our work is a redundancy criterion that characterizes the redundant computation in a buggy trace. Based on the redundancy criterion, we have developed two novel techniques to automatically identify and remove redundant threads and instructions in the bug reproduction execution. Our evaluation results with several real world concurrency bugs in large complex server programs demonstrate that LEAN is able to reduce the size, the number of threads, and the number of thread context switches of the replay trace by orders of magnitude, and accordingly greatly shorten the replay time.
p1627
aVWe propose a new algorithm for dynamic datarace detection. Our algorithm reports no false positives and runs on arbitrary C and C++ code. Unlike previous algorithms, we do not have to instrument every memory access or track a full happensbefore relation. Our datarace detector, which we call IFRit, is based on a runtime abstraction called an interferencefree region (IFR). An IFR is an interval of one thread's execution during which any write to a specific variable by a different thread is a data race. We insert instrumentation at compile time to monitor active IFRs at runtime. If the runtime observes overlapping IFRs for conflicting accesses to the same variable in two different threads, it reports a race. The static analysis aggregates information for multiple accesses to the same variable, avoiding the expense of having to instrument every memory access in the program. We directly compare IFRit to FastTrack and ThreadSanitizer, two stateoftheart fullyprecise datarace detectors. We show that IFRit imposes a fraction of the overhead of these detectors. We show that for the PARSEC benchmarks, and several realworld applications, IFRit finds many of the races detected by a fullyprecise detector. We also demonstrate that sampling can further reduce IFRit's performance overhead without completely forfeiting precision.
p1628
aVTesting multithreaded programs is a hard problem, because it is challenging to expose those rare interleavings that can trigger a concurrency bug. We propose a new thread interleaving coveragedriven testing tool called Maple that seeks to expose untested thread interleavings as much as possible. It memoizes tested interleavings and actively seeks to expose untested interleavings for a given test input to increase interleaving coverage. We discuss several solutions to realize the above goal. First, we discuss a coverage metric based on a set of interleaving idioms. Second, we discuss an online technique to predict untested interleavings that can potentially be exposed for a given test input. Finally, the predicted untested interleavings are exposed by actively controlling the thread schedule while executing for the test input. We discuss our experiences in using the tool to expose several known and unknown bugs in realworld applications such as Apache and MySQL.
p1629
aVMATLAB is a dynamic scientific language used by scientists, engineers and students worldwide. Although MATLAB is very suitable for rapid prototyping and development, MATLAB users often want to convert their final MATLAB programs to a static language such as FORTRAN. This paper presents an extensible objectoriented toolkit for supporting the generation of static programs from dynamic MATLAB programs. Our open source toolkit, called the MATLAB Tamer, identifies a large tame subset of MATLAB, supports the generation of a specialized Tame IR for that subset, provides a principled approach to handling the large number of builtin MATLAB functions, and supports an extensible interprocedural value analysis for estimating MATLAB types and call graphs.
p1630
aVSeveral studies have shown that a large fraction of the work performed inside memory transactions in representative programs is wasted due to the transaction experiencing a conflict and aborting. Aborts inside long running transactions are especially influential to performance and the simplicity of the TM programming model (relative to using finegrained locking) in synchronizing large critical sections means that large transactions are common and this exacerbates the problem of wasted work. In this paper we present a practical transaction checkpoint and recovery scheme in which transactions that experience a conflict can restore their state (including the local context in which they were executing) to some dynamic program point before this access and begin execution from that point. This state saving and restoration is implemented by checkpoint operations that are generated by a compiler into the transaction's body and are also optimized to reduce the amount of state that is saved and restored. We also describe a runtime system that manages these checkpointed states and orchestrates the restoration of the right checkpointed state for a conflict on a particular transactional access. Moreover the synthesis of these save and restore operations, their optimization and invocation at runtime are completely transparent to the programmer. We have implemented the checkpoint generation and optimization scheme in the LLVM compiler and runtime support for the TL2 STM system. Our experiments indicate that for many parallel programs using such checkpoint recovery schemes can result in upto several orders of magnitude reduction in number of aborts and significant execution time speedups relative to plain transactional programs for the same number of threads.
p1631
aVThis paper introduces a novel approach to scale symbolic execution   a program analysis technique for systematic exploration of bounded execution paths for test input generation. While the foundations of symbolic execution were developed over three decades ago, recent years have seen a real resurgence of the technique, specifically for systematic bug finding. However, scaling symbolic execution remains a primary technical challenge due to the inherent complexity of the pathbased exploration that lies at core of the technique. Our key insight is that the state of the analysis can be represented highly compactly: a test input is all that is needed to effectively encode the state of a symbolic execution run. We present ranged symbolic execution, which embodies this insight and uses two test inputs to define a range, i.e., the beginning and end, for a symbolic execution run. As an application of our approach, we show how it enables scalability by distributing the path exploration both in a sequential setting with a single worker node and in a parallel setting with multiple workers. As an enabling technology, we leverage the opensource, stateoftheart symbolic execution tool KLEE. Experimental results using 71 programs chosen from the widely deployed GNU Coreutils set of Unix utilities show that our approach provides a significant speedup over KLEE. For example, using 10 worker cores, we achieve an average speedup of 6.6X for the 71 programs.
p1632
aVWe present a new approach to automated reasoning about higherorder programs by extending symbolic execution to use behavioral contracts as symbolic values, thus enabling symbolic approximation of higherorder behavior. Our approach is based on the idea of an abstract reduction semantics that gives an operational semantics to programs with both concrete and symbolic components. Symbolic components are approximated by their contract and our semantics gives an operational interpretation of contractsasvalues. The result is an executable semantics that soundly predicts program behavior, including contract failures, for all possible instantiations of symbolic components. We show that our approach scales to an expressive language of contracts including arbitrary programs embedded as predicates, dependent function contracts, and recursive contracts. Supporting this rich language of specifications leads to powerful symbolic reasoning using existing program constructs. We then apply our approach to produce a verifier for contract correctness of components, including a sound and computable approximation to our semantics that facilitates fully automated contract verification. Our implementation is capable of verifying contracts expressed in existing programs, and of justifying contractelimination optimizations.
p1633
aVThis paper presents a verification framework that is parametric in a (trusted) operational semantics of some programming language. The underlying proof system is languageindependent and consists of eight proof rules. The proof system is proved partially correct and relatively complete (with respect to the programming language configuration model). To show its practicality, the generic framework is instantiated with a fragment of C and evaluated with encouraging results.
p1634
aVScripting languages are widely used to quickly accomplish a variety of tasks because of the high productivity they enable. Among other reasons, this increased productivity results from a combination of extensive libraries, fast development cycle, dynamic typing, and polymorphism. The dynamic features of scripting languages are traditionally associated with interpreters, which is the approach used to implement most scripting languages. Although easy to implement, interpreters are generally slow, which makes scripting languages prohibitive for implementing large, CPUintensive applications. This efficiency problem is particularly important for PHP given that it is the most commonly used language for serverside web development. This paper presents the design, implementation, and an evaluation of the HipHop compiler for PHP. HipHop goes against the standard practice and implements a very dynamic language through static compilation. After describing the most challenging PHP features to support through static compilation, this paper presents HipHop's design and techniques that support almost all PHP features. We then present a thorough evaluation of HipHop running both standard benchmarks and the Facebook web site. Overall, our experiments demonstrate that HipHop is about 5.5x faster than standard, interpreted PHP engines. As a result, HipHop has reduced the number of servers needed to run Facebook and other web sites by a factor between 4 and 6, thus drastically cutting operating costs.
p1635
aVWe present Dependent JavaScript (DJS), a statically typed dialect of the imperative, objectoriented, dynamic language. DJS supports the particularly challenging features such as runtime typetests, higherorder functions, extensible objects, prototype inheritance, and arrays through a combination of nested refinement types, strong updates to the heap, and heap unrolling to precisely track prototype hierarchies. With our implementation of DJS, we demonstrate that the type system is expressive enough to reason about a variety of tricky idioms found in small examples drawn from several sources, including the popular book JavaScript: The Good Parts and the SunSpider benchmark suite.
p1636
aVEval endows JavaScript developers with great power. It allows developers and endusers, by turning text into executable code, to seamlessly extend and customize the behavior of deployed applications as they are running. With great power comes great responsibility, though not in our experience. In previous work we demonstrated through a large corpus study that programmers wield that power in rather irresponsible and arbitrary ways. We showed that most calls to eval fall into a small number of very predictable patterns. We argued that those patterns could easily be recognized by an automated algorithm and that they could almost always be replaced with safer JavaScript idioms. In this paper we set out to validate our claim by designing and implementing a tool, which we call Evalorizer, that can assist programmers in getting rid of their unneeded evals. We use the tool to remove eval from a realworld website and validated our approach over logs taken from the top 100 websites with a success rate over 97% under an open world assumption.
p1637
aVThe JavaScript programming language, originally developed as a simple scripting language, is now the language of choice for web applications. All the top 100 sites on the web use JavaScript and its use outside web pages is rapidly growing. However, JavaScript is not yet ready for programming in the large: it does not support a module system. Lack of namespaces introduces module patterns, and makes it difficult to use multiple JavaScript frameworks together. In this paper, we propose a formal specification of a JavaScript module system. A module system for JavaScript will allow safe and incremental development of JavaScript web applications. While the next version of the JavaScript standard proposes a module system, it informally describes its design in prose. We formally specify a module system as an extension to the existing JavaScript language, and rigorously describe its semantics via desugaring to LambdaJS, a prior core calculus for JavaScript. We implement the desugaring process and show its faithfulness using realworld test suites. Finally, we define a set of properties for valid JavaScript programs using modules and formally prove that the proposed module system satisfies the validity properties.
p1638
aVHumans can perform many tasks with ease that remain difficult or impossible for computers. Crowdsourcing platforms like Amazon's Mechanical Turk make it possible to harness humanbased computational power at an unprecedented scale. However, their utility as a generalpurpose computational platform remains limited. The lack of complete automation makes it difficult to orchestrate complex or interrelated tasks. Scheduling more human workers to reduce latency costs real money, and jobs must be monitored and rescheduled when workers fail to complete their tasks. Furthermore, it is often difficult to predict the length of time and payment that should be budgeted for a given task. Finally, the results of humanbased computations are not necessarily reliable, both because human skills and accuracy vary widely, and because workers have a financial incentive to minimize their effort. This paper introduces AutoMan, the first fully automatic crowdprogramming system. AutoMan integrates humanbased computations into a standard programming language as ordinary function calls, which can be intermixed freely with traditional functions. This abstraction lets AutoMan programmers focus on their programming logic. An AutoMan program specifies a confidence level for the overall computation and a budget. The AutoMan runtime system then transparently manages all details necessary for scheduling, pricing, and quality control. AutoMan automatically schedules human tasks for each computation until it achieves the desired confidence level; monitors, reprices, and restarts human tasks as necessary; and maximizes parallelism across human workers while staying under budget.
p1639
aVIBM's Jazz initiative offers a stateoftheart collaborative development environment (CDE) facilitating developer interactions around interdependent units of work. In this paper, we analyze development data across two versions of a major IBM product developed on the Jazz platform, covering in total 19 months of development activity, including 17,000+ work items and 61,000+ comments made by more than 190 developers in 35 locations. By examining the relation between developer talk and work, we find evidence that developers maintain a reasonably high level of connectivity with peer developers with whom they share work dependencies, but the span of a developer's communication goes much beyond the known dependencies of his/her work items. Using multiple linear regression models, we find that the number of defects owned by a developer is impacted by the number of other developers (s)he is connected through talk, his/her interpersonal influence in the network of work dependencies, the number of work items (s)he comments on, and the number work items (s)he owns. These effects are maintained even after controlling for workload, role, work dependency, and connection related factors. We discuss the implications of our results for collaborative software development and project governance.
p1640
aVModern integrated development environments make recommendations and automate common tasks, such as refactorings, autocompletions, and error corrections. However, these tools present little or no information about the consequences of the recommended changes. For example, a rename refactoring may: modify the source code without changing program semantics; modify the source code and (incorrectly) change program semantics; modify the source code and (incorrectly) create compilation errors; show a name collision warning and require developer input; or show an error and not change the source code. Having to compute the consequences of a recommendation   either mentally or by making source code changes   puts an extra burden on the developers. This paper aims to reduce this burden with a technique that informs developers of the consequences of code transformations. Using Eclipse Quick Fix as a domain, we describe a plugin, Quick Fix Scout, that computes the consequences of Quick Fix recommendations. In our experiments, developers completed compilationerror removal tasks 10% faster when using Quick Fix Scout than Quick Fix, although the sample size was not large enough to show statistical significance.
p1641
aVWe demonstrate that a practical concurrent language can be extended in a natural way with information security mechanisms that provably enforce strong information security guarantees. We extend the X10 concurrent programming language with coarsegrained informationflow control. Central to X10 concurrency abstractions is the notion of a place: a container for data and computation. We associate a security level with each place, and restrict each place to store only data appropriate for that security level. When places interact only with other places at the same security level, then our security mechanisms impose no restrictions. When places of differing security levels interact, our information security analysis prevents potentially dangerous information flows, including information flow through covert scheduling channels. The X10 concurrency mechanisms simplify reasoning about information flow in concurrent programs. We present a static analysis that enforces a noninterferencebased extensional information security condition in a calculus that captures the key aspects of X10's place abstraction and asyncfinish parallelism. We extend this security analysis to support many of X10's language features, and have implemented a prototype compiler for the resulting language.
p1642
aVAbstract Although the study of static and dynamic type systems plays a major role in research, relatively little is known about the impact of type systems on software development. Perhaps one of the more common arguments for static type systems in languages such as Java or C++ is that they require developers to annotate their code with type names, which is thus claimed to improve the documentation of software. In contrast, one common argument against static type systems is that they decrease flexibility, which may make them harder to use. While these arguments are found in the literature, rigorous empirical evidence is lacking. We report on a controlled experiment where 27 subjects performed programming tasks on an undocumented API with a static type system (requiring type annotations) as well as a dynamic type system (which does not). Our results show that for some tasks, programmers had faster completion times using a static type system, while for others, the opposite held. We conduct an exploratory study to try and theorize why.
p1643
aVThe datatriggered threads (DTT) programming and execution model can increase parallelism and eliminate redundant computation. However, the initial proposal requires significant architecture support, which impedes existing applications and architectures from taking advantage of this model. This work proposes a pure software solution that supports the DTT model without any hardware support. This research uses a prototype compiler and runtime libraries running on top of existing machines. Several enhancements to the initial software implementation are presented, which further improve the performance. The software runtime system improves the performance of serial C SPEC benchmarks by 15% on a Nehalem processor, but by over 7X over the full suite of singlethread applications. It is shown that the DTT model can work in conjunction with traditional parallelism. The DTT model provides up to 64X speedup over parallel applications exploiting traditional parallelism.
p1644
aVThis paper presents PoliC, a language extension, runtime library, and system daemon enabling finegrained, languagelevel, hierarchical resource management policies. PoliC is suitable for use in applications that compose parallel libraries, frameworks, and programs. In particular, we have added a powerful new statement to C for expressing resource limits and guarantees in such a way that programmers can set resource management policies even when the source code of parallel libraries and frameworks is not available. PoliC enables application programmers to manage any resource exposed by the underlying OS, for example cores or IO bandwidth. Additionally, we have developed a domainspecific language for defining highlevel resource management policies, and a facility for extending the kinds of resources that can be managed with our language extension. Finally, through a number of useful variations, our design offers a high degree of composability. We evaluate PoliC by way of three casestudies: a scientific application, an image processing webserver, and a pair of parallel database join implementations. We found that using PoliC yields efficiency gains that require the addition of only a few lines of code to applications.
p1645
aVMaking multithreaded execution less nondeterministic is a promising solution to address the difficulty of concurrent programming plagued by the nondeterministic thread scheduling. In fact, a vast category of concurrent programs are scheduleroblivious: their execution is deterministic, regardless of the scheduling behavior. We present and formally prove a fundamental observation of the privatizability property for scheduleroblivious programs, that paves the theoretical foundation for privatizing shared data accesses on a path segment. With privatization, the nondeterministic thread interleavings on the privatized accesses are isolated and as the consequence many concurrency problems are alleviated. We further present a path and context sensitive privatization algorithm that safely privatizes the program without introducing any additional program behavior. Our evaluation results show that the privatization opportunity pervasively exists in real world large complex concurrent systems. Through privatization, several real concurrency bugs are fixed and notable performance improvements are also achieved on benchmarks.
p1646
aVThis paper introduces a unified concurrent programming model combining the previously developed Actor Model (AM) and the taskparallel AsyncFinish Model (AFM). With the advent of multicore computers, there is a renewed interest in programming models that can support a wide range of parallel programming patterns. The proposed unified model shows how the divideandconquer approach of the AFM and the noshared mutable state and eventdriven philosophy of the AM can be combined to solve certain classes of problems more efficiently and productively than either of the aforementioned models individually. The unified model adds actor creation and coordination to the AFM, while also enabling parallelization within actors. This paper describes two implementations of the unified model as extensions of HabaneroJava and HabaneroScala. The unified model adds to the foundations of parallel programs, and to the tools available for the programmer to aid in productivity and performance while developing parallel software.
p1647
aVModule systems enable a divide and conquer strategy to software development. To implement compiletime variability in software product lines, modules can be composed in different combinations. However, this way, variability dictates a dominant decomposition. As an alternative, we introduce a variabilityaware module system that supports compiletime variability inside a module and its interface. So, each module can be considered a product line that can be type checked in isolation. Variability can crosscut multiple modules. The module system breaks with the antimodular tradition of a global variability model in productline development and provides a path toward software ecosystems and product lines of product lines developed in an open fashion. We discuss the design and implementation of such a module system on a core calculus and provide an implementation for C as part of the TypeChef project. Our implementation supports variability inside modules from #ifdef preprocessor directives and variable linking at the composition level. With our implementation, we type check all configurations of all modules of the open source product line Busybox with 811~compiletime options, perform linker check of all configurations, and report found type and linker errors   without resorting to a bruteforce strategy.
p1648
aVDynamic typechecking and objectoriented programming often go handinhand; scripting languages such as Python, Ruby, and JavaScript all embrace objectoriented (OO) programming. When scripts written in such languages grow and evolve into large programs, the lack of a static type discipline reduces maintainability. A programmer may thus wish to migrate parts of such scripts to a sister language with a static type system. Unfortunately, existing type systems neither support the flexible OO composition mechanisms found in scripting languages nor accommodate sound interoperation with untyped code. In this paper, we present the design of a gradual typing system that supports sound interaction between statically and dynamicallytyped units of classbased code. The type system uses row polymorphism for classes and thus supports mixinbased OO composition. To protect migration of mixins from typed to untyped components, the system employs a novel form of contracts that partially seal classes. The design comes with a theorem that guarantees the soundness of the type system even in the presence of untyped components.
p1649
aVModern objectoriented languages such as X10 require a rich framework for types capable of expressing both valuedependency and genericity, and supporting pluggable, domainspecific extensions. In earlier work, we presented a framework for constrained types in objectoriented languages, parametrized by an underlying constraint system. Types are viewed as formulas C{c} where C is the name of a class or an interface and c is a constraint on the immutable instance state (the properties) of C. Constraint systems are a very expressive framework for partial information. Many (value)dependent type systems for objectoriented languages can be viewed as constrained types. This paper extends the constrained types approach to handle typedependency ("genericity"). The key idea is to introduce constrained kinds: in the same way that constraints on values can be used to define constrained types, constraints on types can define constrained kinds. We develop a core programming language with constrained kinds. Generic types are supported by introducing type variables literally, variables with "type" Type and permitting programs to impose subtyping and equality constraints on such variables. We formalize the typechecking rules and establish soundness. While the language now intertwines constraints on types and values, its type system remains parametric in the choice of the value constraint system (language and solver). We demonstrate that constrained kinds are expressive and practical and sketch possible extensions with a discussion of the design and implementation of X10.
p1650
aVThis paper presents a novel type system to promote and facilitate energyaware programming. Energy Types is built upon a key insight into today's energyefficient systems and applications: despite the popular perception that energy and power can only be described in joules and watts, realworld energy management is often based on discrete phases and modes, which in turn can be reasoned about by type systems very effectively. A phase characterizes a distinct pattern of program workload, and a mode represents an energy state the program is expected to execute in. This paper describes a programming model where phases and modes can be intuitively specified by programmers or inferred by the compiler as type information. It demonstrates how a typebased approach to reasoning about phases and modes can help promote energy efficiency. The soundness of our type system and the invariants related to interphase and intermode interactions are rigorously proved. Energy Types is implemented as the core of a prototyped objectoriented language ET for smartphone programming. Preliminary studies show ET can lead to significant energy savings for Android Apps.
p1651
aVPrediction of program dynamic behaviors is fundamental to program optimizations, resource management, and architecture reconfigurations. Most existing predictors are based on locality of program behaviors, subject to some inherent limitations. In this paper, we revisit the design philosophy and systematically explore a second source of clues: statistical correlations between the behavior sequences of different program entities. Concentrated on loops, it examines the correlations' existence, strength, and values in enhancing the design of program behavior predictors. It creates the first taxonomy of program behavior sequence patterns. It develops a new form of predictors, named sequence predictors, to effectively translate the correlations into largescope, proactive predictions of program behavior sequences. It demonstrates the usefulness of the prediction in dynamic version selection and loop importance estimation, showing 19% average speedup on a number of realworld utility applications. By taking scope and timing of behavior prediction as the firstorder design objectives, the new approach overcomes limitations of existing program behavior predictors, opening up many new opportunities for runtime optimizations at various layers of computing.
p1652
aVOur goal is to develop precise and scalable verification techniques for Java programs that use collections and properties that depend on their content. We apply the popular approach of predicate abstraction to Java programs and collections. The main challenge in this context is precise and compact modeling of collections that enables practical verification. We define a predicate language for modeling the observable state of Java collections at the interface level. Changes of the state by API methods are captured by weakest preconditions. We adapt existing techniques for construction of abstract programs. Most notably, we designed optimizations based on specific features of the predicate language. We evaluated our approach on Java programs that use collections in advanced ways. Our results show that interesting properties, such as consistency between multiple collections, can be verified using our approach. The properties are specified using logic formulas that involve predicates introduced by our language.
p1653
aVCalling context trees are one of the most fundamental data structures for representing the interprocedural control flow of a program, providing valuable information for program understanding and optimization. Nodes of a calling context tree associate performance metrics to whole distinct paths in the call graph starting from the root function. However, no explicit information is provided for detecting short hot sequences of activations, which may be a better optimization target in large modular programs where groups of related functions are reused in many different parts of the code. Furthermore, calling context trees can grow prohibitively large in some scenarios. Another classical approach, called edge profiling, collects performance metrics for callercallee pairs in the call graph, allowing it to detect hot paths of fixed length one. We study a generalization of edge and contextsensitive profiles by introducing a novel data structure called kcalling context forest (kCCF). Nodes in a kCCF associate performance metrics to paths of length at most k that lead to each distinct routine of the program, providing edge profiles for k=1, full contextsensitive profiles for k equal to infinity, as well as any other intermediate point in the spectrum. We study the properties of the kCCF both theoretically and experimentally on a large suite of prominent Linux applications, showing how to construct it efficiently and discussing its relationships with the calling context tree. Our experiments show that the kCCF can provide effective spaceaccuracy tradeoffs for interprocedural contextual profiling, yielding useful clues to the hot spots of a program that may be hidden in a calling context tree and using less space for small values of k, which appear to be the most interesting in practice.
p1654
aVReference immutability ensures that a reference is not used to modify the referenced object, and enables the safe sharing of object structures. A pure method does not cause sideeffects on the objects that existed in the prestate of the method execution. Checking and inference of reference immutability and method purity enables a variety of program analyses and optimizations. We present ReIm, a type system for reference immutability, and ReImInfer, a corresponding type inference analysis. The type system is concise and contextsensitive. The type inference analysis is precise and scalable, and requires no manual annotations. In addition, we present a novel application of the reference immutability type system: method purity inference. To support our theoretical results, we implemented the type system and the type inference analysis for Java. We include a type checker to verify the correctness of the inference result. Empirical results on Java applications and libraries of up to 348kLOC show that our approach achieves both scalability and precision.
p1655
aVSampling is a very important and lowcost approach to uncertain data processing, in which output variations caused by input errors are sampled. Traditional methods tend to treat a program as a blackbox. In this paper, we show that through program analysis, we can expose the internals of sample executions so that the process can become more selective and focused. In particular, we develop a sampling runtime that can selectively sample in input error bounds to expose discontinuity in output functions. It identifies all the program factors that can potentially lead to discontinuity and hash the values of such factors during execution in a costeffective way. The hash values are used to guide the sampling process. Our results show that the technique is very effective for realworld programs. It can achieve the precision of a high sampling rate with the cost of a lower sampling rate.
p1656
aVTesting the components of a distributed system is challenging as it requires consideration of not just the state of a component, but also the sequence of messages it may receive from the rest of the system or the environment. Such messages may vary in type and content, and more particularly, in the frequency at which they are generated. All of these factors, in the right combination, may lead to faulty behavior. In this paper we present an approach to address these challenges by systematically analyzing a component in a distributed system to identify specific message sequences and frequencies at which a failure can occur. At the core of the analysis is the generation of a test driver that defines the space of message sequences to be generated, the exploration of that space through the use of dynamic symbolic execution, and the timing and analysis of the generated tests to identify problematic frequencies. We implemented our approach in the context of the popular Robotic Operating System and investigated its application to three systems of increasing complexity.
p1657
aVSome bugs, among the millions that exist, are similar to each other. One bugfixing tactic is to search for similar bugs that have been reported and resolved in the past. A fix for a similar bug can help a developer understand a bug, or even directly fix it. Studying bugs with similar symptoms, programmers may determine how to detect or resolve them. To speed debugging, we advocate the systematic capture and reuse of debugging knowledge, much of which is currently wasted. The core challenge here is how to search for similar bugs. To tackle this problem, we exploit semantic bug information in the form of execution traces, which precisely capture bug semantics. This paper introduces novel tool and language support for semantically querying and analyzing bugs. We describe OSCILLOSCOPE, an Eclipse plugin, that uses a bug trace to exhaustively search its database for similar bugs and return their bug reports. OSCILLOSCOPE displays the traces of the bugs it returns against the trace of the target bug, so a developer can visually examine the quality of the matches. OSCILLOSCOPE rests on our bug query language (BQL), a flexible query language over traces. To realize OSCILLOSCOPE, we developed an open infrastructure that consists of a trace collection engine, BQL, a Hadoopbased query engine for BQL, a traceindexed bug database, as well as a webbased frontend. OSCILLOSCOPE records and uploads bug traces to its infrastructure; it does so automatically when a JUnit test fails. We evaluated OSCILLOSCOPE on bugs collected from popular opensource projects. We show that OSCILLOSCOPE accurately and efficiently finds similar bugs, some of which could have been immediately used to fix open bugs.
p1658
aVChaperones and impersonators provide runtime support for interposing on primitive operations such as function calls, array access and update, and structure field access and update. Unlike most interposition support, chaperones and impersonators are restricted so that they constrain the behavior of the interposing code to reasonable interposition, which in practice preserves the abstraction mechanisms and reasoning that programmers and compiler analyses rely on. Chaperones and impersonators are particularly useful for implementing contracts, and our implementation in Racket allows us to improve both the expressiveness and the performance of Racket's contract system. Specifically, contracts on mutable data can be enforced without changing the API to that data; contracts on large data structures can be checked lazily on only the accessed parts of the structure; contracts on objects and classes can be implemented with lower overhead; and contract wrappers can preserve object equality where appropriate. With this extension, gradual typing systems, such as Typed Racket, that rely on contracts for interoperation with untyped code can now pass mutable values safely between typed and untyped modules.
p1659
aVSelecting operations based on the runtime type of an object is key to many objectoriented and functional programming techniques. We present a technique for implementing open and efficient type switching on hierarchical extensible data types. The technique is general and copes well with C++ multiple inheritance. To simplify experimentation and gain realistic performance using productionquality compilers and tool chains, we implement a type switch construct as an ISO C++11 library, called Mach7. This libraryonly implementation provides concise notation and outperforms the visitor design pattern, commonly used for case analysis on types in objectoriented programming. For closed sets of types, its performance roughly equals equivalent code in functional languages, such as OCaml and Haskell. The typeswitching code is easier to use and is more expressive than handcoded visitors are. The library is nonintrusive and circumvents most of the extensibility restrictions typical of the visitor design pattern. It was motivated by applications involving large, typed, abstract syntax trees.
p1660
aVApplications that combine general program logic with persistent databases (e.g., threetier applications) often suffer large performance penalties from poor use of the database. We introduce a program analysis technique that combines information flow in the program with commutativity analysis of its database operations to produce a unified dependency graph for database statements, which provides programmers with a highlevel view of how costly database operations are and how they are connected in the program. As an example application of our analysis we describe three optimizations that can be discovered by examining the structure of the dependency graph; each helps remove communication latency from the critical path of a multitier system. We implement our technique in a tool for Java applications using JDBC and experimentally validate it using the multitier component of the Dacapo benchmark.
p1661
aVWe present a novel code search approach for answering queries focused on APIusage with code showing how the API should be used. To construct a search index, we develop new techniques for statically mining and consolidating temporal API specifications from code snippets. In contrast to existing semanticbased techniques, our approach handles partial programs in the form of code snippets. Handling snippets allows us to consume code from various sources such as parts of open source projects, educational resources (e.g. tutorials), and expert code sites. To handle code snippets, our approach (i) extracts a possibly partial temporal specification from each snippet using a relatively precise static analysis tracking a generalized notion of typestate, and (ii) consolidates the partial temporal specifications, combining consistent partial information to yield consolidated temporal specifications, each of which captures a full(er) usage scenario. To answer a search query, we define a notion of relaxed inclusion matching a query against temporal specifications and their corresponding code snippets. We have implemented our approach in a tool called PRIME and applied it to search for API usage of several challenging APIs. PRIME was able to analyze and consolidate thousands of snippets per tested API, and our results indicate that the combination of a relatively precise analysis and consolidation allowed PRIME to answer challenging queries effectively.
p1662
aVFormally verifying a program requires significant skill not only because of complex interactions between program subcomponents, but also because of deficiencies in current verification interfaces. These skill barriers make verification economically unattractive by preventing the use of lessskilled (lessexpensive) workers and distributed workflows (i.e., crowdsourcing). This paper presents VeriWeb, a webbased IDE for verification that decomposes the task of writing verifiable specifications into manageable subproblems. To overcome the information loss caused by task decomposition, and to reduce the skill required to verify a program, VeriWeb incorporates several innovative user interface features: drag and drop condition construction, concrete counterexamples, and specification inlining. To evaluate VeriWeb, we performed three experiments. First, we show that VeriWeb lowers the time and monetary cost of verification by performing a comparative study of VeriWeb and a traditional tool using 14 paid subjects contracted hourly from Exhedra Solution's vWorker online marketplace. Second, we demonstrate the dearth and insufficiency of current adhoc labor marketplaces for verification by recruiting workers from Amazon's Mechanical Turk to perform verification with VeriWeb. Finally, we characterize the minimal communication overhead incurred when VeriWeb is used collaboratively by observing two pairs of developers each use the tool simultaneously to verify a single program.
p1663
aVWe present a technique for verifying race and divergencefreedom of GPU kernels that are written in mainstream kernel programming languages such as OpenCL and CUDA. Our approach is founded on a novel formal operational semantics for GPU programming termed synchronous, delayed visibility (SDV) semantics. The SDV semantics provides a precise definition of barrier divergence in GPU kernels and allows kernel verification to be reduced to analysis of a sequential program, thereby completely avoiding the need to reason about thread interleavings, and allowing existing modular techniques for program verification to be leveraged. We describe an efficient encoding for data race detection and propose a method for automatically inferring loop invariants required for verification. We have implemented these techniques as a practical verification tool, GPUVerify, which can be applied directly to OpenCL and CUDA source code. We evaluate GPUVerify with respect to a set of 163 kernels drawn from public and commercial sources. Our evaluation demonstrates that GPUVerify is capable of efficient, automatic verification of a large number of realworld kernels.
p1664
aVWe study the problem of suggesting code repairs at design time, based on the warnings issued by modular program verifiers. We introduce the concept of a verified repair, a change to a program's source that removes bad execution traces while increasing the number of good traces, where the bad/good traces form a partition of all the traces of a program. Repairs are propertyspecific. We demonstrate our framework in the context of warnings produced by the modular cccheck (a.k.a. Clousot) abstract interpreter, and generate repairs for missing contracts, incorrect locals and objects initialization, wrong conditionals, buffer overruns, arithmetic overflow and incorrect floating point comparisons. We report our experience with automatically generating repairs for the .NET framework libraries, generating verified repairs for over 80% of the warnings generated by cccheck.
p1665
aVToday's compilers have a plethora of optimizations to choose from, and the correct choice of optimizations can have a significant impact on the performance of the code being optimized. Furthermore, choosing the correct order in which to apply those optimizations has been a long standing problem in compilation research. Each of these optimizations interacts with the code and in turn with all other optimizations in complicated ways. Traditional compilers typically apply the same set of optimization in a fixed order to all functions in a program, without regard the code being optimized. Understanding the interactions of optimizations is very important in determining a good solution to the phaseordering problem. This paper develops a new approach that automatically selects good optimization orderings on a per method basis within a dynamic compiler. Our approach formulates the phaseordering problem as a Markov process and uses a characterization of the current state of the code being optimized to creating a better solution to the phase ordering problem. Our technique uses neuroevolution to construct an artificial neural network that is capable of predicting beneficial optimization ordering for a piece of code that is being optimized. We implemented our technique in Jikes RVM and achieved significant improvements on a set of standard Java benchmarks over a wellengineered fixed order.
p1666
aVSome programming languages become widely popular while others fail to grow beyond their niche or disappear altogether. This paper uses survey methodology to identify the factors that lead to language adoption. We analyze large datasets, including over 200,000 SourceForge projects, 590,000 projects tracked by Ohloh, and multiple surveys of 1,00013,000 programmers. We report several prominent findings. First, language adoption follows a power law; a small number of languages account for most language use, but the programming market supports many languages with niche user bases. Second, intrinsic features have only secondary importance in adoption. Open source libraries, existing code, and experience strongly influence developers when selecting a language for a project. Language features such as performance, reliability, and simple semantics do not. Third, developers will steadily learn and forget languages. The overall number of languages developers are familiar with is independent of age. Finally, when considering intrinsic aspects of languages, developers prioritize expressivity over correctness. They perceive static types as primarily helping with the latter, hence partly explaining the popularity of dynamic languages.
p1667
aVDynamically typed language implementations often use more memory and execute slower than their statically typed cousins, in part because operations on collections of elements are unoptimised. This paper describes storage strategies, which dynamically optimise collections whose elements are instances of the same primitive type. We implement storage strategies in the PyPy virtual machine, giving a performance increase of 18% on wideranging benchmarks of real Python programs. We show that storage strategies are simple to implement, needing only 1500LoC in PyPy, and have applicability to a wide range of virtual machines.
p1668
aVAs more applications migrate to the cloud, and as "big data" edges into even more production environments, the performance and simplicity of exchanging data between compute nodes/devices is increasing in importance. An issue central to distributed programming, yet often underconsidered, is serialization or pickling, i.e., persisting runtime objects by converting them into a binary or text representation. Pickler combinators are a popular approach from functional programming; their composability alleviates some of the tedium of writing pickling code by hand, but they don't translate well to objectoriented programming due to qualities like open class hierarchies and subtyping polymorphism. Furthermore, both functional pickler combinators and popular, Javabased serialization frameworks tend to be tied to a specific pickle format, leaving programmers with no choice of how their data is persisted. In this paper, we present objectoriented pickler combinators and a framework for generating them at compiletime, called scala/pickling, designed to be the default serialization mechanism of the Scala programming language. The static generation of OO picklers enables significant performance improvements, outperforming Java and Kryo in most of our benchmarks. In addition to high performance and the need for little to no boilerplate, our framework is extensible: using the type class pattern, users can provide both (1) custom, easily interchangeable pickle formats and (2) custom picklers, to override the default behavior of the pickling framework. In benchmarks, we compare scala/pickling with other popular industrial frameworks, and present results on time, memory usage, and size when pickling/unpickling a number of data types used in realworld, largescale distributed applications and frameworks.
p1669
aVDebugging and analyzing a snapshot of a crashed program's memory is far more difficult than working with a live program, because debuggers can no longer execute code to help make sense of the program state. We present an architecture that supports the restricted execution of ordinary code starting from the snapshot, as if the dead objects within it had been restored, but without access to their original external environment. We demonstrate the feasibility of this approach via an implementation for Java that does not require a custom virtual machine, show that it performs competitively with live execution, and use it to diagnose an unresolved memory leak in a mature mainstream application.
p1670
aVWe present a smallstep operational semantics for the Python programming language. We present both a core language for Python, suitable for tools and proofs, and a translation process for converting Python source to this core. We have tested the composition of translation and evaluation of the core for conformance with the primary Python implementation, thereby giving confidence in the fidelity of the semantics. We briefly report on the engineering of these components. Finally, we examine subtle aspects of the language, identifying scope as a pervasive concern that even impacts features that might be considered orthogonal.
p1671
aVWe propose DelphJ: a Javabased OO language that eschews inheritance completely, in favor of a combination of class morphing and (deep) delegation. Compared to past delegation approaches, the novel aspect of our design is the ability to emulate the best aspects of inheritance while retaining maximum flexibility: using morphing, a class can select any of the methods of its delegatee and export them (if desired) or transform them (e.g., to add extra arguments or modify type signatures), yet without needing to name these methods explicitly and handle them onebyone. Compared to past work on morphing, our approach adopts and adapts advanced delegation mechanisms, in order to add late binding capabilities and, thus, provide a full substitute of inheritance. Additionally, we explore complex semantic issues in the interaction of delegation with late binding. We present our language design both informally, with numerous examples, and formally in a core calculus.
p1672
aVWe present setbased preanalysis: a virtually universal optimization technique for flowinsensitive pointsto analysis. Pointsto analysis computes a static abstraction of how object values flow through a program's variables. Setbased preanalysis relies on the observation that much of this reasoning can take place at the set level rather than the value level. Computing constraints at the set level results in significant optimization opportunities: we can rewrite the input program into a simplified form with the same essential pointsto properties. This rewrite results in removing both local variables and instructions, thus simplifying the subsequent valuebased pointsto computation. Effectively, setbased preanalysis puts the program in a normal form optimized for pointsto analysis.  Compared to other techniques for offline optimization of pointsto analyses in the literature, the new elements of our approach are the ability to eliminate statements, and not just variables, as well as its modularity: setbased preanalysis can be performed on the input just once, e.g., allowing the preoptimization of libraries that are subsequently reused many times and for different analyses. In experiments with Java programs, setbased preanalysis eliminates 30% of the program's local variables and 30% or more of computed contextsensitive pointsto facts, over a wide set of benchmarks and analyses, resulting in a ~20% average speedup (max: 110%, median: 18%).
p1673
aVIn a common use case for cloud computing, clients upload data and computation to servers that are managed by a thirdparty infrastructure provider. We describe MrCrypt, a system that provides data confidentiality in this setting by executing client computations on encrypted data. MrCrypt statically analyzes a program to identify the set of operations on each input data column, in order to select an appropriate homomorphic encryption scheme for that column, and then transforms the program to operate over encrypted data. The encrypted data and transformed program are uploaded to the server and executed as usual, and the result of the computation is decrypted on the client side. We have implemented MrCrypt for Java and illustrate its practicality on three standard benchmark suites for the Hadoop MapReduce framework. We have also formalized the approach and proven several soundness and security guarantees.
p1674
aVThe C++ programming language remains widely used, despite inheriting many unsafe features from C features that often lead to failures of type or memory safety that manifest as buffer overflows, useafterfree vulnerabilities, or abstraction violations. Malicious attackers can exploit such violations to compromise application and system security. This paper introduces Ironclad C++, an approach to bringing the benefits of type and memory safety to C++. Ironclad C++ is, in essence, a libraryaugmented, typesafe subset of C++. All Ironclad C++ programs are valid C++ programs that can be compiled using standard, offtheshelf C++ compilers. However, not all valid C++ programs are valid Ironclad C++ programs: a syntactic sourcecode validator statically prevents the use of unsafe C++ features. To enforce safety properties that are difficult to check statically, Ironclad C++ applies dynamic checks via templated ``smart pointer'' classes. Using a semiautomatic refactoring tool, we have ported nearly 50K lines of code to Ironclad C++. These benchmarks incur a performance overhead of 12% on average, compared to the original unsafe C++ code.
p1675
aVProviding security guarantees for systems built out of untrusted components requires the ability to define and enforce access control policies over untrusted code. In Web 2.0 applications, JavaScript code from different origins is often combined on a single page, leading to wellknown vulnerabilities. We present a security infrastructure which allows users and content providers to specify access control policies over subsets of a JavaScript program by leveraging the concept of delimited histories with revocation. We implement our proposal in WebKit and evaluate it with three policies on 50 widely used websites with no changes to their JavaScript code and report performance overheads and violations.
p1676
aVModern IDEs support automated refactoring for many programming languages, but support for JavaScript is still primitive. To perform renaming, which is one of the fundamental refactorings, there is often no practical alternative to simple syntactic searchandreplace. Although more sophisticated alternatives have been developed, they are limited by wholeprogram assumptions and poor scalability. We propose a technique for semiautomatic refactoring for JavaScript, with a focus on renaming. Unlike traditional refactoring algorithms, semiautomatic refactoring works by a combination of static analysis and interaction with the programmer. With this pragmatic approach, we can provide scalable and effective refactoring support for realworld code, including libraries and incomplete applications. Through a series of experiments that estimate how much manual effort our technique demands from the programmer, we show that our approach is a useful improvement compared to searchandreplace tools.
p1677
aVSymbolic execution is a promising testing and analysis methodology. It systematically explores a program's execution space and can generate test cases with high coverage. One significant practical challenge for symbolic execution is how to effectively explore the enormous number of program paths in realworld programs. Various heuristics have been proposed for guiding symbolic execution, but they are generally inefficient and adhoc. In this paper, we introduce a novel, unified strategy to guide symbolic execution to less explored parts of a program. Our key idea is to exploit a specific type of path spectra, namely the lengthn subpath program spectra, to systematically approximate full path information for guiding path exploration. In particular, we use frequency distributions of explored lengthn subpaths to prioritize "less traveled" parts of the program to improve test coverage and error detection. We have implemented our general strategy in KLEE, a stateoftheart symbolic execution engine. Evaluation results on the GNU Coreutils programs show that (1) varying the length n captures programspecific information and exhibits different degrees of effectiveness, and (2) our general approach outperforms traditional strategies in both coverage and error detection.
p1678
aVRefactoring has become an integral part of modern software development, with wide support in popular integrated development environments (IDEs). Modern IDEs provide a fixed set of supported refactorings, listed in a refactoring menu. But with IDEs supporting more and more refactorings, it is becoming increasingly difficult for programmers to discover and memorize all their names and meanings. Also, since the set of refactorings is hardcoded, if a programmer wants to achieve a slightly different code transformation, she has to either apply a (possibly nonobvious) sequence of several builtin refactorings, or just perform the transformation by hand. We propose a novel approach to refactoring, based on synthesis from examples, which addresses these limitations. With our system, the programmer need not worry how to invoke individual refactorings or the order in which to apply them. Instead, a transformation is achieved via three simple steps: the programmer first indicates the start of a code refactoring phase; then she performs some of the desired code changes manually; and finally, she asks the tool to complete the refactoring. Our system completes the refactoring by first extracting the difference between the starting program and the modified version, and then synthesizing a sequence of refactorings that achieves (at least) the desired changes. To enable scalable synthesis, we introduce local refactorings, which allow for first discovering a refactoring sequence on small program fragments and then extrapolating it to a full refactoring sequence. We implemented our approach as an Eclipse plugin, with an architecture that is easily extendable with new refactorings. The experimental results are encouraging: with only minimal user input, the synthesizer was able to quickly discover complex refactoring sequences for several challenging realistic examples.
p1679
aVUnderstanding and analyzing multithreaded program performance and scalability is far from trivial, which severely complicates parallel software development and optimization. In this paper, we present bottle graphs, a powerful analysis tool that visualizes multithreaded program performance, in regards to both perthread parallelism and execution time. Each thread is represented as a box, with its height equal to the share of that thread in the total program execution time, its width equal to its parallelism, and its area equal to its total running time. The boxes of all threads are stacked upon each other, leading to a stack with height equal to the total program execution time. Bottle graphs show exactly how scalable each thread is, and thus guide optimization towards those threads that have a smaller parallel component (narrower), and a larger share of the total execution time (taller), i.e. to the 'neck' of the bottle. Using lightweight OS modules, we calculate bottle graphs for unmodified multithreaded programs running on real processors with an average overhead of 0.68%. To demonstrate their utility, we do an extensive analysis of 12 Java benchmarks running on top of the Jikes JVM, which introduces many JVM service threads. We not only reveal and explain scalability limitations of several wellknown Java benchmarks; we also analyze the reasons why the garbage collector itself does not scale, and in fact performs optimally with two collector threads for all benchmarks, regardless of the number of application threads. Finally, we compare the scalability of Jikes versus the OpenJDK JVM. We demonstrate how useful and intuitive bottle graphs are as a tool to analyze scalability and help optimize multithreaded applications.
p1680
aVIdentifying the hottest paths in the control flow graph of a routine can direct optimizations to portions of the code where most resources are consumed. This powerful methodology, called path profiling, was introduced by Ball and Larus in the mid 90's [4] and has received considerable attention in the last 15 years for its practical relevance. A shortcoming of the BallLarus technique was the inability to profile cyclic paths, making it difficult to mine execution patterns that span multiple loop iterations. Previous results, based on rather complex algorithms, have attempted to circumvent this limitation at the price of significant performance losses even for a small number of iterations. In this paper, we present a new approach to multiiteration path profiling, based on data structures built on top of the original BallLarus numbering technique. Our approach allows the profiling of all executed paths obtained as a concatenation of up to k BallLarus acyclic paths, where k is a userdefined parameter. We provide examples showing that this method can reveal optimization opportunities that acyclicpath profiling would miss. An extensive experimental investigation on a large variety of Java benchmarks on the Jikes RVM shows that our approach can be even faster than BallLarus due to fewer operations on smaller hash tables, producing compact representations of cyclic paths even for large values of k.
p1681
aVWe present a data driven algorithm for equivalence checking of two loops. The algorithm infers simulation relations using data from test runs. Once a candidate simulation relation has been obtained, offtheshelf SMT solvers are used to check whether the simulation relation actually holds. The algorithm is sound: insufficient data will cause the proof to fail. We demonstrate a prototype implementation, called DDEC, of our algorithm, which is the first sound equivalence checker for loops written in x86 assembly.
p1682
aVWe describe techniques for synthesis and verification of recursive functional programs over unbounded domains. Our techniques build on top of an algorithm for satisfiability modulo recursive functions, a framework for deductive synthesis, and complete synthesis procedures for algebraic data types. We present new counterexampleguided algorithms for constructing verified programs. We have implemented these algorithms in an integrated environment for interactive verification and synthesis from relational specifications. Our system was able to synthesize a number of useful recursive functions that manipulate unbounded numbers and data structures.
p1683
aVFormal program verification offers strong assurance of correctness, backed by the strength of mathematical proof. Constructing these proofs requires humans to identify program invariants, and show that they are always maintained. These invariants are then used to prove that the code adheres to its specification. In this paper, we explore the overlap between formal verification and code optimization. We propose two approaches to reuse the invariants derived in formal proofs and integrate them into compilation. The first applies invariants extracted from the proof, while the second leverages the property of program safety (i.e., the absence of bugs). We reuse this information to improve the performance of generated object code. We evaluated these methods on seL4, a realworld formallyverified microkernel, and obtained improvements in average runtime performance (up to 28%) and in worstcase execution time (up to 25%). In macrobenchmarks, we found the performance of paravirtualized Linux running on the microkernel improved by 616%.
p1684
aVThis paper presents a new method for generating inductive loop invariants that are expressible as boolean combinations of linear integer constraints. The key idea underlying our technique is to perform a backtracking search that combines Hoarestyle verification condition generation with a logical abduction procedure based on quantifier elimination to speculate candidate invariants. Starting with true, our method iteratively strengthens loop invariants until they are inductive and strong enough to verify the program. A key feature of our technique is that it is lazy: It only infers those invariants that are necessary for verifying program correctness. Furthermore, our technique can infer arbitrary boolean combinations (including disjunctions) of linear invariants. We have implemented the proposed approach in a tool called HOLA. Our experiments demonstrate that HOLA can infer interesting invariants that are beyond the reach of existing stateoftheart invariant generation tools.
p1685
aVType systems that permit developers to express themselves more precisely are one of the primary topics in programming language research, as well as in industrial software development. While it seems plausible that an expressive static type system increases developer productivity, there is little empirical evidence for or against this hypothesis. Generic types in Java are an example: as an extension of Java's original type system, some claim that Java 1.5 improves the type system's "expressiveness." Even if this claim is true, there exists little empirical evidence that claimed expressiveness leads to a measurable increase in developer productivity. This paper introduces an experiment where generic types (in comparison to raw types) have been evaluated in three different directions: (1) the documentation impact on undocumented APIs, (2) the time required for fixing type errors, and (3) the extensibility of a generic type hierarchy. The results of the experiment suggest that generic types improve documentation and reduce extensibility   without revealing a difference in the time required for fixing type errors.
p1686
aVMany languages support behavioral software contracts so that programmers can describe a component's obligations and promises via logical assertions in its interface. The contract system monitors program execution, checks whether the assertions hold, and, if not, blames the guilty component. Pinning down the violator gets the debugging process started in the right direction. Quality contracts impose a serious runtime cost, however, and programmers therefore compromise in many ways. Some turn off contracts for deployment, but then contracts and code quickly get out of sync during maintenance. Others test contracts randomly or probabilistically. In all cases, programmers have to cope with lack of blame information when the program eventually fails. In response, we propose option contracts as an addition to the contract tool box. Our key insight is that in ordinary contract systems, server components impose their contract on client components, giving them no choice whether to trust the server's promises or check them. With option contracts, server components may choose to tag a contract as an option and clients may choose to exercise the option or accept it, in which case they also shoulder some responsibility. We show that option contracts permit programmers to specify flexible checking policies, that their cost is reasonable, and that they satisfy a complete monitoring theorem.
p1687
aVApplications written for distributedmemory parallel architectures must partition their data to enable parallel execution. As memory hierarchies become deeper, it is increasingly necessary that the data partitioning also be hierarchical to match. Current language proposals perform this hierarchical partitioning statically, which excludes many important applications where the appropriate partitioning is itself data dependent and so must be computed dynamically. We describe Legion, a regionbased programming system, where each region may be partitioned into subregions. Partitions are computed dynamically and are fully programmable. The division of data need not be disjoint and subregions of a region may overlap, or alias one another. Computations use regions with certain privileges (e.g., expressing that a computation uses a region readonly) and data coherence (e.g., expressing that the computation need only be atomic with respect to other operations on the region), which can be controlled on a perregion (or subregion) basis. We present the novel aspects of the Legion design, in particular the combination of static and dynamic checks used to enforce soundness. We give an extended example illustrating how Legion can express computations with dynamically determined relationships between computations and data partitions. We prove the soundness of Legion's type system, and show Legion type checking improves performance by up to 71% by eliding provably safe memory checks. In particular, we show that the dynamic checks to detect aliasing at runtime at the region granularity have negligible overhead. We report results for three realworld applications running on distributed memory machines, achieving up to 62.5X speedup on 96 GPUs on the Keeneland supercomputer.
p1688
aVEmerging highperformance architectures are anticipated to contain unreliable components that may exhibit soft errors, which silently corrupt the results of computations. Full detection and masking of soft errors is challenging, expensive, and, for some applications, unnecessary. For example, approximate computing applications (such as multimedia processing, machine learning, and big data analytics) can often naturally tolerate soft errors. We present Rely a programming language that enables developers to reason about the quantitative reliability of an application   namely, the probability that it produces the correct result when executed on unreliable hardware. Rely allows developers to specify the reliability requirements for each value that a function produces. We present a static quantitative reliability analysis that verifies quantitative requirements on the reliability of an application, enabling a developer to perform sound and verified reliability engineering. The analysis takes a Rely program with a reliability specification and a hardware specification that characterizes the reliability of the underlying hardware components and verifies that the program satisfies its reliability specification when executed on the underlying unreliable hardware platform. We demonstrate the application of quantitative reliability analysis on six computations implemented in Rely.
p1689
aVWe present the problem of class hierarchy complementation: given a partially known hierarchy of classes together with subtyping constraints ("A has to be a transitive subtype of B") complete the hierarchy so that it satisfies all constraints. The problem has immediate practical application to the analysis of partial programs e.g., it arises in the process of providing a sound handling of "phantom classes" in the Soot program analysis framework. We provide algorithms to solve the hierarchy complementation problem in the single inheritance and multiple inheritance settings. We also show that the problem in a language such as Java, with single inheritance but multiple subtyping and distinguished class vs. interface types, can be decomposed into separate single and multiplesubtyping instances. We implement our algorithms in a tool, JPhantom, which complements partial Java bytecode programs so that the result is guaranteed to satisfy the Java verifier requirements. JPhantom is highly scalable and runs in mere seconds even for large input applications and complex constraints (with a maximum of 14s for a 19MB binary).
p1690
aVAlgorithmic speculation or highlevel speculation is a promising programming paradigm which allows programmers to speculatively branch an execution into multiple independent parallel sections and then choose the best (perhaps fastest) amongst them. The continuing execution after the speculatively branched section sees only the modifications made by the best one. This programming paradigm allows programmers to harness parallelism and can provide dramatic performance improvements. In this paper we present the Multiverse speculative programming model. Multiverse allows programmers to exploit parallelism through highlevel speculation. It can effectively harness large amounts of parallelism by speculating across an entire cluster and is not bound by the parallelism available in a single machine. We present abstractions and a runtime which allow programmers to introduce large scale highlevel speculative parallelism into applications with minimal effort. We introduce a novel ondemand address space sharing mechanism which provide speculations efficient transparent access to the original address space of the application (including the use of pointers) across machine boundaries. Multiverse provides single commit semantics across speculations while guaranteeing isolation between them. We also introduce novel mechanisms to deal with scalability bottlenecks when there are a large number of speculations. We demonstrate that for several benchmarks, Multiverse achieves impressive speedups and good scalability across entire clusters. We study the overheads of the runtime and demonstrate how our special scalability mechanisms are crucial in scaling cluster wide.
p1691
aVDisposal of dead actors in actormodel languages is as important as disposal of unreachable objects in objectoriented languages. In current practice, programmers are required to either manually terminate actors, or they have to rely on garbage collection systems that monitor actor mutation through write barriers, thread coordination through locks etc. These techniques, however, prevent the collector from being fully concurrent. We developed a protocol that allows garbage collection to run fully concurrently with all actors. The main challenges in concurrent garbage collection is the detection of cycles of sleeping actors in the actors graph, in the presence of concurrent mutation of this graph. Our protocol is solely built on message passing: it uses deferred direct reference counting, a dedicated actor for the detection of (cyclic) garbage, and a confirmation protocol (to deal with the mutation of the actor graph). We present our ideas informally through an example, and then present a formal model, prove soundness and argue completeness. We have implemented the protocol as part of a runtime library. As a preliminary performance evaluation, we discuss the performance of our approach as currently used at a financial institution, and use four benchmarks from the literature to compare our approach with other actormodel systems. These preliminary results indicate that the overhead of our approach is small.
p1692
aVIsolation the property that a task can access shared data without interference from other tasks is one of the most basic concerns in parallel programming. Whilethere is a large body of past work on isolated taskparallelism, the integration of isolation, taskparallelism, and nesting of tasks has been a difficult and unresolved challenge. In this pa per, we present a programming and execution model called Otello where isolation is extended to arbitrarily nested parallel tasks with irregular accesses to heap data. At the same time, no additional burden is imposed on the programmer, who only exposes parallelism by creating and synchronizing parallel tasks, leaving the job of ensuring isolation to the underlying compiler and runtime system. Otello extends our past work on Aida execution model and the delegated isolation mechanism [22] to the setting of nested parallelism. The basic runtime construct in Aida and Otello is an assembly: a task equipped with a region in the shared heap that it owns. When an assembly A conflicts with an assembly B, A transfers or delegates its code and owned region to a carefully selected assembly C in a way that will ensure isolation with B, leaving the responsibility of reexecuting task A to C. The choice of C depends on the nesting relationship between A and B.We have implemented Otello on top of the Habanero Java (HJ) parallel programming language [8], and used this implementation to evaluate Otello on collections of nested taskparallel benchmarks and nonnested transactional benchmarks from past work. On the nested taskparallel benchmarks, Otello achieves scalability comparable to HJ programs without builtin isolation, and the relative overhead of Otello is lower than that of many published datarace detection algorithms that detect the isolation violations (but do not enforce isolation). For the transactional benchmarks, Otello incurs lower overhead than a stateoftheart software transactional memory system (Deuce STM).
p1693
aVNondeterminism is a useful and prevalent concept in the design and implementation of software systems. An important property of nondeterminism is its latent parallelism: A nondeterministic action can evaluate to multiple behaviors. If at least one of these behaviors does not conflict with concurrent tasks, then there is an admissible execution of the action in parallel with these tasks. Unfortunately, existing implementations of the atomic paradigm  optimistic as well as pessimistic  are unable to fully exhaust the parallelism potential of nondeterministic actions, lacking the means to guide concurrent tasks toward nondeterministic choices that minimize interference. This paper investigates the problem of utilizing parallelism due to nondeterminism. We observe that nondeterminism occurs in many realworld codes. We motivate the need for devising coordination mechanisms that can utilize available nondeterminism. We have developed a system featuring such mechanisms, which leverages nondeterminism in a wide class of query operations, allowing a task to look into the future of concurrent tasks that mutate the shared state during query evaluation and reduce conflict accordingly. We evaluate our system on a suite of 12 algorithmic benchmarks of wide applicability, as well as an industrial application. The results are encouraging.
p1694
aVDatadependent GPU kernels, whose data or control flow are dependent on the input of the program, are difficult to verify because they require reasoning about shared state manipulated by many parallel threads. Existing verification techniques for GPU kernels achieve soundness and scalability by using a twothread reduction and making the contents of the shared state nondeterministic each time threads synchronise at a barrier, to account for all possible thread interactions. This coarse abstraction prohibits verification of datadependent kernels. We present barrier invariants, a novel abstraction technique which allows key properties about the shared state of a kernel to be preserved across barriers during formal reasoning. We have integrated barrier invariants with the GPUVerify tool, and present a detailed case study showing how they can be used to verify three prefix sum algorithms, allowing efficient modular verification of a stream compaction kernel, a key building block for GPU programming. This analysis goes significantly beyond what is possible using existing verification techniques for GPU kernels.
p1695
aVSmartphones and tablets with rich graphical user interfaces (GUI) are becoming increasingly popular. Hundreds of thousands of specialized applications, called apps, are available for such mobile platforms. Manual testing is the most popular technique for testing graphical user interfaces of such apps. Manual testing is often tedious and errorprone. In this paper, we propose an automated technique, called SwiftHand, for generating sequences of test inputs for Android apps. The technique uses machine learning to learn a model of the app during testing, uses the learned model to generate user inputs that visit unexplored states of the app, and uses the execution of the app on the generated inputs to refine the model. A key feature of the testing algorithm is that it avoids restarting the app, which is a significantly more expensive operation than executing the app on a sequence of inputs. An important insight behind our testing algorithm is that we do not need to learn a precise model of an app, which is often computationally intensive, if our goal is to simply guide test execution into unexplored parts of the state space. We have implemented our testing algorithm in a publicly available tool for Android apps written in Java. Our experimental results show that we can achieve significantly better coverage than traditional random testing and L*based testing in a given time budget. Our algorithm also reaches peak coverage faster than both random and L*based testing.
p1696
aVSystematic exploration of Android apps is an enabler for a variety of app analysis and testing tasks. Performing the exploration while apps run on actual phones is essential for exploring the full range of app capabilities. However, exploring realworld apps on real phones is challenging due to nondeterminism, nonstandard control flow, scalability and overhead constraints. Relying on endusers to conduct the exploration might not be very effective: we performed a 7use study on popular Android apps, and found that the combined 7use coverage was 30.08% of the app screens and 6.46% of the app methods. Prior approaches for automated exploration of Android apps have run apps in an emulator or focused on small apps whose source code was available. To address these problems, we present A3E, an approach and tool that allows substantial Android apps to be explored systematically while running on actual phones, yet without requiring access to the app's source code. The key insight of our approach is to use a static, taintstyle, dataflow analysis on the app bytecode in a novel way, to construct a highlevel control flow graph that captures legal transitions among activities (app screens). We then use this graph to develop an exploration strategy named Targeted Exploration that permits fast, direct exploration of activities, including activities that would be difficult to reach during normal use. We also developed a strategy named Depthfirst Exploration that mimics user actions for exploring activities and their constituents in a slower, but more systematic way. To measure the effectiveness of our techniques, we use two metrics: activity coverage (number of screens explored) and method coverage. Experiments with using our approach on 25 popular Android apps including BBC News, Gas Buddy, Amazon Mobile, YouTube, Shazam Encore, and CNN, show that our exploration techniques achieve 59.39 64.11% activity coverage and 29.53 36.46% method coverage.
p1697
aVEmerging mobile applications that sense context are poised to delight and entertain us with timely news and events, health tracking, and social connections. Unfortunately, sensing algorithms quickly drain the phone's battery. Developers can overcome battery drain by carefully optimizing context sensing but that makes programming with context arduous and ties applications to current sensing hardware. These types of applications embody a twist on the classic tension between programmer productivity and performance due to their combination of requirements. This paper identifies the latency, accuracy, battery (LAB) abstraction to resolve this tension. We implement and evaluate LAB in a system called Senergy. Developers specify their LAB requirements independent of inference algorithms and sensors. Senergy delivers energy efficient context while meeting the requirements and adapts as hardware changes. We demonstrate LAB's expressiveness by using it to implement 22 context sensing algorithms for four types of context (location, driving, walking, and stationary) and six diverse applications. To demonstrate LAB's energy optimizations, we show often an order of magnitude improvements in energy efficiency on applications compared to prior approaches. This relatively simple, priority based API, may serve as a blueprint for future API design in an increasingly complex design space that must tradeoff latency, accuracy, and efficiency to meet application needs and attain portability across evolving, sensorrich, heterogeneous, and power constrained hardware.
p1698
aVWe propose constraining multithreaded execution to small sets of inputcovering schedules, which we define as follows: given a program P, we say that a set of schedules \u2211 covers all inputs of program P if, when given any input, P's execution can be constrained to some schedule in \u2211 and still produce a semantically valid result. Our approach is to first compute a small \u2211 for a given program P, and then, at runtime, constrain P's execution to always follow some schedule in \u2211, and never deviate. We have designed an algorithm that uses symbolic execution to systematically enumerate a set of inputcovering schedules, \u2211. To deal with programs that run for an unbounded length of time, we partition execution into bounded epochs, find inputcovering schedules for each epoch in isolation, and then piece the schedules together at runtime. We have implemented this algorithm along with a constrained execution runtime for pthreads programs, and we report results Our approach has the following advantage: because all possible runtime schedules are known a priori, we can seek to validate the program by thoroughly verifying each schedule in \u2211, in isolation, without needing to reason about the huge space of thread interleavings that arises due to conventional nondeterministic execution.
p1699
aVStateoftheart dynamic bug detectors such as data race and memory leak detectors report program locations that are likely causes of bugs. However, programmers need more than static program locations to understand the behavior of increasingly complex and concurrent software. Dynamic calling context provides additional information, but it is expensive to record calling context frequently, e.g., at every read and write. Contextsensitive dynamic analyses can build and maintain a calling context tree (CCT) to track calling context but in order to reuse existing nodes, CCTbased approaches require an expensive lookup. This paper introduces a new approach for context sensitivity that avoids this expensive lookup. The approach uses a new data structure called the calling context uptree (CCU) that adds low overhead by avoiding the lookup and instead allocating a new node for each context. A key contribution is that the approach can mitigate the costs of allocating many nodes by extending tracing garbage collection (GC): GC collects unused CCU nodes naturally and efficiently, and we extend GC to merge duplicate nodes lazily. We implement our CCUbased approach in a highperformance Java virtual machine and integrate it with a stalenessbased memory leak detector and happensbefore data race detector, so they can report contextsensitive program locations that cause bugs. We show that the CCUbased approach, in concert with an extended GC, provides a compelling alternative to CCTbased approaches for adding context sensitivity to dynamic analyses.
p1700
aVParallel programming is essential for reaping the benefits of parallel hardware, but it is notoriously difficult to develop and debug reliable, scalable software systems. One key challenge is that modern languages and systems provide poor support for ensuring concurrency correctness properties  atomicity, sequential consistency, and multithreaded determinism  because all existing approaches are impractical. Dynamic, softwarebased approaches slow programs by up to an order of magnitude because capturing and controlling crossthread dependences (i.e., conflicting accesses to shared memory) requires synchronization at virtually every access to potentially shared memory. This paper introduces a new softwarebased concurrency control mechanism called OCTET that soundly captures crossthread dependences and can be used to build dynamic analyses for concurrency correctness. OCTET achieves low overheads by tracking the locality state of each potentially shared object. Nonconflicting accesses conform to the locality state and require no synchronization; only conflicting accesses require a state change and heavyweight synchronization. This optimistic tradeoff leads to significant efficiency gains in capturing crossthread dependences: a prototype implementation of OCTET in a highperformance Java virtual machine slows realworld concurrent programs by only 26% on average. A dependence recorder, suitable for record & replay, built on top of OCTET adds an additional 5% overhead on average. These results suggest that OCTET can provide a foundation for developing lowoverhead analyses that check and enforce concurrency correctness.
p1701
aVThe performance of parallel code significantly depends on the parallel task granularity (PTG). If the PTG is too coarse, performance suffers due to load imbalance; if the PTG is too fine, performance suffers from the overhead that is induced by parallel task creation and scheduling. This paper presents a software platform that automatically determines the PTG at runtime. Automatic PTG selection is enabled by concurrent calls, which are special source language constructs that provide a late decision (at runtime) of whether concurrent calls are executed sequentially or concurrently (as a parallel task). Furthermore, the execution semantics of concurrent calls permits the runtime system to merge two (or more) concurrent calls thereby coarsening the PTG. We present an integration of concurrent calls into the Java programming language, the Java Memory Model, and show how the Java Virtual Machine can adapt the PTG based on dynamic profiling. The performance evaluation shows that our runtime system performs competitively to Java programs for which the PTG is tuned manually. Compared to an unfortunate choice of the PTG, this approach performs up to 3x faster than standard Java code.
p1702
aVJavaScript is the most popular language on the web and is a crucial component of HTML5 applications and services that run on consumer platforms ranging from desktops to phones. However, despite ample amount of hardware parallelism available to web applications on such platforms, JavaScript web applications remain predominantly sequential. Common parallel programming solutions accepted by other programming languages failed to transfer themselves to JavaScript due to differences in programming models, the additional requirements of the web and different developer expectations. In this paper we present River Trail  a parallel programming model and API for JavaScript that provides safe, portable, programmerfriendly, deterministic parallelism to JavaScript applications. River Trail allows web applications to effectively utilize multiple cores, vector instructions, and GPUs on client platforms while allowing the web developer to remain within the environment of JavaScript. We describe the implementation of the River Trail compiler and runtime and present experimental results that show the impact of River Trail on performance and scalability for a variety of realistic HTML5 applications. Our experiments show that River Trail has a dramatic positive impact on overall performance and responsiveness of computationally intense JavaScript based applications achieving up to 33.6 times speedup for kernels and up to 11.8 times speedup for realistic web applications compared to sequential JavaScript. Moreover, River Trail enables new interactive web usages that are simply not even possible with standard sequential JavaScript.
p1703
aVFramework based software tends to get bloated by accumulating optional features (or concerns) justincase they are needed. The good news is that such feature bloat need not always cause runtime execution bloat. The bad news is that often enough, only a few statements from an optional concern may cause execution bloat that may result in as much as 50% runtime overhead. We present a novel technique to analyze the connection between optional concerns and the potential sources of execution bloat induced by them. Our analysis automatically answers questions such as (1) whether a given set of optional concerns could lead to execution bloat and (2) which particular statements are the likely sources of bloat when those concerns are not required. The technique combines coarse grain concern input from an external source with a finegrained static analysis. Our experimental evaluation highlights the effectiveness of such concern augmented program analysis in execution bloat assessment of ten programs.
p1704
aVThis paper presents a novel methodology for localizing faults in code as it evolves. Our insight is that the essence of failureinducing edits made by the developer can be captured using mechanical program transformations (e.g., mutation changes). Based on the insight, we present the FIFL framework, which uses both the spectrum information of edits (obtained using the existing FaultTracer approach) as well as the potential impacts of edits (simulated by mutation changes) to achieve more accurate fault localization. We evaluate FIFL on realworld repositories of nine Java projects ranging from 5.7KLoC to 88.8KLoC. The experimental results show that FIFL is able to outperform the stateoftheart FaultTracer technique for localizing failureinducing program edits significantly. For example, all 19 FIFL strategies that use both the spectrum information and simulated impact information for each edit outperform the existing FaultTracer approach statistically at the significance level of 0.01. In addition, FIFL with its default settings outperforms FaultTracer by 2.33% to 86.26% on 16 of the 26 studied version pairs, and is only inferior than FaultTracer on one version pair.
p1705
aVIn the multicore era, it is critical to efficiently test multithreaded software and expose concurrency bugs before software release. Previous work has made significant progress in detecting and validating concurrency bugs under a given input. Unfortunately, software testing always faces large sets of test inputs, and existing techniques are still too expensive to be applied to every test input in practice. In this paper, we use opensource software to study how existing concurrencybug detection tools work for a set of inputs. The study shows that an interleaving pattern, such as a data race or an atomicity violation, can often be exposed by many inputs. Consequently, existing bug detectors would inevitably waste their bug detection effort to generate duplicate bug reports, when applied to a set of inputs. Guided by the above study, we propose a coverage metric, Concurrent Function Pairs (CFP), to efficiently approximate how interleavings overlap across inputs. Using CFP, we have designed a new approach to detecting data races and atomicityviolation bugs for a set of inputs. Our evaluation on opensource C/C++ applications shows that our CFPguided approach can effectively accelerate concurrencybug detection for a set of inputs by reducing redundant detection effort across inputs.
p1706
aVWhen programmers encounter an unfamiliar API library, they often need to refer to its documentations, tutorials, or discussions on development forums to learn its proper usage. These API documents contain valuable information, but may also mislead programmers as they may contain errors (e.g., broken code names and obsolete code samples). Although most API documents are actively maintained and updated, studies show that many new and latent errors do exist. It is tedious and errorprone to find such errors manually as API documents can be enormous with thousands of pages. Existing tools are ineffective in locating documentation errors because traditional natural language (NL) tools do not understand code names and code samples, and traditional code analysis tools do not understand NL sentences. In this paper, we propose the first approach, DOCREF, specifically designed and developed to detect API documentation errors. We formulate a class of inconsistencies to indicate potential documentation errors, and combine NL and code analysis techniques to detect and report such inconsistencies. We have implemented DOCREF and evaluated its effectiveness on the latest documentations of five widelyused API libraries. DOCREF has detected more than 1,000 new documentation errors, which we have reported to the authors. Many of the errors have already been confirmed and fixed, after we reported them.
p1707
aVThe machine representation of floating point values has limited precision such that errors may be introduced during execution. These errors may get propagated and magnified by the following operations, leading to instability problems, e.g., control flow path may be undesirably altered and faulty output may be emitted. In this paper, we develop an onthefly efficient monitoring technique that can predict if an execution is stable. The technique does not explicitly compute errors as doing so incurs high overhead. Instead, it detects possible places where an error becomes substantially inflated regarding the corresponding value, and then tags the value with one bit to denote that it has an inflated error. It then tracks inflation bit propagation, taking care of operations that may cut off such propagation. It reports instability if any inflation bit reaches a critical execution point, such as a predicate, where the inflated error may induce substantial execution difference, such as different execution paths. Our experiment shows that with appropriate thresholds, the technique can correctly detect that over 99.999996% of the inputs of all the programs we studied are stable while a traditional technique relying solely on inflation detection mistakenly classifies majority of the inputs as unstable for some of the programs. Compared to the state of the art technique that is based on high precision computation and causes several hundred times slowdown, our technique only causes 7.91 times slowdown on average and can report all the true unstable executions with the appropriate thresholds.
p1708
aVEliminating concurrency errors is increasingly important as systems rely more on parallelism for performance. Exhaustively exploring the statespace of a program's thread interleavings finds concurrency errors and provides coverage guarantees, but suffers from exponential statespace explosion. Two prior approaches alleviate statespace explosion. (1) Dynamic partialorder reduction (DPOR) provides full coverage and explores only one interleaving of independent transitions. (2) Bounded search provides bounded coverage by enumerating interleavings that do not exceed a bound. In particular, we focus on preemptionbounding. Combining partialorder reduction with preemptionbounding had remained an open problem. We show that preemptionbounded search explores the same partial orders repeatedly and consequently explores more executions than unbounded DPOR, even for small bounds. We further show that if DPOR simply uses the preemption bound to prune the state space as it explores new partial orders, it misses parts of the state space reachable in the bound and is therefore unsound. The bound essentially induces dependences between otherwise independent transitions in the DPOR state space. We introduce Bounded Partial Order Reduction (BPOR), a modification of DPOR that compensates for bound dependences. We identify properties that determine how well bounds combine with partialorder reduction. We prove sound coverage and empirically evaluate BPOR with preemption and fairness bounds. We show that by eliminating redundancies, BPOR significantly reduces testing time compared to bounded search. BPOR's faster incremental guarantees will help testers verify larger concurrent programs.
p1709
aVWhen resolving performance problems, a simple histogram of hot call stacks does not cut it, especially given the highly fluid nature of modern deployments. Why bother tuning, when adding a few CPUs via the management console will quickly resolve the problem? The findings of these tools are also presented without any sense of context: e.g. string conversion may be expensive, but only matters if it contributes greatly to the response time of user logins. Historically, these concerns have been the purview of capacity planning. The power of planners lies in their ability to weigh demand versus capacity, and to do so in terms of the important units of work in the application (such as user logins). Unfortunately, they rely on measurements of rates and latencies, and both quantities are difficult to obtain. Even if possible, when all is said and done, these planners only relate to the code as a blackbox: but, why bother adding CPUs, when easy code changes will fix the problem? We present a way to do planning onthefly: with a few call stack samples taken from an alreadyrunning system, we predict the benefit of a proposed tuning plan. We accomplish this by simulating the effect of a tuning action upon execution speed and the way it shifts resource demand. To identify existing problems, we show how to generate tuning actions automatically, guided by the desire to maximize speedup without needless expense, and that these generated plans may span resource and code changes. We show that it is possible to infer everything needed from these samples alone: levels of resource demand and the units of work in the application. We evaluate our planner on a suite of microbenchmarks and a suite of 15,000 data sets that come from real applications running in the wild.
p1710
aVParametric polymorphism enables code reuse and type safety. Underneath the uniform interface exposed to programmers, however, its low level implementation has to cope with inherently nonuniform data: value types of different sizes and semantics (bytes, integers, floating point numbers) and reference types (pointers to heap objects). On the Java Virtual Machine, parametric polymorphism is currently translated to bytecode using two competing approaches: homogeneous and heterogeneous. Homogeneous translation requires boxing, and thus introduces indirect access delays. Heterogeneous translation duplicates and adapts code for each value type individually, producing more bytecode. Therefore bytecode speed and size are at odds with each other. This paper proposes a novel translation that significantly reduces the bytecode size without affecting the execution speed. The key insight is that larger value types (such as integers) can hold smaller ones (such as bytes) thus reducing the duplication necessary in heterogeneous translations. In our implementation, on the Scala compiler, we encode all primitive value types in long integers. The resulting bytecode approaches the performance of monomorphic code, matches the performance of the heterogeneous translation and obtains speedups of up to 22x over the homogeneous translation, all with modest increases in size.
p1711
aVDespite some clear advantages and recent advances, reference counting remains a poor cousin to highperformance tracing garbage collectors. The advantages of reference counting include a) immediacy of reclamation, b) incrementality, and c) local scope of its operations. After decades of languishing with hopelessly bad performance, recent work narrowed the gap between reference counting and the fastest tracing collectors to within 10%. Though a major advance, this gap remains a substantial barrier to adoption in performanceconscious application domains.  Our work identifies heap organization as the principal source of the remaining performance gap. We present the design, implementation, and analysis of a new collector, RC Immix, that replaces reference counting's traditional freelist heap organization with the line and block heap structure introduced by the Immix collector. The key innovations of RC Immix are 1) to combine traditional reference counts with perline live object counts to identify reusable memory and 2) to eliminate fragmentation by integrating copying with reference counting of new objects and with backup tracing cycle collection. In RC Immix, reference counting offers efficient collection and the line and block heap organization delivers excellent mutator locality and efficient allocation. With these advances, RC Immix closes the 10% performance gap, matching the performance of a highly tuned production generational collector. By removing the performance barrier, this work transforms reference counting into a serious alternative for meeting high performance objectives for garbage collected languages.
p1712
aVModern objectoriented applications commonly suffer from severe performance problems that need to be optimized away for increased efficiency and user satisfaction. Many existing optimization techniques (such as object pooling and pretenuring) require precise identification of object lifetimes. However, it is particularly challenging to obtain object lifetimes both precisely and efficiently: precise profiling techniques such as Merlin introduce several hundred times slowdown even for small programs while efficient approximation techniques often sacrifice precision and produce less useful lifetime information. This paper presents a tunable profiling technique, called Resurrector, that explores the middle ground between high precision and high efficiency to find the precisionefficiency sweetspot for various livenessbased optimization techniques. Our evaluation shows that Resurrector is both more precise and more efficient than the GCbased approximation, and it is ordersofmagnitude faster than Merlin. To demonstrate Resurrector's usefulness, we have developed client analyses to find allocation sites that create large data structures with disjoint lifetimes. By inspecting program source code and reusing data structures created from these allocation sites, we have achieved significant performance gains. We have also improved the precision of an existing optimization technique using the lifetime information collected by Resurrector.
p1713
aVWriting lowlevel concurrent software has traditionally required intimate knowledge of the entire toolchain and often has involved coding in assembly. New language standards have extended C and C++ with support for lowlevel atomic operations and a weak memory model, enabling developers to write portable and efficient multithreaded code. Developing correct lowlevel concurrent code is wellknown to be especially difficult under a weak memory model, where code behavior can be surprising. Building reliable concurrent software using C/C++ lowlevel atomic operations will likely require tools that help developers discover unexpected program behaviors. In this paper we present CDSChecker, a tool for exhaustively exploring the behaviors of concurrent code under the C/C++ memory model. We develop several novel techniques for modeling the relaxed behaviors allowed by the memory model and for minimizing the number of execution behaviors that CDSChecker must explore. We have used CDSChecker to exhaustively unit test several concurrent data structure implementations on specific inputs and have discovered errors in both a recently published C11 implementation of a workstealing queue and a single producer, single consumer queue implementation.
p1714
aVLike sharedmemory multithreaded programs, eventdriven programs such as clientside web applications are susceptible to data races that are hard to reproduce and debug. Race detection for such programs is hampered by their pervasive use of ad hoc synchronization, which can lead to a prohibitive number of false positives. Race detection also faces a scalability challenge, as a large number of shortrunning event handlers can quickly overwhelm standard vectorclockbased techniques. This paper presents several novel contributions that address both of these challenges. First, we introduce race coverage, a systematic method for exposing ad hoc synchronization and other (potentially harmful) races to the user, significantly reducing false positives. Second, we present an efficient connectivity algorithm for computing race coverage. The algorithm is based on chain decomposition and leverages the structure of eventdriven programs to dramatically decrease the overhead of vector clocks. We implemented our techniques in a tool called EventRacer and evaluated it on a number of public web sites. The results indicate substantial performance and precision improvements of our approach over the stateoftheart. Using EventRacer, we found many harmful races, most of which are beyond the reach of current techniques.
p1715
aVThe TypeScript programming language adds optional types to JavaScript, with support for interaction with existing JavaScript libraries via interface declarations. Such declarations have been written for hundreds of libraries, but they can be difficult to write and often contain errors, which may affect the type checking and misguide code completion for the application code in IDEs.  We present a pragmatic approach to check correctness of TypeScript declaration files with respect to JavaScript library implementations. The key idea in our algorithm is that many declaration errors can be detected by an analysis of the library initialization state combined with a lightweight static analysis of the library function code.  Our experimental results demonstrate the effectiveness of the approach: it has found 142 errors in the declaration files of 10 libraries, with an analysis time of a few minutes per library and with a low number of false positives. Our analysis of how programmers use library interface declarations furthermore reveals some practical limitations of the TypeScript type system.
p1716
aVSatisfiability (SAT) and Satisfiability Modulo Theories (SMT) have been used in solving a wide variety of important and challenging problems, including automatic test generation, model checking, and program synthesis. For these applications to scale to larger problem instances, developers cannot rely solely on the sophistication of SAT and SMT solvers to efficiently solve their queries; they must also optimize their own orchestration and construction of queries. We present Smten, a highlevel language for orchestrating and constructing satisfiabilitybased search queries. We show that applications developed using Smten require significantly fewer lines of code and less developer effort to achieve results comparable to standard SMTbased tools.
p1717
aVThere are many domain libraries, but despite the performance benefits of compilation, domainspecific languages are comparatively rare due to the high cost of implementing an optimizing compiler. We propose commensal compilation, a new strategy for compiling embedded domainspecific languages by reusing the massive investment in modern language virtual machine platforms. Commensal compilers use the host language's frontend, use host platform APIs that enable backend optimizations by the host platform JIT, and use an autotuner for optimization selection. The cost of implementing a commensal compiler is only the cost of implementing the domainspecific optimizations. We demonstrate the concept by implementing a commensal compiler for the stream programming language StreamJIT atop the Java platform. Our compiler achieves performance 2.8 times better than the StreamIt native code (via GCC) compiler with considerably less implementation effort.
p1718
aVSurveys can be viewed as programs, complete with logic, control flow, and bugs. Word choice or the order in which questions are asked can unintentionally bias responses. Vague, confusing, or intrusive questions can cause respondents to abandon a survey. Surveys can also have runtime errors: inattentive respondents can taint results. This effect is especially problematic when deploying surveys in uncontrolled settings, such as on the web or via crowdsourcing platforms. Because the results of surveys drive business decisions and inform scientific conclusions, it is crucial to make sure they are correct. We present SurveyMan, a system for designing, deploying, and automatically debugging surveys. Survey authors write their surveys in a lightweight domainspecific language aimed at end users. SurveyMan statically analyzes the survey to provide feedback to survey authors before deployment. It then compiles the survey into JavaScript and deploys it either to the web or a crowdsourcing platform. SurveyMan's dynamic analyses automatically find survey bugs, and control for the quality of responses. We evaluate SurveyMan's algorithms analytically and empirically, demonstrating its effectiveness with case studies of social science surveys conducted via Amazon's Mechanical Turk.
p1719
aVWe introduce RATE TYPES, a novel type system to reason about and optimize dataintensive programs. Built around stream languages, RATE TYPES performs static quantitative reasoning about stream rates   the frequency of data items in a stream being consumed, processed, and produced. Despite the fact that streams are fundamentally dynamic, we find two essential concepts of stream rate control   throughput ratio and natural rate   are intimately related to the program structure itself and can be effectively reasoned about by a type system. RATE TYPES is proven to correspond with a timeaware and parallelismaware operational semantics. The strong correspondence result tolerates arbitrary schedules, and does not require any synchronization between stream filters.We further implement RATE TYPES, demonstrating its effectiveness in predicting stream data rates in realworld stream programs.
p1720
aVA scalable programming language is one in which the same concepts can describe small as well as large parts. Towards this goal, Scala unifies concepts from object and module systems. An essential ingredient of this unification is the concept of objects with type members, which can be referenced through pathdependent types. Unfortunately, pathdependent types are not wellunderstood, and have been a roadblock in grounding the Scala type system on firm theory. We study several calculi for pathdependent types. We present DOT which captures the essence  DOT stands for Dependent Object Types. We explore the design space bottomup, teasing apart inherent from accidental complexities, while fully mechanizing our models at each step. Even in this simple setting, many interesting patterns arise from the interaction of structural and nominal features. Whereas our simple calculus enjoys many desirable and intuitive properties, we demonstrate that the theory gets much more complicated once we add another Scala feature, type refinement, or extend the subtyping relation to a lattice. We discuss possible remedies and tradeoffs in modeling type systems for Scalalike languages.
p1721
aVGradual typing combines static and dynamic typing flexibly and safely in a single programming language. To do so, gradually typed languages implicitly insert casts where needed, to ensure at runtime that typing assumptions are not violated by untyped code. However, the implicit nature of cast insertion, especially on higherorder values, can jeopardize reliability and efficiency: higherorder casts can fail at any time, and are costly to execute. We propose Confined Gradual Typing, which extends gradual typing with two new type qualifiers that let programmers control the flow of values between the typed and the untyped worlds, and thereby trade some flexibility for more reliability and performance. We formally develop two variants of Confined Gradual Typing that capture different flexibility/guarantee tradeoffs. We report on the implementation of Confined Gradual Typing in Gradualtalk, a graduallytyped Smalltalk, which confirms the performance advantage of avoiding unwanted higherorder casts and the low overhead of the approach.
p1722
aVWildcard annotations can improve the generality of Java generic libraries, but require heavy manual effort. We present an algorithm for refactoring and inferring more general type instantiations of Java generics using wildcards. Compared to past approaches, our work is practical and immediately applicable: we assume no changes to the Java type system, while taking into account all its intricacies. Our system allows users to select declarations (variables, method parameters, return types, etc.) to generalize and considers declarations not declared in available source code. It then performs an interprocedural flow analysis and a method body analysis, in order to generalize type signatures. We evaluate our technique on six Java generic libraries. We find that 34% of available declarations of variant type signatures can be generalized  i.e., relaxed with more general wildcard types. On average, 146 other declarations need to be updated when a declaration is generalized, showing that this refactoring would be too tedious and errorprone to perform manually.
p1723
aVToday, Java is regularly used to implement large multithreaded serverclass applications that use locks to protect access to shared data. However, understanding the impact of locks on the performance of a system is complex, and thus the use of locks can impede the progress of threads on configurations that were not anticipated by the developer, during specific phases of the execution. In this paper, we propose Free Lunch, a new lock profiler for Java application servers, specifically designed to identify, invivo, phases where the progress of the threads is impeded by a lock. Free Lunch is designed around a new metric, critical section pressure (CSP), which directly correlates the progress of the threads to each of the locks. Using Free Lunch, we have identified phases of high CSP, which were hidden with other lock profilers, in the distributed Cassandra NoSQL database and in several applications from the DaCapo 9.12, the SPECjvm2008 and the SPECjbb2005 benchmark suites. Our evaluation of Free Lunch shows that its overhead is never greater than 6%, making it suitable for invivo use.
p1724
aVThe accuracy of an approximate computation is the distance between the result that the computation produces and the corresponding fully accurate result. The reliability of the computation is the probability that it will produce an acceptably accurate result. Emerging approximate hardware platforms provide approximate operations that, in return for reduced energy consumption and/or increased performance, exhibit reduced reliability and/or accuracy.  We present Chisel, a system for reliability and accuracyaware optimization of approximate computational kernels that run on approximate hardware platforms. Given a combined reliability and/or accuracy specification, Chisel automatically selects approximate kernel operations to synthesize an approximate computation that minimizes energy consumption while satisfying its reliability and accuracy specification.  We evaluate Chisel on five applications from the image processing, scientific computing, and financial analysis domains. The experimental results show that our implemented optimization algorithm enables Chisel to optimize our set of benchmark kernels to obtain energy savings from 8.7% to 19.8% compared to the fully reliable kernel implementations while preserving important reliability guarantees.
p1725
aVModern demand for energyefficient computation has spurred research at all levels of the stack, from devices to microarchitecture, operating systems, compilers, and languages. Unfortunately, this breadth has resulted in a disjointed space, with technologies at different levels of the system stack rarely compared, let alone coordinated. This work begins to remedy the problem, conducting an experimental survey of the present state of energy management across the stack. Focusing on settings that are exposed to software, we measure the total energy, average power, and execution time of 41 benchmark applications in 220 configurations, across a total of 200,000 program executions. Some of the more important findings of the survey include that effective parallelization and compiler optimizations have the potential to save far more energy than Linux's frequency tuning algorithms; that certain noncomplementary energy strategies can undercut each other's savings by half when combined; and that while the power impacts of most strategies remain constant across applications, the runtime impacts vary, resulting in inconsistent energy impacts.
p1726
aVStatic analysis for JavaScript can potentially help programmers find errors early during development. Although much progress has been made on analysis techniques, a major obstacle is the prevalence of libraries, in particular jQuery, which apply programming patterns that have detrimental consequences on the analysis precision and performance. Previous work on dynamic determinacy analysis has demonstrated how information about program expressions that always resolve to a fixed value in some call context may lead to significant scalability improvements of static analysis for such code. We present a static dataflow analysis for JavaScript that infers and exploits determinacy information onthefly, to enable analysis of some of the most complex parts of jQuery. The analysis combines selective context and path sensitivity, constant propagation, and branch pruning, based on a systematic investigation of the main causes of analysis imprecision when using a more basic analysis. The techniques are implemented in the TAJS analysis tool and evaluated on a collection of small programs that use jQuery. Our results show that the proposed analysis techniques boost both precision and performance, specifically for inferring type information and call graphs.
p1727
aVJava programmers are faced with numerous choices in managing concurrent execution on multicore platforms. These choices often have different tradeoffs (e.g., performance, scalability, and correctness guarantees). This paper analyzes an additional dimension, energy consumption. It presents an empirical study aiming to illuminate the relationship between the choices and settings of thread management constructs and energy consumption. We consider three important thread management constructs in concurrent programming: explicit thread creation, fixedsize thread pooling, and work stealing. We further shed light on the energy/performance tradeoff of three ``tuning knobs'' of these constructs: the number of threads, the task division strategy, and the characteristics of processed data. Through an extensive experimental space exploration over realworld Java programs, we produce a list of findings about the energy behaviors of concurrent programs, which are not always obvious. The study serves as a first step toward improving energy efficiency of concurrent programs on parallel architectures.
p1728
aVReactive programming improves the design of reactive applications by relocating the logic for managing dependencies between dependent values away from the application logic to the language implementation. Many distributed applications are reactive. Yet, existing change propagation algorithms are not suitable in a distributed setting. We propose Distributed REScala, a reactive language with a change propagation algorithm that works without centralized knowledge about the topology of the dependency structure among reactive values and avoids unnecessary propagation of changes, while retaining safety guarantees (glitch freedom). Distributed REScala enables distributed reactive programming, bringing the benefits of reactive programming to distributed applications. We demonstrate the enabled design improvements by a case study. We also empirically evaluate the performance of our algorithm in comparison to other algorithms in a simulated distributed setting.
p1729
aVOliveira and Cook (2012) and Oliveira et al. (2013) have recently introduced object algebras as a program structuring technique to improve the modularity and extensibility of programs. We analyze the relationship between object algebras and attribute grammars (AGs), a formalism to augment contextfree grammars with attributes. We present an extension of the object algebra technique with which the full class of Lattributed grammars  an important class of AGs that corresponds to onepass compilers  can be encoded in Scala. The encoding is modular (attributes can be defined and typechecked separately), scalable (the size of the encoding is linear in the size of the AG specification) and compositional (each AG artifact is represented as a semantic object of the host language). To evaluate these claims, we have formalized the encoding and reimplemented a onepass compiler for a subset of C with our technique. We also discuss how advanced features of modern AG systems, such as higherorder and parameterized attributes, reference attributes, and forwarding can be supported.
p1730
aVValues need to be represented differently when interacting with certain language features. For example, an integer has to take an objectbased representation when interacting with erased generics, although, for performance reasons, the stackbased value representation is better. To abstract over these implementation details, some programming languages choose to expose a unified highlevel concept (the integer) and let the compiler choose its exact representation and insert coercions where necessary. This pattern appears in multiple language features such as value classes, specialization and multistage programming: they all expose a unified concept which they later refine into multiple representations. Yet, the underlying compiler implementations typically entangle the core mechanism with assumptions about the alternative representations and their interaction with other language features. In this paper we present the Late Data Layout mechanism, a simple but versatile typedriven generalization that subsumes and improves the stateoftheart representation transformations. In doing so, we make two key observations: (1) annotated types conveniently capture the semantics of using multiple representations and (2) local type inference can be used to consistently and optimally introduce coercions. We validated our approach by implementing three language features as Scala compiler extensions: value classes, specialization (using the miniboxing representation) and a simplified multistage programming mechanism.
p1731
aVAn incremental computation updates its result based on a change to its input, which is often an order of magnitude faster than a recomputation from scratch. In particular, incrementalization can make expensive computations feasible for settings that require short feedback cycles, such as interactive systems, IDEs, or (soft) realtime systems. This paper presents i3QL, a generalpurpose programming language for specifying incremental computations. i3QL provides a declarative SQLlike syntax and is based on incremental versions of operators from relational algebra, enriched with support for general recursion. We integrated i3QL into Scala as a library, which enables programmers to use regular Scala code for nonincremental subcomputations of an i3QL query and to easily integrate incremental computations into larger software projects. To improve performance, i3QL optimizes userdefined queries by applying algebraic laws and partial evaluation. We describe the design and implementation of i3QL and its optimizations, demonstrate its applicability, and evaluate its performance.
p1732
aVNonvolatile main memory, such as memristors or phase change memory, can revolutionize the way programs persist data. Inmemory objects can themselves be persistent without the need for a separate persistent data storage format. However, the challenge is to ensure that such data remains consistent if a failure occurs during execution. In this paper, we present our system, called Atlas, which adds durability semantics to lockbased code, typically allowing us to automatically maintain a globally consistent state even in the presence of failures. We identify failureatomic sections of code based on existing critical sections and describe a logbased implementation that can be used to recover a consistent state after a failure. We discuss several subtle semantic issues and implementation tradeoffs. We confirm the ability to rapidly flush CPU caches as a core implementation bottleneck and suggest partial solutions. Experimental results confirm the practicality of our approach and provide insight into the overheads of such a system.
p1733
aVWe describe a new algorithm SplitMix for an objectoriented and splittable pseudorandom number generator (PRNG) that is quite fast: 9 64bit arithmetic/logical operations per 64 bits generated. A conventional linear PRNG object provides a generate method that returns one pseudorandom value and updates the state of the PRNG, but a splittable PRNG object also has a second operation, split, that replaces the original PRNG object with two (seemingly) independent PRNG objects, by creating and returning a new such object and updating the state of the original object. Splittable PRNG objects make it easy to organize the use of pseudorandom numbers in multithreaded programs structured using forkjoin parallelism. No locking or synchronization is required (other than the usual memory fence immediately after object creation). Because the generate method has no loops or conditionals, it is suitable for SIMD or GPU implementation. We derive SplitMix from the DotMix algorithm of Leiserson, Schardl, and Sukha by making a series of program transformations and engineering improvements. The end result is an objectoriented version of the purely functional API used in the Haskell library for over a decade, but SplitMix is faster and produces pseudorandom sequences of higher quality; it is also far superior in quality and speed to java.util.Random, and has been included in Java JDK8 as the class java.util.SplittableRandom. We have tested the pseudorandom sequences produced by SplitMix using two standard statistical test suites (DieHarder and TestU01) and they appear to be adequate for "everyday" use, such as in Monte Carlo algorithms and randomized data structures where speed is important.
p1734
aVDesigning and implementing threadsafe multithreaded libraries can be a daunting task as developers of these libraries need to ensure that their implementations are free from concurrency bugs, including deadlocks. The usual practice involves employing software testing and/or dynamic analysis to detect deadlocks. Their effectiveness is dependent on welldesigned multithreaded test cases. Unsurprisingly, developing multithreaded tests is significantly harder than developing sequential tests for obvious reasons. In this paper, we address the problem of automatically synthesizing multithreaded tests that can induce deadlocks. The key insight to our approach is that a subset of the properties observed when a deadlock manifests in a concurrent execution can also be observed in a single threaded execution. We design a novel, automatic, scalable and directed approach that identifies these properties and synthesizes a deadlock revealing multithreaded test. The input to our approach is the library implementation under consideration and the output is a set of deadlock revealing multithreaded tests. We have implemented our approach as part of a tool, named OMEN1. OMEN is able to synthesize multithreaded tests on many multithreaded Java libraries. Applying a dynamic deadlock detector on the execution of the synthesized tests results in the detection of a number of deadlocks, including 35 real deadlocks in classes documented as threadsafe. Moreover, our experimental results show that dynamic analysis on multithreaded tests that are either synthesized randomly or developed by thirdparty programmers are ineffective in detecting the deadlocks.
p1735
aVWe describe an algorithm to perform symbolic execution of a multithreaded program starting from an arbitrary program context. We argue that this can enable more efficient symbolic exploration of deep code paths in multithreaded programs by allowing the symbolic engine to jump directly to program contexts of interest. The key challenge is modeling the initial context with reasonable precision  an overly approximate model leads to exploration of many infeasible paths during symbolic execution, while a very precise model would be so expensive to compute that computing it would defeat the purpose of jumping directly to the initial context in the first place. We propose a contextspecific dataflow analysis that approximates the initial context cheaply, but precisely enough to avoid some common causes of infeasiblepath explosion. This model is necessarily approximate  it may leave portions of the memory state unconstrained, leaving our symbolic execution unable to answer simple questions such as "which thread holds lock A?". For such cases, we describe a novel algorithm for evaluating symbolic synchronization during symbolic execution. Our symbolic execution semantics are sound and complete up to the limits of the underlying SMT solver. We describe initial experiments on an implementation in Cloud 9.
p1736
aVTesting and static analysis can help root out bugs in programs, but not in data. This paper introduces data debugging, an approach that combines program analysis and statistical analysis to automatically find potential data errors. Since it is impossible to know a priori whether data are erroneous, data debugging instead locates data that has a disproportionate impact on the computation. Such data is either very important, or wrong. Data debugging is especially useful in the context of dataintensive programming environments that intertwine data with programs in the form of queries or formulas. We present the first data debugging tool, CheckCell, an addin for Microsoft Excel. CheckCell identifies cells that have an unusually high impact on the spreadsheet's computations. We show that CheckCell is both analytically and empirically fast and effective. We show that it successfully finds injected typographical errors produced by a generative model trained with data entry from 169,112 Mechanical Turk tasks. CheckCell is more precise and efficient than standard outlier detection techniques. CheckCell also automatically identifies a key flaw in the infamous Reinhart and Rogoff spreadsheet.
p1737
aVEventdriven user interface applications typically have a single thread of execution that processes event handlers in response to input events triggered by the user, the network, or other applications. Programmers must ensure that event handlers terminate after a short amount of time because otherwise, the application may become unresponsive. This paper presents EventBreak, a performanceguided test generation technique to identify and analyze event handlers whose execution time may gradually increase while using the application. The key idea is to systematically search for pairs of events where triggering one event increases the execution time of the other event. For example, this situation may happen because one event accumulates data that is processed by the other event. We implement the approach for JavaScriptbased web applications and apply it to three realworld applications. EventBreak discovers events with an execution time that gradually increases in an unbounded way, which makes the application unresponsive, and events that, if triggered repeatedly, reveal a severe scalability problem, which makes the application unusable. The approach reveals two known bugs and four previously unknown responsiveness problems. Furthermore, we show that EventBreak helps in testing that event handlers avoid such problems by bounding a handler's execution time.
p1738
aVAutomatic type inference is a popular feature of functional programming languages. If a program cannot be typed, the compiler typically reports a single program location in its error message. This location is the point where the type inference failed, but not necessarily the actual source of the error. Other potential error sources are not even considered. Hence, the compiler often misses the true error source, which increases debugging time for the programmer. In this paper, we present a general framework for automatic localization of type errors. Our algorithm finds all minimum error sources, where the exact definition of minimum is given in terms of a compilerspecific ranking criterion. Compilers can use minimum error sources to produce more meaningful error reports, and for automatic error correction. Our approach works by reducing the search for minimum error sources to an optimization problem that we formulate in terms of weighted maximum satisfiability modulo theories (MaxSMT). The reduction to weighted MaxSMT allows us to build on SMT solvers to support rich type systems and at the same time abstract from the concrete criterion that is used for ranking the error sources. We have implemented an instance of our framework targeted at HindleyMilner type systems and evaluated it on existing OCaml benchmarks for type error localization. Our evaluation shows that our approach has the potential to significantly improve the quality of type error reports produced by state of the art compilers.
p1739
aVWriting concurrent software while achieving both correctness and efficiency is a grand challenge. To facilitate this task, concurrent data structures have been introduced into the standard library of popular languages like Java and C#. Unfortunately, while the operations exposed by concurrent data structures are atomic (or linearizable), compositions of these operations are not necessarily atomic. Recent studies have found many erroneous implementations of composed concurrent operations.  We address the problem of fixing nonlinearizable composed operations such that they behave atomically. We introduce Flint, an automated fixing algorithm for composed Map operations. Flint accepts as input a composed operation suffering from atomicity violations. Its output, if fixing succeeds, is a composed operation that behaves equivalently to the original operation in sequential runs and is guaranteed to be atomic. To our knowledge, Flint is the first general algorithm for fixing incorrect concurrent compositions.  We have evaluated Flint on 48 incorrect compositions from 27 popular applications, including Tomcat and MyFaces. The results are highly encouraging: Flint is able to correct 96% of the methods, and the fixed version is often the same as the fix by an expert programmer and as efficient as the original code.
p1740
aVDesign and implementation defects that lead to inefficient computation widely exist in software. These defects are difficult to avoid and discover. They lead to severe performance degradation and energy waste during production runs, and are becoming increasingly critical with the meager increase of singlecore hardware performance and the increasing concerns about energy constraints. Effective tools that diagnose performance problems and point out the inefficiency root cause are sorely needed. The state of the art of performance diagnosis is preliminary. Profiling can identify the functions that consume the most computation resources, but can neither identify the ones that waste the most resources nor explain why. Performancebug detectors can identify specific type of inefficient computation, but are not suited for diagnosing general performance problems. Effective failure diagnosis techniques, such as statistical debugging, have been proposed for functional bugs. However, whether they work for performance problems is still an open question. In this paper, we first conduct an empirical study to understand how performance problems are observed and reported by realworld users. Our study shows that statistical debugging is a natural fit for diagnosing performance problems, which are often observed through comparisonbased approaches and reported together with both good and bad inputs. We then thoroughly investigate different design points in statistical debugging, including three different predicates and two different types of statistical models, to understand which design point works the best for performance diagnosis. Finally, we study how some unique nature of performance bugs allows sampling techniques to lower the overhead of runtime performance diagnosis without extending the diagnosis latency.
p1741
aVDespite the advances made by modern parsing strategies such as PEG, LL(*), GLR, and GLL, parsing is not a solved problem. Existing approaches suffer from a number of weaknesses, including difficulties supporting sideeffecting embedded actions, slow and/or unpredictable performance, and counterintuitive matching strategies. This paper introduces the ALL(*) parsing strategy that combines the simplicity, efficiency, and predictability of conventional topdown LL(k) parsers with the power of a GLRlike mechanism to make parsing decisions. The critical innovation is to move grammar analysis to parsetime, which lets ALL(*) handle any nonleftrecursive contextfree grammar. ALL(*) is O(n4) in theory but consistently performs linearly on grammars used in practice, outperforming general strategies such as GLL and GLR by orders of magnitude. ANTLR 4 generates ALL(*) parsers and supports direct leftrecursion through grammar rewriting. Widespread ANTLR 4 use (5000 downloads/month in 2013) provides evidence that ALL(*) is effective for a wide variety of applications.
p1742
aVThe efficiency of a build system is an important factor for developer productivity. As a result, developer teams have been increasingly adopting new build systems that allow higher build parallelization. However, migrating the existing legacy build scripts to new build systems is a tedious and errorprone process. Unfortunately, there is insufficient support for automated migration of build scripts, making the migration more problematic. We propose the first dynamic approach for automated migration of build scripts to new build systems. Our approach works in two phases. First, from a set of execution traces, we synthesize build scripts that accurately capture the intent of the original build. The synthesized build scripts are typically long and hard to maintain. Second, we apply refactorings that raise the abstraction level of the synthesized scripts (e.g., introduce functions for similar fragments). As different refactoring sequences may lead to different build scripts, we use a searchbased approach that explores various sequences to identify the best (e.g., shortest) build script. We optimize searchbased refactoring with partialorder reduction to faster explore refactoring sequences. We implemented the proposed two phase migration approach in a tool called METAMORPHOSIS that has been recently used at Microsoft.
p1743
aVMATLAB is a popular dynamic arraybased language commonly used by students, scientists and engineers who appreciate the interactive development style, the rich set of array operators, the extensive builtin library, and the fact that they do not have to declare static types. Even though these users like to program in MATLAB, their computations are often very computeintensive and are better suited for emerging high performance computing systems. This paper reports on MIX10, a sourcetosource compiler that automatically translates MATLAB programs to X10, a language designed for "Performance and Productivity at Scale"; thus, helping scientific programmers make better use of high performance computing systems. There is a large semantic gap between the arraybased dynamicallytyped nature of MATLAB and the objectoriented, staticallytyped, and highlevel array abstractions of X10. This paper addresses the major challenges that must be overcome to produce sequential X10 code that is competitive with stateoftheart static compilers for MATLAB which target more conventional imperative languages such as C and Fortran. Given that efficient basis, the paper then provides a translation for the MATLAB parfor construct that leverages the powerful concurrency constructs in X10. The MIX10 compiler has been implemented using the McLab compiler tools, is open source, and is available both for compiler researchers and enduser MATLAB programmers. We have used the implementation to perform many empirical measurements on a set of 17 MATLAB benchmarks. We show that our best MIX10generated code is significantly faster than the de facto Mathworks' MATLAB system, and that our results are competitive with stateoftheart static compilers that target C and Fortran. We also show the importance of finding the correct approach to representing the arrays in the generated X10 code, and the necessity of an IntegerOkay' analysis that determines which double variables can be safely represented as integers. Finally, we show that our X10based handling of the MATLAB parfor greatly outperforms the de facto MATLAB implementation.
p1744
aVParsers are ubiquitous in computing, and many applications depend on their performance for decoding data efficiently. Parser combinators are an intuitive tool for writing parsers: tight integration with the host language enables grammar specifications to be interleaved with processing of parse results. Unfortunately, parser combinators are typically slow due to the high overhead of the host language abstraction mechanisms that enable composition. We present a technique for eliminating such overhead. We use staging, a form of runtime code generation, to dissociate input parsing from parser composition, and eliminate intermediate data structures and computations associated with parser composition at staging time. A key challenge is to maintain support for input dependent grammars, which have no clear stage distinction. Our approach applies to topdown recursivedescent parsers as well as bottomup nondeterministic parsers with key applications in dynamic programming on sequences, where we autogenerate code for parallel hardware. We achieve performance comparable to specialized, handwritten parsers.
p1745
aVWe present a novel technique for producing bounded exhaustive test suites from hybrid invariants, i.e., invariants that are expressed imperatively, declaratively, or as a combination of declarative and imperative predicates. Hybrid specifications are processed using known mechanisms for the imperative and declarative parts, but combined in a way that enables us to exploit information from the declarative side, such as tight bounds computed from the declarative specification, to improve the search both on the imperative and declarative sides. Moreover, our technique automatically evaluates different possible ways of processing the imperative side, and the alternative settings (imperative or declarative) for parts of the invariant available both declaratively and imperatively, to decide the most convenient invariant configuration with respect to efficiency in test generation. This is achieved by transcoping, i.e., by assessing the efficiency of the different alternatives on small scopes (where generation times are negligible), and then extrapolating the results to larger scopes. We also show experiments involving collection classes that support the effectiveness of our technique, by demonstrating that (i) bounded exhaustive suites can be computed from hybrid invariants significantly more efficiently than doing so using stateoftheart purely imperative and purely declarative approaches, and (ii) our technique is able to automatically determine efficient hybrid invariants, in the sense that they lead to an efficient computation of bounded exhaustive suites, using transcoping.
p1746
aVMany real programs are written in multiple different programming languages, and supporting this pattern creates challenges for formal compiler verification. We describe our Coq verification of a compiler for a highlevel language, such that the compiler correctness theorem allows us to derive partialcorrectness Hoarelogic theorems for programs built by linking the assembly code output by our compiler and assembly code produced by other means. Our compiler supports such tricky features as storable crosslanguage function pointers, without giving up the usual benefits of being able to verify different compiler phases (including, in our case, two classic optimizations) independently. The key technical innovation is a mixed operational and axiomatic semantics for the source language, with a builtin notion of abstract data types, such that compiled code interfaces with other languages only through axiomatically specified methods that mutate encapsulated private data, represented in whatever formats are most natural for those languages.
p1747
aVWeak memory models formalize the inconsistent behaviors that one can expect to observe in multithreaded programs running on modern hardware. In so doing, however, they complicate the alreadydifficult task of reasoning about correctness of concurrent code. Worse, they render impotent the sophisticated formal methods that have been developed to tame concurrency, which almost universally assume a strong (i.e. sequentially consistent) memory model. This paper introduces GPS, the first program logic to provide a fullfledged suite of modern verification techniques  including ghost state, protocols, and separation logic  for highlevel, structured reasoning about weak memory. We demonstrate the effectiveness of GPS by applying it to challenging examples drawn from the Linux kernel as well as lockfree data structures. We also define the semantics of GPS and prove in Coq that it is sound with respect to the axiomatic C11 weak memory model.
p1748
aVSeveral program analysis tools  such as plagiarism detection and bug finding  rely on knowing a piece of code's relative semantic importance. For example, a plagiarism detector should not bother reporting two programs that have an identical simple loop counter test, but should report programs that share more distinctive code. Traditional program analysis techniques (e.g., finding data and control dependencies) are useful, but do not say how surprising or common a line of code is. Natural language processing researchers have encountered a similar problem and addressed it using an ngram model of text frequency, derived from statistics computed over text corpora. We propose and compute an ngram model for programming languages, computed over a corpus of 2.8 million JavaScript programs we downloaded from the Web. In contrast to previous techniques, we describe a code ngram as a subgraph of the program dependence graph that contains all nodes and edges reachable in n steps from the statement. We can count ngrams in a program and count the frequency of ngrams in the corpus, enabling us to compute tfidfstyle measures that capture the differing importance of different lines of code. We demonstrate the power of this approach by implementing a plagiarism detector with accuracy that beats previous techniques, and a bugfinding tool that discovered over a dozen previously unknown bugs in a collection of real deployed programs.
p1749
aVWe consider the problem of provably verifying that an asynchronous messagepassing system satisfies its local assertions. We present a novel reduction scheme for asynchronous eventdriven programs that finds almostsynchronous invariants  invariants consisting of global states where message buffers are close to empty. The reduction finds almostsynchronous invariants and simultaneously argues that they cover all local states. We show that asynchronous programs often have almostsynchronous invariants and that we can exploit this to build natural proofs that they are correct. We implement our reduction strategy, which is sound and complete, and show that it is more effective in proving programs correct as well as more efficient in finding bugs in several programs, compared to current search strategies which almost always diverge. The high point of our experiments is that our technique can prove the Windows Phone USB Driver written in P [9]correct for the responsiveness property, which was hitherto not provable using stateoftheart modelcheckers.
p1750
aVGenerators offer an elegant way to express iterators. However, their performance has always been their Achilles heel and has prevented widespread adoption. We present techniques to efficiently implement and optimize generators. We have implemented our optimizations in ZipPy, a modern, lightweight AST interpreter based Python 3 implementation targeting the Java virtual machine. Our implementation builds on a framework that optimizes AST interpreters using justintime compilation. In such a system, it is crucial that AST optimizations do not prevent subsequent optimizations. Our system was carefully designed to avoid this problem. We report an average speedup of 3.58x for generatorbound programs. As a result, using generators no longer has downsides and programmers are free to enjoy their upsides.
p1751
aVPredicting a sequence of upcoming function calls is important for optimizing programs written in modern managed languages (e.g., Java, Javascript, C#.) Existing function call predictions are mainly built on statistical patterns, suitable for predicting a single call but not a sequence of calls. This paper presents a new way to enable call sequence prediction, which exploits program structures through Probabilistic Calling Automata (PCA), a new program representation that captures both the inherent ensuing relations among function calls, and the probabilistic nature of execution paths. It shows that PCAbased prediction outperforms existing predictions, yielding substantial speedup when being applied to guide JustInTime compilation. By enabling accurate, efficient call sequence prediction for the first time, PCAbased predictors open up many new opportunities for dynamic program optimizations.
p1752
aVFunction versioning is an approach to addressing inputsensitivity of program optimizations. A major side effect of it is notable code size increase, which has been hindering its broad applications to large code bases and spacestringent environments. In this paper, we initiate a systematic exploration into the problem, providing answers to some fundamental questions: Given a space constraint, to which function we should apply versioning? How many versions of a function should we include in the final executable? Is the optimal selection feasible to do in polynomial time? This study proves selecting the best set of versions under a space constraint is NPcomplete and proposes a heuristic algorithm named CHoGS which yields near optimal results in quadratic time. We implement the algorithm and conduct experiments through the IBM XL compilers. We observe significant performance enhancement with only slight code size increase; the results from CHoGS show factors of higher space efficiency than those from traditional hotnessbased methods.
p1753
aVThe HipHop Virtual Machine (HHVM) is a JIT compiler and runtime for PHP. While PHP values are dynamically typed, real programs often have latent types that are useful for optimization once discovered. Some types can be proven through static analysis, but limitations in the aheadoftime approach leave some types to be discovered at run time. And even though many values have latent types, PHP programs can also contain polymorphic variables and expressions, which must be handled without catastrophic slowdown. HHVM discovers latent types by structuring its JIT around the concept of a tracelet. A tracelet is approximately a basic block specialized for a particular set of runtime types for its input values. Tracelets allow HHVM to exactly and efficiently learn the types observed by the program, while using a simple compiler. This paper shows that this approach enables HHVM to achieve high levels of performance, without sacrificing compatibility or interactivity.
p1754
aVThe C programming language does not prevent outofbounds memory accesses. There exist several techniques to secure C programs; however, these methods tend to slow down these programs substantially, because they populate the binary code with runtime checks. To deal with this problem, we have designed and tested two static analyses  symbolic region and range analysis  which we combine to remove the majority of these guards. In addition to the analyses themselves, we bring two other contributions. First, we describe live range splitting strategies that improve the efficiency and the precision of our analyses. Secondly, we show how to deal with integer overflows, a phenomenon that can compromise the correctness of static algorithms that validate memory accesses. We validate our claims by incorporating our findings into AddressSanitizer. We generate SPEC CINT 2006 code that is 17% faster and 9% more energy efficient than the code produced originally by this tool. Furthermore, our approach is 50% more effective than Pentagons, a stateoftheart analysis to sanitize memory accesses.
p1755
aVWe address the problem of computing semantic differences between a program and a patched version of the program. Our goal is to obtain a precise characterization of the difference between program versions, or establish their equivalence. We focus on infinitestate numerical programs, and use abstract interpretation to compute an overapproximation of program differences. Computing differences and establishing equivalence under abstraction requires abstracting relationships between variables in the two programs. Towards that end, we use a correlating abstract domain to compute a sound approximation of these relationships which captures semantic difference. This approximation can be computed over any interleaving of the two programs. However, the choice of interleaving can significantly affect precision. We present a speculative search algorithm that aims to find an interleaving of the two programs with minimal abstract semantic difference. This method is unique as it allows the analysis to dynamically alternate between several interleavings. We have implemented our approach and applied it to realworld examples including patches from Git, GNU Coreutils, as well as a few handpicked patches from the Linux kernel and the Mozilla Firefox web browser. Our evaluation shows that we compute precise approximations of semantic differences, and report few false differences.
p1756
aVInclusionbased alias analysis for C can be formulated as a contextfree language (CFL) reachability problem. It is well known that the traditional cubic CFLreachability algorithm does not scale well in practice. We present a highly scalable and efficient CFLreachabilitybased alias analysis for C. The key novelty of our algorithm is to propagate reachability information along only original graph edges and bypass a large portion of summary edges, while the traditional CFLreachability algorithm propagates along all summary edges. We also utilize the Four Russians' Trick  a key enabling technique in the subcubic CFLreachability algorithm  in our alias analysis. We have implemented our subcubic alias analysis and conducted extensive experiments on widelyused C programs from the pointer analysis literature. The results demonstrate that our alias analysis scales extremely well in practice. In particular, it can analyze the recent Linux kernel (which consists of 10M SLOC) in about 30 seconds.
p1757
aVMobile app markets have lowered the barrier to market entry for software producers. As a consequence, an increasing number of independent app developers offer their products, and recent platforms such as the MIT App Inventor and Microsoft's TouchDevelop enable even lay programmers to develop apps and distribute them in app markets. A major challenge in this distribution model is to ensure the quality of apps. Besides the usual sources of software errors, mobile apps are susceptible to errors caused by the nondeterminism of an eventbased execution model, a volatile environment, diverse hardware, and others. Many of these errors are difficult to detect during testing, especially for independent app developers, who are not supported by test teams and elaborate test infrastructures. To address this problem, we propose a static program analysis that captures the specifics of mobile apps and is efficient enough to provide feedback during the development process. Experiments involving 51,456 published TouchDevelop scripts show that our analysis analyzes 98% of the scripts in under a minute, and five seconds on average. Manual inspection of the analysis results for a selection of all scripts shows that most of the alarms are real errors.
p1758
aVMany vertexcentric graph algorithms can be expressed using asynchronous parallelism by relaxing certain readafterwrite data dependences and allowing threads to compute vertex values using stale (i.e., not the most recent) values of their neighboring vertices. We observe that on distributed shared memory systems, by converting synchronous algorithms into their asynchronous counterparts, algorithms can be made tolerant to high internode communication latency. However, high internode communication latency can lead to excessive use of stale values causing an increase in the number of iterations required by the algorithms to converge. Although by using bounded staleness we can restrict the slowdown in the rate of convergence, this also restricts the ability to tolerate communication latency. In this paper we design a relaxed memory consistency model and consistency protocol that simultaneously tolerate communication latency and minimize the use of stale values. This is achieved via a coordinated use of best effort refresh policy and bounded staleness. We demonstrate that for a range of asynchronous graph algorithms and PDE solvers, on an average, our approach outperforms algorithms based upon: prior relaxed memory models that allow stale values by at least 2.27x; and Bulk Synchronous Parallel (BSP) model by 4.2x. We also show that our approach frequently outperforms GraphLab, a popular distributed graph processing framework.
p1759
aVDevelopers who set a breakpoint a few statements too late or who are trying to diagnose a subtle bug from a single core dump often wish for a timetraveling debugger. The ability to rewind time to see the exact sequence of statements and program values leading to an error has great intuitive appeal but, due to large time and space overheads, time traveling debuggers have seen limited adoption. A managed runtime, such as the Java JVM or a JavaScript engine, has already paid much of the cost of providing core features  type safety, memory management, and virtual IO  that can be reused to implement a low overhead timetraveling debugger. We leverage this insight to design and build affordable timetraveling debuggers for managed languages. Tardis realizes our design: it provides affordable timetravel with an average overhead of only 7% during normal execution, a rate of 0.6MB/s of history logging, and a worstcase 0.68s timetravel latency on our benchmark applications. Tardis can also debug optimized code using timetravel to reconstruct state. This capability, coupled with its low overhead, makes Tardis suitable for use as the default debugger for managed languages, promising to bring timetraveling debugging into the mainstream and transform the practice of debugging.
p1760
aVPartitioned Global Address Space (PGAS) environments simplify writing parallel code for clusters because they make data movement implicit  dereferencing global pointers automatically moves data around. However, it does not free the programmer from needing to reason about locality  poor placement of data can lead to excessive and even unnecessary communication. For this reason, modern PGAS languages such as X10, Chapel, and UPC allow programmers to express datalayout constraints and explicitly move computation. This places an extra burden on the programmer, and is less effective for applications with limited or datadependent locality (e.g., graph analytics). This paper proposes Alembic, a new static analysis that frees programmers from having to manually move computation to exploit locality in PGAS programs. It works by determining regions of code that access the same cluster node, then transforming the code to migrate parts of the execution to increase the proportion of accesses to local data. We implement the analysis and transformation for C++ in LLVM and show that in irregular application kernels, Alembic can achieve 82% of the performance of handtuned communication (for comparison, nave compilergenerated communication achieves only 13%).
p1761
aVI/O reduction has been a major focus in optimizing dataparallel programs for bigdata processing. While the current stateoftheart techniques use static program analysis to reduce I/O, Cybertron proposes a new direction that incorporates runtime mechanisms to push the limit further on I/O reduction. In particular, Cybertron tracks how data is used in the computation accurately at runtime to filter unused data at finer granularity dynamically, beyond what current staticanalysis based mechanisms are capable of, and to facilitate a new mechanism called constraint based encoding for more efficient encoding. Cybertron has been implemented and applied to production dataparallel programs; our extensive evaluations on real programs and real data have shown its effectiveness on I/O reduction over the existing mechanisms at reasonable CPU cost, and its improvement on endtoend performance in various network environments.
p1762
aVDynamic taint analysis is a wellknown information flow analysis problem with many possible applications. Taint tracking allows for analysis of application data flow by assigning labels to data, and then propagating those labels through data flow. Taint tracking systems traditionally compromise among performance, precision, soundness, and portability. Performance can be critical, as these systems are often intended to be deployed to production environments, and hence must have low overhead. To be deployed in securityconscious settings, taint tracking must also be sound and precise. Dynamic taint tracking must be portable in order to be easily deployed and adopted for real world purposes, without requiring recompilation of the operating system or language interpreter, and without requiring access to application source code. We present Phosphor, a dynamic taint tracking system for the Java Virtual Machine (JVM) that simultaneously achieves our goals of performance, soundness, precision, and portability. Moreover, to our knowledge, it is the first portable general purpose taint tracking system for the JVM. We evaluated Phosphor's performance on two commonly used JVM languages (Java and Scala), on two successive revisions of two commonly used JVMs (Oracle's HotSpot and OpenJDK's IcedTea) and on Android's Dalvik Virtual Machine, finding its performance to be impressive: as low as 3% (53% on average; 220% at worst) using the DaCapo macro benchmark suite. This paper describes our approach toward achieving portable taint tracking in the JVM.
p1763
aVThis paper presents Rubah, the first dynamic software updating system for Java that: is portable, implemented via libraries and bytecode rewriting on top of a standard JVM; is efficient, imposing essentially no overhead on normal, steadystate execution; is flexible, allowing nearly arbitrary changes to classes between updates; and isnondisruptive, employing either a novel eager algorithm that transforms the program state with multiple threads, or a novel lazy algorithm that transforms objects as they are demanded, postupdate. Requiring little programmer effort, Rubah has been used to dynamically update five longrunning applications: the H2 database, the Voldemort keyvalue store, the Jake2 implementation of the Quake 2 shooter game, the CrossFTP server, and the JavaEmailServer.
p1764
aVGarbage collectors are exact or conservative. An exact collector identifies all references precisely and may move referents and update references, whereas a conservative collector treats one or more of stack, register, and heap references as ambiguous. Ambiguous references constrain collectors in two ways. (1) Since they may be pointers, the collectors must retain referents. (2) Since they may be values, the collectors cannot modify them, pinning their referents. We explore conservative collectors for managed languages, with ambiguous stacks and registers. We show that for Java benchmarks they retain and pin remarkably few heap objects: < 0.01% are falsely retained and 0.03% are pinned. The larger effect is collector design. Prior conservative collectors (1) use marksweep and unnecessarily forgo moving all objects, or (2) use mostly copying and pin entire pages. Compared to generational collection, overheads are substantial: 12% and 45% respectively. We introduce high performance conservative Immix and reference counting (RC). Immix is a markregion collector with fine linegrain pinning and opportunistic copying of unambiguous referents. Deferred RC simply needs an object map to deliver the first conservative RC. We implement six exact collectors and their conservative counterparts. Conservative Immix and RC come within 2 to 3% of their exact counterparts. In particular, conservative RC Immix is slightly faster than a welltuned exact generational collector. These findings show that for managed languages, conservative collection is compatible with high performance.
p1765
aVGraphics processing units (GPUs) can effectively accelerate many applications, but their applicability has been largely limited to problems whose solutions can be expressed neatly in terms of linear algebra. Indeed, most GPU programming languages limit the user to simple data structures  typically only multidimensional rectangular arrays of scalar values. Many algorithms are more naturally expressed using higher level language features, such as algebraic data types (ADTs) and first class procedures, yet building these structures in a manner suitable for a GPU remains a challenge. We present a regionbased memory management approach that enables rich data structures in Harlan, a language for data parallel computing. Regions enable rich data structures by providing a uniform representation for pointers on both the CPU and GPU and by providing a means of transferring entire data structures between CPU and GPU memory. We demonstrate Harlan's increased expressiveness on several example programs and show that Harlan performs well on more traditional dataparallel problems.
p1766
aVProgramminglanguage compilers generate code targeted to machines with fixed architectures, either parallel or serial. Compiler techniques can also be used to generate the hardware on which these programming languages are executed. In this paper we demonstrate that many compilation techniques developed for programming languages are applicable to compilation of registertransfer hardware designs. Our approach uses a typical syntaxdirected translation \u2192 global optimization \u2192 local optimization \u2192 code generation \u2192 peephole optimization method. In this paper we will describe ways in which we have both followed and diverged from traditional compiler approaches to these problems and compare our approach to other compiler oriented approaches to hardware compilation.
p1767
aVWe have implemented an illustrated compiler for a simple block structured language. The compiler graphically displays its control and data structures, and so gives its viewers an intuitive understanding of compiler organization and operation. The illustrations were planned by hand and display information naturally and concisely.
p1768
aVAs optimizing compilers become more sophisticated, the problem of debugging the source code of an application becomes more difficult. In order to investigate this problem, we implemented DOC, a prototype solution for Debugging Optimized Code. DOC is a modification of the existing C compiler and sourcelevel symbolic debugger for the HP9000 Series 800. This paper describes our experiences in this effort. We show in an actual implementation that sourcelevel debugging of globally optimized code is viable.
p1769
aVThis paper addresses the design and implementation of an integrated debugging system for parallel programs running on shared memory multiprocessors (SMMP). We describe the use of flowback analysis to provide information on causal relationships between events in a program's execution without reexecuting the program for debugging. We introduce a mechanism called incremental tracing that, by using semantic analyses of the debugged program, makes the flowback analysis practical with only a small amount of trace generated during execution. We extend flowback analysis to apply to parallel programs and describe a method to detect race conditions in the interactions of the cooperating processes.
p1770
aVWe present a case study that illustrates a method of debugging concurrent processes in a parallel programming environment. It uses a new approach called speculative replay to reconstruct the behavior of a program from the histories of its individual processes. Known time dependencies between events in different processes are used to divide the histories into dependence blocks. A graphical representation called a concurrency map displays possibilities for concurrency among processes. The replay technique preserves the known dependencies and compares the process histories generated during replay with those that were logged during the original program execution. If a process generates a replay history that does not match its original history, replay backs up. An alternative ordering of events is created and tested to see if it produces process histories that match the original histories. Successively more controlled replay sequences are generated, by introducing additional dependencies. We describe ongoing work on tools that will control replay without reconstructing the entire space of possible event orderings.\u000aThe case study presents a miniature example of sharedqueue management that can be examined in detail. It demonstrates the replay technique and the construction and use of the concurrency map. Using our techniques, we detect a failure to which a standard algorithm for sharedqueue management is susceptible.
p1771
aVData flow analysis is a timeconsuming part of the optimization process. As transformations are made in a multiple pass global optimizer, the data flow information must be updated to reflect these changes. Various approaches have been used, including complete recalculation as well as partial recalculation over the affected area. The approach presented here has been designed for maximum efficiency. Data flow information is completely calculated only once, using an interval analysis method which is slightly faster than a purely iterative approach, and which allows partial recomputation when appropriate. A minimal set of data flow information is computed, keeping the computation and update cost low. Following each set of transformations, the data flow information is updated based on knowledge of the effect of each change. This approach saves considerable time over complete recalculation, and proper ordering of the various optimizations minimizes the amount of update required.
p1772
aVTraditional flow analysis techniques, such as the ones typically employed by optimizing Fortran compilers, do not work for Schemelike languages. This paper presents a flow analysis technique \u2014 control flow analysis \u2014 which is applicable to Schemelike languages. As a demonstration application, the information gathered by control flow analysis is used to perform a traditional flow analysis problem, induction variable elimination. Extensions and limitations are discussed.\u000aThe techniques presented in this paper are backed up by working code. They are applicable not only to Scheme, but also to related languages, such as Common Lisp and ML.
p1773
aVIn this paper we describe the design of a global machine independent low level optimizer for the Karlsruhe Ada Compiler. We give a short overview on the optimizations and data structures used in the optimizer as well as some experiences with the optimizer. Detailed measurements are provided for a collection of benchmarks. The average improvement of code speed is 40%.
p1774
aVProcessors for programming languages and other formal languages typically use a concrete syntax to describe the user's view of a program and an abstract syntax to represent language structures internally. Grammatical abstraction is defined as a relationship between two contextfree grammars. It formalizes the notion of one syntax being \u201cmore abstract\u201d than another. Two variants of abstraction are presented. Weak grammatical abstraction supports (i) the construction during LR parsing of an internal representation that is closely related to the abstract syntax and (ii) incremental LR parsing using that internal representation as its base. Strong grammatical abstraction tightens the correspondence so that topdown construction of incrementallyparsable internal representations is possible. These results arise from an investigation into languagebased editing systems, but apply to any program that transforms a linguistic object from a representation in its concrete syntax to a representation in its abstract syntax or vice versa.
p1775
aVWe've designed and implemented a copying garbagecollection algorithm that is efficient, realtime, concurrent, runs on commercial uniprocessors and sharedmemory multiprocessors, and requires no change to compilers. The algorithm uses standard virtualmemory hardware to detect references to \u201cfrom space\u201d objects and to synchronize the collector and mutator threads. We've implemented and measured a prototype running on SRC's 5processor Firefly. It will be straightforward to merge our techniques with generational collection. An incremental, nonconcurrent version could be implemented easily on many versions of Unix.
p1776
aVWe describe motivation, design, use, and implementation of higherorder abstract syntax as a central representation for programs, formulas, rules, and other syntactic objects in program manipulation and other formal systems where matching and substitution or unification are central operations. Higherorder abstract syntax incorporates name binding information in a uniform and language generic way. Thus it acts as a powerful link integrating diverse tools in such formal environments. We have implemented higherorder abstract syntax, a supporting matching and unification algorithm, and some clients in Common Lisp in the framework of the Ergo project at Carnegie Mellon University.
p1777
aVWe describe the automatic generation of a complete, realistic compiler from formal specifications of the syntax and semantics of Sol/C, a nontrivial imperative language \u201csort of like C.\u201d The compiler exhibits a three pass structure, is efficient, and produces object programs whose performance characteristics compare favorably with those produced by commercially available compilers. To our knowledge, this is the first time that this has been accomplished.
p1778
aVTraditional compilers are usually sequential programs that serially process source programs through lexical analysis, syntax analysis, semantic analysis and code generation. The availability of multiprocessor computers has made it feasible to consider alternatives to this serial compilation process. The authors are currently engaged in a project to devise ways of structuring compilers so that they can take advantage of modern multiprocessor hardware. This paper is about the most difficult aspect of concurrent compilation: concurrent semantic analysis.
p1779
aVPractical implementations of real languages are often an excellent way of testing the applicability of theoretical principles. Many stresses and strains arise from fitting practicalities, such as performance and standard compatibility, to theoretical models and methods. These stresses and strains are valuable sources of new research and insight, as well as an oftneeded check on the egos of theoreticians.\u000aTwo fertile areas that are often explored by implementations are\u000a\u000aPlaces where tractable models fail to match practice. This can lead to new models, and may also affect practice (e.g., the average programming language has become more context free over the last several decades).\u000aPlaces where existing algorithms fail to deal with practical problems effectively, frequently because the problems are large in some dimension that has not been much explored.\u000a\u000aThe present paper discusses the application of a much studied body of algorithms and techniques [Alle 83, KKLW 80, Bane 76, Wolf 78, Wolf 82, Kenn 80, Lamp 74, Huso 82] for vectorizing and optimizing Fortran to the problem of vectorizing and optimizing C. In the course of this work some algorithms were discarded, others invented, and many were tuned and modified. The experience gave us insight into the strengths and weaknesses of the current theory, as well as into the strong and weak points of C on vector/parallel machines. This paper attempts to communicate some of those insights.
p1780
aVSynchronous message passing via channels is an interprocess communication (IPC) mechanism found in several concurrent languages, such as CSP, occam, and Amber. Such languages provide a powerful selective I/O operation, which plays a vital role in managing communication with multiple processes. Because the channel IPC mechanism is \u201coperationoriented,\u201d only procedural abstraction techniques can be used in structuring the communication/synchronization aspects of a system. This has the unfortunate effect of restricting the use of selective I/O, which in turn limits the communication structure. We propose a new, \u201cvalueoriented\u201d approach to channelbased synchronization. We make synchronous operations firstclass values, called events, in much the same way that functions are firstclass values in functional programming languages. Our approach allows the use of data abstraction techniques for structuring IPC. We have incorporated events into PML, a concurrent functional programming language, and have implemented runtime support for them as part of the Pegasus system.
p1781
aVThis paper deals with the integration of an efficient asynchronous remote procedure call mechanism into a programming language. It describes a new data type called a promise that was designed to support asynchronous calls. Promises allow a caller to run in parallel with a call and to pick up the results of the call, including any exceptions it raises, in a convenient and typesafe manner. The paper also discusses efficient composition of sequences of asynchronous calls to different locations in a network.
p1782
aVWe have adapted an interactive programming system (Smalltalk) to a multiprocessor (the Firefly). The task was not as difficult as might be expected, thanks to the application of three basic strategies: serialization, replication, and reorganization. Serialization of access to resources disallows concurrent access. Replication provides multiple instances of resources when they cannot or should not be serialized. Reorganization allows us to restructure part of the system when the other two strategies cannot be applied.\u000aWe serialized I/O, memory allocation, garbage collection, and scheduling, we replicated the interpreter process, software caches, and a freelist, and we reorganized portions of the scheduling system to deal with some deepseated assumptions. Our changes yielded a fairly low static overhead. We attribute our success to the choice of a small, flexible operating system, a set of constraints which simplified the problem, and the versality of the three strategies for dealing with concurrency. The current system has a moderate amount of overhead when parallelism is being used\u201425% to 65%. It is acceptable, but we believe it can be improved.
p1783
aVTwo references to a record structure conflict if they access the same field and at least one modifies the location. Because structures can be connected by pointers, deciding if two statements conflict requires knowledge of the possible aliases for the locations that they access.\u000aThis paper describes a dataflow computation that produces a conservative description of the aliases visible at any point in a program. The data structure that records aliases is an alias graph. It also labels instances of structures so that the objects referenced at different points in a program can be compared. This paper shows how alias graphs can be used to detect potential conflicts.
p1784
aVPrograms typically spend much of their execution time in loops. This makes the generation of efficient code for loops essential for good performance. Loop optimization of logic programming languages is complicated by the fact that such languages lack the iterative constructs of traditional languages, and instead use recursion to express loops. In this paper, we examine the application of unfold/fold transformations to three kinds of loop optimization for logic programming languages: recursion removal, loop fusion and code motion out of loops. We describe simple unfold/fold transformation sequences for these optimizations that can be automated relatively easily. In the process, we show that the properties of unification and logical variables can sometimes be used to generalize, from traditional languages, the conditions under which these optimizations may be carried out. Our experience suggests that such sourcelevel transformations may be used as an effective tool for the optimization of logic programs.
p1785
aVThis paper shows that software pipelining is an effective and viable scheduling technique for VLIW processors. In software pipelining, iterations of a loop in the source program are continuously initiated at constant intervals, before the preceding iterations complete. The advantage of software pipelining is that optimal performance can be achieved with compact object code.\u000aThis paper extends previous results of software pipelining in two ways: First, this paper shows that by using an improved algorithm, nearoptimal performance can be obtained without specialized hardware. Second, we propose a hierarchical reduction scheme whereby entire control constructs are reduced to an object similar to an operation in a basic block. With this scheme, all innermost loops, including those containing conditional statements, can be software pipelined. It also diminishes the startup cost of loops with small number of iterations. Hierarchical reduction complements the software pipelining technique, permitting a consistent performance improvement be obtained.\u000aThe techniques proposed have been validated by an implementation of a compiler for Warp, a systolic array consisting of 10 VLIW processors. This compiler has been used for developing a large number of applications in the areas of image, signal and scientific processing.
p1786
aVA slice of a program with respect to a program point p and variable x consists of all statements of the program that might affect the value of x at point p. This paper concerns the problem of interprocedural slicing \u2014 generating a slice of an entire program, where the slice crosses the boundaries of procedure calls. To solve this problem, we introduce a new kind of graph to represent programs, called a system dependence graph, which extends previous dependence representations to incorporate collections of procedures (with procedure calls) rather than just monolithic programs. Our main result is an algorithm for interprocedural slicing that uses the new representation.\u000aThe chief difficulty in interprocedural slicing is correctly accounting for the calling context of a called procedure. To handle this problem, system dependence graphs include some datadependence edges that represent transitive dependencies due to the effects of procedure calls, in addition to the conventional directdependence edges. These edges are constructed with the aid of an auxiliary structure that represents calling and parameterlinkage relationships. This structure takes the form of an attribute grammar. The step of computing the required transitivedependence edges is reduced to the construction of the subordinate characteristic graphs for the grammar's nonterminals.
p1787
aVWe present a new method for solving Banning's aliasfree flowinsensitive sideeffect analysis problem. The algorithm employs a new data structure, called the binding multigraph, along with depthfirst search to achieve a running time that is linear in the size of the call multigraph of the program. This method can be extended to produce fast algorithms for dataflow problems with more complex lattice structures.
p1788
aVA large register set can be exploited by keeping variables and constants in registers instead of in memory. Hardware register windows and compiletime or linktime global register allocation are ways to do this. A measure of the effectiveness of any of these register management schemes is how thoroughly they remove loads and stores. This measure must also count extra loads and stores executed because of window overflow or conflicts between procedures.\u000aBy combining profiling, instrumentation, and inline simulation, we measured the effectiveness of several register management schemes. These included compiletime and linktime schemes for allocating registers, and register window schemes using fixedsize or variablesized windows. Linktime allocation based on profile information was the clear winner in some cases and did about as well as windows in the rest. Even linktime allocation based on an estimated profile was about as good as windows. Variablesized windows sometimes did better than fixedsized windows, but the difference was usually small.\u000aRegister windows require extra logic in the data path, which may slow the machine cycle slightly, and often use more chip real estate for additional registers. Proponents of windows suppose that they trade these drawbacks for a reduction in the number of memory references they must make. Our results show that this tradeoff should be made the other way. Keep the hardware simple, because a linktime register allocator can nearly duplicate the improvement in memory reference frequency. Then the cycle time can be as small as possible, resulting in faster programs overall.
p1789
aVInterprocedural register allocation can minimize the register usage penalty at procedure calls by reducing the saving and restoring of registers at procedure boundaries. A onepass interprocedural register allocation scheme based on processing the procedures in a depthfirst traversal of the call graph is presented. This scheme can be overlaid on top of intraprocedural register allocation via a simple extension to the prioritybased coloring algorithm. Using two different usage conventions for the registers, the scheme can distribute register saves/restores throughout the call graph even in the presence of recursion, indirect calls or separate compilation. A natural and efficient way to pass parameters emerges from this scheme. A separate technique uses data flow analysis to optimize the placement of the save/restore code for registers within individual procedures. The techniques described have been implemented in a production compiler suite. Measurements of the effects of these techniques on a set of practical programs are presented and the results analysed.
p1790
aVAbstract interpretation introduced the notion of formal specification of program analyses. Denotational frameworks are convenient for reasoning about such specifications. However, implementation considerations make denotational specifications complex and hard to develop. We present a framework that facilitates the construction and understanding of denotational specifications for program analysis techniques. The framework is exemplified by specifications for program analysis techniques from the literature and from our own research. This approach allows program analysis techniques to be incorporated into automatically generated program synthesizers by including their specifications with the language definition.
p1791
aVWe present a new abstract machine for graph reduction called TIGRE. Benchmark results show that TIGRE's execution speed compares quite favorably with previous combinatorgraph reduction techniques on similar hardware. Furthermore, the mapping of TIGRE onto conventional hardware is simple and efficient. Mainframe implementations of TIGRE provide performance levels exceeding those previously available on custom graph reduction hardware.
p1792
aVThis paper presents aspects of a compiler for a new hardware description language (VHDL) written using attribute grammar techniques. VHDL is introduced, along with the new compiler challenges brought by a language that extends an Ada subset for the purpose of describing hardware. Attribute grammar programming solutions are presented for some of the language challenges.\u000aThe organization of the compiler and of the target virtual machine represented by the simulation kernel are discussed, and performance and codesize figures are presented.\u000aThe paper concludes that attribute grammars can be used for large commercial compilers with excellent results in terms of rapid development time and enhanced maintainability, and without paying any substantial penalty in terms of either the complexity of the language that can be handled or the resulting compilation speed.
p1793
aVA new kind of attribute grammars, called higher order attribute grammars, is defined. In higher order attribute grammars the structure tree can be expanded as a result of attribute computation. A structure tree may be stored in an attribute. The term higher order is used because of the analogy with higher order functions, where a function can be the result or parameter of another function. A relatively simple method, using OAGs, is described to derive an evaluation order on the defining attribute occurrences which comprises all possible direct and indirect attribute dependencies. As in OAGs, visitsequences are computed from which an efficient algorithm for attribute evaluation can be derived.
p1794
aVDynamicallytyped objectoriented languages please programmers, but their lack of static type information penalizes performance. Our new implementation techniques extract static type information from declarationfree programs. Our system compiles several copies of a given procedure, each customized for one receiver type, so that the type of the receiver is bound at compile time. The compiler predicts types that are statically unknown but likely, and inserts runtime type tests to verify its predictions. It splits calls, compiling a copy on each control path, optimized to the specific types on that path. Coupling these new techniques with compiletime message lookup, aggressive procedure inlining, and traditional optimizations has doubled the performance of dynamicallytyped objectoriented languages.
p1795
aVFor a contextfree grammar G, a construction is given to produce an LR parser that recognizes any substring of the language generated by G. The construction yields a conflictfree (deterministic) parser for the bounded context class of grammars (Floyd, 1964). The same construction yields either a lefttoright or righttoleft substring parser, as required to implement Noncorrecting Syntax Error Recovery as proposed by Richter (1985). Experience in constructing a substring parser for Pascal is described.
p1796
aVThe disadvantages of traditional twophase parsing (a scanner phase preprocessing input for a parser phase) are discussed. We present metalanguage enhancements for contextfree grammars that allow the syntax of programming languages to be completely described in a single grammar. The enhancements consist of two new grammar rules, the exclusion rule, and the adjacencyrestriction rule. We also present parser construction techniques for building parsers from these enhanced grammars, that eliminate the need for a scanner phase.
p1797
aVAn LRbased parser generator for arbitrary contextfree grammars is described, which generates parsers by need and processes grammar modifications by updating already existing parsers. We motivate the need for these techniques in the context of interactive language definition environments, present all required algorithms, and give measurements comparing their performance with that of conventional techniques.
p1798
aVA number of recent programming language designs incorporate a type checking system based on the GirardReynolds polymorphic &lgr;calculus. This allows the construction of general purpose, reusable software without sacrificing compiletime type checking. A major factor constraining the implementation of these languages is the difficulty of automatically inferring the lengthy type information that is otherwise required if full use is made of these languages. There is no known algorithm to solve any natural and fully general formulation of this \u201ctype inference\u201d problem. One very reasonable formulation of the problem is known to be undecidable.\u000aHere we define a restricted version of the type inference problem and present an efficient algorithm for its solution. We argue that the restriction is sufficiently weak to be unobtrusive in practice.
p1799
aVWe present the first type reconstruction system which combines the implicit typing of ML with the full power of the explicitly typed secondorder polymorphic lambda calculus. The system will accept MLstyle programs, explicitly typed programs, and programs that use explicit types for all firstclass polymorphic values. We accomplish this flexibility by providing both generic and explicitlyquantified polymorphic types, as well as operators which convert between these two forms of polymorphism. This type reconstruction system is an integral part of the FX89 programming language. We present a type reconstruction algorithm for the system. The type reconstruction algorithm is proven sound and complete with respect to the formal typing rules.
p1800
aVWe present a new static analysis method for firstclass continuations that uses an effect system to classify the control domain behavior of expressions in a typed polymorphic language. We introduce two new control effects, goto and comefrom, that describe the control flow properties of expressions. An expression that does not have a goto effect is said to be continuation following because it will always call its passed return continuation. An expression that does not have a comefrom effect is said to be continuation discarding because it will never preserve its return continuation for later use. Unobservable control effects can be masked by the effect system. Control effect soundness theorems guarantee that the effects computed statically by the effect system are a conservative approximation of the dynamic behavior of an expression.\u000aThe effect system that we describe performs certain kinds of control flow analysis that were not previously feasible. We discuss how this analysis can enable a variety of compiler optimizations, including parallel expression scheduling in the presence of complex control structures, and stack allocation of continuations. The effect system we describe has been implemented as an extension to the FX87 programming language.
p1801
aVOptimizing and parallelizing compilers for procedural languages rely on various forms of program dependence graphs (pdgs) to express the essential control and data dependencies among atomic program operations. In this paper, we provide a semantic justification for this practice by deriving two different forms of program dependence graph \u2014 the output pdg and the deforder pdg\u2014and their semantic definitions from nonstrict generalizations of the denotational semantics of the programming language. In the process, we demonstrate that both the output pdg and the deforder pdg (with minor technical modifications) are conventional dataflow programs. In addition, we show that the semantics of the deforder pdg dominates the semantics of the output pdg and that both of these semantics dominate\u2014rather than preserve\u2014the semantics of sequential execution.
p1802
aVThis paper describes a system that generates compiler back ends from a strictly declarative specification of the code generation process. The generated back ends use tree pattern matching for code selection. Two methods for register allocation supporting a wide range of target architectures are provided. A general bottomup pattern matching method avoids problems that occurred with previous systems using LRparsing. The performance of compilers using generated back ends is comparable to very fast production compilers. Some figures are given about the results of using the system to generate the back end of a Modula2 compiler.
p1803
aVInline function expansion replaces a function call with the function body. With automatic inline function expansion, programs can be constructed with many small functions to handle complexity and then rely on the compilation to eliminate most of the function calls. Therefore, inline expansion serves a tool for satisfying two conflicting goals: minizing the complexity of the program development and minimizing the function call overhead of program execution. A simple inline expansion procedure is presented which uses profile information to address three critical issues: code expansion, stack expansion, and unavailable function bodies. Experiments show that a large percentage of function calls/returns (about 59%) can be eliminated with a modest code expansion cost (about 17%) for twelve UNIX* programs.
p1804
aVGlobal register allocation and spilling is commonly performed by solving a graph coloring problem. In this paper we present a new coherent set of heuristic methods for reducing the amount of spill code generated. This results in more efficient (and shorter) compiled code. Our approach has been compared to both standard and prioritybased coloring algorithms, universally outperforming them.\u000aIn our approach, we extend the capability of the existing algorithms in several ways. First, we use multiple heuristic functions to increase the likelihood that less spill code will be inserted. We have found three complementary heuristic functions which together appear to span a large proportion of good spill decisions. Second, we use a specially tuned greedy heuristic for determining the order of deleting (and hence coloring) the unconstrained vertices. Third, we have developed a \u201ccleaning\u201d technique which avoids some of the insertion of spill code in nonbusy regions.
p1805
aVAlthough graph coloring is widely recognized as an effective technique for global register allocation, the overhead can be quite high, not only in execution time but also in memory, as the size of the interference graph needed in coloring can become quite large. In this paper, we present an algorithm based upon a result by R. Tarjan regarding the colorability of graphs which are decomposable using clique separators, that improves on the overhead of coloring. The algorithm first partitions program code into code segments using the notion of clique separators. The interference graphs for the code partitions are next constructed one at a time and colored independently. The colorings for the partitions are combined to obtain a register allocation for the program code. The technique presented is both efficient in space and time because the graph for only a single code segment needs to be constructed and colored at any given point in time. The partitioning of a graph using clique separators increases the likelihood of obtaining a coloring without spilling and hence an efficient allocation of registers for the program. For straight line code an optimal allocation for the entire program code can be obtained from optimal allocations for individual code segments. In the presence of branches, optimal allocation along one execution path and a near optimal allocation along alternative paths can be potentially obtained. Since the algorithm is highly efficient, it eliminates the need for a local register allocation phase.
p1806
aVWe describe an improvement to a heuristic introduced by Chaitin for use in graph coloring register allocation. Our modified heuristic produces better colorings, with less spill code. It has similar compiletime and implementation requirements. We present experimental data to compare the two methods.
p1807
aVAccess anomalies are a common class of bugs in sharedmemory parallel programs. An access anomaly occurs when two concurrent execution threads both write (or one thread reads and the other writes) the same shared memory location without coordination. Approaches to the detection of access anomalies include static analysis, postmortem trace analysis, and onthefly monitoring.\u000aA general onthefly algorithm for access anomaly detection is presented, which can be applied to programs with both nested forkjoin and synchronization operations. The advantage of onthefly detection over postmortem analysis is that the amount of storage used can be greatly reduced by data compression techniques and by discarding information as soon as it becomes obsolete. In the algorithm presented, the amount of storage required at any time depends only on the number V of shared variables being monitored and the number N of threads, not on the number of synchronizations. Data compression is achieved by the use of two techniques called merging and subtraction. Upper bounds on storage are shown to be V  N2 for merging and V  N for subtraction.
p1808
aVThis paper presents a general framework for determining average program execution times and their variance, based on the program's interval structure and control dependence graph. Average execution times and variance values are computed using frequency information from an optimized counterbased execution profile of the program.
p1809
aVThis paper describes generational reference counting, a new distributed storage reclamation scheme for looselycoupled multiprocessors. It has a significantly lower communication overhead than distributed versions of conventional reference counting. Although generational reference counting has greater computational and space requirements than ordinary reference counting, it may provide a significant saving in overall execution time on machines in which message passing is expensive.\u000aThe communication overhead for generational reference counting is one message for each copy of an interprocessor reference (pointer). Unlike conventional reference counting, when a reference to an object is copied no message is sent to the processor on which the object lies. A message is sent only when a reference is discarded. Unfortunately, generational reference counting shares conventional reference counting's inability to reclaim cyclical structures.\u000aIn this paper, we present the generational reference counting algorithm, prove it correct, and discuss some refinements that make it more efficient. We also compare it with weighted reference counting, another distributed reference counting scheme described in the literature.
p1810
aVCedar is the name for both a language and an environment in use in the Computer Science Laboratory at Xerox PARC since 1980. The Cedar language is a superset of Mesa, the major additions being garbage collection and runtime types. Neither the language nor the environment was originally intended to be portable, and for many years ran only on Dmachines at PARC and a few other locations in Xerox. We recently reimplemented the language to make it portable across many different architectures. Our strategy was, first, to use machinedependent C code as an intermediate language, second, to create a languageindependent layer known as the Portable Common Runtime, and third, to write a relatively large amount of Cedarspecific runtime code in a subset of Cedar itself. By treating C as an intermediate code we are able to achieve reasonably fast compilation, very good eventual machine code, and all with relatively small programmer effort. Because Cedar is a much richer language than C, there were numerous issues to resolve in performing an efficient translation and in providing reasonable debugging. These strategies will be of use to many other porters of highlevel languages who may wish to use C as an assembler language without giving up either ease of debugging or high performance. We present a brief description of the Cedar language, our portability strategy for the compiler and runtime, our manner of making connections to other languages and the Unix* operating system, and some measures of the performance of our \u201cPortable Cedar\u201d.
p1811
aVOur concern is how to determine data dependencies between program constructs in programming languages with pointer variables. We are particularly interested in computing data dependencies for languages that manipulate heapallocated storage, such as Lisp and Pascal. We have defined a family of algorithms that compute safe approximations to the flow, output, and antidependencies of a program written in such a language. Our algorithms account for destructive updates to fields of a structure and thus are not limited to the cases where all structures are trees or acyclic graphs; they are applicable to programs that build cyclic structures.\u000aOur technique extends an analysis method described by Jones and Muchnick that determines an approximation to the actual layouts of memory that can arise at each program point during execution. We extend the domain used in their abstract interpretation so that the (abstract) memory locations are labeled by the program points that set their contents. Data dependencies are then determined from these memory layouts according to the component labels found along the access paths that must be traversed during execution to evaluate the program's statements and predicates.\u000aFor structured programming constructs, the technique can be extended to distinguish between loopcarried and loopindependent dependencies, as well as to determine lower bounds on minimum distances for loopcarried dependencies.
p1812
aVDemonic memory is a form of reconstructive memory for process histories. As a process executes, its states are regularly checkpointed, generating a history of the process at low time resolution. Following the initial generation, any prior state of the process can be reconstructed by starting from a checkpointed state and reexecuting the process up through the desired state, thereby exploiting the redundancy between the states of a process and the description of that process (i.e., a computer program).\u000aThe reconstruction of states is automatic and transparent. The history of a process may be examined as though it were a large twodimensional array, or address spacetime, with a normal address space as one axis and steps of process time as the other. An attempt to examine a state that is not physically stored triggers a \u201cdemon\u201d which reconstructs that memory state before access is allowed.\u000aRegeneration requires an exact description of the original execution of the process. If the original process execution depends on nondeterministic events (e.g., user input), these events are recorded in an exception list, and are replayed at the proper points during reexecution.\u000aWhile more efficient than explicitly storing all state changes, such a checkpointing system is still prohibitively expensive for many applications; each copy (or snapshot) of the system's state may be very large, and many snapshots may be required. Demonic memory saves both space and time by using a virtual copy mechanism. (Virtual copies share unchanging data with the objects that they are copies of, only storing differences from a prototype or original [MiBK86].) In demonic memory, the snapshot at each checkpoint is a virtual copy of the preceding checkpoint's snapshot. Hence it is called a virtual snapshot. In order to make the virtual snapshot mechanism efficient, state information is initially saved in relatively large units of space and time, on the order of pages and seconds, with singleword/singlestep regeneration undertaken only as needed. This permits the costs of indexing and lookup operations to be amortized over many locations.
p1813
aVIn the context of sequential computers, it is common practice to exploit temporal locality of reference through devices such as caches and virtual memory. In the context of multiprocessors, we believe that it is equally important to exploit spatial locality of reference. We are developing a system which, given a sequential program and its domain decomposition, performs process decomposition so as to enhance spatial locality of reference. We describe an application of this method  generating code from sharedmemory programs for the (distributed memory) Intel iPSC/2.
p1814
aVMulT is a parallel Lisp system, based on Multilisp's future construct, that has been developed to run on an Encore Multimax multiprocessor. MulT is an extended version of the Yale T system and uses the T system's ORBIT compiler to achieve \u201cproduction quality\u201d performance on stock hardware \u2014 about 100 times faster than Multilisp. MulT shows that futures can be implemented cheaply enough to be useful in a productionquality system. MulT is fully operational, including a user interface that supports managing groups of parallel tasks.
p1815
aVAn application for a parallel computer with multiple, independent processors often includes different programs (functions) for the individual processors; compilation of such functions can proceed independently. We implemented a compiler that exploits this parallelism by partitioning the input program for parallel translation. The host system for the parallel compiler is an Ethernetbased network of workstations, and different functions of the application program are compiled in parallel on different workstations. For typical programs in our environment, we observe a speedup ranging from 3 to 6 using not more than 9 processors. The paper includes detailed measurements for this parallel compiler; we report the system overhead, implementation overhead, as well as the speedup obtained when compared with sequential compilation.
p1816
aVCST is a programming language based on Smalltalk802 that supports concurrency using locks, asynchronous messages, and distributed objects. In this paper, we describe CST: the language and its implementation. Example programs and initial programming experience with CST are described. Our implementation of CST generates native code for the Jmachine, a finegrained concurrent computer. Some compiler optimizations developed in conjunction with that implementation are also described.
p1817
aVThe constructive reals provide programmers with a useful mechanism for prototyping numerical programs, and for experimenting with numerical algorithms. Unfortunately, the performance of current implementations is inadequate for some potential applications. In particular, these implementations tend to be space inefficient, in that they essentially require a complete computation history to be maintained.\u000aSome numerical analysts (cf. [3]) propose that the programmer instead be provided with variable precision interval arithmetic, and then be required to write code to restart a computation when the intervals become too inaccurate. Though this model is no doubt appropriate at times, it is not an adequate replacement for exact arithmetic. The correct transformation from a program operating on the constructive reals to a reasonable program using iterated interval arithmetic can be nontrivial and error prone. Here we present a technique based on program slicing to both automate this process and reduce the amount of reexecution. Thus the programmer is still free to use the simpler abstraction of exact real arithmetic, but we can provide a more efficient interval arithmetic based implementation. Some preliminary empirical results are presented.
p1818
aVWe present algorithms for accurately converting floatingpoint numbers to decimal representation. The key idea is to carry along with the computation an explicit representation of the required rounding accuracy.\u000aWe begin with the simpler problem of converting fixedpoint fractions. A modification of the wellknown algorithm for radixconversion of fixedpoint fractions by multiplication explicitly determines when to terminate the conversion process; a variable number of digits are produced. The algorithm has these properties:\u000a\u000aNo information is lost; the original fraction can be recovered from the output by rounding.\u000aNo \u201cgarbage digits\u201d are produced.\u000aThe output is correctly rounded.\u000aIt is never necessary to propagate carries on rounding.\u000a\u000aWe then derive two algorithms for freeformal output of floatingpoint numbers. The first simply scales the given floatingpoint number to an appropriate fractional range and then applies the algorithm for fractions. This is quite fast and simple to code but has inaccuracies stemming from roundoff errors and oversimplification. The second algorithm guarantees mathematical accuracy by using multipleprecision integer arithmetic and handling special cases. Both algorithms produce no more digits than necessary (intuitively, the \u201c1.3 prints as 1.2999999\u201d problem does not occur).\u000aFinally, we modify the freeformat conversion algorithm for use in fixedformat applications. Information may be lost if the fixed format provides too few digit positions, but the output is always correctly rounded. On the other hand, no \u201cgarbage digits\u201d are ever produced, even if the fixed format specifies too many digit positions (intuitively, the \u201c4/3 prints as 1.333333328366279602\u201d problem does not occur).
p1819
aVAn algorithm is presented to infer the type and operation parameters of polymorphic functions. Operation parameters are named and typed at the function definition, but are selected from the set of overloaded definitions available wherever the function is used. These parameters are always implicit, implying that the complexity of using a function does not increase with the generality of its type.
p1820
aVMonolithic approaches to functional language arrays, such as Haskell array comprehensions, define elements all at once, at the time the array is created, instead of incrementally. Although monolithic arrays are elegant, a naive implementation can be very inefficient. For example, if a compiler does not know whether an element has zero or many definitions, it must compile runtime tests. If a compiler does not know interelement data dependencies, it must resort to pessimistic strategies such as compiling elements as thunks, or making unnecessary copies when updating an array.\u000aSubscript analysis, originally developed for imperative language vectorizing and parallelizing compilers, can be adapted to provide a functional language compiler with the information needed for efficient compilation of monolithic arrays. Our contribution is to develop the numbertheoretic basis of subscript analysis with assumptions appropriate to functional arrays, detail the kinds of dependence information subscript analysis can uncover, and apply that dependence information to sequential efficient compilation of functional arrays.
p1821
aVObjectoriented languages have suffered from poor performance caused by frequent and slow dynamicallybound procedure calls. The best way to speed up a procedure call is to compile it out, but dynamic binding of objectoriented procedure calls without static receiver type information precludes inlining. Iterative type analysis and extended message splitting are new compilation techniques that extract much of the necessary type information and make it possible to hoist runtime type tests out of loops.\u000aOur system compiles code onthefly that is customized to the actual data types used by a running program. The compiler constructs a control flow graph annotated with type information by simultaneously performing type analysis and inlining. Extended message splitting preserves type information that would otherwise be lost by a controlflow merge by duplicating all the code between the merge and the place that uses the information. Iterative type analysis computes the types of variables used in a loop by repeatedly recompiling the loop until the computed types reach a fixpoint. Together these two techniques enable our SELF compiler to split off a copy of an entire loop, optimized for the commoncase types.\u000aBy the time our SELF compiler generates code for the graph, it has eliminated many dynamicallydispatched procedure calls and type tests. The resulting machine code is twice as fast as that generated by the previous SELF compiler, four times faster than ParcPlace Systems Smalltalk80,* the fastest commercially available dynamicallytyped objectoriented language implementation, and nearly half the speed of optimized C. Iterative type analysis and extended message splitting have cut the performance penalty for dynamicallytyped objectoriented languages in half.
p1822
aVThis paper presents a type system for logic programs that supports parametric polymorphism and subtypes. This system follows most knowledge representation and objectoriented schemes in that subtyping is namebased, i.e., &tgr;1 is considered to be a subtype of &tgr;2 iff it is declared as such. We take this as a fundamental principle in the sense that type declarations have the form of subtype constraints. Types are assigned meaning by viewing such constraints as Horn clauses that, together with a few basic axioms, define a subtype predicate. This technique provides a (least) model for types and, at the same time, a sound and complete proof system for deriving subtypes. Using this proof system, we define welltypedness conditions which ensure that a logic program/query respects a set of predicate types. We prove that these conditions are consistent in the sense that every atom of every resolvent produced during the execution of a welltyped program is consistent with its type.
p1823
aVWhile logic programming languages offer a great deal of scope for parallelism, there is usually some overhead associated with the execution of goals in parallel because of the work involved in task creation and scheduling. In practice, therefore, the \u201cgranularity\u201d of a goal, i.e. an estimate of the work available under it, should be taken into account when deciding whether or not to execute a goal concurrently as a separate task. This paper describes a method for estimating the granularity of a goal at compile time. The runtime overhead associated with our approach is usually quite small, and the performance improvements resulting from the incorporation of grainsize control can be quite good. This is shown by means of experimental results.
p1824
aVThe need for searching a space of solutions appears often. Many problems, such as iteration over a dynamically created domain, can be expressed most naturally using a generateandprocess style. Serial programming languages typically support solutions of these problems by providing some form of generators or backtracking.\u000aA parallel programming language is more demanding since it needs to be able to express parallel generation and processing of elements. Failure driven computation is no longer sufficient and neither is multipleassignment to generated values.\u000aWe describe the replicator control operator used in the high level parallel programming language ALLOY. The replicator control operator provides a new view of generators which deals with these problems.
p1825
aVFNC2 is a new attribute grammar processing system aiming at expressive power, efficiency, ease of use and versatility. Its development at INRIA started in 1986, and a first running prototype is available since early 1989. Its most important features are: efficient exhaustive and incremental visitsequencebased evaluation of strongly (absolutely) noncircular AGs; extensive space optimizations; a speciallydesigned AGdescription language, with provisions for true modularity; portability and versatility of the generated evaluators; complete environment for application development. This paper briefly describes the design and implementation of FNC2 and its peripherals. Then preliminary experience with the system is reported.
p1826
aVThis paper presents the results of our investigation of code positioning techniques using execution profile data as input into the compilation process. The primary objective of the positioning is to reduce the overhead of the instruction memory hierarchy.\u000aAfter initial investigation in the literature, we decided to implement two prototypes for the HewlettPackard Precision Architecture (PARISC). The first, built on top of the linker, positions code based on whole procedures. This prototype has the ability to move procedures into an order that is determined by a \u201cclosest is best\u201d strategy.\u000aThe second prototype, built on top of an existing optimizer package, positions code based on basic blocks within procedures. Groups of basic blocks that would be better as straightline sequences are identified as chains. These chains are then ordered according to branch heuristics. Code that is never executed during the data collection runs can be physically separated from the primary code of a procedure by a technique we devised called procedure splitting.\u000aThe algorithms we implemented are described through examples in this paper. The performance improvements from our work are also summarized in various tables and charts.
p1827
aVThe University of Washington illustrating compiler (UWPI) automatically illustrates the data structures used in simple programs written in a subset of Pascal2. A UWPI user submits a program to UWPI, and can then watch a graphical display show time varying illustrations of the data structures and program source code. UWPI uses the information latent in the program to determine how to illustrate the program. UWPI infers the abstract data types directly from the declarations and operations used in the source program, and then lays out the illustration in a natural way by instantiating wellknown layouts for the abstracts types. UWPI solves program illustration using compiletime pattern matching and type inferencing to link anticipated execution events to display events, rather than relying on user assistance or specialized programming techniques. UWPI has been used to automatically illustrate didactic sorting and searching examples, and can be used to help teach basic data structures, or to help when debugging programs.
p1828
aVTextbased file comparators (e.g., the Unix utility diff), are very general tools that can be applied to arbitrary files. However, using such tools to compare programs can be unsatisfactory because their only notion of change is based on program text rather than program behavior. This paper describes a technique for comparing two versions of a program, determining which program components represents changes, and classifying each changed component as representing either a semantic or a textual change.
p1829
aVProgram slices are useful in debugging, testing, maintenance, and understanding of programs. The conventional notion of a program slice, the static slice, is the set of all statements that might affect the value of a given variable occurrence. In this paper, we investigate the concept of the dynamic slice consisting of all statements that actually affect the value of a variable occurrence for a given program input. The sensitivity of dynamic slicing to particular program inputs makes it more useful in program debugging and testing than static slicing. Several approaches for computing dynamic slices are examined. The notion of a Dynamic Dependence Graph and its use in computing dynamic slices is discussed. The Dynamic Dependence Graph may be unbounded in length; therefore, we introduce the economical concept of a Reduced Dynamic Dependence Graph, which is proportional in size to the number of dynamic slices arising during the program execution.
p1830
aVThe Program Dependence Web (PDW) is a program representation that can be directly interpreted using control, data, or demanddriven models of execution. A PDW combines a singleassignment version of the program with explicit operators that manage the flow of data values. The PDW can be viewed as an augmented Program Dependence Graph. Translation to the PDW representation provides the basis for projects to compile Fortran onto dynamic dataflow architectures and simulators. A second application of the PDW is the construction of various compositional semantics for program dependence graphs.
p1831
aVThis paper describes techniques for optimizing range checks performed to detect array bound violations. In addition to the elimination of range checks, the optimizations discussed in this paper also reduce the overhead due to range checks that cannot be eliminated by compiletime analysis. The optimizations reduce the program execution time and the object code size through elimination of redundant checks, propagation of checks out of loops, and combination of multiple checks into a single check. A minimal control flow graph (MCFG) is constructed using which the minimal amount of data flow information required for range check optimizations is computed. The range check optimizations are performed using the MCFG rather the CFG for the entire program. This allows the global range check optimizations to be performed efficiently since the MCFG is significantly smaller than the CFG. Any array bound violation that is detected by a program with all range checks included, will also be detected by the program after range check optimization and vice versa. Even though the above optimizations may appear to be similar to traditional code optimizations, similar reduction in the number of range checks executed can not be achieved by a traditional code optimizer. Experimental results indicate that the number of range checks performed in executing a program is greatly reduced using the above techniques.
p1832
aVThe recognition of recurrence relations is important in several ways to the compilation of programs. Induction variables, the simplest form of recurrence, are pivotal in loop optimizations and dependence testing. Many recurrence relations, although expressed sequentially by the programmer, lend themselves to efficient vector or parallel computation. Despite the importance of recurrences, vectorizing and parallelizing compilers to date have recognized them only in an adhoc fashion. In this paper we put forth a systematic method for recognizing recurrence relations automatically. Our method has two parts. First, abstract interpretation [CC77, CC79] is used to construct a map that associates each variable assigned in a loop with a symbolic form (expression) of its value. Second, the elements of this map are matched with patterns that describe recurrence relations. The scheme is easily extensible by the addition of templates, and is able to recognize nested recurrences by the propagation of the closed forms of recurrences from inner loops. We present some applications of this method and a proof of its correctness.
p1833
aVThis paper describes an AL compiler for the Warp systolic array. AL is a programming language in which the user programs a systolic array as if it were a sequential computer and relies on the compiler to generate parallel code. This paper introduces the notion of data relations in compiling programs for systolic arrays. Unlike dependence relations among statements of a program, data relations define compatibility relations among data objects of a program. The AL compiler uses data relations to compute data compatibility classes, determine data distribution, and distribute loop iterations. The AL compiler can generate efficient parallel code almost identical to what the user would have written by hand. For example, the AL compiler generates parallel code for the LINPACK LU decomposition (SGEFA) and QR decomposition (SQRDC) routines with a nearly 8fold speedup on the 10cell Warp array for matrices of size 180  180.
p1834
aVThis paper describes a method for compiling programs using interprocedural register allocation. A strategy for handling programs built from multiple modules is presented, as well as algorithms for global variable promotion and register spill code motion. These algorithms attempt to address some of the shortcomings of previous interprocedural register allocation strategies. Results are given for an implementation on a single register file RISCbased architecture.
p1835
aVThough graph coloring algorithms have been shown to work well when applied to register allocation problems, the technique has not been generalized for processor architectures in which some instructions refer to individual operands that are comprised of multiple registers. This paper presents a suitable generalization.
p1836
aVMost conventional compilers fail to allocate array elements to registers because standard dataflow analysis treats arrays like scalars, making it impossible to analyze the definitions and uses of individual array elements. This deficiency is particularly troublesome for floatingpoint registers, which are most often used as temporary repositories for subscripted variables.\u000aIn this paper, we present a sourcetosource transformation, called scalar replacement, that finds opportunities for reuse of subscripted variables and replaces the references involved by references to temporary scalar variables. The objective is to increase the likelihood that these elements will be assigned to registers by the coloringbased register allocators found in most compilers. In addition, we present transformations to improve the overall effectiveness of scalar replacement and show how these transformations can be applied in a variety of loop nest types. Finally, we present experimental results showing that these techniques are extremely effective\u2014capable of achieving integer factor speedups over code generated by good optimizing compilers of conventional design.
p1837
aVLanguages such as Scheme and Smalltalk that provide continuations as firstclass data objects present a challenge to efficient implementation. Allocating activation records in a heap has proven unsatisfactory because of increased frame linkage costs, increased garbage collection overhead, and decreased locality of reference. However, simply allocating activation records on a stack and copying them when a continuation is created results in unbounded copying overhead. This paper describes a new approach based on stack allocation that does not require the stack to be copied when a continuation is created and that allows us to place a small upper bound on the amount copied when a continuation is reinstated. This new approach is faster than the naive stack allocation approach, and it does not suffer from the problems associated with unbounded copying. For continuationintensive programs, our approach is at worst a constant factor slower than the heap allocation approach, and for typical programs, it is significantly faster. An important additional benefit is that recovery from stack overflow is handled gracefully and efficiently.
p1838
aVWe have designed and implemented a fast breakpoint facility. Breakpoints are usually thought of as a feature of an interactive debugger, in which case the breakpoints need not be particularly fast. In our environment breakpoints are often used for noninteractive information gathering; for example, procedure call count and statement execution count profiling [Swinehart, et al.]. When used noninteractively, breakpoints should be as fast as possible, so as to perturb the execution of the program as little as possible. Even in interactive debuggers, a conditional breakpoint facility would benefit from breakpoints that could transfer to the evaluation of the condition rapidly, and continue expeditiously if the condition were not satisfied. Such conditional breakpoints could be used to check assertions, etc. Program advising could also make use of fast breakpoints [Teitelman]. Examples of advising include tracing, timing, and even animation, all of which should be part of an advanced programming environment.\u000aWe have ported the Cedar environment from a machine with microcode support for breakpoints [Lampson and Pier] to commercial platforms running C code [Atkinson, et al.]. Most of our ports run under the Unix* operating system, so one choice for implementing breakpoints for Cedar was to use the breakpoint facility provided by that system. The breakpoints provided by the Unix operating system are several orders of magnitude too slow (and also several process switches too complicated) for the applications we have in mind. So we designed a breakpoint system that was fast enough for our purposes.\u000aBreakpoints for uniprocessors running single threads of control used to be fast and simple to implement. This paper shows that breakpoints can still be fast, even with multiple threads of control on multiprocessors. This paper describes problems in the design of a breakpoint package for modern computer architectures and programming styles, and our solutions to them for a particular architecture.
p1839
aVMuch recent work in polymorphic programming languages allows subtyping and multiple inheritance for records. In such systems, we would like to extract a field from a record with the same efficiency as if we were not making use of subtyping and multiple inheritance. Methods currently used make field extraction 35 times slower, which can produce a significant overall performance slowdown.\u000aWe describe a record layout algorithm that allows us to assign a fixed offset to each field name. This allows field extraction to done just as quickly as in systems that do not provide multiple inheritance. Assigning fixed offsets may require us to leave gaps in some records (and waste space). However, by placing fields at both positive and negative offsets we can drastically reduce the amount of wasted space. Finding an optimal layout is NPhard, so we propose and analyze heuristic algorithms for producing good twodirection record layouts.\u000aIn a trial run, our algorithm produced a fixed layout for the instance variables of the 563 flavors of a Lisp Flavors system; this fixed layout only wastes 6% of the total space consumed by a collection of one instance of each flavor.
p1840
aVConsider the problem of converting decimal scientific notation for a number into the best binary floating point approximation to that number, for some fixed precision. This problem cannot be solved using arithmetic of any fixed precision. Hence the IEEE Standard for Binary FloatingPoint Arithmetic does not require the result of such a conversion to be the best approximation.\u000aThis paper presents an efficient algorithm that always finds the best approximation. The algorithm uses a few extra bits of precision to compute an IEEEconforming approximation while testing an intermediate result to determine whether the approximation could be other than the best. If the approximation might not be the best, then the best approximation is determined by a few simple operations on multipleprecision integers, where the precision is determined by the input. When using 64 bits of precision to compute IEEE double precision results, the algorithm avoids higherprecision arithmetic over 99% of the time.\u000aThe input problem considered by this paper is the inverse of an output problem considered by Steele and White: Given a binary floating point number, print a correctly rounded decimal representation of it using the smallest number of digits that will allow the number to be read without loss of accuracy. The Steele and White algorithm assumes that the input problem is solved; an imperfect solution to the input problem, as allowed by the IEEE standard and ubiquitous in current practice, defeats the purpose of their algorithm.
p1841
aVDebugging optimized code is a desirable capability not provided by most current debuggers. Users are forced to debug the unoptimized code when a bug occurs in the optimized version. Current research offers partial solutions for a small class of optimizations, but not a unified approach that handles a wide range of optimizations, such as the sophisticated optimizations performed by supercomputer compilers.\u000aThe trend with current research is to make the effects of optimization transparent, i.e., provide the same behavior as that of the unoptimized program. We contend that this approach is neither totally feasible nor entirely desirable. Instead, we propose a new approach based on the premise that one should be able to debug the optimized code. This implies mapping the current  state of execution back to the original source, tracking the location of variables, and mapping compilersynthesized variables back to userdefined induction variables. To aid the user in understanding program behavior, various visual means are provided, e.g., different forms of highlighting and annotating of the source/assembly code. While this unavoidably requires the user to have a basic understanding of the optimizations performed, it permits the user to see what is actually happening, infer the optimizations performed, and detect bugs. An example illustrates the effectiveness of visual feedback.\u000aTo support conventional debugger functionality for optimized code, the compiler must generate additional information. Current compilerdebugger interfaces (CDIs) were neither  designed to handle this new information nor are they extensible in a straight forward manner. Therefore, a new CDI was designed that supports providing visual feedback and the debugging of optimized code. This paper specifies the details of a new CDI and relates each feature back to the debugger functionality it supports.
p1842
aVAbstract interpretation is a technique extensively used for global dataflow analyses of logic programs. Existing implementations of abstract interpretation are slow due to interpretive or transforming overhead and the inefficiency in manipulation of global information. Since abstract interpretation mimics standard interpretation, it is a promising alternative to compile abstract interpretation into the framework of the WAM (Warren Abstract Machine) for better performance. In this paper, we show how this approach can be effectively implemented in a lowcost manner. To evaluate the possible benefits of this approach, a prototype dataflow analyzer has been implemented for inference of mode, type and variable aliasing information of logic programs. For a subset of benchmark programs in [15], it significantly improves the performance by a factor of over 150 on the average.
p1843
aVHigher order functional programs constantly allocate objects dynamically. These objects are typically cons cells, closures, and records and are generally allocated in the heap and reclaimed later by some garbage collection process. This paper describes a compile time analysis, called escape analysis, for determining the lifetime of dynamically created objects in higher order functional programs, and describes optimizations that can be performed, based on the analysis, to improve storage allocation and reclamation of such objects. In particular, our analysis can be applied to programs manipulating lists, in which case optimizations can be performed to allow cons cells in spines of lists to be either reclaimed immediately or reused without incurring any garbage collection overhead. In  a previous paper on escape analysis [10], we had left open the problem of performing escape analysis on lists.\u000aEscape analysis simply determines when the argument (or some part of the argument) to a function call is returned by that call. This simple piece of information turns out to be sufficiently powerful to allow stack allocation of objects, compiletime garbage collection, reduction of runtime storage reclamation overhead, and other optimizations that are possible when the lifetimes of objects can be computed statically.\u000aOur approach is to define a highlevel nonstandard semantics that, in many ways, is similar to the standard semantics and captures the escape behavior caused by the constructs in a functional language. The advantage of our analysis lies in its conceptual simplicity and portability (i.e. no assumption is made about an underlying abstract machine).
p1844
aVAn abstract machine is described for the CLP( R ) programming language. It is intended as a first step toward enabling CLP( R ) programs to be executed with efficiency approaching that of conventional languages. The core Constraint Logic Arithmetic Machine (CLAM) extends the Warren Abstract Machine (WAM) for compiling Prolog with facilities for handling real arithmetic constraints. The full CLAM includes facilities for taking advantage of information obtained from global program analysis.
p1845
aVArray data dependence analysis methods currently in use generate false dependences that can prevent useful program transformations. These false dependences arise because the questions asked are conservative approximations to the questions we really should be asking. Unfortunately, the questions we really should be asking go beyond integer programming and require decision procedures for a sublcass of Presburger formulas. In this paper, we describe how to extend the Omega test so that it can answer these queries and allow us to eliminate these false data dependences. We have implemented the techniques described here and believe they are suitable for use in production compilers.
p1846
aVExact and efficient data dependence testing is a key to success of loopparallelizing compiler for computationally intensive programs. A number of algorithms has been created to test array references contained in parameter loops for dependence but most of them are unable to answer the following question correctly: Are references C(i1 + 10j1) and C(i2 + 5), 0 \u2264 i1, i2 \u2264 4, 0 \u2264 j1,j2 \u2264 9 independent? The technique introduced in this paper recognizes that  i1, i2 and j1, j2 make different order contributions to the subscript index, and breaks dependence equation i1 + 10j1 = i2 + 10j2 + 5 into two equations i1 = i2 and 10j1 = 10j2 which then can be solved independently. Since resulting equations contain less variables it is less expensive to solve them. We call this technique  delinearization because it is reverse of the linearization much discussed in the literature.\u000aIn the introduction we demonstrate that linearized references are used not infrequently in scientific FORTRAN and C codes. Then we present a theorem on which delinearization algorithm is based and the algorithm itself. The algorithm is fairly simple and inexpensive. As a byproduct it tests equations it produces for independence as exactly as it is done by GCDtest and Banerjee inequalities combined. The algorithm has been implemented at Moscow State University in a vectorizer named VIC.
p1847
aVInduction variable detection is usually closely tied to the strength reduction optimization. This paper studies induction variable analysis from a different perspective, that of finding induction variables for data dependence analysis. While classical induction variable analysis techniques have been used successfully up to now, we have found a simple algorithm based on the Static Single Assignment form of a program that finds all linear induction variables in a loop. Moreover, this algorithm is easily extended to find induction variables in multiple nested loops, to find nonlinear induction variables, and to classify other integer scalar assignments in loops, such as monotonic, periodic and wraparound variables. Some of these other variables are now classified using ad hoc pattern recognition, while others are not analyzed by current compilers. Giving a unified approach improves the speed of compilers and allows a more general classification scheme. We also show how to use these variables in data dependence testing.
p1848
aVThis paper describes a general framework for representing iterationreordering transformations. These transformations can be both matrixbased and nonmatrixbased. Transformations are defined by rules for mapping dependence vectors, rules for mapping loop bound expressions, and rules for creating new initialization statements. The framework is extensible, and can be used to represent any iterationreordering transformation. Mapping rules for several common transformations are included in the paper.
p1849
aVMany loop nests in scientific codes contain a parallelizable outer loop but have an inner loop for which the number of iterations varies between different iterations of the outer loop. When running this kind of loop nest on a SIMD machine, the SIMDinherent restriction to single program counter common to all processors will cause a performance degradation relative to comparable MIMD implementations. This problem is not due to limited parallelism or bad load balance, it is merely a problem of control flow.\u000aThis paper presents a loop transformation, which we call loop flattening, that overcomes this limitation by letting each processor advance to the next loop iteration containing useful computation, if there is such an iteration for the given processor. We  study a concrete example derived from a molecular dynamics code and compare performance results for flattened and unflattened versions of this kernel on two SIMD machines, the CM2 and the DECmpp 12000. We then evaluate loop flattening from the compiler's perspective in terms of applicability, cost, profitability, and safety. We conclude with arguing that loop flattening, whether performed by the programmer or by the compiler, introduces negligible overhead and can significantly improve the performance of scientific codes for solving irregular problems.
p1850
aVThis paper develops a methodology for compiling and executing irregular parallel programs. Such programs implement parallel operations whose size and work distribution depend on input data. We show a fundamental relationship between three quantities that characterize an irregular parallel computation: the total available parallelism, the optimal grain size, and the statistical variance of execution times for individual tasks. This relationship yields a dynamic scheduling algorithm that substantially reduces the overhead of executing irregular parallel operations.\u000aWe incorporated this algorithm into an extended Fortran compiler. The compiler accepts as input a subset of Fortran D which includes blocked and cyclic decompositions and perfect alignment; it outputs Fortran 77 augmented with calls to library routines written in C. For irregular parallel operations, the compiled code gathers information about available parallelism and task execution time variance and uses this information to schedule the operation. On distributed memory architectures, the compiler encodes information about data access patterns for the runtime scheduling system so that it can preserve communication locality.\u000aWe evaluated these compilation techniques using a set of application programs including climate modeling, circuit simulation, and xray tomography, that contain irregular parallel operations. The results demonstrate that, for these applications, the dynamic techniques described here achieve nearoptimal efficiency on large numbers of processors. In addition, they perform significantly better, on these problems, than any previously proposed static or dynamic scheduling algorithm.
p1851
aVThis paper introduces program directing, a new way of program interaction. Directing enables one program, the director, to monitor and to control another program, the executor. One important application of program directing is human interaction with complex computer simulations.\u000aThe Dynascope programming environment is designed specifically to support directing in traditional, compiled languages. It provides a framework and building blocks for easy construction of sophisticated directors. Directors are regular programs that perform the directing of executors through Dynascope primitives. Dynascope is built around the concept of the execution stream which provides a complete description of the executor's computational behavior. The source code of executors requires no changes in order to be subjected to directing.\u000aThis paper gives an overview of the Dynascope system. Sample applications are presented: debugging register allocation, animation of procedure calls, and a complex artificial life simulation. The performance of Dynascope is illustrated by real time measurements.
p1852
aVWe present a bitvector algorithm for the optimal and economical placement of computations within flow graphs, which is as efficient as standard unidirectional analyses. The point of our algorithm is the decomposition of the bidirectional structure of the known placement algorithms into a sequence of a backward and a forward analysis, which directly implies the efficiency result. Moreover, the new compositional structure opens the algorithm for modification: two further unidirectional analysis components exclude any unnecessary code motion. This laziness of our algorithm minimizes the register pressure, which has drastic effects on the runtime behaviour of the optimized programs in practice, where an economical use of registers is essential.
p1853
aVDuring execution, when two or more names exist for the same location at some program point, we call them aliases. In a language which allows arbitrary pointers, the problem of determining aliases at a program point is &rgr;spacehard [Lan92]. We present an algorithm for the Conditional May Alias problem, which can be used to safely approximate Interprocedural May Alias in the presence of pointers. This algorithm is as precise as possible in the worst case and has been implemented in a prototype analysis tool for C programs. Preliminary speed and precision results are presented.
p1854
aVEven though impressive progress has been made in the area of optimizing and parallelizing programs with arrays, the application of similar techniques to programs with pointer data structures has remained difficult. In this paper we introduce a new approach that leads to improved analysis and transformation of programs with recursivelydefined pointer data structures.\u000aWe discuss how an abstract data structure description can improve program analysis by presenting an analysis approach that combines an alias analysis technique, path matrix, with information available from an ADDS declaration. Given this improved alias analysis technique, we provide a concrete example of applying a software pipelining transformation to loops involving pointer data structures.
p1855
aVAlphonse is a program transformation system that uses dynamic dependency analysis and incremental computation techniques to automatically generate efficient dynamic implementations from simple exhaustive imperative program specifications.
p1856
aVWe consider the problem of supporting compacting garbage collection in the presence of modern compiler optimizations. Since our collector may move any heap object, it must accurately locate, follow, and update all pointers and values derived from pointers. To assist the collector, we extend the compiler to emit tables describing live pointers, and values derived from pointers, at each program location where collection may occur. Significant results include identification of a number of problems posed by optimizations, solutions to those problems, a working compiler, and experimental data concerning table sizes, table compression, and time overhead of decoding tables during collection. While gc support can affect the code produced, our sample programs show no significant changes, the table sizes are a modest fraction of the size of the optimized code, and stack tracing is a small fraction of total gc time. Since the compiler enhancements are also modest, we conclude that the approach is practical.
p1857
aVSoftware pipelining is an important instruction scheduling technique for efficiently overlapping successive iterations of loops and executing them in parallel. This paper studies the task of register allocation for software pipelined loops, both with and without hardware features that are specifically aimed at supporting software pipelines. Register allocation for software pipelines presents certain novel problems leading to unconventional solutions, especially in the presence of hardware support. This paper formulates these novel problems and presents a number of alternative solution strategies. These alternatives are comprehensively tested against over one thousand loops to determine the best register allocation strategy, both with and without the hardware support for software pipelining.
p1858
aVA new global register allocation technique, probabilistic register allocation, is described. Probabilistic register allocation quantifies the costs and benefits of allocating variables to registers over live ranges so that excellent allocation choices can be made. Local allocation is done first, and then global allocation is done iteratively beginning in the most deeply nested loops. Because local allocation precedes global allocation, probabilistic allocation does not interfere with the use of wellknown, highquality local register allocation and instruction scheduling techniques.
p1859
aVThis paper examines a problem that arises during global register allocation \u2013 rematerialization. If a value cannot be kept in a register, the allocator should recognize when it is cheaper to recompute the value (rematerialize it) than to store and reload it. Chaitin's original graphcoloring allocator handled simple instance of this problem correctly. This paper details a general solution to the problem and presents experimental evidence that shows its importance.\u000aOur approach is to tag individual values in the procedure's SSA graph with information specifying how it should be spilled. We use a variant of Wegman and Zadeck's sparse simple constant algorithm to propagate tags throughout the graph. The allocator then splits live ranges into values with different tags. This isolates those values that can be easily rematerialized from values that require general spilling. We modify the base allocator to use this information when estimating spill costs and introducing spill code.\u000aOur presentation focuses on rematerialization in the context of Chaitin's allocator; however, the problem arises in any global allocator. We believe that our approach will work in other allocators\u2013while the details of implementation will vary, the key insights should carry over directly.
p1860
aVThis study evaluates a global optimization technique that avoids unconditional jumps by replicating code. When implemented in the backend of an optimizing compiler, this technique can be generalized to work on almost all instances of unconditional jumps, including those generated from conditional statements and unstructured loops. The replication method is based on the idea of finding a replacement for each unconditional jump which minimizes the growth in code size. This is achieved by choosing the shortest sequence of instructions as a replacement. Measurements taken from a variety of programs showed that not only the number of executed instructions decreased, but also that the total cache work was reduced (except for small caches) despite increases in code size. Pipelined and superscalar machines may also benefit from an increase in the average basic block size.
p1861
aVA simple and efficient algorithm for generating bottomup rewrite system (BURS) tables is described. A small prototype implementation produces tables 10 to 30 times more quickly than the best current techniques. The algorithm does not require novel data structures or complicated algorithmic techniques. Previously published methods for the onthefly elimination of states are generalized and simplified to create a new method, triangle trimming, that is employed in the algorithm.
p1862
aVWe are developing techniques for building retargetable debuggers. Our prototype, 1db, debugs C programs compiled for the MIPS R3000, Motorola 68020, SPARC, and VAX architectures. It can use a network to connect to faulty processes and can do crossarchitecture debugging. 1db's total code size is about 16,000 lines, but it needs only 250\u2013550 lines of machinedependent code for each target.\u000a1db owes its retargetability to three techniques: getting help from the compiler, using a machineindependent embedded interpreter, and choosing abstractions that minimize and isolate machinedependent code. 1db reuses existing compiler function by having the compiler emit PostScript code that 1db later interprets; PostScript works well in this unusual context.
p1863
aVSELF's debugging system provides complete sourcelevel debugging (expected behavior) with globally optimized code. It shields the debugger from optimizations performed by the compiler by dynamically deoptimizing code on demand. Deoptimization only affects the procedure activations that are actively being debugged; all other code runs at full speed. Deoptimization requires the compiler to supply debugging information at discrete interrupt points; the compiler can still perform extensive optimizations between interrupt points without affecting debuggability. At the same time, the inability to interrupt between interrupt points is invisible to the user. Our debugging system also handles programming changes during debugging. Again, the system provides expected behavior: it is possible to change a running program and immediately observe the effects of the change. Dynamic deoptimization transforms old compiled code (which may contain inlined copies of the old version of the changed procedure) into new versions reflecting the current sourcelevel state. To the best of our knowledge, SELF is the first practical system providing full expected behavior with globally optimized code.
p1864
aVHoME is a version of Smalltalk which can be efficiently executed on a multiprocessor and can be executed in parallel by combining a Smalltalk process with a Mach thread and executing the process on the thread. HoME is nearly the same as ordinary Smalltalk except that multiple processes may execute in parallel. Thus, almost all applications running on ordinary Smalltalk can be executed on HoME without changes in their code.\u000aHoME was designed and implemented based on the following fundamental policies: (1) theoretically, an infinite number of processes can become active; (2) the moment a process is scheduled, it becomes active; (3) no process switching occurs; (4) HoME is equivalent to ordinary Smalltalk except for the previous three policies.\u000aThe performance of the current implementation of HoME running on OMRON LUNA88K, which had four processors, was measured by benchmarks which execute in parallel with multiple processes. In all benchmarks, the results showed that HoME's performance is much better than HPS on the same workstation.
p1865
aVWe describe an approach to implementing a widerange of concurrency paradigms in highlevel (symbolic) programming languages. The focus of our discussion is STING, a dialect of Scheme, that supports lightweight threads of control and virtual processors as firstclass objects. Given the significant degree to which the behavior of these objects may be customized, we can easily express a variety of concurrency paradigms and linguistic structures within a common framework without loss of efficiency.\u000aUnlike parallel systems that rely on operating system services for managing concurrency, STING implements concurrency management entirely in terms of Scheme objects and procedures. It, therefore, permits users to optimize the runtime behavior of their applications without requiring knowledge of the underlying runtime system.\u000aThis paper concentrates on (a) the implications of the design for building asynchronous concurrency structures, (b) organizing largescale concurrent computations, and (c) implementing robust programming environments for symbolic computing.
p1866
aVIn this paper we describe a collection of techniques for the design and implementation of concurrent compilers. We begin by describing a technique for dividing a source program into many streams so that each stream can be compiled concurrently. We discuss several compiler design issues unique to concurrent compilers including source program partitioning, symbol table management, compiler task scheduling and information flow constraints. The application of our techniques is illustrated by a complete design for a concurrent Modula2+ compiler. After describing the structure of this compiler's performance that demonstrates that significant improvements in compilation time can be achieved through the use of concurrency.
p1867
aVA complex and timeconsuming function of a modern compiler is global optimization. Unlike other functions of a compiler such as parsing and code generation which examine only one statement or one basic block at a time, optimizers are much larger in scope, examining and changing large portions of a program all at once. The larger scope means optimizers must perform many program transformations. Each of these transformations makes its own particular demands on the internal representation of programs; each can interact with and depend on the others in different ways. This makes optimizers large and complex.\u000aDespite their complexity, few tools exist to help in building optimizers. This is in stark contrast with other parts of the compiler where years of experience have culminated in tools with which these other parts can be constructed easily. For example, parser generators are used to build frontends, and peephole optimizers and tree matchers are used to build code generators.\u000aThis paper presents Sharlit, a tool to support the construction of modular and extensible global optimizers. We will show how Sharlit helps in constructing dataflow analyzers and the transformations that use dataflow analysis information, both are major components of any optimizer.\u000aSharlit is implemented in C++ and uses C++ in the same way that YACC uses C. Thus we assume the reader has some familiarity with C++[9].
p1868
aVMassively parallel architectures, and the languages used to program them, are among both the most difficult and the most rapidlychanging subjects for compilation. This has created a demand for new compiler prototyping technologies that allow novel style of compilation and optimization to be tested in a reasonable amount of time.\u000aUsing formal specification techniques, we have produced a dataparallel Fortran90 subset compiler for Thinking Machines' Connection Machine/2 and Connection Machine/5. The prototype produces code from initial Fortran90 benchmarks demonstrating sustained performance superior to handcoded Lisp and competitive with Thinking Machines' CM Fortran compiler. This paper presents some new specification techniques necessary to construct competitive, easily retargetable prototype compilers.
p1869
aVA data breakpoint associates debugging actions with programmerspecified conditions on the memory state of an executing program. Data breakpoints provide a means for discovering program bugs that are tedious or impossible to isolate using control breakpoints alone. In practice, programmers rarely use data breakpoints, because they are either unimplemented or prohibitively slow in available debugging software. In this paper, we present the design and implementation of a practical data breakpoint facility.\u000aA data breakpoint facility must monitor all memory updates performed by the program being debugged. We implemented and evaluated two complementary techniques for reducing the overhead of monitoring memory updates. First, we checked write instructions by inserting checking  code directly into the program being debugged. The checks use a segmented bitmap data structure that minimizes address lookup complexity. Second, we developed data flow algorithms that eliminate checks on some classes of write instructions but may increase the complexity of the remaining checks.\u000aWe evaluated these techniques on the SPARC using the SPEC benchmarks. Checking each write instruction using a segmented bitmap achieved an average overhead of 42%. This overhead is independent of the number of breakpoints in use. Data flow analysis eliminated an average of 79% of the dynamic write checks. For scientific programs such the NAS kernels, analysis reduced write checks by a factor of ten or more. On the SPARC these optimizations reduced the  average overhead to 25%.
p1870
aVMany parallel programs contain multiple subcomputations, each with distinct communication and load balancing requirements. The traditional approach to compiling such programs is to impose a processor synchronization barrier between subcomputations, optimizing each as a separate entity. This paper develops a methodology for managing the interactions among subcomputations, avoiding strict synchronization where concurrent or pipelined relationships are possible.\u000aOur approach to compiling parallel programs has two components: symbolic data access analysis and adaptive runtime support. We summarize the data access behavior of subcomputations (such as loop nests) and split them to expose concurrency and pipelining opportunities. The split transformation has been incorporated into an extended FORTRAN compiler, which outputs a FORTRAN 77 program augmented with calls to library routines written in C and a coarsegrained dataflow graph summarizing the exposed parallelism.\u000aThe compiler encodes symbolic information, including loop bounds and communication requirements, for an adaptive runtime system, which uses runtime information to improve the scheduling efficiency of irregular subcomputations. The runtime system incorporates algorithms that allocate processing resources to concurrently executing subcomputations and choose communication granularity. We have demonstrated that these dynamic techniques substantially improve performance on a range of production applications including climate modeling and xray tomography, expecially when large numbers of processors are available.
p1871
aVThis paper presents several algorithms to solve code generation and optimization problems specific to machines with distributed address spaces. Given a description of how the computation is to be partitioned across the processors in a machine, our algorithms produce an SPMD (single program multiple data) program to be run on each processor. Our compiler generated the necessary receive and send instructions, optimizes the communication by eliminating redundant communication and aggregating small messages into large messages, allocates space locally on each processor, and translates global data addresses to local addresses.\u000aOur techniques are based on an exact dataflow analysis on individual array element accesses. Unlike data dependence analysis, this analysis determines if two dynamic instances refer to the same value, and not just to the same location. Using this information, our compiler can handle more flexible data decompositions and find more opportunities for communication optimization than systems based on data dependence analysis.\u000aOur technique is based on a uniform framework, where data decompositions, computation decompositions and the data flow information are all represented as systems of linear inequalities. We show that the problems of communication code generation, local memory management, message aggregation and redundant data communication elimination can all be solved by projecting polyhedra represented by sets of inequalities onto lower dimensional spaces.
p1872
aVIn most programming language implementations, the compiler has detailed knowledge of the representations of and operations on primitive data typed and datatype constructors. In SCHEMEXEROX, this knowledge is almost entirely external to the compiler, in ordinary, procedural user code. The primitive representations and operations are embodied in firstclass \u201crepresentation types\u201d that are constructed and implemented in an abstract and highlevel fashion. Despite this abstractness, a few generallyuseful optimizing transformations are sufficient to allow the SCHEMEXEROX compiler to generate efficient code for the primitive operations, essentially as good as could be achieved using more contorted, traditional techniques.
p1873
aVNonlocal control transfer and exception handling have a long tradition in higherorder programming languages such as Common Lisp, Scheme and ML. However, each language stops short of providing a full and complementary approach\u2014control handling is provided only if the corresponding control operator is firstorder. In this work, we describe handlers in a higherorder control setting. We invoke our earlier theoretical result that all denotational models of control languages invariably include capabilities that handle control. These capabilities, when incorporated into the language, form an elegant and powerful higherorder generalization of the firstorder exceptionhandling mechanism.
p1874
aVLisp has shown that a programmable syntax macro system acts as an adjunct to the compiler that gives the programmer important and powerful abstraction facilities not provided by the language. Unlike simple token substitution macros, such as are provided by CPP (the C preprocessor), syntax macros operate on Abstract Syntax Trees (ASTs). Programmable syntax macro systems have not yet been developed for syntactically rich languages such as C because rich concrete syntax requires the manual construction of syntactically valid program fragments, which is a tedious, difficult, and error prone process. Also, using two languages, one for writing the program, and one for writing macros, is another source of complexity. This research solves these problems by having the macro language be a  minimal extension of the programming language, by introducing explicit code template operators into the macro language, and by using a type system to guarantee, at macro definition time, that all macros and macro functions only produce syntactically valid program fragments. The code template operators make the language context sensitive, which requires changes to the parser. The parser must perform type analysis in order to parse macro definitions, or to parse user code that invokes macros.
p1875
aVWe present a programming language with firstclass timing constructs, whose semantics is based on timeconstrained relationships between observable events. Since a system specification postulates timing relationships between events, realizing the specification in a program becomes a more straightforward process.\u000aUsing these constraints, as well as those imposed by data and control flow properties, our objective is to transform the code so that its worstcase execution time is consistent with its realtime requirements. To accomplish this goal we first translate an eventbased source program into intermediate code, in which the timing constraints are imposed on the code itself, and then use a compilation technique which synthesizes feasible code from the original source program.
p1876
aVThe allocation and disposal of memory is a ubiquitous operation in most programs. Rarely do programmers concern themselves with details of memory allocators; most assume that memory allocators provided by the system perform well. This paper presents a performance evaluation of the reference locality of dynamic storage allocation algorithms based on tracedriven simualtion of five large allocationintensive C programs. In this paper, we show how the design of a memory allocator can significantly affect the reference locality for various applications. Our measurements show that poor locality in sequentialfit allocation algorithms reduces program performance, both by increasing paging and cache miss rates. While increased paging can be debilitating on any architecture, cache misses rates are also important for modern computer architectures. We show that algorithms attempting to be spaceefficient by coalescing adjacent free objects show poor reference locality, possibly negating the benefits of space efficiency. At the other extreme, algorithms can expend considerable effort to increase reference locality yet gain little in total execution performance. Our measurements suggest an allocator design that is both very fast and has good locality of reference.
p1877
aVDynamic storage allocation is used heavily in many application areas including interpreters, simulators, optimizers, and translators. We describe research that can improve all aspects of the performance of dynamic storage allocation by predicting the lifetimes of shortlived objects when they are allocated. Using five significant, allocationintensive C programs, we show that a great fraction of all bytes allocated are shortlived (> 90% in all cases). Furthermore, we describe an algorithm for liftetime prediction that accurately predicts the lifetimes of 42\u201399% of all objects allocated. We describe and simulate a storage allocator that takes adavantage of lifetime prediction of shortlived objects and show that it can significantly improve a program's memory overhead and reference locality, and even, at times, improve CPU performance as well.
p1878
aVWe call a garbage collector conservative if it has only partial information about the location of pointers, and is thus forced to treat arbitrary bit patterns as though they might be pointers, in at least some cases. We show that some very inexpensive, but previously unused techniques can have dramatic impact on the effectiveness of conservative garbage collectors in reclaiming memory. Our most significant observation is that static data that appears to point to the heap should not result in misidentified references to the heap. The garbage collector has enough information to allocate around such references. We also observe that programming style has a significant impact on the amount of spuriously retained storage, typically even if the collector is not terribly conservative. Some fairly common C and C++ programming style significantly decrease the effectiveness of any garbage collector. These observations suffice to explain some of the different assessments of conservative collection that have appeared in the literature.
p1879
aVInstruction scheduling reorders and interleaves instruction sequences from different source statements. This impacts the task of a symbolic debugger, which attempts to present the user a picture of program execution that matches the source program. At a breakpoint B, if the value in the runtime location of a variable V may not correspond to the value the user expects V to have, then this variable is endangered at B. This paper describes an approach to detecting and recovering endangered variables caused by instruction scheduling. We measure the effects of instruction scheduling on a symbolic debugger's ability to recover source values at a breakpoint. This paper reports measurements for three C programs from the SPEC suite and a collection of programs from the Numerical Recipes, which have been compiled with a variant of a commercial C compiler.
p1880
aVThis paper describes a new language feature that allows dynamically allocated objects to be saved from deallocation by an automatic storage management system so that cleanup or other actions can be performed using the data stored within the objects. The program has full control over the timing of cleanup actions, which eliminates several potential problems and often  eliminates the need for critical sections in code that interacts with cleanup actions. Our implementation is \u201cgenerationfriendly\u201d in the sense that the additional overhead within a generationbased garbage collector is proportional to the work already done there, and the overhead within the mutator is proportional to the number of cleanup actions actually performed.
p1881
aVWe have implemented the first copying garbage collector that permits continuous unimpeded mutator access to the original objects during copying. The garbage collector incrementally replicates all accessible objects and uses a mutation log to bring the replicas uptodate with changes made by the mutator. An experimental implementation demonstrates that the costs of using our algorithm are small and that bounded pause times of 50 milliseconds can be readily achieved.
p1882
aVWe describe the implementation of a type checker for the functional programming language Haskell that supports the use of type classes. This extends the type system of ML to support overloading (adhoc polymorphism) and can be used to implement features such as equality types and numeric overloading in a simple and general way.\u000aThe theory of type classes is well understood, but the practical issues involved in the implementation of such systems have not received a great deal of attention. In addition to the basic type checking algorithm, an implmenentation of type classes also requires some form of program transformation. In all current Haskell compilers this takes the form of dictionary conversion, using functions as hidden parameters to overloaded values. We present efficient techniques for type checking and dictionary conversion. A number of optimizations and extensions to the basic type class sytems are also described.
p1883
aVIn order to simplify the compilation process, many compilers for higherorder languages use the continuationpassing style (CPS) transformation in a first phase to generate an intermediate representation of the source program. The salient aspect of this intermediate form is that all procedures take an argument that represents the rest of the computation (the \u201ccontinuation\u201d). Since the naive CPS transformation considerably increases the size of programs, CPS compilers perform reductions to produce a more compact intermediate representation. Although often implemented as a part of the CPS transformation, this step is conceptually a second phase. Finally, code generators for typical CPS compilers treat continuations specially in order to optimize the interpretation of continuation parameters.\u000aA thorough analysis of the abstract machine for CPS terms show that the actions of the code generator invert the naive CPS translation step. Put differently, the combined effect of the three phases is equivalent to a sourcetosource transformation that simulates the compaction phase. Thus, fully developed CPS compilers do not need to employ the CPS transformation but can achieve the same results with a simple sourcelevel transformation.
p1884
aVWe present a new framework in which considerations of both register allocation and instruction scheduling can be applied uniformly and simultaneously. In this framework an optimal coloring of a graph, called the parallel interference graph, provides an optimal register allocation and preserves the property that no false dependences are introduced, thus all the options for parallelism are kept for the scheduler to handle. For this framework we provide heuristics for trading off parallel scheduling with register spilling.
p1885
aVThis paper shows how to software pipeline a loop for minimal register pressure without sacrificing the loop's minimum execution time. This novel bidirectional slackscheduling method has been implemented in a FORTRAN compiler and tested on many scientific benchmarks. The empirical results\u2014when measured against an absolute lower bound on execution time, and against a novel scheduleindependent absolute lower bound on register pressure\u2014indicate nearoptimal performance.
p1886
aVLive range splitting techniques improve global register allocation by splitting the live ranges of variables into segments that are individually allocated registers. Load/store range analysis is a new technique for live range splitting that is based on reaching definition and live variable analyses. Our analysis localizes the profits and the register requirements of every access to every variable to provide a fine granularity of candidates for register allocation. Experiments on a suite of C and FORTRAN benchmark programs show that a graph coloring register allocator operating on load/store ranges often provides better allocations than the same allocator operating on live ranges. Experimental results also show that the computational cost of using load/store ranges for register allocation is moderately more than the cost of using live ranges.
p1887
aVTraditional list schedulers order instructions based on an optimistic estimate of the load delay imposed by the implementation. Therefore they cannot respond to variations in load latencies (due to cache hits or misses, congestion in the memory interconnect, etc.) and cannot easily be applied across different implementations. We have developed an alternative algorithm, known as balanced scheduling, that schedule instructions based on an estimate of the amount of instruction level parallelism in the program. Since scheduling decisions are programrather than machinebased, balanced scheduling is unaffected by implementation changes. Since it is based on the amount of instruction level parallelism that a program can support, it can respond better to variations in load latencies. Performance improvements over a traditional list scheduler on a Fortran workload and simulating several different machine types (cachebased workstations, large parallel machines with a multipath interconnect and a combination, all with nonblocking processors) are quite good, averaging between 3% and 18%.
p1888
aVIn this paper we present a set of isomorphic control transformations that allow the compiler to apply local scheduling techniques to acyclic subgraphs of the control flow graph. Thus, the code motion complexities of global scheduling are eliminated. This approach relies on a new technique, Reverse IfConversion (RIC), that transforms scheduled IfConverted code back to the control flow graph representation. This paper presents the predicate internal representation, the algorithms for RIC, and the correctness of RIC. In addition, the scheduling issues are addressed and an application to software pipelining is presented.
p1889
aVWe present an algorithm for incrementally including mayalias information into Static Single Assignment form by computing a sequence of increasingly precise (and correspondingly larger) partial SSA forms. Our experiments show significant speedup of our method over exhaustive use of mayalias information, as optimization problems converge well before most mayaliases are needed.
p1890
aVAbstract interpretation is a formal method that enables the static determination (i.e. at compiletime) of the dynamic properties (i.e. at runtime) of programs. We present an abstract interpretationbased method, called abstract debugging, which enables the static and formal debugging of programs, prior to their execution, by finding the origin of potential bugs as well as necessary conditions for these bugs not to occur at runtime. We show how invariant assertions and intermittent assertions, such as termination, can be used to formally debug programs. Finally, we show how abstract debugging can be effectively and efficiently applied to higherorder imperative programs with exceptions and jumps to nonlocal labels, and present the Syntox system that enables the abstract debugging of the Pascal language by the determination of the range of the scalar variables of programs.
p1891
aVWe present a new interprocedural modification side effects algorithm for C programs, that can discern side effects through generalpurpose pointer usage. Ours is the first complete design and implementation of such an algorithm. Preliminary performance findings support the practicality of the technique, which is based on our previous approximation algorithm for pointer aliases [LR92]. Each indirect store through a pointer variable is found, on average, to correspond to a store into 1.2 locations. This indicates that our programpointspecific pointer aliasing information is quite precise when used to determine the effects of these stores.
p1892
aVData flow analysis techniques have traditionally been restricted to the analysis of scalar variables. This retriction, however, imposes a limitation on the kinds of optimizations that can be performed in loops containing array references. We present a data flow framework for array reference analysis that provides the information needed in various optimizations targeted at sequential or finegrained parallel architectures. The framework extends the traditional scalar framework by incorporating iteration distance values into the analysis to qualify the computed data flow solution during the fixed point iteration. Analyses phrased in this framework are capable of discovering recurrent access patterns among array references that evolve during the execution of a loop. Applications of our  framework are discussed for register allocation, load/store optimizations, and controlled loop unrolling.
p1893
aVProgram analysis and optimization can be speeded up through the use of the dependence flow graph (DFG), a representation of program dependences which generalizes defuse chains and static single assignment (SSA) form. In this paper, we give a simple graphtheoretic description of the DFG and show how the DFG for a program can be constructed in O(EV) time. We then show how forward and backward dataflow analyses can be performed efficiently on the DFG, using constant propagation and elimination of partial redundancies as examples. These analyses can be framed as solutions of dataflow equations in the DFG. Our construction algorithm is of independent interest because it can be used to construct a program's control dependence graph in O(E) time and its SSA representation in O(EV) time, which are improvements over existing algorithms.
p1894
aVAn implementation of interprocedural constant propagation must model the transmission of values through each procedure. In the framework proposed by Callahan, Cooper, Kennedy, and Torczon in 1986, this intraprocedural propagation is modeled with a jump function. While Callahan et al. propose several kinds of jump functions, they give no data to help choose between them. This paper reports on a comparative study of jump function implementations. It shows that different jump functions produce different numbers of useful constants; it suggests a particular function, called the passthrough parameter jump function, as the most costeffective in practice.
p1895
aVThe widespread use of the continuationpassing style (CPS) transformation in compilers, optimizers, abstract interpreters, and partial evaluators reflects a common belief that the transformation has a positive effect on the analysis of programs. Investigations by Nielson [13] and Burn/Filho [5,6] support, to some degree, this belief with theoretical results. However, they do not pinpoint the source of increased abstract information and do not explain the observation of many people that continuationpassing confuses some conventional data flow analyses.\u000aTo study the impact of the CPS transformation on program analysis, we derive three canonical data flow analyzers for the core of an applicative higherorder programming language. The first analyzer is based on a direct  semantics  of the language, the second on a continuationsemantics of the language, and the last on the direct semantics of CPS terms. All analyzers compute the control flow graph of the source program and hence our results apply to a large class of data flow analyses. A comparison of the information gathered by our analyzers establishes the following points:1. The results of a direct analysis of a source program are incomparable to the results of an analysis of the equivalent CPS program. In other words, the translation of the source program to a CPS version may increase or decrease static information. The gain of information occurs in nondistributive analyses and is solely due to the duplication of the analysis of the continuation. The  loss of information is due to the confusion of distinct procedure returns.2. The analyzer based on the continuation semantics produces more accurate results than both direct analyzers, but again only in nondistributive analyses due to the duplication of continuations along every execution path. However, when the analyzer explicitly accounts for looping constructs, the results of the semanticCPS analysis are no longer computable.In view of these results, we argue that, in practice, a direct data flow analysis that relies on some amount of duplication would be as satisfactory as a CPS analysis.
p1896
aVGIVENTAKE is a code placement framework which uses a general producerconsumer concept. An advantage of GIVENTAKE over existing partial redundancy elimination techniques is its concept of production regions, instead of single locations, which can be beneficial for general latency hiding. GIVENTAKE guaranteed balanced production, that is, each production will be started and stopped once. The framework can also take advantage of production coming \u201cfor free,\u201d as induced by side effects, without disturbing balance. GIVENTAKE can place production either before or after consumption, and it also provides the option to hoist code out of potentially zerotrip loop (nest) constructs. GIVENTAKE uses a fast elimination method based on  Tarjan intervals, with a complexity linear in the program size in most cases.\u000aWe have implemented GIVENTAKE as part of a Fortran D compiler prototype, where it solves various communication generation problems associated with compiling dataparallel languages onto distributedmemory architectures.
p1897
aVWe describe methods that are able to count the number of integer solutions to selected free variables of a Presburger formula, or sum a polynomial over all integer solutions of selected free variables of a Presburger formula. This answer is given symbolically, in terms of symbolic constants (the remaining free variables in the Presburger formula).\u000aFor example, we can create a Presburger formula who's solutions correspond to the iterations of a loop. By counting these, we obtain an estimate of the execution time of the loop.\u000aIn more complicated applications, we can create Presburger formulas who's solutions correspond to the distinct memory locations or cache lines touched by a loop, the flops executed by a loop, or the array elements that need to be communicated at a particular point in a distributed computation. By counting the number of solutions, we can evaluate the computation/memory balance of a computation, determine if a loop is load balanced and evaluate message traffic and allocate message buffers.
p1898
aVWe present a method for automatically extracting parallel prefix programs from sequential loops, even in the presence of complicated conditional statements. Rather than searching for associative operators in the loop body directly, the method rests on the observation that functional composition itself is associative. Accordingly, we model the loop body as a multivalued function of multiple parameters, and look for a closedform representation of arbitrary compositions of loop body instances. Careful analysis of conditionals allows this search to succeed in cases where existing automatic methods fail. The method has been implemented and used to generate code for the iWarp parallel computer.
p1899
aVA new aggressive algorithm for the elimination of partially dead code is presented, i.e., of code which is only dead on some program paths. Besides being more powerful than the usual approaches to dead code elimination, this algorithm is optimal in the following sense: partially dead code remaining in the resulting program cannot be eliminated without changing the branching structure or the semantics of the program, or without impairing some program executions.\u000aOur approach is based on techniques for partial redundancy elimination. Besides some new technical problems there is a significant difference here: partial dead code elimination introduces second order effects, which we overcome by means of exhaustive motion and elimination steps. The optimality and  the uniqueness of the program obtained is proved by means of a new technique which is universally applicable and particularly useful in the case of mutually interdependent program optimizations.
p1900
aVPartial redundancy elimination is a code optimization with a long history of literature and implementation. In practice, its effectiveness depends on issues of naming and code shape. This paper shows that a combination of global reassociation and global value numbering can increase the effectiveness of partial redundancy elimination. By imposing a discipline on the choice of names and the shape of expressions, we are able to expose more redundancies.\u000aAs part of the work, we introduce a new algorithm for global reassociation of expressions. It uses global information to reorder expressions, creating opportunities for other optimizations. The new algorithm generalizes earlier work that ordered FORTRAN array address expressions to improve  otpimization [25].
p1901
aVIn this paper, we describe the program structure tree (PST), a hierarchical representation of program structure based on single entry single exit (SESE) regions of the control flow graph. We give a lineartime algorithm for finding SESE regions and for building the PST of arbitrary control flow graphs (including irreducible ones). Next, we establish a connection between SESE regions and control dependence equivalence classes, and show how to use the algorithm to find control regions in linear time. Finally, we discuss some applications of the PST. Many control flow algorithms, such as construction of Static Single Assignment form, can be speeded up by applying the algorithms in a divideandconquer style to each SESE region on its own. The PST is also used to speed up data flow  analysis by exploiting \u201csparsity\u201d. Experimental results from the Perfect Club and SPEC89 benchmarks confirm that the PST approach finds and exploits program structure.
p1902
aVAs microprocessor speeds increase, memory bandwidth is increasingly the performance bottleneck for microprocessors. This has occurred because innovation and technological improvements in processor design have outpaced advances in memory design. Most attempts at addressing this problem have involved hardware solutions. Unfortunately, these solutions do little to help the situation with respect to current microprocessors. In previous work, we developed, implemented, and evaluated an algorithm that exploited the ability of newer machines with widebuses to load/store multiple floatingpoint operands in a single memory reference. This paper describes a general code improvement algorithm that transforms code to better exploit the available memory bandwidth on existing microprocessors as  well as widebus machines. Where possible and advantageous, the algorithm coalesces narrow memory references into wide ones. An interesting characteristic of the algorithm is that some decisions about the applicability of the transformation are made at run time. This dynamic analysis significantly increases the probability of the transformation being applied. The code improvement transformation was implemented and added to the repertoire of code improvements of an existing retargetable optimizing back end. Using three current architectures as evaluation platforms, the effectiveness of the transformation was measured on a set of compute and memoryintensive programs. Interestingly, the effectiveness of the transformation varied significantly with respect to the instructionset   architecture of the tested platform. For one of the tested architectures, improvements in execution speed ranging from 5 to 40 percent were observed. For another, the improvements in execution speed ranged from 5 to 20 percent, while for yet another, the transformation resulted in slower code for all programs.
p1903
aVATOM (Analysis Tools with OM) is a single framework for building a wide range of customized program analysis tools. It provides the common infrastructure present in all codeinstrumenting tools; this is the difficult and timeconsuming part. The user simply defines the toolspecific details in instrumentation and analysis routines. Building a basic block counting tool like Pixie with ATOM requires only a page of code.\u000aATOM, using OM linktime technology, organizes the final executable such that the application program and user's analysis routines run in the same address space. Information is directly passed from the application program to the analysis routines through simple procedure calls instead of interprocess communication or files on disk. ATOM takes care that analysis routines do not interfere with the program's execution, and precise information about the program is presented to the analysis routines at all times. ATOM uses no simulation or interpretation.\u000aATOM has been implemented on the Alpha AXP under OSF/1. It is efficient and has been used to build a diverse set of tools for basic block counting, profiling, dynamic memory recording, instruction and data cache simulation, pipeline simulation, evaluating branch prediction, and instruction scheduling.
p1904
aVAs processor speeds continue to improve relative to mainmemory access times, cache performance is becoming an increasingly important component of program performance. Prior work on the cache performance of garbagecollected programs either argues or assumes that conventional garbagecollection methods will yield poor performance, and has therefore concentrated on new collection algorithms designed specifically to improve cachelevel reference locality.\u000aThis paper argues to the contrary: Many programs written in garbagecollected languages are naturally wellsuited to the directmapped caches typically found in modern computer systems. Garbagecollected programs written in a mostlyfunctional style should perform well when simple linear storage allocation and an infrequentlyrun generational compacting collector are employed; sophisticated collectors intended to improve cache performance are unlikely to be necessary. As locality becomes ever more important to program performance, programs of this kind may turn out to have a significant performance advantage over programs written in traditional languages.
p1905
aVOptimizing compilers require accurate dependence testing to enable numerous, performanceenhancing transformations. However, data dependence testing is a difficult problem, particularly in the presence of pointers. Though existing approaches work well for pointers to named memory locations (i.e. other variables), they are overly conservative in the case of pointers to unnamed memory locations. The latter occurs in the context of dynamic, pointerbased data structures, used in a variety of applications ranging from system software to computational geometry to Nbody and circuit simulations.\u000aIn this paper we present a new technique for performing more accurate data dependence testing in the presence of dynamic, pointerbased data structures. We will demonstrate its effectiveness by breaking false dependences that existing approaches cannot, and provide results which show that removing these dependences enables significant parallelization of a real application.
p1906
aVLanguages that support abstraction and modular structure, such as Standard ML, Modula, Ada, and (more or less) C++, may have deeply nested dependency hierarchies among source files. In ML the problem is particularly severe because ML's powerful parameterized module (functor) facility entails dependencies among implementation modules, not just among interfaces.To efficiently compile individual modules in such languages, it is useful (in ML, necessary) to infer, digest, and cache the static environment resulting from the compilation of each module.\u000aOur system provides a simple model of compilation and linkage that supports incremental recompilation (a restricted form of separate compilation) with typesafe linkage. This model is made available to user programs in the form of a set of internal compiler modules, a feature that we call the \u201cvisible compiler\u201d. The chief client of this interface is the IRM incremental recompilation manager from CMU.
p1907
aVExisting methods for alias analysis of recursive pointer data structures are based on two approximation techniques: klimiting, and storebased (or equivalently location or regionbased) approximations, which blur distinction between elements of recursive data structures. Although notable progress in interprocedural alias analysis has been recently accomplished, very little progress in the precision of analysis of recursive pointer data structures has been seen since the inception of these approximation techniques by Jones and Muchnick a decade ago. As a result, optimizing, verifying and parallelizing programs with pointers has remained difficult.\u000aWe present a new parametric framework for analyzing recursive pointer data structures which can express a new natural class of alias information not accessible to existing methods. The key idea is to represent alias information by pairs of symbolic access paths which are qualified by symbolic descriptions of the positions for which the alias pair holds.\u000aBased on this result, we present an algorithm for interprocedural mayalias analysis with pointers which on numerous examples that occur in practice is much more precise than recently published algorithms [CWZ90, He90, LR92, CBC93].
p1908
aVThis paper reports on the design, implementation, and empirical results of a new method for dealing with the aliasing problem in C. The method is based on approximating the pointsto relationships between accessible stack locations, and can be used to generate alias pairs, or used directly for other analyses and transformations.\u000aOur method provides contextsensitive interprocedural information based on analysis over invocation graphs that capture all calling contexts including recursive and mutuallyrecursive calling contexts. Furthermore, the method allows the smooth integration for handling general function pointers in C.\u000aWe illustrate the effectiveness of the method with empirical results from an implementation in the McCAT optimizing/parallelizing C compiler.
p1909
aVThis paper presents a new optimization technique that uses empty delay slots to improve code scheduling. We are able to split live ranges for free, by inserting spill code into empty delay slots. Splitting a live range can reduce interferences with other live ranges and can sometimes free registers. Live ranges no longer interfering with the split live range can sometimes make use of the extra register.\u000aOur algorithm, as a final pass over the code, exploits empty delay slots that would remain unused if spill code was not inserted. This paper proposes a variety of optimizations that use the extra registers generated from live range splitting, including coalescing live ranges and improving code scheduling. We present an algorithm for improving code scheduling and present implementation results.
p1910
aVThis paper describes RAP, a Register Allocator that allocates registers over the Program Dependence Graph (PDG) representation of a program in a hierarchical manner. The PDG program representation has been used successfully for scalar optimizations, the detection and improvement of parallelism for vector machines, multiple processor machines, and machines that exhibit instruction level parallelism, as well as debugging, the integration of different versions of a program, and translation of imperative programs for data flow machines. By basing register allocation on the PDG, the register allocation phase may be more easily integrated and intertwined with other optimization analyses and transformations. In addition, the advantages of a hierarchical approach to global register allocation can be attained without constructing an additional structure used solely for register allocation. Our experimental results have shown that on average, code allocated registers via RAP executed 2.7% faster than code allocated registers via a standard global register allocator.
p1911
aVAdvanced processor and machine architectures need optimizing compilers to be efficiently programmed in high level languages. Therefore the need for source level debuggers that can handle optimized programs is rising. One difficulty in debugging optimized code arises from the problem to determine the values of source code variables. To ensure correct debugger behaviour with optimized programs, the debugger not only has to determine the variable's storage location or associated register. It must also verify that the variable is current, i.e. the value determined from that location is really the value that the variable would have in unoptimized code. We will deduce requirements on algorithms for currentness determination and present an algorithm meeting this requirements that is more general than previous work. We will also give first experiences with an implementation. To our knowledge this is the first implementation of a currentness determination algorithm for globally optimized code.
p1912
aVWe present a pointer and array access checking technique that provides complete error coverage through a simple set of program transformations. Our technique, based on an extended safe pointer representation, has a number of novel aspects. Foremost, it is the first technique that detects all spatial and temporal access errors. Its use is not limited by the expressiveness of the language; that is, it can be applied successfully to compiled or interpreted languages with subscripted and mutable pointers, local references, and explicit and typeless dynamic storage management, e.g., C. Because it is a source level transformation, it is amenable to both compile and runtime optimization. Finally, its performance, even without compiletime optimization, is quite good. We implemented a prototype translator for the C language and analyzed the checking overheads of six nontrivial, pointer intensive programs. Execution overheads range from 130% to 540%; with text and data size overheads typically below 100%.
p1913
aVProgram slices have potential uses in many software engineering applications. Traditional slicing algorithms, however, do not work correctly on programs that contain explicit jump statements. Two similar algorithms were proposed recently to alleviate this problem. Both require the flowgraph and the program dependence graph of the program to be modified. In this paper, we propose an alternative algorithm that leaves these graphs intact and uses a separate graph to store the additional required information. We also show that this algorithm permits an extremely efficient, conservative adaptation for use with programs that contain only \u201cstructured\u201d jump statements.
p1914
aVType analysis of Prolog is of primary importance for highperformance compilers, since type information may lead to better indexing and to sophisticated specializations of unification and builtin predicates to name a few. However, these optimizations often require a sophisticated type inference system capable of inferring disjunctive and recursive types and hence expensive in computation time.\u000aThe purpose of this paper is to describe a type analysis system for Prolog based on abstract interpretation and type graphs (i.e. disjunctive rational trees) with this functionality. The system (about 15,000 lines of C) consists of the combination of a generic fixpoint algorithm, a generic pattern domain, and a type graph domain. The main contribution of the paper is to show that this approach can be engineered to be practical for mediumsized programs without sacrificing accuracy. The main technical contributions to achieve this result are (1) a novel widening operator for type graphs which appears to be accurate and effective in keeping the sizes of the graphs, and hence the computation time, reasonably small; (2) the use of the generic pattern domain to obtain a compact representation of equality constraints between subterms and to factor out sure structural information.
p1915
aVSome algorithms make critical internal use of updatable state, even though their external specification is purely functional. Based on earlier work on monads, we present a way of securely encapsulating stateful computations that manipulate multiple, named, mutable objects, in the context of a nonstrict, purelyfunctional language.The security of the encapsulation is assured by the type system, using parametricity. Intriguingly, this parametricity requires the provision of a (single) constant with a rank2 polymorphic type.
p1916
aVWe describe techniques for converting the intermediate code representation of a given program, as generated by a modern compiler, to another representation which produces the same runtime results, but can run faster on a superscalar machine. The algorithms, based on novel parallelization techniques for Very Long Instruction Word (VLIW) architectures, find and place together independently executable operations that may be far apart in the original code. i.e., they may be separated by many conditional branches or belong to different iterations of a loop. As a result, the functional units in the superscalar are presented with more work that can proceed in parallel, thus achieving higher performance than the approach of using hardware instruction dispatch techniques alone.While general scheduling techniques improve performance by removing idle pipeline cycles, to further improve performance on a superscalar with only a few functional units requires a reduction in the pathlength. We have designed a set of new algorithms for reducing pathlength and removing stalls due to branches, namely speculative loadstore motion out of loops, unspeculation, limited combining, basic block expansion, and prolog tailoring. These algorithms were implemented in a prototype version of the IBM RS/6000 xlc compiler and have shown significant improvement in SPEC integer benchmarks on the IBM POWER machines.Also, we describe a new technique to obtain profiling information with low overhead, and some applications of profiling directed feedback, including scheduling heuristics, code reordering and branch reversal.
p1917
aVCompilers for new machines with 64bit addresses must generate code that works when the memory used by the program is large. Procedures and global variables are accessed indirectly via global address tables, and calling conventions include code to establish the addressability of the appropriate tables. In the common case of a program that does not require a lot of memory, all of this can be simplified considerably, with a corresponding reduction in program size and execution time.We have used our linktime code modification system OM to perform program transformations related to global address use on the Alpha AXP. Though simple, many of these arewholeprogram optimizations that can be done only when we can see the entire  program at once, so linktime is an ideal occasion to perform them.This paper describes the optimizations performed and shows their effects on program size and performance. Relatively modest transformations, possible without moving code, improve the performance of SPEC benchmarks by an average of 1.5%. More ambitious transformations, requiring an understanding of program structure that is thorough but not difficult at linktime, can do even better, reducing program size by 10% or more, and improving performance by an average of 3.8%.Even a program compiled monolithically with interprocedural optimization can benefit nearly as much from this technique, if it contains staticallylinked precompiled library code. When the benchmark sources were compiled in this way, we  were still able to improve their performance by 1.35% with the modest transformations and 3.4% with the ambitious transformations.
p1918
aVInteger division remains expensive on today's processors as the\u000acost of integer multiplication declines. We present code sequences for\u000adivision by arbitrary nonzero integer constants and runtime invariants\u000ausing integer multiplication. The algorithms assume a two's complement\u000aarchitecture. Most also require that the upper half of an integer\u000aproduct be quickly accessible. We treat unsigned division, signed\u000adivision where the quotient rounds towards zero, signed division where\u000athe quotient rounds towards \u221e, and division where the result is known\u000aa priori to be exact. We give some\u000aimplementation results using the C compiler GCC.
p1919
aVOptimizing compilers (particularly parallel compilers) are constrained by their ability to predict performance consequences of the transformations they apply. Many factors, such as unknowns in control structures, dynamic behavior of programs, and complexity of the underlying hardware, make it very difficult for compilers to estimate the performance of the transformations accurately and efficiently. In this paper, we present a performance prediction framework that combines several innovative approaches to solve this problem. First, the framework employs a detailed, architecturespecific, but portable, cost model that can be used to estimate the cost of straight line code efficiently. Second, aggregated costs of loops and conditional statements are computed and represented symbolically. This avoids unnecessary, premature guesses and preserves the precision of the prediction. Third, symbolic comparison allows compilers to choose the best transformation dynamically and systematically. Some methodologies for applying the framework to optimizing parallel compilers to support automatic, performanceguided program restructuring are discussed.
p1920
aVDetermining the relative execution frequency of program regions is essential for many important optimization techniques, including register allocation, function inlining, and instruction scheduling. Estimates derived from profiling with sample inputs are generally regarded as the most accurate source of this information; static (compiletime) estimates are considered to be distinctly inferior. If static estimates were shown to be competitive, however, their convenience would outweigh minor gains from profiling, and they would provide a sound basis for optimization when profiling is impossible.We use quantitative metrics to compare estimates from static analysis to those derived from profiles. For C programs, simple techniques for predicting branches and loop counts suffice  to estimate intraprocedural frequency patterns with high accuracy. To determine interprocedural estimates successfully, we combine functionlevel information with a Markov model of control flow over the call graph to produce arc and basic block frequency estimates for the entire program.For a suite of 14 programs, including the C programs from the SPEC92 benchmark suite, we demonstrate that static estimates are competitive with those derived from profiles. Using simple heuristics, we can determine the most frequently executed blocks in each function with 81% accuracy. With the Markov model, we identify 80% of the frequently called functions. Combining the two techniques, we identify 76% of the most frequently executed call sites.
p1921
aVSpeculative execution on superscalar processors demands substantially better branch prediction than what has been previously available. In this paper we present code replication techniques that improve the accuracy of semistatic branch prediction to a level comparable to dynamic branch prediction schemes. Our technique uses profiling to collect information about the correlation between different branches and about the correlation between the subsequent outcomes of a single branch. Using this information and code replication the outcome of branches is represented in the program state. Our experiments have shown that the misprediction rate can almost be halved while the code size is increased by one third.
p1922
aVThis paper proposes an efficient technique for contextsensitive pointer analysis that is applicable to real C programs. For efficiency, we summarize the effects of procedures using partial transfer functions. A partial transfer function (PTF) describes the behavior of a procedure assuming that certain alias relationships hold when it is called. We can reuse a PTF in many calling contexts as long as the aliases among the inputs to the procedure are the same. Our empirical results demonstrate that this technique is successful\u2014a single PTF per procedure is usually sufficient to obtain completely contextsensitive results. Because many C programs use features such as type casts and pointer arithmetic to circumvent the highlevel type system, our algorithm is based on a lowlevel representation of memory locations that safely handles all the features of C. We have implemented our algorithm in the SUIF compiler system and we show that it runs efficiently for a set of C benchmarks.
p1923
aVModula3 supports development of modular programs by separating an object's interface from its implementation. This separation induces a runtime overhead in the implementation of objects, because it prevents the compiler from having complete information about a program's type hierarchy. This overhead can be reduced at link time, when the entire type hierarchy becomes available. We describe opportunities for linktime optimization of Modula3, present two linktime optimizations that reduce the runtime costs of Modula3's opaque types and methods, and show how linktime optimization could provide C++ which the benefits of opaques types at no additional runtime cost.Our optimization techniques are implemented in mld, a retargetable linker for the MIPS, SPARC, and Intel 486, mld links a machineindependent intermediate code that is suitable for linktime optimization and code generation. Linking intermediate code simplifies implementation of the optimizations and makes it possible to evaluate them on a wide range of architectures. mld's optimizations are effective: they reduce the total number of instructions executed by up to 14% and convert as many as 79% of indirect calls to direct calls.
p1924
aVCompiletime type information should be valuable in efficient compilation of statically typed functional languages such as Standard ML. But how should typedirected compilation work in real compilers, and how much performance gain will typebased optimizations yield? In order to support more efficient data representations and gain more experience about typedirected compilation, we have implemented a new typebased middle end and back end for the Standard ML of New Jersey compiler. We describe the basic design of the new compiler, identify a number of practical issues, and then compare the performance of our new compiler with the old nontypebased compiler. Our measurement shows that a combination of several simple typebased optimizations reduces heap allocation by 36%; and  improves the alreadyefficient code generated by the old nontypebased compiler by about 19% on a DECstation 500.
p1925
aVThis paper presents a fast and effective linear intraprocedural register allocation strategy that optimizes register usage across procedure calls. It capitalizes on our observation that while procedures that do not contain calls (syntactic leaf routines) account for under one third of all procedure activations, procedures that actually make no calls (effective leaf routines) account for over two thirds of all procedure activations. Wellsuited for both callerand callesave registers, our strategy employs a \u201clazy\u201d save mechanism that avoids saves for all effective leaf routines, an \u201ceager\u201d restore mechanism that reduces the effect of memory latency, and a \u201cgreedy\u201d register shuffling algorithm that does a remarkbly  good job  of minimizing the need for temporaries in setting up procedure calls.
p1926
aVRecently, software pipelining methods based on an ILP (Integer Linear Programming) framework have been successfully applied to derive rateoptimal schedules for architectures involving clean pipelines  pipelines without structural hazards. The problem for architectures beyond such clean pipelines remains open. One challenge is how, under a unified ILP framework, to simultaneously represent resource constraints for unclean pipelines, and the assignment or mapping of operations from a loop to those pipelines. In this paper we provide a framework which does exactly this, and in addition constructs rateoptimal software pipelined schedules. The proposed formulation and a solution method have been implemented and tested on a set of 1006 loops taken from various scientific and integer benchmark suites. The formulation found a rateoptimal schedule for 75% of the loops, and required a median time of only 2 seconds per loop on a Sparc 10/30.
p1927
aVTraditional list schedulers order instructions based on an optimistic  estimate of the load latency imposed by the hardware and therefore cannot respond to variations in memory latency caused by cache hits and misses on nonblocking architectures. In contrast, balanced scheduling schedules instructions based on an estimate of the amount of instructionlevel parallelism in the program. By scheduling independent instructions behind loads based on what the program can provide, rather than what the implementation stipulates in the best case (i.e., a cache hit), balanced scheduling can hide variations in memory latencies more effectively.Since its success depends on the amount of instructionlevel parallelism in the code, balanced scheduling should perform even better when more parallelism is available. In this study, we combine balanced scheduling with three compiler optimizations that increase instructionlevel parallelism: loop unrolling, trace scheduling and cache locality analysis. Using code generated for the DEC Alpha by the Multiflow compiler, we simulated a nonblocking processor architecture that closely models the Alpha 21164. Our results show that balanced scheduling benefits from all three optimizations, producing average speedups that range from 1.15 to 1.40, across the optimizations. More importantly, because of its ability to tolerate variations in load interlocks, it improves its advantage over traditional scheduling. Without the optimizations, balanced scheduled code is, on average, 1.05 times faster than that generated by a traditional scheduler; with them, its lead increases to 1.18.
p1928
aVThis paper presents the techniques used for the compilation of the dataflow, synchronous language SIGNAL. The key feature of the compiler is that it performs formal calculus on systems of boolean equations. The originality of the implementation of the compiler lies in the use of a tree structure to solve the equations.
p1929
aVStatic memory management replaces runtime garbage collection with compiletime annotations that make all memory allocation and deallocation explicit in a program. We improve upon the Tofte/Talpin regionbased scheme for compiletime memory management[TT94]. In the Tofte/Talpin approach, all values, including closures, are stored in regions. Region lifetimes coincide with lexical scope, thus forming a runtime stack of regions and eliminating the need for garbage collection. We relax the requirement that region lifetimes be lexical. Rather, regions are allocated late and deallocated as early as possible by explicit memory operations. The placement of allocation and deallocation annotations is determined by solving a system of constraints that expresses all possible annotations. Experiments show that our approach reduces memory requirements significantly, in some cases asymptotically.
p1930
aVDSP architectures typically provide indirect addressing modes with autoincrement and decrement. In addition, indexing mode is not available, and there are usually few, if any, generalpurpose registers. Hence, it is necessary to use address registers and perform address arithmetic to access automatic variables. Subsuming the address arithmetic into autoincrement and autodecrement modes improves the size of the generated code.In this paper we present a formulation of the problem of optimal storage assignment such that explicit instructions for address arithmetic are minimized. We prove that for the case of a single address register the decision problem is NPcomplete. We then generalize the problem to multiple address registers. For both cases heuristic algorithms are given. Our experimental results indicate an improvement of 3.
p1931
aVWe present compiler analyses and optimizations for explicitly parallel programs that communicate through a shared address space. Any type of code motion on explicitly parallel programs requires a new kind of analysis to ensure that operations reordered on one processor cannot be observed by another. The analysis, based on work by Shasha and Snir, checks for cycles among interfering accesses. We improve the accuracy of their analysis by using additional information from postwait synchronization, barriers, and locks.We demonstrate the use of this analysis by optimizing remote access on distributed memory machines. The optimizations include message pipelining, to allow multiple outstanding remote memory operations, conversion of twoway to oneway communication, and elimination of communication through data reuse. The performance improvements are as high as 2035% for programs running on a CM5 multiprocessor using the SplitC language as a global address layer.
p1932
aVWe present a unified approach to locality optimization that employs both data and control transformations. Data transformations include changing the array layout in memory. Control transformations involve changing the execution order of programs. We have developed new techniques for compiler optimizations for distributed sharedmemory machines, although the same techniques can be used for sequential machines with a memory hierarchy.Our compiler optimizations are based on an algebraic representation of data mappings and a new data locality model. We present a pure data transformation algorithm and an algorithm unifying data and control transformations. While there has been much work on control transformations, the opportunities for data transformations have been largely neglected. In fact, data transformations have the advantage of being applicable to programs that cannot be optimized with control transformations. The unified algorithm, which performs data and control transformations simultaneously, offers improvement over optimizations obtained by applying data and control transformations separately.The experimental results using a set of applications on a parallel machine show that the new optimizations improve performance significantly. These results are further analyzed using locality metrics with instrumentation and simulation.
p1933
aVRecent work on alias analysis in the presence of pointers has concentrated on contextsensitive interprocedural analyses, which treat multiple calls to a single procedure independently rather than constructing a single approximation to a procedure's effect on all of its callers. While contextsensitive modeling offers the potential for greater precision by considering only realizable callreturn paths, its empirical benefits have yet to be measured.This paper compares the precision of a simple, efficient, contextinsensitive pointsto analysis for the C programming language with that of a maximally contextsensitive version of the same analysis. We demonstrate that, for a number of pointerintensive benchmark programs, contextinsensitivity exerts little to no precision penalty. We also describe techniques for using the output of contextinsensitive analysis to improve the efficiency of contextsensitive analysis without affecting precision.
p1934
aVCurrent parallelizing compilers cannot identify a significant fraction of parallelizable loops because they have complex or statically insufficiently defined access patterns. As parallelizable loops arise frequently in practice, we advocate a novel framework for their identification: speculatively execute the loop as a doall, and apply a fully parallel data dependence test to determine if it had any crossiteration dependences; if the test fails, then the loop is reexecuted serially. Since, from our experience, a significant amount of the available parallelism in Fortran programs can be exploited by loops transformed through privatization and reduction parallelization, our methods can speculatively apply these transformations and then check their validity at runtime. Another important contribution of this paper is a novel method for reduction recognition which goes beyond syntactic pattern matching; it detects at runtime if the values stored in an array participate in a reduction operation, even if they are transferred through private variables and/or are affected by statically unpredictable control flow. We present experimental results on loops from the PERFECT Benchmarks which substantiate our claim that these techniques can yield significant speedups which are often superior to those obtainable by inspector/executor methods.
p1935
aVAssignment motion (AM) and expression motion (EM) are the basis of powerful and at the first sight incomparable techniques for removing partially redundant code from a program. Whereas AM aims at the elimination of complete assignments, a transformation which is always desirable, the more flexible EM requires temporaries to remove partial redundancies. Based on the observation that a simple program transformation enhances AM to subsume EM, we develop an algorithm that for the first time captures all second order effects between AM and EM transformations. Under usual structural restrictions, the worst case time complexity of our algorithm is essentially quadratic, a fact which explains the promising experience with our implementation.
p1936
aVPartial Redundancy Elimination (PRE) is a general scheme for suppressing partial redundancies which encompasses traditional optimizations like loop invariant code motion and redundant code elimination. In this paper we address the problem of performing this optimization interprocedurally. We use interprocedural partial redundancy elimination for placement of communication and communication preprocessing statements while compiling for distributed memory parallel machines.
p1937
aVThis paper presents a compiler optimization algorithm to reduce the run time overhead of array subscript range checks in programs without compromising safety. The algorithm is based on partial redundancy elimination and it incorporates previously developed algorithms for range check optimization. We implemented the algorithm in our research compiler, Nascent, and conducted experiments on a suite of 10 benchmark programs to obtain four results: (1) the execution overhead of naive range checking is high enough to merit optimization, (2) there are substantial differences between various optimizations, (3) loopbased optimizations that hoist checks out of loops are effective in eliminating about 98% of the range checks, and (4) more sophisticated analysis and optimization algorithms produce very marginal benefits.
p1938
aVWhen dense matrix computations are too large to fit in cache, previous research proposes tiling to reduce or eliminate capacity misses. This paper presents a new algorithm for choosing problemsize dependent tile sizes based on the cache size and cache line size for a directmapped cache. The algorithm eliminates both capacity and selfinterference misses and reduces crossinterference misses. We measured simulated miss rates and execution times for our algorithm and two others on a variety of problem sizes and cache organizations. At higher set associativity, our algorithm does not always achieve the best performance. However on directmapped caches, our algorithm improves simulated miss rates and measured execution times when compared with previous work.
p1939
aVEEL (Executable Editing Library) is a library for building tools to analyze and modify an executable (compiled) program. The systems and languages communities have built many tools for error detection, fault isolation, architecture translation, performance measurement, simulation, and optimization using this approach of modifying executables. Currently, however, tools of this sort are difficult and timeconsuming to write and are usually closely tied to a particular machine and operating system. EEL supports a machine and systemindependent editing model that enables tool builders to modify an executable without being aware of the details of the underlying architecture or operating system or being concerned with the consequences of deleting instructions or adding foreign code.
p1940
aVGenerational techniques have been very successful in reducing the impact of garbage collection algorithms upon the performance of programs. However, all generational algorithms occasionally promote objects that later become garbage, resulting in an accumulation of garbage in older generations. Reclaiming this tenured garbage without resorting to collecting the entire heap is a difficult problem. In this paper, we describe a mechanism that extends existing generational collection algorithms by allowing them to reclaim tenured garbage more effectively. In particular, our dynamic threatening boundary mechanism divides memory into two spaces, one for shortlived, and another for longlived objects. Unlike previous work, our collection mechanism can  dynamically adjust the boundary between these two spaces either forward or backward in time, essentially allowing data to become untenured. We describe an implementation of the dynamic threatening boundary mechanism and quantify its associated costs. We also describe a policy for setting the threatening boundary and evaluate its performance relative to existing generational collection algorithms. Our results show that a policy that uses the dynamic threatening boundary mechanism is effective at reclaiming tenured garbage.
p1941
aVWe present a flowsensitive interprocedural constant propagation algorithm, which supports recursion while only performing one flowsensitive analysis of each procedure. We present experimental results which show that this method finds substantially more constants than previous methods and is efficient in practice. We introduce new metrics for evaluating interprocedural constant propagation algorithms which measure the number of interprocedural constant values that are propagated. We use these metrics to provide further experimental results for our algorithm.
p1942
aVThe control dependence relation is used extensively in restructuring compilers. This relation is usually represented using the control dependence graph; unfortunately, the size of this data structure can be quadratic in the size of the program, even for some structured programs. In this paper, we introduce a data structure called the augmented postdominator tree (APT) which is constructed in space and time proportional to the size of the program, and which can answer control dependence queries in time proportional to the size of the output. Therefore, APT is an optimal representation of control dependence. We also show that using APT, we can compute SSA graphs, as well as sparse dataflow evaluator graphs, in time proportional to the size of the program. Finally, we put APT in perspective by showing that it can be viewed as a factored representation of control dependence graph in which filtered search is used to answer queries.
p1943
aVIn this paper, we present an almostlinear time algorithm for constructing Gated Single Assignment (GSA), which is SSA augmented with gating functions at nodes. The gating functions specify the control dependences for each reaching definition at a node. We introduce a new concept of gating path, which is path in the control flow graph from the immediate dominator u of a node v to v, such that every node in the path is dominated by u. Previous algorithms start with function placement, and then traverse the control flow graph to compute the gating functions. By formulating the problem into gating path construction, we are able to identify not only a node, but also a gating path expression which defines a gating function for the node.
p1944
aVThe ability to predict at compile time the likelihood of a particular branch being taken provides valuable information for several optimizations, including global instruction scheduling, code layout, function inlining, interprocedural register allocation and many high level optimizations. Previous attempts at static branch prediction have either used simple heuristics, which can be quite inaccurate, or put the burden onto the programmer by using execution profiling data or source code hints.This paper presents a new approach to static branch prediction called value range propagation. This method tracks the weighted value ranges of variables through a program, much like constant propagation. These value ranges may be either numeric of symbolic in nature.    Branch prediction is then performed by simply consulting the value range of the appropriate variable. Heuristics are used as a fallback for cases where the value range of the variable cannot be determined statically. In the process, value range propagationsubsumes both constant propagation and copy propagation.Experimental results indicate that this approach produces significantly more accurate predictions than the best existing heuristic techniques. The value range propagation method can be implemented over any \u201cfactored\u201d dataflow representation with a static single assignment property (such as SSA form or a dependence flow graph where the variables have been renamed to achieve single assignment). Experimental results indicate that   the technique maintains the linear runtime behavior of constant propagation experienced in practice.
p1945
aVCorrectly predicting the direction that branches will take is increasingly important in today's wideissue computer architectures. The name programbased branch prediction is given to static branch prediction techniques that base their prediction on a program's structure. In this paper, we investigate a new approach to programbased branch prediction that uses a body of existing programs to predict the branch behavior in a new program. We call this approach to programbased branch prediction, evidencebased static prediction, or ESP. The main idea of ESP is that the behavior of a corpus of programs can be used to infer the behavior of new programs. In this paper, we use a neural network to map static features associated with each  branch to the probability that the branch will be taken. ESP shows significant advantages over other prediction mechanisms. Specifically, it is a programbased technique, it is effective across a range of programming languages and programming styles, and it does not rely on the use of expertdefined heuristics. In this paper, we describe the application of ESP to the problem of branch prediction and compare our results to existing programbased branch predictors. We also investigate the applicability of ESP across computer architectures, programming languages, compilers, and runtime systems. Averaging over a body of 43 C and Fortran programs, ESP branch prediction results in a miss rate of 20%, as compared with the 25% miss rate obtained using the best existing  programbased   heuristics.
p1946
aVDynamic dispatching is a major source of runtime overhead in objectoriented languages, due both to the direct cost of method lookup and to the indirect effect of preventing other optimizations. To reduce this overhead, optimizing compilers for objectoriented languages analyze the classes of objects stored in program variables, with the goal of bounding the possible classes of message receivers enough so that the compiler can uniquely determine the target of a message send at compile time and replace the message send with a direct procedure call. Specialization is one important technique for improving the precision of this static class information: by compiling multiple versions of a method, each applicable to a subset of the possible argument classes of the  method, more precise static information about the classes of the method's arguments is obtained. Previous specialization strategies have not been selective about where this technique is applied, and therefore tended to significantly increase compile time and code space usage, particularly for large applications. In this paper, we present a more general framework for specialization in objectoriented languages and describe a goal directed specialization algorithm that makes selective decisions to apply specialization to those cases where it provides the highest benefit. Our results show that our algorithm improves the performance of a group of sizeable programs by 65% to 275% while increasing  compiled code space requirements by only 4% to 10%. Moreover, when compared to the previous  stateoftheart specialization scheme, our algorithm improves performance by 11% to 67% while simultaneously reducing code space requirements by 65% to 73%.
p1947
aVThis paper is a scientific comparison of two code generation techniques with identical goals   generation of the best possible software pipelined code for computers with instruction level parallelism. Both are variants of modulo scheduling, a framework for generation of software pipelines pioneered by Rau and Glaser [RaG181], but are otherwise quite dissimilar.One technique was developed at Silicon Graphics and is used in the MIPSpro compiler. This is the production compiler for SGI's systems which are based on the MIPS R8000 processor [Hsu94]. It is essentially a branch and bound enumeration of possible schedules with extensive pruning. This method is heuristic because of the way it prunes and also because of the interaction between register allocation and scheduling.The second technique aims to produce optimal results by formulating the scheduling and register allocation problem as an integrated integer linear programming (ILP1) problem. This idea has received much recent exposure in the literature [AlGoGa95, Feautrier94, GoAlGa94a, GoAlGa94b, Eichenberger95], but to our knowledge all previous implementations have been too preliminary for detailed measurement and evaluation. In particular, we believe this to be the first published measurement of runtime performance for ILP based generation of software pipelines.A particularly valuable result of this study was evaluation of the heuristic pipelining technology in the SGI compiler. One of the motivations behind the McGill research was the hope that optimal software pipelining, while not in itself practical for use in production compilers, would be useful for their evaluation and validation. Our comparison has indeed provided a quantitative validation of the SGI compiler's pipeliner, leading us to increased confidence in both techniques.
p1948
aVTraditional firstclass continuation mechanisms allow a captured continuation to be invoked multiple times. Many continuations, however, are invoked only once. This paper introduces oneshot continuations, shows how they interact with traditional multishot continuations, and describes a stackbased implementation of control that handles both oneshot and multishot continuations. The implementation eliminates the copying overhead for oneshot continuations that is inherent in multishot continuations.
p1949
aVThis paper presents a fast and accurate algorithm for printing floatingpoint numbers in both free and fixedformat modes. In freeformat mode, the algorithm generates the shortest, correctly rounded output string that converts to the same number when read back in, accommodating whatever rounding mode the reader uses. In fixedformat mode, the algorithm generates a correctly rounded output string using special # marks to denote insignificant trailing digits. For both modes, the algorithm employs a fast estimator to scale floatingpoint numbers efficiently.
p1950
aVMany analysis problems can be cast in the form of evaluating minimal models of a logic program. Although such formulations are appealing due to their simplicity and declarativeness, they have not been widely used in practice because, either existing logic programming systems do not guarantee completeness, or those that do have been viewed as too inefficient for integration into a compiler. The objective of this paper is to reexamine this issue in the context of recent advances in implementation technologies of logic programming systems.We find that such declarative formulations can indeed be used in practical systems, when combined with the appropriate tool for evaluation. We use existing formulations of analysis problems   groundness analysis of logic programs, and strictness analysis of functional programs   in this case study, and the XSB system, a tablebased logic programming system, as the evaluation tool of choice. We give experimental evidence that the resultant groundness and strictness analysis systems are practical in terms of both time and space. In terms of implementation effort, the analyzers took less than 2 manweeks (in total), to develop, optimize and evaluate. The analyzer itself consists of about 100 lines of tabled Prolog code and the entire system, including the components to read and preprocess input programs and to collect the analysis results, consists of about 500 lines of code.
p1951
aVThis paper evaluates the design and implementation of Omniware: a safe, efficient, and languageindependent system for executing mobile program modules. Previous approaches to implementing mobile code rely on either language semantics or abstract machine interpretation to enforce safety. In the former case, the mobile code system sacrifices universality to gain safety by dictating a particular source language or type system. In the latter case, the mobile code system sacrifices performance to gain safety through abstract machine interpretation.Omniware uses software fault isolation, a technology developed to provide safe extension code for databases and operating systems, to achieve a unique combination of languageindependence and excellent performance. Software fault isolation uses only the semantics of the underlying processor to determine whether a mobile code module can corrupt its execution environment. This separation of programming language implementation from program module safety enables our mobile code system to use a radically simplified virtual machine as its basis for portability. We measured the performance of Omniware using a suite of four SPEC92 programs on the Pentium, PowerPC, Mips, and Sparc processor architectures. Including the overhead for enforcing safety on all four processors, OmniVM executed the benchmark programs within 21% as fast as the optimized, unsafe code produced by the vendorsupplied compiler.
p1952
aVWe describe the design and implementation of a compiler that automatically translates ordinary programs written in a subset of ML into code that generates native code at run time. Runtime code generation can make use of values and invariants that cannot be exploited at compile time, yielding code that is often superior to statically optimal code. But the cost of optimizing and generating code at run time can be prohibitive. We demonstrate how compiletime specialization can reduce the cost of runtime code generation by an order of magnitude without greatly affecting code quality. Several benchmark programs are examined, which exhibit an average cost of only six cycles per instruction generated at run time.
p1953
aVDynamic compilation enables optimization based on the values of invariant data computed at runtime. Using the values of these runtime constants, a dynamic compiler can eliminate their memory loads, perform constant propagation and folding, remove branches they determine, and fully unroll loops they bound. However, the performance benefits of the more efficient, dynamicallycompiled code are offset by the runtime cost of the dynamic compile. Our approach to dynamic compilation strives for both fast dynamic compilation and highquality dynamicallycompiled code: the programmer annotates regions of the programs that should be compiled dynamically; a static, optimizing compiler automatically produces preoptimized machinecode templates, using a pair of dataflow analyses that identify which variables will be constant at runtime; and a simple, dynamic compiler copies the templates, patching in the computed values of the runtime constants, to produce optimized, executable code. Our work targets general purpose, imperative programming languages, initially C. Initial experiments applying dynamic compilation to C programs have produced speedups ranging from 1.2 to 1.8.
p1954
aVDynamic code generation is the creation of executable code at runtime. Such "onthefly" code generation is a powerful technique, enabling applications to use runtime information to improve performance by up to an order of magnitude [4, 8,20, 22, 23].Unfortunately, previous generalpurpose dynamic code generation systems have been either inefficient or nonportable. We present VCODE, a retargetable, extensible, very fast dynamic code generation system. An important feature of VCODE is that it generates machine code "inplace" without the use of intermediate data structures. Eliminating the need to construct and consume an intermediate representation at runtime makes VCODE both efficient and extensible. VCODE dynamically generates code at an approximate cost of six to ten instructions per generated instruction, making it over an order of magnitude faster than the most efficient generalpurpose code generation system in the literature [10].Dynamic code generation is relatively well known within the compiler community. However, due in large part to the lack of a publicly available dynamic code generation system, it has remained a curiosity rather than a widely used technique. A practical contribution of this work is the free, unrestricted distribution of the VCODE system, which currently runs on the MIPS, SPARC, and Alpha architectures.
p1955
aVWe propose a new approach to adding objects to Standard ML (SML) based on explicit declarations of object types, object constructors, and subtyping relationships, with a generalization of the SML case statement to a "typecase" on object types. The language, called Object ML (OML), has a type system that conservatively extends the SML type system, preserves sound static typing, and permits type inference. The type system sacrifices some of the expressiveness found in recently proposed schemes, but has the virtue of simplicity. We give examples of how features found in other objectoriented languages can be emulated in OML, discuss the formal properties of OML, and describe some implementation issues.
p1956
aVA flowdirected inlining strategy uses information derived from controlflow analysis to specialize and inline procedures for functional and objectoriented languages. Since it uses controlflow analysis to identify candidate call sites, flowdirected inlining can inline procedures whose relationships to their call sites are not apparent. For instance, procedures defined in other modules, passed as arguments, returned as values, or extracted from data structures can all be inlined. Flowdirected inlining specializes procedures for particular call sites, and can selectively inline a particular procedure at some call sites but not at others. Finally, flowdirected inlining encourages modular implementations: controlflow analysis, inlining, and postinlining optimizations are all orthogonal components. Results from a prototype implementation indicate that this strategy effectively reduces procedure call overhead and leads to significant reduction in execution time.
p1957
aVHigh performance compilers increasingly rely on accurate modeling of the machine resources to efficiently exploit the instruction level parallelism of an application. In this paper, we propose a reduced machine description that results in faster detection of resource contentions while preserving the scheduling constraints present in the original machine description. The proposed approach reduces a machine description in an automated, errorfree, and efficient fashion, Moreover, it fully supports schedulers that backtrack and process operations in arbitrary order. Reduced descriptions for the DEC Alpha 21064, MIPS R3000/R3010, and Cydra 5 result in 4 to 7 times faster detection of resource contentions and require 22 to 90% of the memory storage used by the original machine descriptions. Precise measurement for the Cydra 5 indicates that reducing the machine description results in a 2.9 times faster contention query module.
p1958
aVTwo key steps in the compilation of strict functional languages are the conversion of higherorder functions to data structures (closures) and the transformation to tailrecursive style. We show how to perform both steps at once by applying firstorder offline partial evaluation to a suitable interpreter. The resulting code is easy to transliterate to lowlevel C or native code. We have implemented the compilation to C; it yields a performance comparable to that of other modern SchemetoC compilers. In addition, we have integrated various optimizations such as constant propagation, higherorder removal, and arity raising simply by modifying the underlying interpreter. Purely firstorder methods suffice to achieve the transformations. Our approach is an instance of semanticsdirected compiler generation.
p1959
aVGiven a repeated computation, part of whose input context remains invariant across all repetitions, program staging improves performance by separating the computation into two phases. An early phase executes only once, performing computations depending only on invariant inputs, while a late phase repeatedly performs the remainder of the work given the varying inputs and the results of the early computations.Common staging techniques based on dynamic compilation statically construct an early phase that dynamically generates object code customized for a particular input context. In effect, the results of the invariant computations are encoded as the compiled code for the late phase.This paper describes an alternative approach in which the results of early computations are encoded as a data structure, allowing both the early and late phases to be generated statically. By avoiding dynamic code manipulation, we give up some optimization opportunities in exchange for significantly lower dynamic space/time overhead and reduced implementation complexity.
p1960
aVRelocation adjusts machine instructions to account for changes in the locations of the instructions themselves or of external symbols to which they refer. Standard linkers implement a finite set of relocation transformations, suitable for a single architecture. These transformations are enumerated, named, and engraved in a machinedependent objectfile format, and linkers must recognize them by name. These names and their associated transformations are an unnecessary source of machinedependence.The New Jersey MachineCode Toolkit is an application generator. It helps programmers create applications that manipulate machine code, including linkers. Guided by a short instructionset specification, the toolkit generates the bitmanipulating code. Instructions are described by constructors, which denote functions mapping lists of operands to instructions' binary representations. Any operand can be designated as "relocatable," meaning that the operand's value need not be known at the time the instruction is encoded. For instructions with relocatable operands, the toolkit computes relocating transformations. Tool writers can use the toolkit to create machineindependent software that relocates machine instructions. mld, a retargetable linker built with the toolkit, needs only 20 lines of C code for relocation, and that code is machineindependent.The toolkit discovers relocating transformations by currying encoding functions. An attempt to encode an instruction with a relocatable operand results in the creation of a closure. The closure can be applied when the values of the relocatable operands become known. Currying provides a general, machineindependent method of relocation.Currying rewrites a &lambda;term into two nested &lambda;terms. The standard implementation has the first &lambda; allocate a closure and store therein its operands and a pointer to the second &lambda;. Using this strategy in the toolkit means that, when it builds an application, the toolkit generates code for many different inner &lambda;terms one for each instruction that uses a relocatable address. Hoisting some of the computation out of the second &lambda; into the first makes many of the second &lambda;s identical a handful are enough for a whole instruction set. This optimization reduces the size of machinedependent assembly and linking code by 15 20% for the MIPS, SPARC, and PowerPC, and by about 30% for the Pentium. It also makes the second &lambda;s equivalent to relocating transformations named in standard objectfile formats.
p1961
aVRecent sharedmemory parallel computer systems offer the exciting possibility of customizing memory coherence protocols to fit an application's semantics and sharing patterns. Custom protocols have been used to achieve messagepassing performance while retaining the convenient programming model of a global address space and to implement highlevel language constructs. Unfortunately, coherence protocols written in a conventional language such as C are difficult to write, debug, understand, or modify. This paper describes Teapot, a small, domainspecific language for writing coherence protocols. Teapot uses continuations to help reduce the complexity of writing protocols. Simple static analysis in the Teapot compiler eliminates much of the overhead of continuations and results in protocols that run nearly as fast as handwritten C code. A Teapot specification can be compiled both to an executable coherence protocol and to input for a model checking system, which permits the specification to be verified. We report our experiences coding and verifying several protocols written in Teapot, along with measurements of the overhead incurred by writing a protocol in a higherlevel language.
p1962
aVBuilding compilers that generate correct code is difficult. In this paper we present a compiler testing technique that closes the gap between actual compiler implementations and correct compilers. Using formal specifications of procedure calling conventions, we have built a targetsensitive test suite generator that builds test cases for a specific aspect of compiler code generators the procedure calling sequence generator. By exercising compilers with these targetspecific test suites, our automated testing tool has exposed bugs in every compiler tested. These compilers include ones that have been in heavy use for many years. The detected bugs cause more than 14,000 test cases to fail.
p1963
aVReplay of sharedmemory program execution is desirable in many domains including cyclic debugging, fault tolerance and performance monitoring. Past approaches to repeatable execution have focused on the problem of reexecuting the sharedmemory access patterns in parallel programs. With the proliferation of operating system supported threads and shared memory for uniprocessor programs, there is a clear need for efficient replay of concurrent applications. The solutions for parallel systems can be performance prohibitive when applied to the uniprocessor case. We present an algorithm, called the repeatable scheduling algorithm, combining scheduling and instruction counts to provide an invariant for efficient, language independent replay of concurrent sharedmemory applications. The approach is shown to have trace overheads that are independent of the amount of sharing that takes place. An implementation for cyclic debugging on Mach 3.0 is evaluated and benchmarks show typical performance overheads of around 10%. The algorithm implemented is compared with optimal eventbased tracing and shown to do better with respect to the number of events monitored or number of events logged, in most cases by several orders of magnitude.
p1964
aVConventional dataflow analysis computes information about what facts may or will not hold during the execution of a program. Sometimes it is useful, for program optimization, to know how often or with what probability a fact holds true during program execution. In this paper, we provide a precise formulation of this problem for a large class of dataflow problems   the class of finite bidistributive subset problems. We show how it can be reduced to a generalization of the standard dataflow analysis problem, one that requires a sumoverallpaths quantity instead of the usual meetoverallpaths quantity. We show that Kildall's result expressing the meetoverallpaths value as a maximalfixedpoint carries over to the generalized setting. We then outline ways to adapt the standard dataflow analysis algorithms to solve this generalized problem, both in the intraprocedural and the interprocedural case.
p1965
aVWe present a new eliminationbased framework for exhaustive and incremental data flow analysis using the DJ graph representation of a program. Unlike the previous approaches to eliminationbased incremental data flow analysis, our approach can handle arbitrary nonstructural and structural changes to program flowgraphs, including those causing irreducibility. We show how our approach is related to (iterated) dominance frontiers, and exploit this relationship to establish the complexity of our exhaustive analysis and to aid the design of our incremental analysis.
p1966
aVMrSpidey is a userfriendly, interactive static debugger for Scheme. A static debugger supplements the standard debugger by analyzing the program and pinpointing those program operations that may cause runtime errors such as dereferencing the null pointer or applying nonfunctions. The program analysis of MrSpidey computes value set descriptions for each term in the program and constructs a value flow graph connecting the set descriptions. Using the set descriptions, MrSpidey can identify and highlight potentially erroneous program operations, whose cause the programmer can then explore by selectively exposing portions of the value flow graph.
p1967
aVAlthough compiler optimizations play a crucial role in the performance of modern computer systems, debugger technology has lagged behind in its support of optimization. Yet debugging the unoptimized translation is often impossible or futile, so handling of code optimizations in the debugger is necessary. But compiler optimizations make it difficult to provide sourcelevel debugger functionality: Global optimizations can cause the runtime value of a variable to be inconsistent with the sourcelevel value expected at a breakpoint; such variables are called endangered variables. A debugger must detect and warn the user of endangered variables otherwise the user may draw incorrect conclusions about the program. This paper presents a new algorithm for detecting variables that are endangered due to global scalar optimization. Our approach provides more precise classifications of variables and is still simpler than past approaches. We have implemented and evaluated our techniques in the context of the cmcc optimizing C compiler. We describe the compiler extensions necessary to perform the required bookkeeping of compiler optimization. We present measurements of the effect of optimizations on a debugger's ability to present the expected values of variables to the user.
p1968
aVMany important classes of bugs result from invalid assumptions about the results of functions and the values of parameters and global variables. Using traditional methods, these bugs cannot be detected efficiently at compiletime, since detailed crossprocedural analyses would be required to determine the relevant assumptions. In this work, we introduce annotations to make certain assumptions explicit at interface points. An efficient static checking tool that exploits these annotations can detect a broad class of errors including misuses of null pointers, uses of dead storage, memory leaks, and dangerous aliasing. This technique has been used successfully to fix memory management problems in a large program.
p1969
aVThis paper presents a new analysis technique, commutativity analysis, for automatically parallelizing computations that manipulate dynamic, pointerbased data structures. Commutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e. generate the same final result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code.We have implemented a prototype compilation system that uses commutativity analysis as its primary analysis framework. We have used this system to automatically parallelize two complete scientific computations: the BarnesHut Nbody solver and the Water code. This paper presents performance results for the generated parallel code running on the Stanford DASH machine. These results provide encouraging evidence that commutativity analysis can serve as the basis for a successful parallelizing compiler.
p1970
aVReducing communication cost is crucial to achieving good performance on scalable parallel machines. This paper presents a new compiler algorithm for global analysis and optimization of communication in dataparallel programs. Our algorithm is distinct from existing approaches in that rather than handling loopnests and array references one by one, it considers all communication in a procedure and their interactions under different placements before making a final decision on the placement of any communication. It exploits the flexibility resulting from this advanced analysis to eliminate redundancy, reduce the number of messages, and reduce contention for cache and communication buffers, all in a unified framework. In contrast, single loopnest analysis often retains redundant communication, and more aggressive dataflow analysis on array sections can generate too many messages or cache and buffer contention. The algorithm has been implemented in the IBM pHPF compiler for High Performance Fortran. During compilation, the number of messages per processor goes down by as much as a factor of nine for some HPF programs. We present performance results for the IBM SP2 and a network of Sparc workstations (NOW) connected by a Myrinet switch. In many cases, the communication cost is reduced by a factor of two.
p1971
aVGUM is a portable, parallel implementation of the Haskell functional language. Despite sustained research interest in parallel functional programming, GUM is one of the first such systems to be made publicly available.GUM is messagebased, and portability is facilitated by using the PVM communications harness that is available on many multiprocessors. As a result, GUM is available for both sharedmemory (Sun SPARCserver multiprocessors) and distributedmemory (networks of workstations) architectures. The high messagelatency of distributed machines is ameliorated by sending messages asynchronously, and by sending large packets of related data in each message.Initial performance figures demonstrate absolute speedups relative to the best sequential compiler technology. To improve the performance of a parallel Haskell program GUM provides tools for monitoring and visualising the behaviour of threads and of processors during execution.
p1972
aVA conservative garbage collector can typically be used with conventionally compiled programs written in C or C++. But two safety issues must be considered. First, the source code must not hide pointers from the garbage collector. This primarily requires stricter adherence to existing restrictions in the language definition. Second, we must ensure that the compiler will not perform transformations that invalidate this requirement.We argue that the same technique can be used to address both issues. We present an algorithm for annotating source or intermediate code to either check the validity of pointer arithmetic in the source, or to guarantee that under minimal, clearly defined assumptions about the compiler, the optimizer cannot "disguise" pointers. We discuss an implementation based on a preprocessor for the GNU C compiler (gcc), and give some measurements of program slow down.
p1973
aVThis paper presents a simple, powerful and flexible technique for reasoning about and translating the goaldirected evaluation of programming language constructs that either succeed (and generate sequences of values) or fail. The technique generalizes the Byrd Box, a wellknown device for describing Prolog backtracking.
p1974
aVtcc is a compiler that provides efficient and highlevel access to dynamic code generation. It implements the 'C ("TickC") programming language, an extension of ANSI C that supports dynamic code generation [15]. 'C gives power and flexibility in specifying dynamically generated code: whereas most other systems use annotations to denote runtime invariants. 'C allows the programmer to specify and compose arbitrary expressions and statements at run time. This degree of control is needed to efficiently implement some of the most important applications of dynamic code generation, such as "just in time" compilers [17] and efficient simulators [10, 48, 46].The paper focuses on the techniques that allow tcc to provide 'C's flexibility and expressiveness without sacrificing runtime code generation efficiency. These techniques include fast register allocation, efficient creation and composition of dynamic code specifications, and linktime analysis to reduce the size of dynamic code generators. tcc also implements two different dynamic code generation strategies, designed to address the tradeoff of dynamic compilation speed versus generated code quality. To characterize the effects of dynamic compilation, we present performance measurements for eleven programs compiled using tcc. On these applications, we measured performance improvements of up to one order of magnitude.To encourage further experimentation and use of dynamic code generation, we are making the tcc compiler available in the public domain. This is, to our knowledge, the first highlevel dynamic compilation system to be made available.
p1975
aVInterprocedural dataflow information enables linktime and postlinktime optimizers to perform analyses and code transformations that are not possible in a traditional compiler. This paper describes the interprocedural dataflow analysis techniques used by Spike, a postlinktime optimizer for Alpha/NT executables. Spike uses dataflow analysis to summarize the register definitions, uses, and kills that occur external to each routine, allowing Spike to perform a variety of optimizations that require interprocedural dataflow information. Because Spike is designed to optimize large PC applications, the time required to perform interprocedural dataflow analysis could potentially be unacceptably long, limiting Spike's effectiveness and applicability. To decrease dataflow analysis time, Spike uses a compact representation of a program's intraprocedural and interprocedural control flow that efficiently summarizes the register definitions and uses that occur in the program. Experimental results are presented for the SPEC95 integer benchmarks and eight large PC applications. The results show that the compact representation allows Spike to compute interprocedural dataflow information in less than 2 seconds for each of the SPEC95 integer benchmarks. Even for the largest PC application containing over 1.7 million instructions in 340 thousand basic blocks, interprocedural dataflow analysis requires just 12 seconds.
p1976
aVExisting research understates the benefits that can be obtained from inlining and cloning, especially when guided by profile information. Our implementation of inlining and cloning yields excellent results on average and very rarely lowers performance. We believe our good results can be explained by a number of factors: inlining at the intermediatecode level removes most technical restrictions on what can be inlined; the ability to inline across files and incorporate profile information enables us to choose better inline candidates; a highquality back end can exploit the scheduling and register allocation opportunities presented by larger subroutines; an aggressive processor architecture benefits from more predictable branch behavior; and a large instruction cache mitigates the impact of code expansion. We describe the often dramatic impact of our inlining and cloning on performance: for example, the implementations of our inlining and cloning algorithms in the HPUX 10.20 compilers boost SPECint95 performance on a PA8000based workstation by a factor of 1.32.
p1977
aVThe existence of statically detectable correlation among conditional branches enables their elimination, an optimization that has a number of benefits. This paper presents techniques to determine whether an interprocedural execution path leading to a conditional branch exists along which the branch outcome is known at compile time, and then to eliminate the branch along this path through code restructuring. The technique consists of a demand driven interprocedural analysis that determines whether a specific branch outcome is correlated with prior statements or branch outcomes. The optimization is performed using a code restructuring algorithm that replicates code to separate out the paths with correlation. When the correlated path is affected by a procedure call, the restructuring is based on procedure entry splitting and exit splitting. The entry splitting transformation creates multiple entries to a procedure, and the exit splitting transformation allows a procedure to return control to one of several return points in the caller. Our technique is efficient in that the correlation detection is demand driven, thus avoiding exhaustive analysis of the entire program, and the restructuring never increases the number of operations along a path through an interprocedural control flow graph. We describe the benefits of our interprocedural branch elimination optimization (ICBE). Our experimental results show that, for the same amount of code growth, the estimated reduction in executed conditional branches is about 2.5 times higher with the ICBE optimization than when only intraprocedural conditional branch elimination is applied.
p1978
aVWe present an approach for optimizing programs that uncovers additional opportunities for optimization of a statement by predicating the statement. In this paper predication algorithms for achieving partial dead code elimination (PDE) are presented. The process of predication embeds a statement in a control flow structure such that the statement is executed only if the execution follows a path along which the value computed by the statement is live. The control flow restructuring performed to achieve predication is expressed through slicing transformations. This approach achieves PDE that is not realizable by existing algorithms. We prove that our algorithm never increases the operation count along any path, and that for acyclic code all partially dead statements are eliminated. The slicing transformation that achieves predication introduces into the program additional conditional branches. These branches are eliminated in a branch deletion step based upon code duplication. We also show how PDE can be used by acyclic schedulers for VLIW processors to reduce critical path lengths along frequently executed paths.
p1979
aVAs the gap between memory and processor performance continues to widen, it becomes increasingly important to exploit cache memory eflectively. Both hardware and aoftware approaches can be explored to optimize cache performance. Hardware designers focus on cache organization issues, including replacement policy, associativity, line size and the resulting cache access time. Software writers use various optimization techniques, including software prefetching, data scheduling and code reordering. Our focus is on improving memory usage through code reordering compiler techniques.In this paper we present a linktime procedure mapping algorithm which can significantly improve the eflectiveness of the instruction cache. Our algorithm produces an improved program layout by performing a color mapping of procedures to cache lines, taking into consideration the procedure size, cache size, cache line size, and call graph. We use cache line coloring to guide the procedure mapping, indicating which cache lines to avoid when placing a procedure in the program layout. Our algorithm reduces on average the instruction cache miss rate by 40% over the original mapping and by 17% over the mapping algorithm of Pettis and Hansen [12].
p1980
aVBranch alignment reorders the basic blocks of a program to minimize pipeline penalties due to controltransfer instructions. Prior work in branch alignment has produced useful heuristic methods. We present a branch alignment algorithm that usually achieves the minimum possible pipeline penalty and on our benchmarks averages within 0.3% of a provable optimum. We compare the control penalties and running times of our algorithm to an older, greedy approach and observe that both the greedy method and our method are close to the lower bound on control penalties, suggesting that greedy is good enough. Surprisingly, in actual execution our method produces programs that run noticeably faster than the greedy method. We also report results from training and testing on different data sets, validating that our results can be achieved in realworld usage. Training and testing on different data sets slightly reduced the benefits from both branch alignment algorithms, but the ranking of the algorithms does not change, and the bulk of the benefits remain.
p1981
aVModulo scheduling algorithms based on optimal solvers have been proposed to investigate and tune the performance of modulo scheduling heuristics. While recent advances have broadened the scope for which the optimal approach is applicable, this approach increasingly suffers from large execution times. In this paper, we propose a more efficient formulation of the modulo scheduling space that significantly decreases the execution time of solvers based on integer linear programs. For example, the total execution time is reduced by a factor of 8.6 when 782 loops from the Perfect Club, SPEC, and Livermore Fortran Kernels are scheduled for minimum register requirements using the more efficient formulation instead of the traditional formulation. Experimental evidence further indicates that significantly larger loops can be scheduled under realistic machine constraints.
p1982
aVWe present an approach for specialising large programs, such as programs consisting of several modules, or libraries. This approach is based on the idea of using a compiler generator (cogen) for creating generating extensions. Generating extensions are specialisers specialised with respect to some input program. When run on some input data the generating extension produces a specialised version of the input program. Here we use the cogen to tailor modules for specialisation. This happens once and for all, independently of all other modules. The resulting module can then be used as a building block for generating extensions for complete programs, in much the same way as the original modules can be put together into complete programs. The result of running the final generating extension is a collection of residual modules, with a module structure derived from the original program.
p1983
aVOne of the flagship applications of partial evaluation is compilation and compiler generation. However, partial evaluation is usually expressed as a sourcetosource transformation for highlevel languages, whereas realistic compilers produce object code.We close this gap by composing a partial evaluator with a compiler by automatic means. Our work is a successful application of several metacomputation techniques to build the system, both in theory and in practice. The composition is an application of deforestation or fusion.The result is a runtime code generation system built from existing components. Its applications are numerous. For example, it allows the language designer to perform interpreterbased experiments with a sourcetosource version of the partial evaluator before building a realistic compiler which generates object code automatically.
p1984
aVObjectoriented languages like Java and Smalltalk provide a uniform object model that simplifies programming by providing a consistent, abstract model of object behavior. But direct implementations introduce overhead, removal of which requires aggressive implementation techniques (e.g. type inference, function specialization); in this paper, we introduce object inlining, an optimization that automatically inline allocates objects within containers (as is done by hand in C++) within a uniform model. We present our technique, which includes novel program analyses that track how inlinable objects are used throughout the program. We evaluated object inlining on several objectoriented benchmarks. It produces performance up to three times as fast as a dynamic model without inlining and roughly equal to that of manuallyinlined codes.
p1985
aVWe present a technique for automatic verification of pointer programs based on a decision procedure for the monadic secondorder logic on finite strings.We are concerned with a whilefragment of Pascal, which includes recursivelydefined pointer structures but excludes pointer arithmetic.We define a logic of stores with interesting basic predicates such as pointer equality, tests for nil pointers, and garbage cells, as well as reachability along pointers.We present a complete decision procedure for Hoare triples based on this logic over loopfree code. Combined with explicit loop invariants, the decision procedure allows us to answer surprisingly detailed questions about small but nontrivial programs. If a program fails to satisfy a certain property, then we can automatically supply an initial store that provides a counterexample.Our technique had been fully and efficiently implemented for linear linked lists, and it extends in principle to tree structures. The resulting system can be used to verify extensive properties of smaller pointer programs and could be particularly useful in a teaching environment.
p1986
aVSet based analysis is a constraintbased whole program analysis that is applicable to functional and objectoriented programming language. Unfortunately, the analysis is useless for large programs, since it generates descriptions of data flow relationships that grow quadratically in the size of the program.This paper presents componential setbased analysis, which is faster and handles larger programs without any loss of accuracy over setbased analysis. The design of the analysis exploits a number of theoretical results concerning constraint systems, including a completeness result and a decision algorithm concerning the observable equivalance of constraint systems. Experimental results validate the practically of the analysis.
p1987
aVThis paper presents a method called relational constraint for finding binary relations among the variables and constants of a program. The method constructs a table of binary relations and treats the program as a collection of constraints on tuples of relations in the table. An experimental optimizer called Thinner uses this method to analyze programs of size n in O(n2) time.
p1988
aVWe present a lineartime algorithm for boundedtype programs that builds a directed graph whose transitive closure gives exactly the results of the standard (cubictime) ControlFlow Analysis (CFA) algorithm. Our algorithm can be used to list all functions calls from all call sites in (optimal) quadratic time. More importantly, it can be used to give lineartime algorithms for CFAconsuming applications such as:&bull; effects analysis: find the sideeffecting expressions in a program.&bull; klimited CFA: for each callsite, list the functions if there are only a few of them (&le; k) and otherwise output "many".&bull; calledonce analysis: identify all functions called from only one callsite.
p1989
aVA new algorithm, SSAPRE, for performing partial redundancy elimination based entirely on SSA form is presented. It achieves optimal code motion similar to lazy code motion [KRS94a, DS93], but is formulated independently and does not involve iterative data flow analysis and bit vectors in its solution. It not only exhibits the characteristics common to other sparse approaches, but also inherits the advantages shared by other SSAbased optimization techniques. SSAPRE also maintains its output in the same SSA form as its input. In describing the algorithm, we state theorems with proofs giving our claims about SSAPRE. We also give additional description about our practical implementation of SSAPRE, and analyze and compare its performance with a bitvectorbased implementation of PRE. We conclude with some discussion of the implications of this work.
p1990
aVMany optimizing compilers perform global register allocation using a Chaitinstyle graph coloring algorithm. Live ranges that cannot be allocated to registers are spilled to memory. The amount of code required to spill the live range depends on the spilling heuristic used. Chaitin's spilling heuristic offers some guidance in reducing the amount of spill code produced. However, this heuristic does not allow the partial spilling of live ranges and the reduction in spill code is limited to a local level. In this paper, we present a global technique called interference region spilling that improves the spilling granularity of any local spilling heuristic. Our technique works above the local spilling heuristic, limiting the normal insertion of spill code to a portion of each spilled live range. By partially spilling live ranges, we can achieve large reductions in dynamically executed spill code; up to 75% in some cases and an average of 33.6% across the benchmarks tested.
p1991
aVChoosing the right kind of register for a live range plays a major role in eliminating the registerallocation overhead when the compiled function is frequently executed or function tails are on the most frequently executed paths. Picking the wrong kind of register for a live range incurs a high penalty that may dominate the total overhead of register allocation. In this paper, we present three improvements, storageclass analysis, benefitdriven simplification, and preference decision that are effective in selecting the right kind of register for a live range. Then we compare an enhanced Chaitinstyle register allocator (with these three improvements) with prioritybased and optimistic coloring.
p1992
aVThe combination of pointers and pointer arithmetic in C makes the task of improving C programs somewhat more difficult than improving programs written in simpler languages like Fortran. While much work has been published that focuses on the analysis of pointers, little has appeared that uses the results of such analysis to improve the code compiled for C. This paper examines the problem of register promotion in C and presents experimental results showing that it can have dramatic effects on memory traffic.
p1993
aVIt is difficult to map the execution model of multithreading languages (languages which support finegrain dynamic thread creation) onto the single stack execution model of C. Consequently, previous work on efficient multithreading uses elaborate frame formats and allocation strategy, with compilers customized for them. This paper presents an alternative costeffective implementation strategy for multithreading languages which can maximally exploit current sequential C compilers. We identify a set of primitives whereby efficient dynamic thread creation and switch can be achieved and clarify implementation issues and solutions which work under the stack frame layout and calling conventions of current C compilers. The primitives are implemented as a C library and named StackThreads. In StackThreads, a thread creation is done just by a C procedure call, maximizing thread creation performance. When a procedure suspends an execution, the context of the procedure, which is roughly a stack frame of the procedure, is saved into heap and resumed later. With StackThreads, the compiler writer can straightforwardly translate sequential constructs of the source language into corresponding C statements or expressions, while using StackThreads primitives as a blackbox mechanism which switches execution between C procedures.
p1994
aVCachecoherent multiprocessors with distributed shared memory are becoming increasingly popular for parallel computing. However, obtaining high performance on these machines mquires that an application execute with good data locality. In addition to making efiective use of caches, it is often necessary to distribute data structures across the local memories of the processing nodes, thereby reducing the latency of cache misses.We have designed a set of abstractions for performing data distribution in the context of explicitly parallel programs and implemented them within the SGI MIPSpro compiler system. Our system incorporates many unique features to enhance both programmability and performance. We address the former by providing a very simple programmming model with extensive support for error detection. Regarding performance, we carefully design the user abstractions with the underlying compiler optimizations in mind, we incorporate several optimization techniques to generate efficient code for accessing distributed data, and we provide a tight integration of these techniques with other optimizations within the compiler Our initial experience suggests that the directives are easy to use and can yield substantial performance gains, in some cases by as much as a factor of 3 over the same codes without distribution.
p1995
aVThe member lookup problem in C++ is the problem of resolving a specified member name in the context of a specified class. Member lookup in C++ is complicated by the presence of virtual inheritance and multiple inheritance. In this paper, we present an efficient algorithm for member lookup in C++. We also present a formalism for the multiple inheritance mechanism of C++, which we use as the basis for deriving our algorithm. The formalism may also be of use as a formal basis for deriving other C++ compiler algorithms.
p1996
aVWe present a simple and novel framework for generating blocked codes for highperformance machines with a memory hierarchy. Unlike traditional compiler techniques like tiling, which are based on reasoning about the control flow of programs, our techniques are based on reasoning directly about the flow of data through the memory hierarchy. Our datacentric transformations permit a more direct solution to the problem of enhancing data locality than current controlcentric techniques do, and generalize easily to multiple levels of memory hierarchy. We buttress these claims with performance numbers for standard benchmarks from the problem domain of dense numerical linear algebra. The simplicity and intuitive appeal of our approach should make it attractive to compiler writers as well as to library writers.
p1997
aVA major research goal for compilers and environments is the automatic derivation of tools from formal specifications. However, the formal model of the language is often inadequate; in particular, LR(k) grammars are unable to describe the natural syntax of many languages, such as C++ and Fortran, which are inherently nondeterministic. Designers of batch compilers work around such limitations by combining generated components with ad hoc techniques (for instance, performing partial type and scope analysis in tandem with parsing). Unfortunately, the complexity of incremental systems precludes the use of batch solutions. The inability to generate incremental tools for important languages inhibits the widespread use of languagerich interactive environments.We address this problem by extending the language model itself, introducing a program representation based on parse dags that is suitable for both batch and incremental analysis. Ambiguities unresolved by one stage are retained in this representation until further stages can complete the analysis, even if the reaolution depends on further actions by the user. Representing ambiguity explicitly increases the number and variety of languages that can be analyzed incrementally using existing methods.To create this representation, we have developed an efficient incremental parser for general contextfree grammars. Our algorithm combines Tomita's generalized LR parser with reuse of entire subtrees via statematching. Disambiguation can occur statically, during or after parsing, or during semantic analysis (using existing incremental techniques); program errors that preclude disambiguation retsin multiple interpretations indefinitely. Our representation and analyses gain efficiency by exploiting the local nature of ambiguities: for the SPEC95 C programs, the explicit representation of ambiguity requires only 0.5% additional space and less than 1% additional time during reconstruction.
p1998
aVAn interface definition language (IDL) is a nontraditional language for describing interfaces between software components. IDL compilers generate "stubs" that provide separate communicating processes with the abstraction of local object invocation or procedure call. Highquality stub generation is essential for applications to benefit from componentbased designs, whether the components reside on a single computer or on multiple networked hosts. Typical IDL compilers, however, do little code optimization, incorrectly assuming that interprocess communication is always the primary bottleneck. More generally, typical IDL compilers are "rigid" and limited to supporting only a single IDL, a fixed mapping onto a target language, and a narrow range of data encodings and transport mechanisms.Flick, our new IDL compiler, is based on the insight that IDLs are true languages amenable to modern compilation techniques. Flick exploits concepts from traditional programming language compilers to bring both flexibility and optimization to the domain of IDL compilation. Through the use of carefully chosen intermediate representations, Flick supports multiple IDLs, diverse data encodings, multiple transport mechanisms, and applies numerous optimizations to all of the code it generates. Our experiments show that Flickgenerated stubs marshal data between 2 and 17 times faster than stubs produced by traditional IDL compilers, and on today's generic operating systems, increase endtoend throughput by factors between 1.2 and 3.7.
p1999
aVThere are three popular methods for constructing highly retargetable compilers: (1) the compiler emits abstract machine code which is interpreted at runtime, (2) the compiler emits C code which is subsequently compiled to machine code by the native C compiler, or (3) the compiler's codegenerator is generated by a backend generator from a formal machine description produced by the compiler writer.These methods incur high costs at runtime, compiletime, or compilerconstruction time, respectively.In this paper we will describe a novel method which promises to significantly reduce the effort required to retarget a compiler to a new architecture, while at the same time producing fast and effective compilers. The basic idea is to use the native C compiler at compiler construction time to discover architectural features of the new architecture. From this information a formal machine description is produced. Given this machine description, a native codegenerator can be generated by a backend generator such as BEG or burg.A prototype Automatic Architecture Discovery Unit has been implemented. The current version is general enough to produce machine descriptions for the integer instruction sets of common RISC and CISC architectures such as the Sun SPARC, Digital Alpha, MIPS, DEC VAX, and Intel x86. The tool is completely automatic and requires minimal input from the user: principally, the user needs to provide the internet address of the target machine and the commandlines by which the C compiler, assembler, and linker are invoked.
p2000
aVThis paper presents dynamic feedback, a technique that enables computations to adapt dynamically to different execution environments. A compiler that uses dynamic feedback produces several different versions of the same source code; each version uses a different optimization policy. The generated code alternately performs sampling phases and production phases. Each sampling phase measures the overhead of each version in the current environment. Each production phase uses the version with the least overhead in the previous sampling phase. The computation periodically resamples to adjust dynamically to changes in the environment.We have implemented dynamic feedback in the context of a parallelizing compiler for objectbased programs. The generated code uses dynamic feedback to automatically choose the best synchronization optimization policy. Our experimental results show that the synchronization optimization policy has a significant impact on the overall performance of the computation, that the best policy varies from program to program, that the compiler is unable to statically choose the best policy, and that dynamic feedback enables the generated code to exhibit performance that is comparable to that of code that has been manually tuned to use the best policy. We have also performed a theoretical analysis which provides, under certain assumptions, a guaranteed optimality bound for dynamic feedback relative to a hypothetical (and unrealizable) optimal algorithm that uses the best policy at every point during the execution.
p2001
aVA program profile attributes runtime costs to portions of a program's execution. Most profiling systems suffer from two major deficiencies: first, they only apportion simple metrics, such as execution frequency or elapsed time to static, syntactic units, such as procedures or statements; second, they aggressively reduce the volume of information collected and reported, although aggregation can hide striking differences in program behavior.This paper addresses both concerns by exploiting the hardware counters available in most modern processors and by incorporating two concepts from data flow analysis flow and context sensitivity to report more context for measurements. This paper extends our previous work on efficient path profiling to flow sensitive profiling, which associates hardware performance metrics with a path through a procedure. In addition, it describes a data structure, the calling context tree, that efficiently captures calling contexts for procedurelevel measurements.Our measurements show that the SPEC95 benchmarks execute a small number (3 28) of hot paths that account for 9 98% of their L1 data cache misses. Moreover, these hot paths are concentrated in a few routines, which have complex dynamic behavior.
p2002
aVIf a fixed exponentially decreasing probability distribution function is used to model every object's lifetime, then the age of an object gives no information about its future life expectancy. This radioactive decay model implies there can be no rational basis for deciding which live objects should be promoted to another generation. Yet there remains a rational basis for deciding how many objects to promote, when to collect garbage, and which generations to collect.Analysis of the model leads to a new kind of generational garbage collector whose effectiveness does not depend upon heuristics that predict which objects will live longer than others.This result provides insight into the computational advantages of generational garbage collection, with implications for the management of objects whose life expectancies are difficult to predict.
p2003
aVPartial redundancy elimination (PRE), the most important component of global optimizers, generalizes the removal of common subexpressions and loopinvariant computations. Because existing PRE implementations are based on code motion, they fail to completely remove the redundancies. In fact, we observed that 73% of loopinvariant statements cannot be eliminated from loops by code motion alone. In dynamic terms, traditional PRE eliminates only half of redundancies that are strictly partial. To achieve a complete PRE, control flow restructuring must be applied. However, the resulting code duplication may cause code size explosion.This paper focuses on achieving a complete PRE while incurring an acceptable code growth. First, we present an algorithm for complete removal of partial redundancies, based on the integration of code motion and control flow restructuring. In contrast to existing complete techniques, we resort to restructuring merely to remove obstacles to code motion, rather than to carry out the actual optimization.Guiding the optimization with a profile enables additional code growth reduction through selecting those duplications whose cost is justified by sufficient executiontime gains. The paper develops two methods for determining the optimization benefit of restructuring a program region, one based on pathprofiles and the other on dataflow frequency analysis. Furthermore, the abstraction underlying the new PRE algorithm enables a simple formulation of speculative code motion guaranteed to have positive dynamic improvements. Finally, we show how to balance the three transformations (code motion, restructuring, and speculation) to achieve a nearcomplete PRE with very little code growth.We also present algorithms for efficiently computing dynamic benefits. In particular, using an eliminationstyle dataflow framework, we derive a demanddriven frequency analyzer whose cost can be controlled by permitting a bounded degree of conservative imprecision in the solution.
p2004
aVThis paper evaluates three alias analyses based on programming language types. The first analysis uses type compatibility to determine aliases. The second extends the first by using additional highlevel information such as field names. The third extends the second with a flowinsensitive analysis. Although other researchers suggests using types to disambiguate memory references, none evaluates its effectiveness. We perform both static and dynamic evaluations of typebased alias analyses for Modula3, a staticallytyped typesafe language. The static analysis reveals that type compatibility alone yields a very imprecise alias analysis, but the other two analyses significantly improve alias precision. We use redundant load elimination (RLE) to demonstrate the effectiveness of the three alias algorithms in terms of the opportunities for optimization, the impact on simulated execution times, and to compute an upper bound on what a perfect alias analysis would yield. We show modest dynamic improvements for (RLE), and more surprisingly, that on average our alias analysis is within 2.5% of a perfect alias analysis with respect to RLE on 8 Modula3 programs. These results illustrate that to explore thoroughly the effectiveness of alias analyses, researchers need static, dynamic, and upperbound analysis. In addition, we show that for typesafe languages like Modula3 and Java, a fast and simple alias analysis may be sufficient for many applications.
p2005
aVIn conventional superscalar microarchitectures with partitioned integer and floatingpoint resources, all floatingpoint resources are idle during execution of integer programs. Palacharla and Smith [26] addressed this drawback and proposed that the floatingpoint subsystem be augmented to support integer operations. The hardware changes required are expected to be fairly minimal.To exploit these idle floating resources, the compiler must identify integer code that can be profitably offloaded to the augmented floatingpoint subsystem. In this paper, we present two compiler algorithms to do this. The basic scheme offloads integer computation to the floatingpoint subsystem using existing program loads/stores for interpartition communication. For the SPECINT95 benchmarks, we show that this scheme offloads from 5% to 29% of the total dynamic instructions to the floatingpoint subsystem. The advanced scheme inserts copy instructions and duplicates some instructions to further offload computation. We evaluate the effectiveness of the two schemes using timing simulation. We show that the advanced scheme can offload from 9% to 41% of the total dynamic instructions to the floatingpoint subsystem. In doing so, speedups from 3% to 23% are achieved over a conventional microarchitecture.
p2006
aVThe conditional branch has long been considered an expensive operation. The relative cost of conditional branches has increased as recently designed machines are now relying on deeper pipelines and higher multiple issue. Reducing the number of conditional branches executed can often result in a substantial performance benefit. This paper describes a codeimproving transformation to reorder sequences of conditional branches. First, sequences of branches that can be reordered are detected in the control flow. Second, profiling information is collected to predict the probability that each branch will transfer control out of the sequence. Third, the cost of performing each conditional branch is estimated. Fourth, the most beneficial ordering of the branches based on the estimated probability and cost is selected. The most beneficial ordering often included the insertion of additional conditional branches that did not previously exist in the sequence. Finally, the control flow is restructured to refflect the new ordering. The results of applying the transformation were significant reductions in the dynamic number of instructions and branches, as well as decreases in execution time.
p2007
aVA linearscan algorithm directs the global allocation of register candidates to registers based on a simple linear sweep over the program being compiled. This approach to register allocation makes sense for systems, such as those for dynamic compilation, where compilation speed is important. In contrast, most commercial and research optimizing compilers rely on a graphcoloring approach to global register allocation. In this paper, we compare the performance of a linearscan method against a modern graphcoloring method. We implement both register allocators within the Machine SUIF extension of the Stanford SUIF compiler system. Experimental results show that linear scan is much faster than coloring on benchmarks with large numbers of register candidates. We also describe improvements to the linearscan approach that do not change its linear character, but allow it to produce code of a quality near to that produced by graph coloring.
p2008
aVMost existing referencebased distributed object systems include some kind of acyclic garbage collection, but fail to provide acceptable collection of cyclic garbage. Those that do provide such GC currently suffer from one or more problems: synchronous operation, the need for expensive global consensus or termination algorithms, susceptibility to communication problems, or an algorithm that does not scale. We present a simple, complete, faulttolerant, asynchronous extension to the (acyclic) cleanup protocol of the SSP Chains system. This extension is scalable, consumes few resources, and could easily be adapted to work in other referencebased distributed object systems rendering them usable for very largescale applications.
p2009
aVThis paper presents two techniques for improving garbage collection performance: generational stack collection and profiledriven pretenuring. The first is applicable to stackbased implementations of functional languages while the second is useful for any generational collector. We have implemented both techniques in a generational collector used by the TIL compiler (Tarditi, Morrisett, Cheng, Stone, Harper, and Lee 1996), and have observed decreases in garbage collection times of as much as 70% and 30%, respectively.Functional languages encourage the use of recursion which can lead to a long chain of activation records. When a collection occurs, these activation records must be scanned for roots. We show that scanning many activation records can take so long as to become the dominant cost of garbage collection. However, most deep stacks unwind very infrequently, so most of the root information obtained from the stack remains unchanged across successive garbage collections. Generational stack collection greatly reduces the stack scan cost by reusing information from previous scans.Generational techniques have been successful in reducing the cost of garbage collection (Ungar 1984). Various complex heap arrangements and tenuring policies have been proposed to increase the effectiveness of generational techniques by reducing the cost and frequency of scanning and copying. In contrast, we show that by using profile information to make lifetime predictions, pretenuring can avoid copying data altogether. In essence, this technique uses a refinement of the generational hypothesis (most data die young) with a locality principle concerning the age of data: most allocations sites produce data that immediately dies, while a few allocation sites consistently produce data that survives many collections.
p2010
aVThe IEEE/ANSI standard for Scheme requires implementations to be properly tail recursive. This ensures that portable code can rely upon the space efficiency of continuationpassing style and other idioms. On its face, proper tail recursion concerns the efficiency of procedure calls that occur within a tail context. When examined closely, proper tail recursion also depends upon the fact that garbage collection can be asymptotically more spaceefficient than Algollike stack allocation.Proper tail recursion is not the same as ad hoc tail call optimization in stackbased languages. Proper tail recursion often precludes stack allocation of variables, but yields a welldefined asymptotic space complexity that can be relied upon by portable programs.This paper offers a formal and implementationindependent definition of proper tail recursion for Scheme. It also shows how an entire family of reference implementations can be used to characterize related safeforspace properties, and proves the asymptotic inequalities that hold between them.
p2011
aVIn this paper, we describe our experience with using an abstract integerset framework to develop the Rice dHPF compiler, a compiler for High Performance Fortran. We present simple, yet general formulations of the major computation partitioning and communication analysis tasks as well as a number of important optimizations in terms of abstract operations on sets of integer tuples. This approach has made it possible to implement a comprehensive collection of advanced optimizations in dHPF, and to do so in the context of a more general computation partitioning model than previous compilers. One potential limitation of the approach is that the underlying class of integer set problems is fundamentally unable to represent HPF data distributions on a symbolic number of processors. We describe how we extend the approach to compile codes for a symbolic number of processors, without requiring any changes to the set formulations for the above optimizations. We show experimentally that the set representation is not a dominant factor in compile times on both small and large codes. Finally, we present preliminary performance measurements to show that the generated code achieves good speedups for a few benchmarks. Overall, we believe we are the first to demonstrate by implementation experience that it is practical to build a compiler for HPF using a general and powerful integerset framework.
p2012
aVThis paper presents algorithms for reducing the communication overhead for parallel C programs that use dynamicallyallocated data structures. The framework consists of an analysis phase called possibleplacement analysis, and a transformation phase called communication selection.The fundamental idea of possibleplacement analysis is to find all possible points for insertion of remote memory operations. Remote reads are propagated upwards, whereas remote writes are propagated downwards. Based on the results of the possibleplacement analysis, the communication selection transformation selects the "best" place for inserting the communication, and determines if pipelining or blocking of communication should be performed.The framework has been implemented in the EARTHMcCAT optimizing/parallelizing C compiler, and experimental results are presented for five pointerintensive benchmarks running on the EARTHMANNA distributedmemory parallel architecture. These experiments show that the communication optimization can provide performance improvements of up to 16% over the unoptimized benchmarks.
p2013
aVThe fifth release of the multithreaded language Cilk uses a provably good "workstealing" scheduling algorithm similar to the first system, but the language has been completely redesigned and the runtime system completely reengineered. The efficiency of the new implementation was aided by a clear strategy that arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads that contribute to the work, even at the expense of overheads that contribute to the critical path. Although it may seem counterintuitive to move overheads onto the critical path, this "workfirst" principle has led to a portable Cilk5 implementation in which the typical cost of spawning a parallel thread is only between 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk programs run on one processor with virtually no degradation compared to equivalent C programs. This paper describes how the workfirst principle was exploited in the design of Cilk5's compiler and its runtime system. In particular, we present Cilk5's novel "twoclone" compilation strategy and its Dijkstralike mutualexclusion protocol for implementing the ready deque in the workstealing scheduler.
p2014
aVWe present a new register promotion algorithm based on Static Single Assignment (SSA) form. Register promotion is aimed at promoting program names from memory locations to registers. Our algorithm is profiledriven and is based on the scope of intervals. In cases where a complete promotion is not possible because of the presence of function calls or pointer references, the proposed algorithm is capable of eliminating loads and stores on frequently executed paths by placing loads and stores on less frequently executed paths. We also describe an efficient method to incrementally update SSA form when new definitions are cloned from an existing name during register promotion. On SPECInt95 benchmarks, our algorithm removes about ~12% of memory operations which access scalar variables.
p2015
aVThis paper presents a typed programming language and compiler for runtime code generation. The language, called ML', extends ML with modal operators in the style of the MiniML'e language of Davies and Pfenning. ML' allows programmers to use types to specify precisely the stages of computation in a program. The types also guide the compiler in generating target code that exploits the staging information through the use of runtime code generation. The target machine is currently a version of the Categorical Abstract Machine, called the CCAM, which we have extended with facilities for runtime code generation.This approach allows the programmer to express the staging that he wants directly to the compiler. It also provides a typed framework in which to verify the correctness of his staging intentions, and to discuss his staging decisions with other programmers. Finally, it supports in a natural way multiple stages of runtime specialization, so that dynamically generated code can be used in the generation of yet further specialized code.This paper presents an overview of the language, with several examples of programs that illustrate key concepts and programming techniques. Then, it discusses the CCAM and the compilation of ML' programs into CCAM code. Finally, the results of some experiments are shown, to demonstrate the benefits of this style of runtime code generation for some applications.
p2016
aVA module system ought to enable assemblyline programming using separate compilation and an expressive linking language. Separate compilation allows programmers to develop parts of a program independently. A linking language gives programmers precise control over the assembly of parts into a whole. This paper presents models of program units, MzScheme's module language for assemblyline programming. Units support separate compilation, independent module reuse, cyclic dependencies, hierarchical structuring, and dynamic linking. The models explain how to integrate units with untyped and typed languages such as Scheme and ML.
p2017
aVWe present a typebased approach to eliminating array bound checking and list tag checking by conservatively extending Standard ML with a restricted form of dependent types. This enables the programmer to capture more invariants through types while typechecking remains decidable in theory and can still be performed efficiently in practice. We illustrate our approach through concrete examples and present the result of our preliminary experiments which support support the feasibility and effectiveness of our approach.
p2018
aVLanguagesupported synchronization is a source of serious performance problems in many Java programs. Even singlethreaded applications may spend up to half their time performing useless synchronization due to the threadsafe nature of the Java libraries. We solve this performance problem with a new algorithm that allows lock and unlock operations to be performed with only a few machine instructions in the most common cases. Our locks only require a partial word per object, and were implemented without increasing object size. We present measurements from our implementation in the JDK 1.1.2 for AIX, demonstrating speedups of up to a factor of 5 in microbenchmarks and up to a factor of 1.7 in real programs.
p2019
aVFull precision in garbage collection implies retaining only those heap allocated objects that will actually be used in the future. Since full precision is not computable in general, garbage collectors use safe (i.e., conservative) approximations such as reachability from a set of root references. Ambiguous roots collectors (commonly called "conservative") can be overly conservative because they overestimate the root set, and thereby retain unexpectedly large amounts of garbage. We consider two more precise collection schemes for Java virtual machines (JVMs). One uses a type analysis to obtain a typeprecise root set (only those variables that contain references); the other adds a live variable analysis to reduce the root set to only the live reference variables. Even with the Java programming language's strong typing, it turns out that the JVM specification has a feature that makes typeprecise root sets difficult to compute. We explain the problem and ways in which it can be solved.Our experimental results include measurements of the costs of the type and liveness analyses at load time, of the incremental benefits at run time of the liveness analysis over the type analysis alone, and of various map sizes and counts. We find that the liveness analysis often produces little or no improvement in heap size, sometimes modest improvements, and occasionally the improvement is dramatic. While further study is in order, we conclude that the main benefit of the liveness analysis is preventing bad surprises.
p2020
aVA "JustInTime" (JIT) Java compiler produces native code from Java byte code instructions during program execution. As such, compilation speed is more important in a Java JIT compiler than in a traditional compiler, requiring optimization algorithms to be lightweight and effective. We present the structure of a Java JIT compiler for the Intel Architecture, describe the lightweight implementation of JIT compiler optimizations (e.g., common subexpression elimination, register allocation, and elimination of array bounds checking), and evaluate the performance benefits and tradeoffs of the optimizations. This JIT compiler has been shipped with version 2.5 of Intel's VTune for Java product.
p2021
aVAchieving good performance in bytecoded language interpreters is difficult without sacrificing both simplicity and portability. This is due to the complexity of dynamic translation ("justintime compilation") of bytecodes into native code, which is the mechanism employed universally by highperformance interpreters.We demonstrate that a few simple techniques make it possible to create highlyportable dynamic translators that can attain as much as 70% the performance of optimized C for certain numerical computations. Translators based on such techniques can offer respectable performance without sacrificing either the simplicity or portability of much slower "pure" bytecode interpreters.
p2022
aVLarge applications are typically partitioned into separately compiled modules. Large performance gains in these applications are available by optimizing across module boundaries. One barrier to applying crossmodule optimization (CMO) to large applications is the potentially enormous amount of time and space consumed by the optimization process.We describe a framework for scalable CMO that provides large gains in performance on applications that contain millions of lines of code. Two major techniques are described. First, careful management of inmemory data structures results in sublinear memory occupancy when compared to the number of lines of code being optimized. Second, profile data is used to focus optimization effort on the performancecritical portions of applications. We also present practical issues that arise in deploying this framework in a production environment. These issues include debuggability and compatibility with existing development tools, such as make. Our framework is deployed in HewlettPackard's (HP) UNIX compiler products and speeds up shipped independent software vendors' applications by as much as 71%.
p2023
aVMuch research has been devoted to studies of and algorithms for memory management based on garbage collection or explicit allocation and deallocation. An alternative approach, regionbased memory management, has been known for decades, but has not been wellstudied. In a regionbased system each allocation specifies a region, and memory is reclaimed by destroying a region, freeing all the storage allocated therein. We show that on a suite of allocationintensive C programs, regions are competitive with malloc/free and sometimes substantially faster. We also show that regions support safe memory management with low overhead. Experience with our benchmarks suggests that modifying many existing programs to use regions is not difficult.
p2024
aVObjectoriented applications may contain data members that can be removed from the application without affecting program behavior. Such "dead" data members may occur due to unused functionality in class libraries, or due to the programmer losing track of member usage as the application changes over time. We present a simple and efficient algorithm for detecting dead data members in C++ applications. This algorithm has been implemented using a prototype version of the IBM VisualAge C++ compiler, and applied to a number of realistic benchmark programs ranging from 600 to 58,000 lines of code. For the nontrivial benchmarks, we found that up to 27.3% of the data members in the benchmarks are dead (average 12.5%), and that up to 11.6% of the object space of these applications may be occupied by dead data members at runtime (average 4.4%).
p2025
aVAn algorithm for register promotion is presented based on the observation that the circumstances for promoting a memory location's value to register coincide with situations where the program exhibits partial redundancy between accesses to the memory location. The recent SSAPRE algorithm for eliminating partial redundancy using a sparse SSA representation forms the foundation for the present algorithm to eliminate redundancy among memory accesses, enabling us to achieve both computational and live range optimality in our register promotion results. We discuss how to effect speculative code motion in the SSAPRE framework. We present two different algorithms for performing speculative code motion: the conservative speculation algorithm used in the absence of profile data, and the the profiledriven speculation algorithm used when profile data are available. We define the static single use (SSU) form and develop the dual of the SSAPRE algorithm, called SSUPRE, to perform the partial redundancy elimination of stores. We provide measurement data on the SPECint95 benchmark suite to demonstrate the effectiveness of our register promotion approach in removing loads and stores. We also study the relative performance of the different speculative code motion strategies when applied to scalar loads and stores.
p2026
aVThis paper presents the design and implementation of a compiler that translates programs written in a typesafe subset of the C programming language into highly optimized DEC Alpha assembly language programs, and a certifier that automatically checks the type safety and memory safety of any assembly language program produced by the compiler. The result of the certifier is either a formal proof of type safety or a counterexample pointing to a potential violation of the type system by the target program. The ensemble of the compiler and the certifier is called a certifying compiler.Several advantages of certifying compilation over previous approaches can be claimed. The notion of a certifying compiler is significantly easier to employ than a formal compiler verification, in part because it is generally easier to verify the correctness of the result of a computation than to prove the correctness of the computation itself. Also, the approach can be applied even to highly optimizing compilers, as demonstrated by the fact that our compiler generates target code, for a range of realistic C programs, which is competitive with both the cc and gcc compilers with all optimizations enabled. The certifier also drastically improves the effectiveness of compiler testing because, for each test case, it statically signals compilation errors that might otherwise require many executions to detect. Finally, this approach is a practical way to produce the safety proofs for a ProofCarrying Code system, and thus may be useful in a system for safe mobile code.
p2027
aVMany cache misses in scientific programs are due to conflicts caused by limited set associativity. We examine two compiletime datalayout transformations for eliminating conflict misses, concentrating on misses occuring on every loop iteration. Intervariable padding adjusts variable base addresses, while intravariable padding modifies array dimension sizes. Two levels of precision are evaluated. PADLITE only uses array and column dimension sizes, relying on assumptions about common array reference patterns. PAD analyzes programs, detecting conflict misses by linearizing array references and calculating conflict distances between uniformlygenerated references. The Euclidean algorithm for computing the gcd of two numbers is used to predict conflicts between different array columns for linear algebra codes. Experiments on a range of programs indicate PADLITE can eliminate conflicts for benchmarks, but PAD is more effective over a range of cache and problem sizes. Padding reduces cache miss rates by 16% on average for a 16K directmapped cache. Execution times are reduced by 6% on average, with some SPEC95 programs improving up to 15%.
p2028
aVArray languages such as Fortran 90, HPF and ZPL have many benefits in simplifying arraybased computations and expressing data parallelism. However, they can suffer large performance penalties because they introduce intermediate arrays both at the source level and during the compilation process which increase memory usage and pollute the cache. Most compilers address this problem by simply scalarizing the array language and relying on a scalar language compiler to perform loop fusion and array contraction. We instead show that there are advantages to performing a form of loop fusion and array contraction at the array level. This paper describes this approach and explains its advantages. Experimental results show that our scheme typically yields runtime improvements of greater than 20% and sometimes up to 400%. In addition, it yields superior memory use when compared against commercial compilers and exhibits comparable memory use when compared with scalar languages. We also explore the interaction between these transformations and communication optimizations.
p2029
aVExisting array region representation techniques are sensitive to the complexity of array subscripts. In general, these techniques are very accurate and efficient for simple subscript expressions, but lose accuracy or require potentially expensive algorithms for complex subscripts. We found that in scientific applications, many access patterns are simple even when the subscript expressions are complex. In this work, we present a new, general array access representation and define operations for it. This allows us to aggregate and simplify the representation enough that precise region operations may be applied to enable compiler optimizations. Our experiments show that these techniques hold promise for speeding up applications.
p2030
aVDataflow analysis computes its solutions over the paths in a controlflow graph. These paths whether feasible or infeasible, heavily or rarely executed contribute equally to a solution. However, programs execute only a small fraction of their potential paths and, moreover, programs' execution time and cost is concentrated in a far smaller subset of hot paths.This paper describes a new approach to analyzing and optimizing programs, which improves the precision of data flow analysis along hot paths. Our technique identifies and duplicates hot paths, creating a hot path graph in which these paths are isolated. After flow analysis, the graph is reduced to eliminate unnecessary duplicates of unprofitable paths. In experiments on SPEC95 benchmarks, path qualification identified 2 112 times more nonlocal constants (weighted dynamically) than the WegmanZadek conditional constant algorithm, which translated into 1 7% more dynamic instructions with constant results.
p2031
aVMany program analyses are naturally formulated and implemented using inclusion constraints. We present new results on the scalable implementation of such analyses based on two insights: first, that online elimination of cyclic constraints yields ordersofmagnitude improvements in analysis time for large problems; second, that the choice of constraint representation affects the quality and efficiency of online cycle elimination. We present an analytical model that explains our design choices and show that the model's predictions match well with results from a substantial experiment.
p2032
aVA pointeranalysis algorithm can be either flowsensitive or flowinsensitive. While flowsensitive analysis usually provides more precise information, it is also usually considerably more costly in terms of time and space. The main contribution of this paper is the presentation of another option in the form of an algorithm that can be 'tuned' to provide a range of results that fall between the results of flowinsensitive and flowsensitive analysis. The algorithm combines a flowinsensitive pointer analysis with static single assignment (SSA) form and uses an iterative process to obtain progressively better results.
p2033
aVHardware trends have produced an increasing disparity between processor speeds and memory access times. While a variety of techniques for tolerating or reducing memory latency have been proposed, these are rarely successful for pointermanipulating programs.This paper explores a complementary approach that attacks the source (poor reference locality) of the problem rather than its manifestation (memory latency). It demonstrates that careful data organization and layout provides an essential mechanism to improve the cache locality of pointermanipulating programs and consequently, their performance. It explores two placement techniques clustering and coloring that improve cache performance by increasing a pointer structure's spatial and temporal locality, and by reducing cacheconflicts.To reduce the cost of applying these techniques, this paper discusses two strategies cacheconscious reorganization and cacheconscious allocation and describes two semiautomatic tools ccmorph and ccmalloc that use these strategies to produce cacheconscious pointer structure layouts. ccmorph is a transparent tree reorganizer that utilizes topology information to cluster and color the structure. ccmalloc is a cacheconscious heap allocator that attempts to colocate contemporaneously accessed data elements in the same physical cache block. Our evaluations, with microbenchmarks, several small benchmarks, and a couple of large realworld applications, demonstrate that the cacheconscious structure layouts produced by ccmorph and ccmalloc offer large performance benefits in most cases, significantly outperforming stateoftheart prefetching.
p2034
aVA highperformance implementation of a Java Virtual Machine1 requires a compiler to translate Java bytecodes into native instructions, as well as an advanced garbage collector (e.g., copying or generational). When the Java heap is exhausted and the garbage collector executes, the compiler must report to the garbage collector all live object references contained in physical registers and stack locations. Typical compilers only allow certain instructions (e.g., call instructions and backward branches) to be GCsafe; if GC happens at some other instruction, the compiler may need to advance execution to the next GCsafe point. Until now, no one has ever attempted to make every compilergenerated instruction GCsafe, due to the perception that recording this information would require too much space. This kind of support could improve the GC performance in multithreaded applications. We show how to use simple compression techniques to reduce the size of the GC map to about 20% of the generated code size, a result that is competitive with the best previously published results. In addition, we extend the work of Agesen, Detlefs, and Moss, regarding the socalled "JSR Problem" (the single exception to Java's type safety property), in a way that eliminates the need for extra runtime overhead in the generated code.
p2035
aVDSP architectures typically provide dedicated memory address generation units and indirect addressing modes with autoincrement and autodecrement that subsume address arithmetic calculation. The heavy use of autoincrement and autodecrement indirect addressing require DSP compilers to perform a careful placement of variables in storage to minimize address arithmetic instructions to generate compact and efficient DSP code. Liao et al. formulated the problem of storage assignment as the simple offset assignment problem (SOA) and the general offset assignment problem (GOA), and proposed heuristic solutions.The storage allocation of variables critically depends on the sequence of variable accesses. In this paper we present techniques to optimize the access sequence of variables by applying algebraic transformations (such as commutativity and associativity) on expression trees to obtain the least cost offset assignment. We develop a new formulation of this problem as the least cost access sequence problem (LCAS). Based on the proposed framework, we develop heuristic algorithms that determine empirically nearoptimal solutions resulting in fewer address arithmetic instructions. We have implemented the proposed heuristic algorithms by extending the storage assignment optimization in the SPAM compiler backend targeted for the TMS320C25 DSP. In the case of SOA, experimental results for programs from the DSPstone benchmark suite show an average improvement of 3.36% in static code size and an average relative speedup of 7.28% over results obtained using existing SOA algorithms. The average code size reduction over code compiled with a naive storage assignment algorithm is 7.04%. The proposed framework has also been applied to the GOA problem and shows average code size reductions of 2.04% over results obtained using existing GOA algorithms, and average code size reductions of 10.84% over a naive GOA algorithm. Code size reduction and improvement in dynamic instruction counts could be valuable given limited memory and realtime response requirements placed on embedded systems.
p2036
aVThis paper explores compiler techniques for reducing the memory needed to load and run program executables. In embedded systems, where economic incentives to reduce both RAM and ROM are strong, the size of compiled code is increasingly important. Similarly, in mobile and network computing, the need to transmit an executable before running it places a premium on code size. Our work focuses on reducing the size of a program's code segment, using patternmatching techniques to identify and coalesce together repeated instruction sequences. In contrast to other methods, our framework preserves the ability to run program executables directly, without an intervening decompression stage. Our compression framework is integrated into an industrialstrength optimizing compiler, which allows us to explore the interaction between code compression and classical code optimization techniques, and requires that we contend with the difficulties of compressing previously optimized code. The specific contributions in this paper include a comprehensive experimental evaluation of code compression for a RISClike architecture, a more powerful patternmatching scheme for improved identification of repeated code fragments, and a new form of profiledriven code compression that reduces the speed penalty arising from compression.
p2037
aVA Chaitinstyle register allocator often blocks during its simplification phase because no node in the interference graph has a degree that is sufficiently small. Typically, this is handled by nodesplitting, or by optimistically continuing and hoping that a legal Ncoloring will still be found. We observe that the merging of two nodes in a graph causes a reduction in the degree of any node that had been adjacent to both. We have enhanced Chaitin's coloring algorithm so that it attempts nodemerging during graph simplification; this often allows simplification to continue, while still guaranteeing a coloring for the graph. We have tested this algorithm using Appel's database of registercoloring graphs, and have compared it with Chaitin's algorithm. The mergeenhanced algorithm yields a better coloring about 8% of the time, and a worse coloring less than 0.1% of the time.
p2038
aVThe challenge of exploiting high degrees of instructionlevel parallelism is often hampered by frequent branching. Both exposed branch latency and low branch throughput can restrict parallelism. Control critical path reduction (control CPR) is a compilation technique to address these problems. Control CPR can reduce the dependence height of critical paths through branch operations as well as decrease the number of executed branches. In this paper, we present an approach to control CPR that recognizes sequences of branches using profiling statistics. The control CPR transformation is applied to the predominant path through this sequence. Our approach, its implementation, and experimental results are presented. This work demonstrates that control CPR enhances instructionlevel parallelism for a variety of application programs and improves their performance across a range of processors.
p2039
aVThe FFTW library for computing the discrete Fourier transform (DFT) has gained a wide acceptance in both academia and industry, because it provides excellent performance on a variety of machines (even competitive with or faster than equivalent libraries supplied by vendors). In FFTW, most of the performancecritical code was generated automatically by a specialpurpose compiler, called genfft, that outputs C code. Written in Objective Caml, genfft can produce DFT programs for any input length, and it can specialize the DFT program for the common case where the input data are real instead of complex. Unexpectedly, genfft "discovered" algorithms that were previously unknown, and it was able to reduce the arithmetic complexity of some other existing algorithms. This paper describes the internals of this specialpurpose compiler in some detail, and it argues that a specialized compiler is a valuable tool.
p2040
aVWith an increasing number of executable binaries generated by optimizing compilers today, providing a clear and correct sourcelevel debugger for programmers to debug optimized code has become a necessity. In this paper, a new framework for debugging globally optimized code is proposed. This framework consists of a new code location mapping scheme, a data location tracking scheme, and an emulationbased forward recovery model. By taking over the control early and emulating instructions selectively, the debugger can preserve and gather the required program state for the recovery of expected variable values at source breakpoints. The framework has been prototyped in the IMPACT compiler and GDB4.16. Preliminary experiments conducted on several SPEC95 integer programs have yielded encouraging results. The extra time needed for the debugger to calculate the limits of the emulated region and to emulate instructions is hardly noticeable, while the increase in executable file size due to the extra debug information is on average 76% of that of the executable file with no debug information.
p2041
aVWe describe a framework for adding type qualifiers to a language. Type qualifiers encode a simple but highly useful form of subtyping. Our framework extends standard type rules to model the flow of qualifiers through a program, where each qualifier or set of qualifiers comes with additional rules that capture its semantics. Our framework allows types to be polymorphic in the type qualifiers. We present a constinference system for C as an example application of the framework. We show that for a set of real C programs, many more consts can be used than are actually present in the original code.
p2042
aVStatic Single Assignment (SSA) is an effective intermediate representation in optimizing compilers. However, traditional SSA form and optimizations are not applicable to programs represented as native machine instructions because the use of dedicated registers imposed by calling conventions, the runtime system, and target architecture must be made explicit. We present a simple scheme for converting between programs in machine code and in SSA, such that references to dedicated physical registers in machine code are preserved. Our scheme ignores all output and antidependences imposed by physical registers while a program is in SSA form, but inserts compensation code during machine code reconstruction if any naming requirements have been violated. By resolving all mismatches between the two representations in separate phases, we are able to utilize existing SSA algorithms unaltered to perform machine code optimizations.
p2043
aVTiling is a wellknown loop transformation to improve temporal locality of nested loops. Current compiler algorithms for tiling are limited to loops which are perfectly nested or can be transformed, in trivial ways, into a perfect nest. This paper presents a number of program transformations to enable tiling for a class of nontrivial imperfectlynested loops such that cache locality is improved. We define a program model for such loops and develop compiler algorithms for their tiling. We propose to adopt oddeven variable duplication to break anti and output dependences without unduly increasing the workingset size, and to adopt speculative execution to enable tiling of loops which may terminate prematurely due to, e.g. convergence tests in iterative algorithms. We have implemented these techniques in a research compiler, Panorama. Initial experiments with several benchmark programs are performed on SGI workstations based on MIPS R5K and R10K processors. Overall, the transformed programs run faster by 9% to 164%.
p2044
aVA program's cache performance can be improved by changing the organization and layout of its data even complex, pointerbased data structures. Previous techniques improved the cache performance of these structures by arranging distinct instances to increase reference locality. These techniques produced significant performance improvements, but worked best for small structures that could be packed into a cache block.This paper extends that work by concentrating on the internal organization of fields in a data structure. It describes two techniques structure splitting and field reordering that improve the cache behavior of structures larger than a cache block. For structures comparable in size to a cache block, structure splitting can increase the number of hot fields that can be placed in a cache block. In five Java programs, structure splitting reduced cache miss rates 10 27% and improved performance 6 18% beyond the benefits of previously described cacheconscious reorganization techniques.For large structures, which span many cache blocks, reordering fields, to place those with high temporal affinity in the same cache block can also improve cache utilization. This paper describes bbcache, a tool that recommends C structure field reorderings. Preliminary measurements indicate that reordering fields in 5 active structures improves the performance of Microsoft SQL Server 7.0 2 3%.
p2045
aVWith the rapid improvement of processor speed, performance of the memory hierarchy has become the principal bottleneck for most applications. A number of compiler transformations have been developed to improve data reuse in cache and registers, thus reducing the total number of direct memory accesses in a program. Until now, however, most data reuse transformations have been static applied only at compile time. As a result, these transformations cannot be used to optimize irregular and dynamic applications, in which the data layout and data access patterns remain unknown until run time and may even change during the computation.In this paper, we explore ways to achieve better data reuse in irregular and dynamic applications by building on the inspectorexecutor method used by Saltz for runtime parallelization. In particular, we present and evaluate a dynamic approach for improving both computation and data locality in irregular programs. Our results demonstrate that runtime program transformations can substantially improve computation and data locality and, despite the complexity and cost involved, a compiler can automate such transformations, eliminating much of the associated runtime overhead.
p2046
aVThis paper describes experiments that apply machine learning to compress computer programs, formalizing and automating decisions about instruction encoding that have traditionally been made by humans in a more ad hoc manner. A program accepts a large training set of program material in a conventional compiler intermediate representation (IR) and automatically infers a decision tree that separates IR code into streams that compress much better than the undifferentiated whole. Driving a conventional arithmetic compressor with this model yields code 30% smaller than the previous record for IR code compression, and 24% smaller than an ambitious optimizing compiler feeding an ambitious generalpurpose data compressor.
p2047
aVJava class files are often distributed as jar files, which are collections of individually compressed class files (and possibility other files). Jar files are typically about 1/2 the size of the original class files due to compression. I have developed a wirecode format for collections of Java class files. This format is typically 1/2 to 1/5 of the size of the corresponding compressed jar file (1/4 to 1/10 the size of the original class files).
p2048
aVWhole program paths (WPP) are a new approach to capturing and representing a program's dynamic actually executed control flow. Unlike other path profiling techniques, which record intraprocedural or acyclic paths, WPPs produce a single, compact description of a program's entire control flow, including loop iteration and interprocedural paths.This paper explains how to collect and represent WPPs. It also shows how to use WPPs to find hot subpaths, which are the heavily executed sequences of code that should be the focus of performance tuning and compiler optimization.
p2049
aVThis paper describes GBURG, which generates tiny, fast code generators based on finitestate machine pattern matching. The code generators translate postfix intermediate code into machine instructions in one pass (except, of course, for backpatching addresses). A stackbased virtual machine known as the Lean Virtual Machine (LVM) tuned for fast code generation is also described. GBURG translates the twopage LVMtox86 specification into a code generator that fits entirely in an 8 KB Icache and that emits x86 code at 3.6 MB/set on a 266MHz P6. Our justintime code generator translates and executes small benchmarks at speeds within a factor of two of executables derived from the conventional compiletime code generator on which it is based.
p2050
aVAvailability of data in a program determines computation stages. Incremental partial evaluation exploit these stages for optimization: it allows further specialization to be performed as data become available at later stages. The fundamental advantage of incremental specialization is to factorize the specialization process. As a result, specializing a program at a given stage costs considerably less than specializing it once all the data are available.We present a realistic and flexible approach to achieve efficient incremental runtime specialization. Rather than developing specific techniques, as previously proposed, we are able to reuse existing technology by iterating a specialization process. Moreover, in doing so, we do not lose any specialization opportunities. This approach makes it possible to exploit nested quasiinvariants and to speed up the runtime specialization process.This approach has been implemented in Tempo, a specializer for C programs that is publicly available. A preliminary experiment confirm that incremental that incremental specialization can greatly speed up the specialization process.
p2051
aVSome modern superscalar microprocessors provide only imprecise exceptions. That is, they do not guarantee to report the same exception that would be encountered by a straightforward sequential execution of the program. In exchange, they offer increased performance or decreased chip area (which amount to much the same thing).This performance/precision tradeoff has not so far been much explored at the programming language level. In this paper we propose a design for imprecise exceptions in the lazy functional programming language Haskell. We discuss several designs, and conclude that imprecision is essential if the language is still to enjoy its current rich algebra of transformations. We sketch a precise semantics for the language extended with exceptions.The paper shows how to extend Haskell with exceptions without crippling the language or its compilers. We do not yet have enough experience of using the new mechanism to know whether it strikes an appropriate balance between expressiveness and performance.
p2052
aVTypical classbased languages, such as C++ and JAVA, provide complex class mechanisms but only weak module systems. In fact, classes in these languages incorporate many of the features found in richer module mechanisms. In this paper, we describe an alternative approach to designing a language that has both classes and modules. In our design, we rely on a rich MLstyle module system to provide features such as visibility control and parameterization, while providing a minimal class mechanism that includes only those features needed to support inheritance. Programmers can then use the combination of modules and classes to implement the full range of classbased features and idioms. Our approach has the advantage that it provides a fullfeatured module system (useful in its own right), while keeping the class mechanism quite simple.We have incorporated this design in MOBY, which is an MLstyle language that supports classbased objectoriented programming. In this paper, we describe our design via a series of simple examples, show how various classbased features and idioms are realized in MOBY, compare our design with others, and sketch its formal semantics.
p2053
aVA hierarchical module system is an effective tool for structuring large programs. Strictly hierarchical module systems impose an acyclic ordering on import dependencies among program units. This can impede modular programming by forcing mutuallydependent components to be consolidated into a single module. Recently there have been several proposals for module systems that admit cyclic dependencies, but it is not clear how these proposals relate to one another, nor how one might integrate them into an expressive module system such as that of ML.To address this question we provide a typetheoretic analysis of the notion of a recursive module in the context of a "phasedistinction" formalism for higherorder module systems. We extend this calculus with a recursive module mechanism and a new form of signature, called a recursively dependent signature, to support the definition of recursive modules. These extensions are justified by an interpretation in terms of more primitive language constructs. This interpretation may also serve as a guide for implementation.
p2054
aVLoadreuse analysis finds instructions that repeatedly access the same memory location. This location can be promoted to a register, eliminating redundant loads by reusing the results of prior memory accesses. This paper develops a loadreuse analysis and designs a method for evaluating its precision.In designing the analysis, we aspire for completeness the goal of exposing all reuse that can be harvested by a subsequent program transformation. For register promotion, a suitable transformation is partial redundancy elimination (PRE). To approach the ideal goal of PREcompleteness, the loadreuse analysis is phrased as a dataflow problem on a program representation that is pathsensitive, as it detects reuse even when it originates in a different instruction along each control flow path. Furthermore, the analysis is comprehensive, as it treats scalar, array and pointerbased loads uniformly.In evaluating the analysis, we compare it with an ideal analysis. By observing the runtime stream of memory references, we collect all PREexploitable reuse and treat it as the ideal analysis performance. To compare the (static) loadreuse analysis with the (dynamic) ideal reuse, we use an estimator algorithm that computes, given a dataflow solution and a program profile, the dynamic amount of reuse detected by the analysis. We developed a family of estimators that differ in how well they bound the profiling error inherent in the edge profile. By bounding the error, the estimators offer a precise and practical method for determining the runtime optimization benefit.Our experiments show that about 55% of loads executed in Spec95 exhibit reuse. Of those, our analysis exposes about 80%.
p2055
aVThis paper presents a novel interprocedural, flowsensitive, and contextsensitive pointer analysis algorithm for multithreaded programs that may concurrently update shared pointers. For each pointer and each program point, the algorithm computes a conservative approximation of the memory locations to which that pointer may point. The algorithm correctly handles a full range of constructs in multithreaded programs, including recursive functions, function pointers, structures, arrays, nested structures and arrays, pointer arithmetic, casts between pointer variables of different types, heap and stack allocated memory, shared global variables, and threadprivate global variables.We have implemented the algorithm in the SUIF compiler system and used the implementation to analyze a sizable set of multithreaded programs written in the Cilk multithreaded programming language. Our experimental results show that the analysis has good precision and converges quickly for our set of Cilk programs.
p2056
aVType casting allows a program to access an object as if it had a type different from its declared type. This complicates the design of a pointeranalysis algorithm that treats structure fields as separate objects; therefore, some previous pointeranalysis algorithms "collapse" a structure into a single variable. The disadvantage of this approach is that it can lead to very imprecise pointsto information. Other algorithms treat each field as a separate object based on its offset and size. While this approach leads to more precise results, the results are not portable because the memory layout of structures is implementation dependent.This paper first describes the complications introduced by type casting, then presents a tunable pointeranalysis framework for handling structures in the presence of casting. Different instances of this framework produce algorithms with different levels of precision, portability, and efficiency. Experimental results from running our implementations of four instances of this framework show that (i) it is important to distinguish fields of structures in pointer analysis, but (ii) making conservative approximations when casting is involved usually does not cost much in terms of time, space, or the precision of the results.
p2057
aVThis paper presents the first multiprocessor garbage collection algorithm with provable bounds on time and space. The algorithm is a realtime sharedmemory copying collector. We prove that the algorithm requires at most 2(R(l + 2/k) + N + 5PD) memory locations, where P is the number of processors, R is the maximum reachable space during a computation (number of locations accessible from the root set), N is the maximum number of reachable objects, D is the maximum depth of any data object, and k is a parameter specifying how many locations are copied each time a location is allocated. Furthermore we show that client threads are never stopped for more than time proportional to k nonblocking machine instructions. The bounds are guaranteed even with arbitrary length arrays. The collector only requires writebarriers (reads are unaffected by the collector), makes few assumptions about the threads that are generating the garbage, and allows them to run mostly asynchronously.
p2058
aVWe describe the design and implementation of Dynamo, a software dynamic optimization system that is capable of transparently improving the performance of a native instruction stream as it executes on the processor. The input native instruction stream to Dynamo can be dynamically generated (by a JIT for example), or it can come from the execution of a statically compiled native binary. This paper evaluates the Dynamo system in the latter, more challenging situation, in order to emphasize the limits, rather than the potential, of the system. Our experiments demonstrate that even statically optimized native binaries can be accelerated Dynamo, and often by a significant degree. For example, the average performance of O optimized SpecInt95 benchmark binaries created by the HP product C   compiler is improved to a level comparable to their O4 optimized version running without Dynamo. Dynamo achieves this by focusing its efforts on optimization opportunities that tend to manifest only at runtime, and hence opportunities that might be difficult for a static compiler to exploit. Dynamo's operation is transparent in the sense that it does not depend on any user annotations or binary instrumentation, and does not require multiple runs, or any special compiler, operating system or hardware support. The Dynamo prototype presented here is a realistic implementation running on an HP PA8000 workstation under the HPUX 10.20 operating system.
p2059
aVThis paper introduces Bitwise, a compiler that \u000aminimizes the bitwidth the number of bits used to represent each operand for both integers and pointers in a program. By propagating 70\u000astatic information both forward and backward in the program dataflow graph, Bitwise frees the programmer from declaring bitwidth invariants in cases where the compiler can determine bitwidths automatically.  Because loop instructions comprise the bulk of dynamically executed instructions, Bitwise incorporates sophisticated loop analysis techniques for identifying bitwidths. We find a rich opportunity for bitwidth reduction in modern multimedia and streaming application workloads. For new architectures that support subword datatypes, we expect that our bitwidth reductions will  save power and increase processor performance.\u000a This paper also applies our analysis to silicon compilation, the\u000atranslation of programs into custom hardware, to realize the full benefits of bitwidth reduction. We describe our integration of Bitwise with the DeepC Silicon Compiler. By taking advantage of bitwidth information during architectural synthesis, we reduce silicon real estate by 15  86%, improve clock speed by 3  249%, and reduce power by 46  73%. The next era of general purpose and reconfigurable architectures should strive to capture a portion of these gains.
p2060
aVThis paper presents a new approach to local instruction scheduling  based on integer programming that produces optimal instruction schedules in a reasonable time, even for very large basic blocks. The new approach first uses a set of graph transformations to simplify the datadependency graph while preserving the optimality of the final schedule. The simplified graph results in a simplified integer program which can be solved much faster. A new integerprogramming formulation is then applied to the simplified graph. Various techniques are used to simplify the formulation, resulting in fewer integerprogram variables, fewer integerprogram constraints and fewer terms in some of the remaining constraints, thus reducing integerprogram solution time. The new formulation also uses certain adaptively added constraints (cuts) to reduce solution time. The proposed optimal instruction scheduler is built within the Gnu Compiler Collection (GCC) and is evaluated experimentally using the SPEC95 floating point benchmarks. Although optimal scheduling for the target processor is considered intractable, all of the benchmarks' basic blocks are optimally scheduled, including blocks with up to 1000 instructions, while total compile time increases by only 14%.
p2061
aVSoftware pipelining is a loop scheduling technique that extracts\u000aparallelism out of loops by overlapping the execution of several\u000aconsecutive iterations. Due to the overlapping of iterations, \u000aschedules impose high register requirements during their execution. \u000aA schedule is valid if it requires at most the number of registers \u000aavailable in the target architecture. If not, its register requirements\u000ahave to be reduced either by decreasing the iteration overlapping or by \u000aspilling registers to memory. In this paper we describe a set of heuristics  \u000ato increase the quality of registerconstrained modulo schedules. The heuristics decide between the two previous alternatives and  define criteria for effectively selecting spilling candidates. The heuristics proposed for reducing the  register pressure can be applied to any software pipelining technique. The proposals are evaluated using a registerconscious software pipeliner on a workbench composed of a large set of loops from the Perfect Club benchmark and a set of processor configurations. Proposals in this paper are compared against a previous proposal already described in the literature. For one of these processor configurations and the set of loops that do not fit in the available registers (32), a speedup of 1.68 and a reduction of the memory traffic by a factor of 0.57 are achieved with an affordable increase in compilation time. For all the loops, this represents a speedup of 1.38 and a reduction of the memory traffic by a factor of 0.7.
p2062
aVIncreasing focus on multimedia applications has prompted the addition\u000aof multimedia extensions to most existing general purpose microprocessors.  This added functionality comes primarily with the addition of short SIMD instructions.  Unfortunately, access to these instructions is limited to inline assembly and library calls. Generally, it has been assumed that vector compilers provide the most promising means of exploiting multimedia instructions. Although vectorization technology is well understood, it is inherently complex and fragile. In addition, it is incapable of locating SIMDstyle parallelism within a basic block.\u000aIn this paper we introduce the concept of Superword Level Parallelism (SLP) ,a novel way of viewing parallelism in multimedia and scientific applications. We believe SLPP is  fundamentally different from the loop level parallelism exploited by traditional vector processing, and therefore demands a new method of extracting it.  We have developed a simple and robust compiler for detecting SLPP that targets basic blocks rather than loop nests.  As with techniques designed to extract ILP, ours is able to exploit parallelism both across loop iterations and within basic blocks. The result is an algorithm that provides excellent performance in several application domains. In our experiments, dynamic instruction counts were reduced by 46%. Speedups ranged from 1.24 to 6.70.
p2063
aVIrregular array accesses are array accesses whose array subscripts  do  not have closedform expressions in terms of loop indices. Traditional array analysis and loop transformation techniques cannot handle irregular array accesses. In this paper, we study two kinds of simple and common cases of irregular array accesses: singleindexed access and indirect array access. We present techniques to analyze these two cases at compiletime, and we provide experimental results showing the effectiveness of these techniques in finding more implicit loop parallelism at compiletime and improved speedups.
p2064
aVRecently, there have been several experimental and theoretical results \u000ashowing significant performance benefits of recursive algorithms on both\u000amultilevel memory hierarchies and on sharedmemory systems.  In particular, such algorithms have the data reuse characteristics of a blocked algorithm that is simultaneously blocked at many different levels.  Most existing applications, however, are written using ordinary loops.  We present a new compiler transformation that can be used to convert loop nests into recursive form automatically.  We show that the algorithm is fast and effective, handling loop nests with arbitrary nesting and control flow.  The transformation achieves substantial performance improvements for several linear algebra codes even on a current system with a two level  cache hierarchy. As a sideeffect of this work, we also develop an improved algorithm for transitive dependence analysis (a powerful technique used in the recursion transformation and other loop transformations)that is much faster than the best previously known algorithm in practice.
p2065
aVThis paper presents a novel framework for the symbolic bounds analysis of pointers, array indices, and accessed memory regions.  Our\u000aframework formulates each analysis problem as a system of  inequality constraints between symbolic bound polynomials. It then reduces the constraint system to a linear program. The solution to the linear program provides symbolic lower and upper bounds for the values of pointer and array index variables and for the regions of memory that each statement and procedure accesses. This approach eliminates fundamental problems associated with applying standard fixedpoint approaches to symbolic \u000aanalysis problems. Experimental results from our implemented compiler show that the analysis can solve several important problems, including static\u000arace detection, automatic parallelization, static detection of array\u000abounds violations, elimination of array bounds checks, and \u000areduction of the number of bits used to store computed values.
p2066
aVDynamic class loading during program execution in the Java Programming Language is an impediment for generating code that is as efficient as code generated using static wholeprogram analysis and optimization. Wholeprogram analysis and optimization is possible for languages, such as C++, that do not allow new classes and/or methods to be  loaded during program execution. One solution for performing wholeprogram analysis and\u000aavoiding incorrect execution after a new class is loaded is to invalidate and recompile affected methods. Runtime invalidation and recompilation mechanisms can be expensive in both space and time, and, therefore, generally restrict optimization.\u000aTo address these drawbacks, we propose a new framework, called the extant analysis  framework, for interprocedural optimization  of programs that support dynamic class (or method)loading. Given a set of classes comprising the  closed world, we perform an offline static analysis \u000awhich partitions references into two categories:(1) unconditionally extant references which point only to objects whose runtime type is guaranteed to be in the closed world; and (2) conditionally extant references which point to objects whose runtime type is not guaranteed to be in the closed world. Optimizations solely dependent on the first categorycan be statically performed, and are guaranteed to be correct even with any future class/method loading. Optimizations dependent on the second category are guarded by  dynamic tests, called   extant safety tests, for correct execution behavior.We describe the properties for extant safety tests, and provide algorithms for their generation and placement.
p2067
aVWe present a new technique for removing unnecessary synchronization operations from statically compiled Java programs. Our approach improves upon current efforts based on escape analysis, as it can eliminate synchronization operations even on objects that escape their allocating threads. It makes use of a compact, equivalenceclassbased representation that eliminates the need for fixed point operations during the analysis. \u000aWe describe and evaluate the performance of an implementation in the\u000aMarmot native Java compiler.  For the benchmark programs examined, the optimization removes 100% of the dynamic synchronization operations in singlethreaded programs, and 099% in multithreaded programs, at a low cost in additional compilation time and code growth.
p2068
aVThis paper presents a static race detection analysis for multithreaded Java programs. Our analysis is based on a formal type system that is capable of capturing many common synchronization patterns. These patterns include classes with internal synchronization, classes thatrequire clientside synchronization, and threadlocal classes. Experience checking over 40,000 lines of Java code with the type system demonstrates that it is an effective approach for eliminating races conditions. On large examples, fewer than 20 additional type annotations per 1000 lines of code were required by the type checker, and we found a number of races in the standard Java libraries and other test programs.
p2069
aVA highperformance implementation of a Java  Virtual Machine (JVM) consists of efficient implementation of JustInTime (JIT) compilation, exception handling, synchronization mechanism, and garbage collection (GC).  These components are tightly coupled to achieve high performance. In this paper, we present some static anddynamic techniques implemented in the JIT compilation and exception handling of the Microprocessor Research Lab Virtual Machine (MRL VM), i.e., lazy exceptions, lazy GC mapping, dynamic patching, and bounds checking elimination. Our experiments used IA32 as the hardware platform, but the optimizations can be generalized to other architectures.
p2070
aVFunctional Reactive Programming, or FRP, is a general framework for programming hybrid systems in a highlevel, declarative manner. The key ideas in FRP are its notions of behaviors and events. Behaviors are timevarying, reactive values, while events are timeordered sequences of discretetime event occurrences. FRP is the essence of Fran, a domainspecific language embedded in Haskell for programming reactive animations, but FRP is now also being used in vision, robotics and other control systems applications. \u000aIn this paper we explore the formal semantics of FRP and how it\u000arelates to an implementation based on streams that represent (and therefore only approximate) continuous behaviors. We show that, in the limit as the sampling interval goes to zero, the implementation is faithful to the formal, continuous semantics, but only when certain constraints on behaviors are observed. We explore the nature of these constraints, which vary amongst the FRP primitives. Our results show both the power and limitations of this approach to language design and implementation. As an example of a limitation, we show that streams are incapable of representing instantaneous predicate events over behaviors.
p2071
aVThis paper shows that a type graph (obtained via polymorphic type\u000ainference) harbors explicit directional flow paths between functions. These flow paths arise from the instantiations of polymorphic types and correspond to callreturn sequences in firstorder programs. We show that flow information can be computed efficiently while considering only paths with well matched callreturn sequences, even in the higherorder case.  Furthermore, we present a practical algorithm for inferring type instantiation graphs and provide empirical evidence to the scalability of the presented techniques by applying them in the context of pointsto analysis for C programs.
p2072
aVWe describe a new method for determining when an object can be\u000agarbage collected. The method does not require marking live objects.  \u000aInstead, each object X is dynamically\u000aassociated with a stack frame M, such that Xis collectable when M pops. Because X could have been dead earlier, our method is conservative. Our results demonstrate that the method nonetheless identifies a large percentage of collectable objects. The method has been implemented in Sun's Java Virtual Machine interpreter, and results are presented based on this implementation
p2073
aVAn onthefly garbage collector does not stop the program threads to perform the collection. Instead, the collector executes in a separate thread (or process) in parallel to the program. Onthefly collectors are useful for multithreaded applications running on multiprocessor servers, where it is important to fully utilize all processors and provide even response time, especially for systems for which \u000astopping the threads is a costly operation. \u000aIn this work, we report on the incorporation of generations into \u000aan onthefly garbage collector. The incorporation is nontrivial since an onthefly collector avoids explicit synchronization with the program threads. To the best of our knowledge, such an incorporation has not been tried before. We have implemented the collector for a prototype Java Virtual Machine on AIX, and measured its performance on a 4way multiprocessor.\u000aAs for other generational collectors, an onthefly generational\u000acollector has the potential for reducing the overall running time and working set of an application by concentrating collection efforts on the young objects. However, in contrast to other generational collectors,\u000aonthefly collectors do not move the objects; thus, there is no segregation between the old and the young objects. Furthermore, onthefly collectors do not stop the threads, so there is no extra benefit for the short pauses obtained by generational collection. Nevertheless, comparing our onthefly collector with and without generations, it turns out that the generational collector performs better for most applications. The best reduction in overall running time for the benchmarks we measured was 25%. However, there were some benchmarks for which it had no effect and one for which the overall running time increased by 4%.
p2074
aVWe present mechanisms that enable our compilertarget language, C , to express four of the best known techniques for implementing exceptions, all within a single, uniform framework. We define the mechanisms precisely, using a formal operational semantics. We also show that exceptions need not require special treatment in the optimizer; by introducing extra dataflow edges, we make standard optimization techniques work even on programs that use exceptions. Our approach clarifies the design space of exceptionhandling techniques, and it allows a single optimizer to handle a variety of implementation techniques. Our ultimate goal is to allow a sourcelanguage compiler the freedom to choose its exceptionhandling policy, while encapsulating the architecturedependent mechanisms and their  optimization in an implementation of C that can be used by compilers for many source languages.
p2075
aVThis paper discusses our research into algorithms for creating an\u000aefficient bidirectional debugger in which all traditional forward movement commands can be performed with equal ease in the reverse direction. We expect that adding these backwards movement capabilities to a debugger will greatly increase its efficacy as a programming tool.\u000aThe efficiency of our methods arises from our use of event counters\u000athat are embedded into the program being debugged.  These counters are\u000aused to precisely identify the desired target event on the fly as the\u000atarget program executes.  This is in contrast to traditional debuggers that may trap back to the debugger many times for some movements.  For reverse movements we reexecute the program (possibly using two passes) to identify and stop at the desired earlier point. Our counter based techniques are essential for these reverse movements because they allow us to efficiently execute through the millions of events encountered during reexecution.\u000aTwo other important components of this debugger are its I/O logging and checkpointing.  We log and later replay the results of system calls\u000ato ensure deterministic reexecution, and we use checkpointing to bound the\u000aamount of reexecution used for reverse movements.  Short movements generally appear instantaneous, and the time for longer movements is usually bounded within a small constant factor of the temporal distance moved back.
p2076
aVThis paper describes the implementation of a purely functional\u000aprogramming language for building software systems. In this language,\u000aexternal tools like compilers and linkers are invoked by function calls. Because some function calls are extremely expensive, it is obviously important to reuse the results of previous function calls whenever possible. Caching a function call requires the language interpreter to record all values on which the function call depends. For optimal caching, it is important to record precise dependencies that are both dynamic and finegrained. The paper sketches how we compute such dependencies, describes the implementation of an efficient function cache, and evaluates our implementation's performance.
p2077
aVTo guarantee typesafe execution, Java and other strongly typed languages require bounds checking of array accesses. Because arraybounds checks may raise exceptions, they block code motion of instructions with side effects, thus preventing many useful code optimizations, such as partial redundancy elimination or instruction scheduling of memory operations. Furthermore, because it is not expressible at bytecode level, the elimination of bounds checks can only be performed at run time, after the bytecode program is loaded. Using existing powerful boundscheck optimizers at run time is not feasible, however, because they are too heavyweight for the dynamic compilation setting.\u000aABCD is a lightweight algorithm for elimination of Array Bounds Checks on Demand.  Its design emphasizes simplicity and efficiency. In essence, ABCD works by adding a few edges to the SSA value graph and performing a simple traversal of the graph. Despite its simplicity, ABCD is surprisingly powerful. On our benchmarks, ABCD removes on average 45% of dynamic bound check instructions, sometimes achieving nearideal optimization. \u000aThe efficiency of ABCD stems from two factors. First, ABCD works on\u000aa sparse representation. As a result, it requires on average fewer than 10 simple analysis steps per bounds check.  Second, ABCD is demanddriven. It can be applied to a set of frequently executed (hot) bounds checks, which makes it suitable for the dynamiccompilation setting, in which compiletime cost is constrained but hot  statements are known.
p2078
aVWe present a new limited form of interprocedural analysis called   field analysis that can be used by a compiler to reduce the costs of modern language features such as objectoriented programming,\u000aautomatic memory management, and runtime checks required for type safety.\u000aUnlike many previous interprocedural analyses, our analysis is cheap, and does not require access to the entire program. Field analysis exploits the declared access restrictions placed on fields in a modular language (e.g. field access modifiers in Java) in order to determine useful properties of fields of an object.\u000a We describe our implementation of field analysis in the Swift\u000aoptimizing compiler for Java, as well a set of optimizations that\u000aexploit the results of field  analysis. These optimizations include removal of runtime tests, compiletime resolution of method calls, object inlining, removal of unnecessary synchronization, and stack allocation.  Our results demonstrate that field analysis is efficient and effective.  Speedups average 7% on a wide range of applications, with some times reduced by up to 27%.  Compile time overhead of field analysis is about 10%.
p2079
aVThis paper describes splitstream dictionary (SSD) compression, a new  technique for transforming programs into a compact, interpretable form. We define a compressed program as interpretable when it can be decompressed at basicblock granularity with reasonable efficiency. The granularity requirement enables interpreters or justintime (JIT) translators to decompress basic blocks incrementally during program execution. Our previous approach to interpretable compression, the Bytecoded RISC (BRISC) program format [1], achieved unprecedented decompression speed in excess of 5 megabytes per second on a 450MHz Pentium II while compressing benchmark programs to an average of threefifths the size of their optimized x86 representation. SSD compression combines the key idea behind BRISC with new observations about instruction reuse frequencies to yield four advantages over BRISC and other competing techniques. First, SSD is simple, requiring only a few pages of code for an effective implementation. Second, SSD compresses programs more effectively than any interpretable program compression scheme known to us. For example, SSD compressed a set of programs including the spec95 benchmarks and Microsoft Word97 to less than half the size, on average, of their optimized x86 representation. Third, SSD exceeds BRISC's decompression and JIT translation rates by over 50%. Finally, SSD's twophased approach to JIT translation enables a virtual machine to provide graceful degradation of program execution time in the face of increasing RAM constraints. For example, using SSD, we ran   Word97 using a JITtranslation buffer onethird the size of Word97's optimized x86 code, yet incurred only 27% execution time overhead.
p2080
aVThis paper describes a new algorithm for flow and context insensitive  pointer analysis of C programs. Our studies show that the most common use of pointers in C programs is in passing the addresses of composite objects or updateable values as arguments to procedures. Therefore, we have designed a lowcost algorithm that handles this common case accurately. In terms of both precision and running time, this algorithm lies between  Steensgaard's algorithm, which treats assignments bidirectionally using unification, and Andersen's algorithm, which treats assignments directionally using subtyping. Our \u201cone level flow\u201d algorithm uses a restricted form of subtyping to avoid unification of symbols at the top levels of pointer chains in the pointsto graph, while using   unification  elsewhere in the graph. The method scales easily to large programs. For instance, we are able to analyze a 1.4 MLOC (million lines of code) program in two minutes, using less than 200MB of memory. At the same time, the precision of our algorithm is very close to that of Andersen's algorithm. On all of the integer benchmark programs from SPEC95, the one level flow algorithm and Andersen's algorithm produce either identical or essentially identical pointsto information. Therefore, we claim that our algorithm provides a method for obtaining precise flowinsensitive pointsto information for large C programs.
p2081
aVMost compiler optimizations and software productivity tools rely on\u000ainformation about the effects of pointer dereferences in a program.\u000aThe purpose of pointsto analysis is to compute this information\u000asafely, and as accurately as is practical. Unfortunately, accurate\u000apointsto information is difficult to obtain for large programs,\u000abecause the time and space requirements of the analysis become\u000aprohibitive.\u000aWe consider the problem of scaling flow and contextinsensitive\u000apointsto analysis to large programs, perhaps containing hundreds of\u000athousands of lines of code. Our approach is based on a variable substitution transformation, which is performed offline, i.e.,\u000abefore a standard pointsto analysis is performed. The general idea of\u000avariable substitution is that a   set  of variables in a program can be replaced by a single representative variable, thereby reducing the input size of the problem. Our main contribution is a lineartime algorithm which finds a particular variable substitution that maintains the precision of the standard analysis, and is also very effective in reducing the size of the problem.\u000aWe report our experience in performing pointsto analysis on large\u000aC programs, including some industrialsized ones. Experiments show that\u000aour algorithm can reduce the cost of Andersen's pointsto analysis substantially: on average, it reduced the running time by 53% and the memory cost by 59%, relative to an efficient baseline implementation of the analysis.
p2082
aVIn this paper we present a modular interprocedural pointer analysis \u000aalgorithm based on accesspaths for C programs. We argue that access paths can reduce the overhead of representing contextsensitive transfer functions and effectively distinguish nonrecursive heap objects. And when the modular analysis paradigm is used together with other techniques to handle type casts and function pointers, we are able to handle significant programs like those in the SPECcint92 and SPECcint95 suites. We have implemented the algorithm and tested it on a Pentium II 450 PC running Linux. The observed resource consumption and performance improvement are very encouraging.
p2083
aVWe show how to determine statically whether it is safe for untrusted \u000amachine code to be loaded into a trusted host system.\u000aOur safetychecking technique operates directly on the untrusted machinecode program, requiring only that the initial inputs to the untrusted program be annotated with typestate information and linear constraints. This approach opens up the possibility of being able to certify code produced by any compiler from any source language, which gives the code producers more freedom in choosing the language in which they write their programs. It eliminates the dependence of safety on the correctness of the compiler because the final product of the compiler is checked. It leads to the decoupling of the safety policy from the language in which the untrusted   code is written, and consequently, makes it possible for safety checking to be performed with respect to an extensible set of safety properties that are specified on the host side.\u000aWe have implemented a prototype safety checker for SPARC machinelanguage programs, and applied the safety checker to several examples. The safety checker was able to either prove that an example met the necessary safety conditions, or identify the places where the safety conditions were violated. The checking times ranged from less than a second to 14 seconds on an UltraSPARC machine.
p2084
aVWe describe a translation validation infrastructure for the GNU C \u000acompiler. During the compilation the infrastructure compares the intermediate form of the program before and after each compiler pass and verifies the preservation of semantics. We discuss a general framework that the optimizer can use to communicate to the validator what transformations were performed. Our implementation however does not rely on help from the optimizer and it is quite successful by using instead a few heuristics to detect the transformations that take place.\u000aThe main message of this paper is that a practical translation validation infrastructure, able to check the correctness of many of the transformations performed by a realistic compiler, can be implemented with about the effort typically required to implement one compiler pass. We demonstrate this in the context of the GNU C compiler for a number of its optimizations while compiling realistic programs such as the compiler itself or the Linux kernel. We believe that the price of such an infrastructure is small considering the qualitative increase in the ability to isolate compilation errors during compiler testing and maintenance.
p2085
aVThis paper presents the initial results of a project to determine if\u000athe techniques of  proofcarrying code and \u000acertifying compilers can be applied to programming languages of realistic size and complexity. The experiment shows that: (1) it is possible to implement a certifying nativecode compiler for a large subset of the Java programming language; (2) the compiler is freely able to apply many standard local and global optimizations; and (3) the PCC binaries it produces are of reasonable size and can be rapidly checked for type safety by a small proofchecker. This paper also presents further evidence that PCC provides several advantages for compiler development. In particular, generating proofs of the target code helps to identify compiler bugs, many of which would have been difficult to discover by testing.
p2086
aVThe Microsoft.NET Common Language Runtime provides a shared type system, intermediate language and dynamic execution environment for the implementation and interoperation of multiple source languages. In this paper we extend it with direct support for parametric polymorphism (also known as generics), describing the design through examples written in an extended version of the C# programming language, and explaining aspects of implementation by reference to a prototype extension to the runtime.\u000aOur design is very expressive, supporting parameterized types, polymorphic static, instance and virtual methods, \u201cFbounded\u201d type parameters, instantiation at pointer and value types, polymorphic recursion, and exact runtime types. The implementation takes advantage of the dynamic nature of the runtime, performing justintime type specialization, representationbased code sharing and novel techniques for efficient creation and use of runtime types.\u000aEarly performance results are encouraging and suggest that programmers will not need to pay an overhead for using generics, achieving performance almost matching handspecialized code.
p2087
aVWe present a heapprofiling tool for exploring the potential for space savings in Java programs. The output of the tool is used to direct rewriting of application source code in a way that allows more timely garbage collection (GC) of objects, thus saving space. The rewriting can also avoid allocating some objects that are never used.\u000aThe tool measures the difference between the actual collection time and the potential earliest collection time of objects for a Java application. This time difference indicates potential savings. Then the tool sorts the allocation sites in the application source code according to the accumulated potential space saving for the objects allocated at the sites. A programmer can investigate the source code surrounding the sites with the highest savings to find opportunities for code rewriting that could save space. Our experience shows that in many cases simple code rewriting leads to actual space savings and in some cases also to improvements in program runtime.\u000aExperimental results using the tool and manually rewriting code show average space savings of 18% for the SPECjvm98 benchmark suite. Results for other benchmarks are also promising. We have also classified the program transformations that we have used and argue that in many cases improvements can be achieved by an optimizing compiler.
p2088
aVWe describe a parallel, realtime garbage collector and present experimental results that demonstrate good scalability and good realtime bounds. The collector is designed for sharedmemory multiprocessors and is based on an earlier collector algorithm [2], which provided fixed bounds on the time any thread must pause for collection. However, since our earlier algorithm was designed for simple analysis, it had some impractical features. This paper presents the extensions necessary for a practical implementation: reducing excessive interleaving, handling stacks and global variables, reducing double allocation, and special treatment of large and small objects. An implementation based on the modified algorithm is evaluated on a set of 15 SML benchmarks on a Sun Enterprise 10000, a 64way UltraSparcII multiprocessor. To the best of our knowledge, this is the first implementation of a parallel, realtime garbage collector.\u000aThe average collector speedup is 7.5 at 8 processors and 17.7 at 32 processors. Maximum pause times range from 3 ms to 5 ms. In contrast, a nonincremental collector (whether generational or not) has maximum pause times from 10 ms to 650 ms. Compared to a nonparallel, stopcopy collector, parallelism has a 39% overhead, while realtime behavior adds an additional 12% overhead. Since the collector takes about 15% of total execution time, these features have an overall time costs of 6% and 2%.
p2089
aVThis paper describes the design and implementation of a method for producing compact, bytecoded instruction sets and interpreters for them. It accepts a grammar for programs written using a simple bytecoded stackbased instruction set, as well as a training set of sample programs. The system transforms the grammar, creating an expanded grammar that represents the same language as the original grammar, but permits a shorter derivation of the sample programs and others like them. A program's derivation under the expanded grammar forms the compressed bytecode representation of the program. The interpreter for this bytecode is automatically generated from the original bytecode interpreter and the expanded grammar. Programs expressed using compressed bytecode can be substantially smaller than their original bytecode representation and even their machine code representation. For example, compression cuts the bytecode for lcc from 199KB to 58KB but increases the size of the interpreter by just over 11KB.
p2090
aVDynamic compilation and optimization are widely used in heterogenous computing environments, in which an intermediate form of the code is compiled to native code during execution. An important trade off exists between the amount of time spent dynamically optimizing the program and the running time of the program. The time to perform dynamic optimizations can cause significant delays during execution and also prohibit performance gains that result from more complex optimization.\u000aIn this research, we present an annotation framework that substantially reduces compilation overhead of Java programs. Annotations consist of analysis information collected offline and are incorporated into Java programs. The annotations are then used by dynamic compilers to guide optimization. The annotations we present reduce compilation overhead incurred at all stages of compilation and optimization as well as enable complex optimizations to be performed dynamically. On average, our annotation optimizations reduce optimized compilation overhead by 78% and enable speedups of 7% on average for the programs examined.
p2091
aVInstrumenting code to collect profiling information can cause substantial execution overhead. This overhead makes instrumentation difficult to perform at runtime, often preventing many known offline feedbackdirected optimizations from being used in online systems. This paper presents a general framework for performing instrumentation sampling to reduce the overhead of previously expensive instrumentation. The framework is simple and effective, using codeduplication and counterbased sampling to allow switching between instrumented and noninstrumented code.\u000aOur framework does not rely on any hardware or operating system support, yet provides a high frequency sample rate that is tunable, allowing the tradeoff between overhead and accuracy to be adjusted easily at runtime. Experimental results are presented to validate that our technique can collect accurate profiles (9398% overlap with a perfect profile) with low overhead (averaging 6% total overhead with a naive implementation). A Jalape~ nospecific optimization is also presented that reduces overhead further, resulting in an average total overhead of 3%.
p2092
aVA whole program path (WPP) is a complete control flow trace of a program's execution. Recently Larus [18] showed that although WPP is expected to be very large (100's of MBytes), it can be greatly compressed (to 10's of MBytes) and therefore saved for future analysis. While the compression algorithm proposed by Larus is highly effective, the compression is accompanied with a loss in the ease with which subsets of information can be accessed. In particular, path traces pertaining to a particular function cannot generally be obtained without examining the entire compressed WPP representation. To solve this problem we advocate the application of compaction techniques aimed at providing easy access to path traces on a per function basis.\u000aWe present a WPP compaction algorithm in which the WPP is broken in to path traces corresponding to individual function calls. All of the path traces for a given function are stored together as a block. Ability to construct the complete WPP from individual path traces is preserved by maintaining a dynamic call graph. The compaction is achieved by eliminating redundant path traces that result from different calls to a function and by replacing a sequence of static basic block ids that correspond to a dynamic basic block by a single id. We transform a compacted WPP representation into a timestamped WPP (TWPP) representation in which the path traces are organized from the perspective of dynamic basic blocks. TWPP representation also offers additional opportunities for compaction.\u000aExperiments show that our algorithm compacts the WPPs by factors ranging from 7 to 64. At the same time information is organized in a highly accessible form which speeds up the responses to queries requesting the path traces of a given function by over 3 orders of magnitude.
p2093
aVWith the growing processormemory performance gap, understanding and optimizing a program's reference locality, and consequently, its cache performance, is becoming increasingly important. Unfortunately, current reference locality optimizations rely on heuristics and are fairly adhoc. In addition, while optimization technology for improving instruction cache performance is fairly mature (though heuristicbased), data cache optimizations are still at an early stage. We believe the primary reason for this imbalance is the lack of a suitable representation of a program's dynamic data reference behavior and a quantitative basis for understanding this behavior.\u000aWe address these issues by proposing a quantitative basis for understanding and optimizing reference locality, and by describing efficient data reference representations and an exploitable locality abstraction that support this framework. Our data reference representations (Whole Program Streams and Stream Flow Graphs) are compact\u2014two to four orders of magnitude smaller than the program's data reference trace\u2014and permit efficient analysis\u2014on the order of seconds to a few minutes\u2014even for complex applications. These representations can be used to efficiently compute our exploitable locality abstraction (hot data streams). We demonstrate that these representations and our hot data stream abstraction are useful for quantifying and exploiting data reference locality. We applied our framework to several SPECint 2000 benchmarks, a graphics program, and a commercial Microsoft database application. The results suggest significant opportunity for hot data streambased locality optimizations.
p2094
aVModel checking has been widely successful in validating and debugging designs in the hardware and protocol domains. However, statespace explosion limits the applicability of model checking tools, so model checkers typically operate on abstractions of systems.\u000aRecently, there has been significant interest in applying model checking to software. For infinitestate systems like software, abstraction is even more critical. Techniques for abstracting software are a prerequisite to making software model checking a reality.\u000aWe present the first algorithm to automatically construct a predicate abstraction of programs written in an industrial programming language such as C, and its implementation in a tool \u2014 C2BP. The C2BP tool is part of the SLAM toolkit, which uses a combination of predicate abstraction, model checking, symbolic reasoning, and iterative refinement to statically check temporal safety properties of programs.\u000aPredicate abstraction of software has many applications, including detecting program errors, synthesizing program invariants, and improving the precision of program analyses through predicate sensitivity. We discuss our experience applying the C2BP predicate abstraction tool to a variety of problems, ranging from checking that listmanipulating code preserves heap invariants to finding errors in Windows NT device drivers.
p2095
aVMany important applications must run continuously and without interruption, yet must be changed to fix bugs or upgrade functionality. No prior generalpurpose methodology for dynamic updating achieves a practical balance between flexibility, robustness, low overhead, and ease of use.\u000aWe present a new approach for Clike languages that provides typesafe dynamic updating of native code in an extremely flexible manner (code, data, and types may be updated, at programmerdetermined times) and permits the use of automated tools to aid the programmer in the updating process. Our system is based on dynamic patches that both contain the updated code and the code needed to transition from the old version to the new. A novel aspect of our patches is that they consist of verifiable native code (e.g. ProofCarrying Code [17] or Typed Assembly Language [16]), which is native code accompanied by annotations that allow online verification of the code's safety. We discuss how patches are generated mostly automatically, how they are applied using dynamiclinking technology, and how code is compiled to make it updateable.\u000aTo concretely illustrate our system, we have implemented a dynamicallyupdateable web server, FlashEd. We discuss our experience building and maintaining FlashEd. Performance experiments show that for FlashEd, the overhead due to updating is typically less than 1%.
p2096
aVWe present an extension of field analysis (sec [4]) called related field analysis which is a general technique for proving relationships between two or more fields of an object. We demonstrate the feasibility and applicability of related field analysis by applying it to the problem of removing array bounds checks. For array bounds check removal, we define a pair of related fields to be an integer field and an array field for which the integer field has a known relationship to the length of the array. This related field information can then be used to remove array bounds checks from accesses to the array field. Our results show that related field analysis can remove an average of 50% of the dynamic array bounds checks on a wide range of applications.\u000aWe describe the implementation of related field analysis in the Swift optimizing compiler for Java, as well as the optimizations that exploit the results of related field analysis.
p2097
aVWe present a new framework for verifying partial specifications of programs in order to catch type and memory errors and check data structure invariants. Our technique can verify a large class of data structures, namely all those that can be expressed as graph types. Earlier versions were restricted to simple special cases such as lists or trees. Even so, our current implementation is as fast as the previous specialized tools.\u000aPrograms are annotated with partial specifications expressed in Pointer Assertion Logic, a new notation for expressing properties of the program store. We work in the logical tradition by encoding the programs and partial specifications as formulas in monadic secondorder logic. Validity of these formulas is checked by the MONA tool, which also can provide explicit counterexamples to invalid formulas.\u000aTo make verification decidable, the technique requires explicit loop and function call invariants. In return, the technique is highly modular: every statement of a given program is analyzed only once.\u000aThe main target applications are safetycritical datatype algorithms, where the cost of annotating a program with invariants is justified by the value of being able to automatically verify complex properties of the program.
p2098
aVWe present a unified mathematical framework for analyzing the tradeoffs between parallelism and storage allocation within a parallelizing compiler. Using this framework, we show how to find a good storage mapping for a given schedule, a good schedule for a given storage mapping, and a good storage mapping that is valid for all legal schedules. We consider storage mappings that collapse one dimension of a multidimensional array, and programs that are in a single assignment form with a onedimensional schedule. Our technique combines affine scheduling techniques with occupancy vector analysis and incorporates general affine dependences across statements and loop nests. We formulate the constraints imposed by the data dependences and storage mappings as a set of linear inequalities, and apply numerical programming techniques to efficiently solve for the shortest occupancy vector. We consider our method to be a first step towards automating a procedure that finds the optimal tradeoff between parallelism and storage space.
p2099
aVMany graphcoloring registerallocation algorithms don't work well for machines with few registers. Heuristics for liverange splitting are complex or suboptimal; heuristics for register assignment rarely factor the presence of fancy addressing modes; these problems are more severe the fewer registers there are to work with. We show how to optimally split live ranges and optimally use addressing modes, where the optimality condition measures dynamically weighted loads and stores but not registerregister moves. Our algorithm uses integer linear programming but is much more efficient than previous ILPbased approaches to register allocation. We then show a variant of Park and Moon's optimistic coalescing algorithm that does a very good (though not provably optimal) job of removing the registerregister moves. The result is Pentium code that is 9.5% faster than code generated by SSAbased splitting with iterated register coalescing.
p2100
aVWe describe the design and implementation of a system for very fast pointsto analysis. On code bases of about million lines of unpreprocessed C code, our system performs fieldbased Andersenstyle pointsto analysis in less than a second and uses less than 10MB of memory. Our two main contributions are a databasecentric analysis architecture called compilelinkanalyze (CLA), and a new algorithm for implementing dynamic transitive closure. Our pointsto analysis system is built into a forward datadependence analysis tool that is deployed within Lucent to help with consistent type modifications to large legacy C code bases.
p2101
aVMost programming languages use static scope rules for associating uses of identifiers with their declarations. Static scope helps catch errors at compile time, and it can be implemented efficiently. Some popular languages\u2014Perl, Tel, TeX, and Postscript\u2014offer dynamic scope, because dynamic scope works well for variables that \u201ccustomize\u201d the execution environment, for example. Programmers must simulate dynamic scope to implement this kind of usage in statically scoped languages. This paper describes the design and implementation of imperative language constructs for introducing and referencing dynamically scoped variables\u2014dynamic variables for short. The design is a minimalist one, because dynamic variables are best used sparingly, much like exceptions. The facility does, however, cater to the typical uses for dynamic scope, and it provides a cleaner mechanism for socalled threadlocal variables. A particularly simple implementation suffices for languages without exception handling. For languages with exception handling, a more efficient implementation builds on existing compiler infrastructure. Exception handling can be viewed as a control construct with dynamic scope. Likewise, dynamic variables are a data construct with dynamic scope.
p2102
aVAsynchronous exceptions, such as timeouts are important for robust, modular programs, but are extremely difficult to program with \u2014 so much so that most programming languages either heavily restrict them or ban them altogether. We extend our earlier work, in which we added synchronous exceptions to Haskell, to support asynchronous exceptions too. Our design introduces scoped combinators for blocking and unblocking asynchronous interrupts, along with a somewhat surprising semantics for operations that can suspend. Uniquely, we also give a formal semantics for our system.
p2103
aVWe develop from first principles an exact model of the behavior of loop nests executing in a memory hicrarchy, by using a nontraditional classification of misses that has the key property of composability. We use Presburger formulas to express various kinds of misses as well as the state of the cache at the end of the loop nest. We use existing tools to simplify these formulas and to count cache misses. The model is powerful enough to handle imperfect loop nests and various flavors of nonlinear array layouts based on bit interleaving of array indices. We also indicate how to handle modest levels of associativity, and how to perform limited symbolic analysis of cache behavior. The complexity of the formulas relates to the static structure of the loop nest rather than to its dynamic trip count, allowing our model to gain efficiency in counting cache misses by exploiting repetitive patterns of cache behavior. Validation against cache simulation confirms the exactness of our formulation. Our method can serve as the basis for a static performance predictor to guide program and data transformations to improve performance.
p2104
aVWe discuss the design and implementation of a compiler that translates formulas representing signal processing transforms into efficient C or Fortran programs. The formulas are represented in a language that we call SPL, an acronym from Signal Processing Language. The compiler is a component of the SPIRAL system which makes use of formula transformations and intelligent search strategies to automatically generate optimized digital signal processing (DSP) libraries. After a discussion of the translation and optimization techniques implemented in the compiler, we use SPL formulations of the fast Fourier transform (FFT) to evaluate the compiler. Our results show that SPIRAL, which can be used to implement many classes of algorithms, produces programs that perform as well as \u201chardwired\u201d systems like FFTW.
p2105
aVThis paper presents the design and implementation of Eventdriven Statemachines Programming (ESP)\u2014a language for programmable devices. In traditional languages, like C, using eventdriven statemachine forces a tradeoff that requires giving up ease of development and reliability to achieve high performance. ESP is designed to provide all of these three properties simultaneously.\u000aESP provides a comprehensive set of features to support development of compact and modular programs. The ESP compiler compiles the programs into two targets\u2014a C file that can be used to generate efficient firmware for the device; and a specification that can be used by a verifier like SPlN to extensively test the firmware.\u000aAs a case study, we reimplemented VMMC firmware that runs on Myrinet network interface cards using ESP. We found that ESP simplifies the task of programming with eventdriven state machines. It required an order of magnitude fewer lines of code than the previous implementation. We also found that modelchecking verifiers like SPIN can be used to effectively debug the firmware. Finally, our measurements indicate that the performance overhead of using ESP is relatively small.
p2106
aVKnown algorithms for pointer analysis are \u201cglobal\u201d in the sense that they perform an exhaustive analysis of a program or program component. In this paper we introduce a demanddriven approach for pointer analysis. Specifically, we describe a demanddriven flowinsensitive, subsetbased, con textinsensitive pointsto analysis. Given a list of pointer variables (a query), our analysis performs just enough computation to determine the pointsto sets for these query variables. Using deductive reachability formulations of both the exhaustive and the demanddriven analyses, we prove that our algorithm is correct. We also show that our analysis is optimal in the sense that it does not do more work than necessary. We illustrate the feasibility and efficiency of our analysis with an implementation of demanddriven pointsto analysis for computing the callgraphs of C programs with function pointers. The performance of our system varies substantially across benchmarks  the main factor is how much of the pointsto graph must be computed to determine the callgraph. For some benchmarks, only a small part of the pointsto graph is needed (e.g pouray emacs and gcc), and here we see more than a 10x speedup. For other benchmarks (e.g. burlap and gimp), we need to compute most (> 95%) of the pointsto graph, and here the demanddriven algorithm is considerably slower, because using the demanddriven algorithm is a slow method of computing the full pointsto graph.
p2107
aVWe present a new pointer and escape analysis. Instead of analyzing the whole program, the algorithm incrementally analyzes only those parts of the program that may deliver useful results. An analysis policy monitors the analysis results to direct the incremental investment of analysis resources to those parts of the program that offer the highest expected optimization return.\u000aOur experimental results show that almost all of the objects are allocated at a small number of allocation sites and that an incremental analysis of a small region of the program surrounding each site can deliver almost all of the benefit of a wholeprogram analysis. Our analysis policy is usually able to deliver this benefit at a fraction of the wholeprogram analysis cost.
p2108
aVIn this paper, we evaluate the benefits achievable from pointer analysis and other memory disambiguation techniques for C/C++ programs, using the framework of the production compiler for the Intel Itanium\u2122 processor. Most of the prior work on memory disambiguation has primarily focused on pointer analysis, and either presents only static estimates of the accuracy of the analysis (such as average pointsto set size), or provides performance data in the context of certain individual optimizations. In contrast, our study is based on a complete memory disambiguation framework that uses a whole set of techniques including pointer analysis. Further, it presents how various compiler analyses and optimizations interact with the memory disambiguator, evaluates how much they benefit from disambiguation, and measures the eventual impact on the performance of the program. The paper also analyzes the types of disambiguation queries that are typically received by the disambiguator, which disambiguation techniques prove most effective in resolving them, and what type of queries prove difficult to be resolved. The study is based on empirical data collected for the SPEC CINT2000 C/C++ programs, running on the Itanium processor.
p2109
aVRegionbased memory management systems structure memory by grouping objects in regions under program control. Memory is reclaimed by deleting regions, freeing all objects stored therein. Our compiler for C with regions, RC, prevents unsafe region deletions by keeping a count of references to each region. Using type annotations that make the structure of a program's regions more explicit, we reduce the overhead of reference counting from a maximum of 27% to a maximum of 11% on a suite of realistic benchmarks. We generalise these annotations in a region type system whose main novelty is the use of existentially quantified abstract regions to represent pointers to objects whose region is partially or totally unknown. A distribution of RC is available at http://www.cs.berkeley.edu/~dgay/rc.tar.gz.
p2110
aVProofcarrying code and typed assembly languages aim to minimize the trusted computing base by directly certifying the actual machine code. Unfortunately, these systems cannot get rid of the dependency on a trusted garbage collector. Indeed, constructing a provably typesafe garbage collector is one of the major open problems in the area of certifying compilation.\u000aBuilding on an idea by Wang and Appel, we present a series of new techniques for writing typesafe stopandcopy garbage collectors. We show how to use intensional type analysis to capture the contract between the mutator and the collector, and how the same method can be applied to support forwarding pointers and generations. Unlike Wang and Appel (which requires wholeprogram analysis), our new framework directly supports higherorder funtions and is compatible with separate compilation; our collectors are written in provably typesafe languages with rigorous semantics and fully formalized soundness proofs.
p2111
aVThe deployment of Java as a concurrent programming language has created a critical need for highperformance, concurrent, and incremental multiprocessor garbage collection. We present the Recycler, a fully concurrent pure reference counting garbage collector that we have implemented in the Jalapeo Java virtual machine running on shared memory multiprocessors.\u000aWhile a variety of multiprocessor collectors have been proposed and some have been implemented, experimental data is limited and there is little quantitative basis for comparison between different algorithms. We present measurements of the Recycler and compare it against a nonconcurrent but parallel loadbalancing markandsweep collector (that we also implemented in Jalapeo), and evaluate the classical tradeoff between response time and throughput.\u000aWhen processor or memory resources are limited, the Recycler runs at about 90% of the speed of the markandsweep collector. However, with an extra processor to run collection and with a moderate amount of memory headroom, the Recycler is able to operate without ever blocking the mutators and achieves a maximum measured mutator delay of only 2.6 milliseconds for our benchmarks. Endtoend execution time is usually within 5%.
p2112
aVWe present a system for extending standard type systems with flowsensitive type qualifiers. Users annotate their programs with type qualifiers, and inference checks that the annotations are correct. In our system only the type qualifiers are modeled flowsensitively the underlying standard types are unchanged, which allows us to obtain an efficient constraintbased inference algorithm that integrates flowinsensitive alias analysis, effect inference, and ideas from linear type systems to support strong updates. We demonstrate the usefulness of flowsensitive type qualifiers by finding a number of new locking bugs in the Linux kernel.
p2113
aVEvents are used as a fundamental abstraction in programs ranging from graphical user interfaces (GUIs) to systems for building customized network protocols. While providing a flexible structuring and execution paradigm, events have the potentially serious drawback of extra execution overhead due to the indirection between modules that raise events and those that handle them. This paper describes an approach to addressing this issue using static optimization techniques. This approach, which exploits the underlying predictability often exhibited by eventbased programs, is based on first profiling the program to identify commonly occurring event sequences. A variety of techniques that use the resulting profile information are then applied to the program to reduce the overheads associated with such mechanisms as indirect function calls and argument marshaling. In addition to describing the overall approach, experimental results are given that demonstrate the effectiveness of the techniques. These results are from eventbased programs written for X Windows, a system for building GUIs, and Cactus, a system for constructing highly configurable distributed services and network protocols.
p2114
aVRecently, a number of threadbased prefetching techniques have been proposed. These techniques aim at improving the latency of singlethreaded applications by leveraging multithreading resources to perform memory prefetching via speculative prefetch threads. Softwarebased speculative precomputation (SSP) is one such technique, proposed for multithreaded Itanium models. SSP does not require expensive hardware supportinstead it relies on the compiler to adapt binaries to perform prefetching on otherwise idle hardware thread contexts at run time. This paper presents a postpass compilation tool for generating SSPenhanced binaries. The tool is able to: (1) analyze a singlethreaded application to generate prefetch threads; (2) identify and embed trigger points in the original binary; and (3) produce a new binary that has the prefetch threads attached. The execution of the new binary spawns the speculative prefetch threads, which are executed concurrently with the main thread. Our results indicate that for a set of pointerintensive benchmarks, the prefetching performed by the speculative threads achieves an average of 87% speedup on an inorder processor and 5% speedup on an outoforder processor.
p2115
aVMultithreaded applications with multigigabyte heaps running on modern servers provide new challenges for garbage collection (GC). The challenges for "serveroriented" GC include: ensuring short pause times on a multigigabyte heap, while minimizing throughput penalty, good scaling on multiprocessor hardware, and keeping the number of expensive multicycle fence instructions required by weak ordering to a minimum. We designed and implemented a fully parallel, incremental, mostly concurrent collector, which employs several novel techniques to meet these challenges. First, it combines incremental GC to ensure short pause times with concurrent lowpriority background GC threads to take advantage of processor idle time. Second, it employs a lowoverhead work packet mechanism to enable full parallelism among the incremental and concurrent collecting threads and ensure load balancing. Third, it reduces memory fence instructions by using batching techniques: one fence for each block of small objects allocated, one fence for each group of objects marked, and no fence at all in the write barrier. When compared to the mature welloptimized parallel stoptheworld marksweep collector already in the IBM JVM, our collector prototype reduces the maximum pause time from 284 ms to 101 ms, and the average pause time from 266 ms to 66 ms while only losing 10% throughput when running the SPECjbb2000 benchmark on a 256 MB heap on a 4way 550 MHz Pentium multiprocessor.
p2116
aVThis paper describes a memory discipline that combines regionbased memory management and copying garbage collection by extending Cheney's copying garbage collection algorithm to work with regions. The paper presents empirical evidence that region inference very significantly reduces the number of garbage collections; and evidence that the fastest execution is obtained by using regions alone, without garbage collection. The memory discipline is implemented for Standard ML in the ML Kit compiler and measurements show that for a variety of benchmark programs, code generated by the compiler is as efficient, both with respect to execution time and memory usage, as programs compiled with Standard ML of New Jersey, another stateoftheart Standard ML compiler.
p2117
aVWe present the design and implementation of a new garbage collection framework that significantly generalizes existing copying collectors. The Beltway framework exploits and separates object age and incrementality. It groups objects in one or more increments on queues called belts, collects belts independently, and collects increments on a belt in firstinfirstout order. We show that Beltway configurations, selected by command line options, act and perform the same as semispace, generational, and olderfirst collectors, and encompass all previous copying collectors of which we are aware. The increasing reliance on garbage collected languages such as Java requires that the collector perform well. We show that the generality of Beltway enables us to design and implement new collectors that are robust to variations in heap size and improve total execution time over the best generational copying collectors of which we are aware by up to 40%, and on average by 5 to 10%, for small to moderate heap sizes. New garbage collection algorithms are rare, and yet we define not just one, but a new family of collectors that subsumes previous work. This generality enables us to explore a larger design space and build better collectors.
p2118
aVThe current practice of mapping computations to custom hardware implementations requires programmers to assume the role of hardware designers. In tuning the performance of their hardware implementation, designers manually apply loop transformations such as loop unrolling. designers manually apply loop transformations. For example, loop unrolling is used to expose instructionlevel parallelism at the expense of more hardware resources for concurrent operator evaluation. Because unrolling also increases the amount of data a computation requires, too much unrolling can lead to a memory bound implementation where resources are idle. To negotiate inherent hardware spacetime tradeoffs, designers must engage in an iterative refinement cycle, at each step manually applying transformations and evaluating their impact. This process is not only errorprone and tedious but also prohibitively expensive given the large search spaces and with long synthesis times. This paper describes an automated approach to hardware design space exploration, through a collaboration between parallelizing compiler technology and highlevel synthesis tools. We present a compiler algorithm that automatically explores the large design spaces resulting from the application of several program transformations commonly used in applicationspecific hardware designs. Our approach uses synthesis estimation techniques to quantitatively evaluate alternate designs for a loop nest computation. We have implemented this design space exploration algorithm in the context of a compilation and synthesis system called DEFACTO, and present results of this implementation on five multimedia kernels. Our algorithm derives an implementation that closely matches the performance of the fastest design in the design space, and among implementations with comparable performance, selects the smallest design. We search on average only 0.3% of the design space. This technology thus significantly raises the level of abstraction for hardware design and explores a design space much larger than is feasible for a human designer.
p2119
aVThe accurate modeling of the electronic structure of atoms and molecules is very computationally intensive. Many models of electronic structure, such as the Coupled Cluster approach, involve collections of tensor contractions. There are usually a large number of alternative ways of implementing the tensor contractions, representing different tradeoffs between the space required for temporary intermediates and the total number of arithmetic operations. In this paper, we present an algorithm that starts with an operationminimal form of the computation and systematically explores the possible spacetime tradeoffs to identify the form with lowest cost that fits within a specified memory limit. Its utility is demonstrated by applying it to a computation representative of a component in the CCSD(T) formulation in the NWChem quantum chemistry suite from Pacific Northwest National Laboratory.
p2120
aVComputer designs are shifting from 32bit architectures to 64bit architectures, while most of the programs available today are still designed for 32bit architectures. Java\u2122, for example, specifies the frequently used int" as a 32bit data type. If such Java programs are executed on a 64bit architecture, many 32bit values must be signextended to 64bit values for integer operations. This causes serious performance overhead. In this paper, we present a fast and effective algorithm for eliminating sign extensions. We implemented this algorithm in the IBM Java JustinTime (JIT) compiler for IA64\u2122. Our experimental results show that our algorithm effectively eliminates the majority of sign extensions. They also show that it significantly improves performance, while it increases JIT compilation time by only 0.11%. We implemented our algorithm for programs in Java, but it can be applied to any language requiring sign extensions.
p2121
aVPrefetching data ahead of use has the potential to tolerate the grow ing processormemory performance gap by overlapping long latency memory accesses with useful computation. While sophisti cated prefetching techniques have been automated for limited domains, such as scientific codes that access dense arrays in loop nests, a similar level of success has eluded generalpurpose pro grams, especially pointerchasing codes written in languages such as C and C++. We address this problem by describing, implementing and evaluating a dynamic prefetching scheme. Our technique runs on stock hardware, is completely automatic, and works for generalpurpose programs, including pointerchasing codes written in weaklytyped languages, such as C and C++. It operates in three phases. First, the profiling phase gathers a temporal data reference profile from a running program with lowoverhead. Next, the profiling is turned off and a fast analysis algorithm extracts hot data streams, which are data reference sequences that frequently repeat in the same order, from the temporal profile. Then, the system dynamically injects code at appropriate program points to detect and prefetch these hot data streams. Finally, the process enters the hibernation phase where no profiling or analysis is performed, and the program continues to execute with the added prefetch instructions. At the end of the hibernation phase, the program is deoptimized to remove the inserted checks and prefetch instructions, and control returns to the profiling phase. For longrunning programs, this profile, analyze and optimize, hibernate, cycle will repeat multiple times. Our initial results from applying dynamic prefetching are promising, indicating overall execution time improvements of 5.19% for several memoryperformancelimited SPECint2000 benchmarks running their largest (ref) inputs.
p2122
aVIrregular data references are difficult to prefetch, as the future memory address of a load instruction is hard to anticipate by a compiler. However, recent studies as well as our experience indicate that some important load instructions in irregular programs contain stride access patterns. Although the load instructions with stride patterns are difficult to identify with static compiler techniques, we developed an efficient profiling method to discover these load instructions. The new profiling method integrates the profiling for stride information and the traditional profiling for edge frequency into a single profiling pass. The integrated profiling pass runs only 17% slower than the frequency profiling alone. The collected stride information helps the compiler to identify load instructions with stride patterns that can be prefetched efficiently and beneficially. We implemented the new profiling and prefetching techniques in a research compiler for Itanium Processor Family (IPF), and obtained significant performance improvement for the SPECINT2000 programs running on Itanium machines. For example, we achieved a 1.59x speedup for 181.mcf, 1.14x for 254.gap, and 1.08x for 197.parser. We also showed that the performance gain is stable across input data sets. These benefits make the new profiling and prefetching techniques suitable for production compilers.
p2123
aVA type system with linearity is useful for checking software protocols andresource management at compile time. Linearity provides powerful reasoning about state changes, but at the price of restrictions on aliasing. The hard division between linear and nonlinear types forces the programmer to make a tradeoff between checking a protocol on an object and aliasing the object. Most onerous is the restriction that any type with a linear component must itself be linear. Because of this, checking a protocol on an object imposes aliasing restrictions on any data structure that directly or indirectly points to the object. We propose a new type system that reduces these restrictions with the adoption and focus constructs. Adoption safely allows a programmer to alias objects on which she is checking protocols, and focus allows the reverse. A programmer can alias data structures that point to linear objects and use focus for safe access to those objects. We discuss how we implemented these ideas in the Vault programming language.
p2124
aVWhile caches are effective at avoiding most mainmemory accesses, the few remaining memory references are still expensive. Even one cache miss per one hundred accesses can double a program's execution time. To better tolerate the datacache miss latency, architects have proposed various speculation mechanisms, including loadvalue prediction. A loadvalue predictor guesses the result of a load so that the dependent instructions can immediately proceed without having to wait for the memory access to complete. To use the prediction resources most effectively, speculation should be restricted to loads that are likely to miss in the cache and that are likely to be predicted correctly. Prior work has considered hardware and profilebased methods to make these decisions. Our work focuses on making these decisions at compile time. We show that a simple compiler classification is effective at separating the loads that should be speculated from the loads that should not. We present results for a number of C and Java programs and demonstrate that our results are consistent across programming languages and across program inputs.
p2125
aVSoftware development and maintenance are costly endeavors. The cost can be reduced if more software defects are detected earlier in the development cycle. This paper introduces the Extended Static Checker for Java (ESC/Java), an experimental compiletime program checker that finds common programming errors. The checker is powered by verificationcondition generation and automatic theoremproving techniques. It provides programmers with a simple annotation language with which programmer design decisions can be expressed formally. ESC/Java examines the annotated software and warns of inconsistencies between the design decisions recorded in the annotations and the actual code, and also warns of potential runtime errors in the code. This paper gives an overview of the checker architecture and annotation language and describes our experience applying the checker to tens of thousands of lines of Java programs.
p2126
aVReasoning precisely about the side effects of procedure calls is important to many program analyses. This paper introduces a technique for specifying and statically checking the side effects of methods in an objectoriented language. The technique uses data groups, which abstract over variables that are not in scope, and limits program behavior by two aliasconfining restrictions, pivot uniqueness and owner exclusion. The technique is shown to achieve modular soundness and is simpler than previous attempts at solving this problem.
p2127
aVWe present a novel approach to dynamic datarace detection for multithreaded objectoriented programs. Past techniques for onthefly datarace detection either sacrificed precision for performance, leading to many false positive datarace reports, or maintained precision but incurred significant overheads in the range of 3x to 30x. In contrast, our approach results in very few false positives and runtime overhead in the 13% to 42% range, making it both efficient and precise. This performance improvement is the result of a unique combination of complementary static and dynamic optimization techniques.
p2128
aVWe have designed and implemented Maya, a version of Java that allows programmers to extend and reinterpret its syntax. Maya generalizes macro systems by treating grammar productions as generic functions, and semantic actions on productions as multimethods on the corresponding generic functions. Programmers can write new generic functions (i.e., grammar productions) and new multimethods (i.e., semantic actions), through which they can extend the grammar of the language and change the semantics of its syntactic constructs, respectively. Maya's multimethods are compiletime metaprograms that transform abstract syntax: they execute at program compiletime, because they are semantic actions executed by the parser. Maya's multimethods can be dispatched on the syntactic structure of the input, as well as the static, sourcelevel types of expressions in the input. In this paper we describe what Maya can do and how it works. We describe how its novel parsing techniques work and how Maya can statically detect certain kinds of errors, such as code that generates references to free variables. Finally, to demonstrate Maya's expressiveness, we describe how Maya can be used to implement the MultiJava language, which was described by Clifton et al. at OOPSLA 2000.
p2129
aVCyclone is a typesafe programming language derived from C. The primary design goal of Cyclone is to let programmers control data representation and memory management without sacrificing typesafety. In this paper, we focus on the regionbased memory management of Cyclone and its static typing discipline. The design incorporates several advancements, including support for region subtyping and a coherent integration with stack allocation and a garbage collector. To support separate compilation, Cyclone requires programmers to write some explicit region annotations, but a combination of default annotations, local type inference, and a novel treatment of region effects reduces this burden. As a result, we integrate C idioms in a regionbased framework. In our experience, porting legacy C to Cyclone has required altering about 8% of the code; of the changes, only 6% (of the 8%) were region annotations.
p2130
aVThis paper presents and evaluates techniques to improve the execution performance of MATLAB. Previous efforts concentrated on source to source translation and batch compilation; MaJIC provides an interactive frontend that looks like MATLAB and compiles/optimizes code behind the scenes in real time, employing a combination of justintime and speculative aheadoftime compilation. Performance results show that the proper mixture of these two techniques can yield nearzero response time as well as performance gains previously achieved only by batch compilers.
p2131
aVThis paper provides a preliminary report on a new research project that aims to construct a code generator that uses an automatic theorem prover to produce very highquality (in fact, nearly mathematically optimal) machine code for modern architectures. The code generator is not intended for use in an ordinary compiler, but is intended to be used for inner loops and critical subroutines in those cases where peak performance is required, no available compiler generates adequately efficient code, and where current engineering practice is to use handcoded machine language. The paper describes the design of the superoptimizer, and presents some encouraging preliminary results.
p2132
aVThis paper presents a fast new algorithm for modeling and reasoning about interferences for variables in a program without constructing an interference graph. It then describes how to use this information to minimize copy insertion for &fgr;node instantiation during the conversion of the static single assignment (SSA) form into the controlflow graph (CFG), effectively yielding a new, very fast copy coalescing and liverange identification algorithm.This paper proves some properties of the SSA form that enable construction of data structures to compute interference information for variables that are considered for folding. The asymptotic complexity of our SSAtoCFG conversion algorithm is whereis the number of instructions in the program.Performing copy folding during the SSAtoCFG conversion eliminates the need for a separate coalescing phase while simplifying the intermediate code. This may make graphcoloring register allocation more practical in just in time (JIT) and other timecritical compilers For example, Sun's Hotspot Server Compiler already employs a graphcoloring register allocator[10].This paper also presents an improvement to the classical interferencegraph based coalescing optimization that shows adecrease in memory usage of up to three orders of magnitude and a decrease of a factor of two in compilation time, while providing the exact same results.We present experimental results that demonstrate that our algorithm is almost as precise (within one percent on average) as the improved interferencegraphbased coalescing algorithm, while requiring three times less compilation time.
p2133
aVThis paper describes a new framework of register allocation based on Chaitinstyle coloring. Our focus is on maximizing the chances for live ranges to be allocated to the most preferred registers while not destroying the colorability obtained by graph simplification. Our coloring algorithm uses a graph representation of preferences called a Register Preference Graph, which helps find a good register selection. We then try to relax the register selection order created by the graph simplification. The relaxed order is defined as a partial order, represented using a graph called a Coloring Precedence Graph. Our algorithm utilizes such a partial order for the register selection instead of using the traditional simplificationdriven order so that the chances of honoring the preferences are effectively increased. Experimental results show that our coloring algorithm is powerful to simultaneously handle spill decisions, register coalescing, and preference resolutions.
p2134
aVThis paper presents a new algorithm for performing global value numbering on a routine in static single assignment form. Our algorithm has all the strengths of the most powerful existing practical methods of global value numbering; it unifies optimistic value numbering with constant folding, algebraic simplification and unreachable code elimination. It goes beyond existing methods by unifying optimistic value numbering with further analyses: it canonicalizes the structure of expressions in order to expose more congruences by performing global reassociation, it exploits the congruences induced by the predicates of conditional jumps (predicate inference and value inference), and it associates the arguments of acyclic  functions with the predicates controlling their arrival ( predication), thus enabling congruence finding on conditional control structures. Finally, it implements an efficient sparse formulation and offers a range of tradeoffs between compilation time and optimization strength. We describe an implementation of the algorithm and present measurements of its strength and efficiency collected when optimizing the SPEC CINT2000 C benchmarks.
p2135
aVIn this paper, we present a new algorithm for partial program verification that runs in polynomial time and space. We are interested in checking that a program satisfies a given temporal safety property. Our insight is that by accurately modeling only those branches in a program for which the propertyrelated behavior differs along the arms of the branch, we can design an algorithm that is accurate enough to verify the program with respect to the given property, without paying the potentially exponential cost of full pathsensitive analysis.We have implemented this "property simulation" algorithm as part of a partial verification tool called ESP. We present the results of applying ESP to the problem of verifying the file I/O behavior of a version of the GNU C compiler (gcc, 140,000 LOC). We are able to prove that all of the 646 calls to .fprintf in the source code of gcc are guaranteed to print to valid, open files. Our results show that property simulation scales to large programs and is accurate enough to verify meaningful properties.
p2136
aVThis paper presents a novel approach to bugfinding analysis and an implementation of that approach. Our goal is to find as many serious bugs as possible. To do so, we designed a flexible, easytouse extension language for specifying analyses and an efficent algorithm for executing these extensions. The language, metal, allows the users of our system to specify a broad class of analyses in terms that resemble the intuitive description of the rules that they check. The system, xgcc, executes these analyses efficiently using a contextsensitive, interprocedural analysis. Our prior work has shown that the approach described in this paper is effective: it has successfully found thousands of bugs in real systems code. This paper describes the underlying system used to achieve these results. We believe that our system is an effective framework for deploying new bugfinding analyses quickly and easily.
p2137
aVWe are concerned with the problem of statically certifying (verifying) whether the client of a software component conforms to the component's constraints for correct usage. We show how conformance certification can be efficiently carried out in a staged fashion for certain classes of firstorder safety (FOS) specifications, which can express relationship requirements among potentially unbounded collections of runtime objects. In the first stage of the certification process, we systematically derive an abstraction that is used to model the component state during analysis of arbitrary clients. In general, the derived abstraction will utilize firstorder predicates, rather than the propositions often used by model checkers. In the second stage, the generated abstraction is incorporated into a static analysis engine to produce a certifier. In the final stage, the resulting certifier is applied to a client to conservatively determine whether the client violates the component's constraints. Unlike verification approaches that analyze a specification and client code together, our technique can take advantage of computationallyintensive symbolic techniques during the abstraction generation phase, without affecting the performance of client analysis. Using as a running example the Concurrent Modification Problem (CMP), which arises when certain classes defined by the Java Collections Framework are misused, we describe several different classes of certifiers with varying time/space/precision tradeoffs. Of particular note are precise, polynomialtime, flow and contextsensitive certifiers for certain classes of FOS specifications and client programs. Finally, we evaluate a prototype implementation of a certifier for CMP on a variety of test programs. The results of the evaluation show that our approach, though conservative, yields very few "false alarms," with acceptable performance.
p2138
aVAs computers are increasingly used in contexts where the amount of available memory is limited, it becomes important to devise techniques that reduce the memory footprint of application programs while leaving them in an executable form. This paper describes an approach to applying data compression techniques to reduce the size of infrequently executed portions of a program. The compressed code is decompressed dynamically (via software) if needed, prior to execution. The use of data compression techniques increases the amount of code size reduction that can be achieved; their application to infrequently executed code limits the runtime overhead due to dynamic decompression; and the use of software decompression renders the approach generally applicable, without requiring specialized hardware. The code size reductions obtained depend on the threshold used to determine what code is "infrequently executed" and hence should be compressed: for low thresholds, we see size reductions of 13.7% to 18.8%, on average, for a set of embedded applications, without excessive runtime overhead.
p2139
aVWe present nesC, a programming language for networked embedded systems that represent a new design space for application developers. An example of a networked embedded system is a sensor network, which consists of (potentially) thousands of tiny, lowpower "motes," each of which execute concurrent, reactive programs that must operate with severe memory and power constraints.nesC's contribution is to support the special needs of this domain by exposing a programming model that incorporates eventdriven execution, a flexible concurrency model, and componentoriented application design. Restrictions on the programming model allow the nesC compiler to perform wholeprogram analyses, including datarace detection (which improves reliability) and aggressive function inlining (which reduces resource consumption).nesC has been used to implement TinyOS, a small operating system for sensor networks, as well as several significant sensor applications. nesC and TinyOS have been adopted by a large number of sensor network research groups, and our experience and evaluation of the language shows that it is effective at supporting the complex, concurrent programming style demanded by this new class of deeply networked systems.
p2140
aVA compiler for multithreaded objectoriented programs needs information about the sharing of objects for a variety of reasons: to implement optimizations, to issue warnings, to add instrumentation to detect access violations that occur at runtime. An Object Use Graph (OUG) statically captures accesses from different threads to objects. An OUG extends the Heap Shape Graph (HSG), which is a compiletime abstraction for runtime objects (nodes) and their reference relations (edges). An OUG specifies for a specific node in the HSG a partial order of events relevant to the corresponding runtime object(s). Relevant events include read and write access, object escape, thread start and join.OUGs have been implemented in a Java compiler. Initial experience shows that OUGs are effective to identify object accesses that potentially conflict at runtime and isolate accesses that never cause a problem at runtime. The capabilities of OUGs are compared with an advanced program analysis that has been used for lock elimination. For the set of benchmarks investigated here, OUGs report only a fraction of shared objects as conflicting and reduce the number of compiletime reports in terms of allocation sites of conflicting objects by 28 92% (average 64%). For benchmarks of up to 30 KLOC, the time taken to construct OUGs is, with one exception, in the order of seconds.The information collected in the OUG has been used to instrument Java programs with checks for object races. OUGs provide precise information about object sharing and static protection, so runtime instrumentation that checks those cases that cannot be disambiguated at compiletime is sparse, and the total runtime overhead of checking for object races is only 3 86% (average 47%).
p2141
aVIn prior work [15] we studied a language construct <tt>restrict</tt> that allows programmers to specify that certain pointers are not aliased to other pointers used within a lexical scope. Among other applications, programming with these constructs helps program analysis tools locally recover strong updates, which can improve the tracking of state in flowsensitive analyses. In this paper we continue the study of <tt>restrict</tt> and introduce the construct <tt>confine</tt>. We present a type and effect system for checking the correctness of these annotations, and we develop efficient constraintbased algorithms implementing these type checking systems. To make it easier to use <tt>restrict</tt> and <tt>confine</tt> in practice, we show how to automatically infer such annotations without programmer assistance. In experiments on locking in 589 Linux device drivers, <tt>confine</tt> inference can automatically recover strong updates to eliminate 95% of the type errors resulting from weak updates.
p2142
aVWe propose a lowoverhead sampling infrastructure for gathering information from the executions experienced by a program's user community. Several example applications illustrate ways to use sampled instrumentation to isolate bugs. Assertiondense code can be transformed to share the cost of assertions among many users. Lacking assertions, broad guesses can be made about predicates that predict program errors and a process of elimination used to whittle these down to the true bug. Finally, even for nondeterministic bugs such as memory corruption, statistical modeling based on logistic regression allows us to identify program behaviors that are strongly correlated with failure and are therefore likely places to look for the error.
p2143
aVErroneous string manipulations are a major source of software defects in C programs yielding vulnerabilities which are exploited by software viruses. We present C String Static Verifyer (CSSV), a tool that statically uncovers all string manipulation errors. Being a conservative tool, it reports all such errors at the expense of sometimes generating false alarms. Fortunately, only a small number of false alarms are reported, thereby proving that statically reducing software vulnerability is achievable. CSSV handles large programs by analyzing each procedure separately. To this end procedure contracts are allowed which are verified by the tool.We implemented a CSSV prototype and used it to verify the absence of errors in real code from EADS Airbus. When applied to another commonly used string intensive application, CSSV uncovered real bugs with very few false alarms.
p2144
aVThis paper presents a static analysis tool that can automatically find memory leaks and deletions of dangling pointers in large C and C++ applications.We have developed a type system to formalize a practical ownership model of memory management. In this model, every object is pointed to by one and only one owning pointer, which holds the exclusive right and obligation to either delete the object or to transfer the right to another owning pointer. In addition, a pointertyped class member field is required to either always or never own its pointee at public method boundaries. Programs satisfying this model do not leak memory or delete the same object more than once.We have also developed a flowsensitive and contextsensitive algorithm to automatically infer the likely ownership interfaces of methods in a program. It identifies statements inconsistent with the model as sources of potential leaks or double deletes. The algorithm is sound with respect to a large subset of the C and C++ language in that it will report all possible errors. It is also practical and useful as it identifies those warnings likely to correspond to errors and helps the user understand the reported errors by showing them the assumed method interfaces.Our techniques are validated with an implementation of a tool we call Clouseau. We applied Clouseau to a suite of applications: two web servers, a chat client, secure shell tools, executable object manipulation tools, and a compiler. The tool found a total of 134 serious memory errors in these applications. The tool analyzes over 50K lines of C++ code in about 9 minutes on a 2 GHz Pentium 4 machine and over 70K lines of C code in just over a minute.
p2145
aVProgram verification tools (such as model checkers and static analyzers) can find many errors in programs. These tools need formal specifications of correct program behavior, but writing a correct specification is difficult, just as writing a correct program is difficult. Thus, just as we need methods for debugging programs, we need methods for debugging specifications.This paper describes a novel method for debugging formal, temporal specifications. Our method exploits the short program execution traces that program verification tools generate from specification violations and that specification miners extract from programs. Manually examining these traces is a straightforward way to debug a specification, but this method is tedious and errorprone because there may be hundreds or thousands of traces to inspect. Our method uses concept analysis to automatically group the traces into highly similar clusters. By examining clusters instead of individual traces, a person can debug a specification with less work.To test our method, we implemented a tool, Cable, for debugging specifications. We have used Cable to debug specifications produced by Strauss, our specification miner. We found that using Cable to debug these specifications requires, on average, less than one third as many user decisions as debugging by examining all traces requires. In one case, using Cable required only 28 decisions, while debugging by examining all traces required 224.
p2146
aVWe show that abstract interpretationbased static program analysis can be made efficient and precise enough to formally verify a class of properties for a family of large programs with few or no false alarms. This is achieved by refinement of a general purpose static analyzer and later adaptation to particular programs of the family by the enduser through parametrization. This is applied to the proof of soundness of data manipulation operations at the machine level for periodic synchronous safety critical embedded software.The main novelties are the design principle of static analyzers by refinement and adaptation through parametrization (Sect. 3 and 7), the symbolic manipulation of expressions to improve the precision of abstract transfer functions (Sect. 6.3), the octagon (Sect. 6.2.2), ellipsoid (Sect. 6.2.3), and decision tree (Sect. 6.2.4) abstract domains, all with sound handling of rounding errors in oating point computations, widening strategies (with thresholds: Sect. 7.1.2, delayed: Sect. 7.1.3) and the automatic determination of the parameters (parametrized packing: Sect. 7.2).
p2147
aVTyped assembly languages provide a way to generate machinecheckable safety proofs for machinelanguage programs. But the soundness proofs of most existing typed assembly languages are handwritten and cannot be machinechecked, which is worrisome for such large calculi. We have designed and implemented a lowlevel typed assembly language (LTAL) with a semantic model and established its soundness from the model. Compared to existing typed assembly languages, LTAL is more scalable and more secure; it has no macro instructions that hinder lowlevel optimizations such as instruction scheduling; its type constructors are expressive enough to capture dataflow information, support the compiler's choice of data representations and permit typed positionindependent code; and its typechecking algorithm is completely syntaxdirected.We have built a prototype system, based on Standard ML of New Jersey, that compiles most of core ML to Sparc code. We explain how we were able to make the untyped back end in SML/NJ preserve types during instruction selection and register allocation, without restricting lowlevel optimizations and without knowledge of any type system pervading the instruction selector and register allocator.
p2148
aVWe describe a technique for automatically proving compiler optimizations sound, meaning that their transformations are always semanticspreserving. We first present a domainspecific language, called Cobalt, for implementing optimizations as guarded rewrite rules. Cobalt optimizations operate over a Clike intermediate representation including unstructured control flow, pointers to local variables and dynamically allocated memory, and recursive procedures. Then we describe a technique for automatically proving the soundness of Cobalt optimizations. Our technique requires an automatic theorem prover to discharge a small set of simple, optimizationspecific proof obligations for each optimization. We have written a variety of forward and backward intraprocedural dataflow optimizations in Cobalt, including constant propagation and folding, branch folding, full and partial redundancy elimination, full and partial dead assignment elimination, and simple forms of pointsto analysis. We implemented our soundnesschecking strategy using the Simplify automatic theorem prover, and we have used this implementation to automatically prove our optimizations correct. Our checker found many subtle bugs during the course of developing our optimizations. We also implemented an execution engine for Cobalt optimizations as part of the Whirlwind compiler infrastructure.
p2149
aVCCured is a program transformation system that adds memory safety guarantees to C programs by verifying statically that memory errors cannot occur and by inserting runtime checks where static verification is insufficient.This paper addresses major usability issues in a previous version of CCured, in which many type casts required the use of pointers whose representation was expensive and incompatible with precompiled libraries. We have extended the CCured type inference algorithm to recognize and verify statically a large number of type casts; this goal is achieved by using physical subtyping and pointers with runtime type information to allow parametric and subtype polymorphism. In addition, we present a new instrumentation scheme that splits CCured's metadata into a separate data structure whose shape mirrors that of the original user data. This scheme allows instrumented programs to invoke external functions directly on the program's data without the use of a wrapper function.With these extensions we were able to use CCured on realworld securitycritical network daemons and to produce instrumented versions without memorysafety vulnerabilities.
p2150
aVAs more complex DSP algorithms are realized in practice, there is an increasing need for highlevel stream abstractions that can be compiled without sacrificing efficiency. Toward this end, we present a set of aggressive optimizations that target linear sections of a stream program. Our input language is StreamIt, which represents programs as a hierarchical graph of autonomous filters. A filter is linear if each of its outputs can be represented as an affine combination of its inputs. Linearity is common in DSP components; examples include FIR filters, expanders, compressors, FFTs and DCTs.We demonstrate that several algorithmic transformations, traditionally handtuned by DSP experts, can be completely automated by the compiler. First, we present a linear extraction analysis that automatically detects linear filters from the Clike code in their work function. Then, we give a procedure for combining adjacent linear filters into a single filter, as well as for translating a linear filter to operate in the frequency domain. We also present an optimization selection algorithm, which finds the sequence of combination and frequency transformations that will give the maximal benefit.We have completed a fullyautomatic implementation of the above techniques as part of the StreamIt compiler, and we demonstrate a 450% performance improvement over our benchmark suite.
p2151
aVProfiling can accurately analyze program behavior for select data inputs. We show that profiling can also predict program locality for inputs other than profiled ones. Here locality is defined by the distance of data reuse. Studying wholeprogram data reuse may reveal global patterns not apparent in shortdistance reuses or local control flow. However, the analysis must meet two requirements to be useful. The first is efficiency. It needs to analyze all accesses to all data elements in fullsize benchmarks and to measure distance of any length and in any required precision. The second is predication. Based on a few training runs, it needs to classify patterns as regular and irregular and, for regular ones, it should predict their (changing) behavior for other inputs. In this paper, we show that these goals are attainable through three techniques: approximate analysis of reuse distance (originally called LRU stack distance), pattern recognition, and distancebased sampling. When tested on 15 integer and floatingpoint programs from SPEC and other benchmark suites, our techniques predict with on average 94% accuracy for data inputs up to hundreds times larger than the training inputs. Based on these results, the paper discusses possible uses of this analysis.
p2152
aVStatic array storage optimization in MATLAB.
p2153
aVSoftware prefetching is a promising technique to hide cache miss latencies, but it remains challenging to effectively prefetch pointerbased data structures because obtaining the memory address to be prefetched requires pointer dereferences. The recently proposed stride prefetching overcomes this problem, but it only exploits  interiteration stride patterns and relies on an offline profiling method.We propose a new algorithm for stride prefetching which is intended for use in a dynamic compiler. We exploit both inter and intraiteration stride patterns, which we discover using an ultralightweight profiling technique, called object inspection. This is a kind of partial interpretation that only a dynamic compiler can perform. During the compilation of a method, the dynamic compiler gathers the profile information by partially interpreting the method using the actual values of parameters and causing no side effects.We evaluated an implementation of our prefetching algorithm in a productionlevel Java justin time compiler. The results show that the algorithm achieved up to an 18.9% and 25.1% speedup in industrystandard benchmarks on the Pentium 4 and the Athlon MP, respectively, while it increased the compilation time by less than 3.0%.
p2154
aVInterpreters designed for efficiency execute a huge number of indirect branches and can spend more than half of the execution time in indirect branch mispredictions. Branch target buffers are the best widely available form of indirect branch prediction; however, their prediction accuracy for existing interpreters is only 2% 50%. In this paper we investigate two methods for improving the prediction accuracy of BTBs for interpreters: replicating virtual machine (VM) instructions and combining sequences of VM instructions into superinstructions. We investigate static (interpreter buildtime) and dynamic (interpreter runtime) variants of these techniques and compare them and several combinations of these techniques. These techniques can eliminate nearly all of the dispatch branch mispredictions, and have other benefits, resulting in speedups by a factor of up to 3.17 over efficient threadedcode interpreters, and speedups by a factor of up to 1.3 over techniques relying on superinstructions alone.
p2155
aVSpeculative execution, such as control speculation and data speculation, is an effective way to improve program performance. Using edge/path profile information or simple heuristic rules, existing compiler frameworks can adequately incorporate and exploit control speculation. However, very little has been done so far to allow existing compiler frameworks to incorporate and exploit data speculation effectively in various program transformations beyond instruction scheduling. This paper proposes a speculative SSA form to incorporate information from alias profiling and/or heuristic rules for data speculation, thus allowing existing program analysis frameworks to be easily extended to support both control and data speculation. Such a general framework is very useful for EPIC architectures that provide checking (such as advanced load address table (ALAT) [10]) on data speculation to guarantee the correctness of program execution. We use SSAPRE [21] as one example to illustrate how to incorporate data speculation in those important compiler optimizations such as partial redundancy elimination (PRE), register promotion, strength reduction and linear function test replacement. Our extended framework allows both control and data speculation to be performed on top of SSAPRE and, thus, enables more aggressive speculative optimizations. The proposed framework has been implemented on Intel's Open Research Compiler (ORC). We present experimental data on some SPEC2000 benchmark programs to demonstrate the usefulness of this framework and how data speculation benefits partial redundancy elimination.
p2156
aVClustered architectures are a solution to the bottleneck of centralized register files in superscalar and VLIW processors. The main challenge associated with clustered architectures is compiler support to effectively partition operations across the available resources on each cluster. In this work, we present a novel technique for clustering operations based on graph partitioning methods. Our approach incorporates new methods of assigning weights to nodes and edges within the dataflow graph to guide the partitioner. Nodes are assigned weights to reflect their resource usage within a cluster, while a slack distribution method intelligently assigns weights to edges to reflect the cost of inserting moves across clusters. A multilevel graph partitioning algorithm, which globally divides a dataflow graph into multiple parts in a hierarchical manner, uses these weights to efficiently generate estimates for the quality of partitions. We found that our algorithm was able to achieve an average of 20% improvement in DSP kernels and 5% improvement in SPECint2000 for a fourcluster architecture.
p2157
aVMethod inlining and data flow analysis are two major optimization components for effective program transformations, however they often suffer from the existence of rarely or never executed code contained in the target method. One major problem lies in the assumption that the compilation unit is partitioned at method boundaries. This paper describes the design and implementation of a regionbased compilation technique in our dynamic compilation system, in which the compiled regions are selected as code portions without rarely executed code. The key part of this technique is the region selection, partial inlining, and region exit handling. For region selection, we employ both static heuristics and dynamic profiles to identify rare sections of code. The region selection process and method inlining decision are interwoven, so that method inlining exposes other targets for region selection, while the region selection in the inline target conserves the inlining budget, leading to more method inlining. Thus the inlining process can be performed for parts of a method, not for the entire body of the method. When the program attempts to exit from a region boundary, we trigger recompilation and then rely on onstack replacement to continue the execution from the corresponding entry point in the recompiled code. We have implemented these techniques in our Java JIT compiler, and conducted a comprehensive evaluation. The experimental results show that the approach of regionbased compilation achieves approximately 5% performance improvement on average, while reducing the compilation overhead by 20 to 30%, in comparison to the traditional functionbased compilation techniques.
p2158
aVThe Real Time Specification for Java (RTSJ) allows a program to create realtime threads with hard realtime constraints. Realtime threads use regionbased memory management to avoid unbounded pauses caused by interference from the garbage collector. The RTSJ uses runtime checks to ensure that deleting a region does not create dangling references and that realtime threads do not access references to objects allocated in the garbagecollected heap. This paper presents a static type system that guarantees that these runtime checks will never fail for welltyped programs. Our type system therefore 1) provides an important safety guarantee for realtime programs and 2) makes it possible to eliminate the runtime checks and their associated overhead.Our system also makes several contributions over previous work on region types. For objectoriented programs, it combines the benefits of region types and ownership types in a unified type system framework. For multithreaded programs, it allows longlived threads to share objects without using the heap and without memory leaks. For realtime programs, it ensures that realtime threads do not interfere with the garbage collector. Our experience indicates that our type system is sufficiently expressive and requires little programming overhead, and that eliminating the RTSJ runtime checks using a static type system can significantly decrease the execution time of realtime programs.
p2159
aVWe compile Nova, a new language designed for writing network processing applications, using a back end based on integerlinear programming (ILP) for register allocation, optimal bank assignment, and spills. The compiler's optimizer employs CPS as its intermediate representation; some of the invariants that this IR guarantees are essential for the formulation of a practical ILP model.Appel and George used a similar ILPbased technique for the IA32 to decide which variables reside in registers but deferred the actual assignment of colors to a later phase. We demonstrate how to carry over their idea to an architecture with many more banks, register aggregates, variables with multiple simultaneous register assignments, and, very importantly, one where bank and registerassignment cannot be done in isolation from each other. Our approach performs well in practise without causing an explosion in size or solve time of the generated integer linear programs.
p2160
aVThis paper presents the design and implementation of a compiler algorithm that effectively optimizes programs for energy usage using dynamic voltage scaling (DVS). The algorithm identifies program regions where the CPU can be slowed down with negligible performance loss. It is implemented as a sourcetosource level transformation using the SUIF2 compiler infrastructure. Physical measurements on a highperformance laptop show that total system (i.e., laptop) energy savings of up to 28% can be achieved with performance degradation of less than 5% for the SPECfp95 benchmarks. On average, the system energy and energydelay product are reduced by 11% and 9%, respectively, with a performance slowdown of 2%. It was also discovered that the energy usage of the programs using our DVS algorithm is within 6% from the theoretical lower bound. To the best of our knowledge, this is one of the first work that evaluates DVS algorithms by physical measurements.
p2161
aVWith powerrelated concerns becoming dominant aspects of hardware and software design, significant research effort has been devoted towards system power minimization. Among runtime powermanagement techniques, dynamic voltage scaling (DVS) has emerged as an important approach, with the ability to provide significant power savings. DVS exploits the ability to control the power consumption by varying a processor's supply voltage (V) and clock frequency (f). DVS controls energy by scheduling different parts of the computation to different (V, f) pairs; the goal is to minimize energy while meeting performance needs. Although processors like the Intel XScale and Transmeta Crusoe allow software DVS control, such control has thus far largely been used at the process/task level under operating system control. This is mainly because the energy and time overhead for switching DVS modes is considered too large and difficult to manage within a single program.In this paper we explore the opportunities and limits of compiletime DVS scheduling. We derive an analytical model for the maximum energy savings that can be obtained using DVS given a few known program and processor parameters. We use this model to determine scenarios where energy consumption benefits from compiletime DVS and those where there is no benefit. The model helps us extrapolate the benefits of compiletime DVS into the future as processor parameters change. We then examine how much of these predicted benefits can actually be achieved through optimal settings of DVS modes. This is done by extending the existing Mixedinteger Linear Program (MILP) formulation for this problem by accurately accounting for DVS energy switching overhead, by providing finergrained control on settings and by considering multiple data categories in the optimization. Overall, this research provides a comprehensive view of compiletime DVS management, providing both practical techniques for its immediate deployment as well theoretical bounds for use into the future.
p2162
aVEmpirical program optimizers estimate the values of key optimization parameters by generating different program versions and running them on the actual hardware to determine which values give the best performance. In contrast, conventional compilers use models of programs and machines to choose these parameters. It is widely believed that modeldriven optimization does not compete with empirical optimization, but few quantitative comparisons have been done to date. To make such a comparison, we replaced the empirical optimization engine in ATLAS (a system for generating a dense numerical linear algebra library called the BLAS) with a modeldriven optimization engine that used detailed models to estimate values for optimization parameters, and then measured the relative performance of the two systems on three different hardware platforms. Our experiments show that modeldriven optimization can be surprisingly effective, and can generate code whose performance is comparable to that of code generated by empirical optimizers for the BLAS.
p2163
aVCompiler writers have crafted many heuristics over the years to approximately solve NPhard problems efficiently. Finding a heuristic that performs well on a broad range of applications is a tedious and difficult process. This paper introduces Meta Optimization, a methodology for automatically finetuning compiler heuristics. Meta Optimization uses machinelearning techniques to automatically search the space of compiler heuristics. Our techniques reduce compiler design complexity by relieving compiler writers of the tedium of heuristic tuning. Our machinelearning system uses an evolutionary algorithm to automatically find effective compiler heuristics. We present promising experimental results. In one mode of operation Meta Optimization creates applicationspecific heuristics which often result in impressive speedups. For hyperblock formation, one optimization we present in this paper, we obtain an average speedup of 23% (up to 73%) for the applications in our suite. Furthermore, by evolving a compiler's heuristic over several benchmarks, we can create effective, generalpurpose heuristics. The best generalpurpose heuristic our system found for hyperblock formation improved performance by an average of 25% on our training set, and 9% on a completely unrelated test set. We demonstrate the efficacy of our techniques on three different optimizations in this paper: hyperblock formation, register allocation, and data prefetching.
p2164
aVMany important applications, such as those using sparse data structures, have memory reference patterns that are unknown at compiletime. Prior work has developed runtime reorderings of data and computation that enhance locality in such applications.This paper presents a compiletime framework that allows the explicit composition of runtime data and iterationreordering transformations. Our framework builds on the iterationreordering framework of Kelly and Pugh to represent the effects of a given composition. To motivate our extension, we show that new compositions of runtime reordering transformations can result in better performance on three benchmarks.We show how to express a number of runtime data and iterationreordering transformations that focus on improving data locality. We also describe the space of possible runtime reordering transformations and how existing transformations fit within it. Since sparse tiling techniques are included in our framework, they become more generally applicable, both to a larger class of applications, and in their composition with other reordering transformations. Finally, within the presented framework data need be remapped only once at runtime for a given composition thus exhibiting one example of overhead reductions the framework can express.
p2165
aVThis paper reports on a new approach to solving a subsetbased pointsto analysis for Java using Binary Decision Diagrams (BDDs). In the model checking community, BDDs have been shown very effective for representing large sets and solving very large verification problems. Our work shows that BDDs can also be very effective for developing a pointsto analysis that is simple to implement and that scales well, in both space and time, to large programs.The paper first introduces BDDs and operations on BDDs using some simple pointsto examples. Then, a complete subsetbased pointsto algorithm is presented, expressed completely using BDDs and BDD operations. This algorithm is then refined by finding appropriate variable orderings and by making the algorithm propagate sets incrementally, in order to arrive at a very efficient algorithm. Experimental results are given to justify the choice of variable ordering, to demonstrate the improvement due to incrementalization, and to compare the performance of the BDDbased solver to an efficient handcoded graphbased solver. Finally, based on the results of the BDDbased solver, a variety of BDDbased queries are presented, including the pointsto query.
p2166
aVSoftware model checking has been successful for sequential programs, where predicate abstraction offers suitable models, and counterexampleguided abstraction refinement permits the automatic inference of models. When checking concurrent programs, we need to abstract threads as well as the contexts in which they execute. Stateless context models, such as predicates on global variables, prove insufficient for showing the absence of race conditions in many examples. We therefore use richer context models, which combine (1) predicates for abstracting data state, (2) control flow quotients for abstracting control state, and (3) counters for abstracting an unbounded number of threads. We infer suitable context models automatically by a combination of counterexampleguided abstraction refinement, bisimulation minimization, circular assumeguarantee reasoning, and parametric reasoning about an unbounded number of threads. This algorithm, called CIRC, has been implemented in BLAST and succeeds in checking many examples of NESC code for data races. In particular, BLAST proves the absence of races in several cases where previous race checkers give false positives.
p2167
aVSoftware watermarking is a tool used to combat software piracy by embedding identifying information into a program. Most existing proposals for software watermarking have the shortcoming that the mark can be destroyed via fairly straightforward semanticspreserving code transformations. This paper introduces pathbased watermarking, a new approach to software watermarking based on the dynamic branching behavior of programs. The advantage of this technique is that errorcorrecting and tamperproofing techniques can be used to make pathbased watermarks resilient against a wide variety of attacks. Experimental results, using both Java bytecode and IA32 native code, indicate that even relatively large watermarks can be embedded into programs at modest cost.
p2168
aVMany programs can be invoked under different execution options, input parameters and data files. Such different execution contexts may lead to strikingly different execution instances. The optimal code generation may be sensitive to the execution instances. In this paper, we show how to use parametric program analysis to deal with this issue for the optimization problem of computation offloading.Computation offloading has been shown to be an effective way to improve performance and energy saving on mobile devices. Optimal program partitioning for computation offloading depends on the tradeoff between the computation workload and the communication cost. The computation workload and communication requirement may change with different execution instances. Optimal decisions on program partitioning must be made at run time when sufficient information about workload and communication requirement becomes available.Our cost analysis obtains program computation workload and communication cost expressed as functions of runtime parameters, and our parametric partitioning algorithm finds the optimal program partitioning corresponding to different ranges of runtime parameters. At run time, the transformed program selfschedules its tasks on either the mobile device or the server, based on the optimal program partitioning that corresponds to the current values of runtime parameters. Experimental results on an HP IPAQ handheld device show that different runtime parameters can lead to quite different program partitioning decisions.
p2169
aVThis paper presents the first scalable contextsensitive, inclusionbased pointer alias analysis for Java programs. Our approach to context sensitivity is to create a clone of a method for every context of interest, and run a contextinsensitive algorithm over the expanded call graph to get contextsensitive results. For precision, we generate a clone for every acyclic path through a program's call graph, treating methods in a strongly connected component as a single node. Normally, this formulation is hopelessly intractable as a call graph often has 10 14 acyclic paths or more. We show that these exponential relations can be computed efficiently using binary decision diagrams (BDDs). Key to the scalability of the technique is a context numbering scheme that exposes the commonalities across contexts. We applied our algorithm to the most popular applications available on Sourceforge, and found that the largest programs, with hundreds of thousands of Java bytecodes, can be analyzed in under 20 minutes.This paper shows that pointer analysis, and many other queries and algorithms, can be described succinctly and declaratively using Datalog, a logic programming language. We have developed a system called bddbddb that automatically translates Datalog programs into highly efficient BDD implementations. We used this approach to develop a variety of contextsensitive algorithms including side effect analysis, type analysis, and escape analysis.
p2170
aVPointer analysis is a critical problem in optimizing compiler, parallelizing compiler, software engineering and most recently, hardware synthesis. While recent efforts have suggested symbolic method, which uses Bryant's Binary Decision Diagram as an alternative to capture the pointto relation, no speed advantage has been demonstrated for contextinsensitive analysis, and results for contextsensitive analysis are only preliminary.In this paper, we refine the concept of symbolic transfer function proposed earlier and establish a common framework for both contextinsensitive and contextsensitive pointer analysis. With this framework, our transfer function of a procedure can abstract away the impact of its callers and callees, and represent its pointto information completely, compactly and canonically. In addition, we propose a symbolic representation of the invocation graph, which can otherwise be exponentially large. In contrast to the classical frameworks where contextsensitive pointto information of a procedure has to be obtained by the application of its transfer function exponentially many times, our method can obtain pointto information of all contexts in a single application. Our experimental evaluation on a wide range of C benchmarks indicates that our contextsensitive pointer analysis can be made almost as fast as its contextinsensitive counterpart.
p2171
aVIn this paper we present Jedd, a language extension to Java that supports a convenient way of programming with Binary Decision Diagrams (BDDs). The Jedd language abstracts BDDs as databasestyle relations and operations on relations, and provides static type rules to ensure that relational operations are used correctly.The paper provides a description of the Jedd language and reports on the design and implementation of the Jedd translator and associated runtime system. Of particular interest is the approach to assigning attributes from the highlevel relations to physical domains in the underlying BDDs, which is done by expressing the constraints as a SAT problem and using a modern SAT solver to compute the solution. Further, a runtime system is defined that handles memory management issues and supports a browsable profiling tool for tuning the key BDD operations.The motivation for designing Jedd was to support the development of whole program analyses based on BDDs, and we have used Jedd to express five key interrelated whole program analyses in our Soot compiler framework. We provide some examples of this application and discuss our experiences using Jedd.
p2172
aVLanguagebased security is a protection mechanism that allows software components to interact in a shared address space, such that each component is guaranteed to respect its interfaces and not steal or corrupt internal data of other components. This protection mechanism is complicated to implement correctly, so we might want a formal verification of it.But we know by a famous result of DeMillo, Lipton, and Perlis (POPL 1978) that formal verification (1) is not what mathematicians do, (2) can never be practical, and (3) cannot tell us anything truly useful. Is this still true 25 years later?The question is, then, how can we carefully skirt the legitimate objections of DeMillo et al. and successfully use formal verification in a context where it can do some good. I'll talk about Foundational ProofCarrying Code, a machinechecked soundness proof for a protection mechanism usable in Javalike virtual machines.
p2173
aVIt has long been known that a fixed ordering of optimization phases will not produce the best code for every application. One approach for addressing this phase ordering problem is to use an evolutionary algorithm to search for a specific sequence of phases for each module or function. While such searches have been shown to produce more efficient code, the approach can be extremely slow because the application is compiled and executed to evaluate each sequence's effectiveness. Consequently, evolutionary or iterative compilation schemes have been promoted for compilation systems targeting embedded applications where longer compilation times may be tolerated in the final stage of development. In this paper we describe two complementary general approaches for achieving faster searches for effective optimization sequences when using a genetic algorithm. The first approach reduces the search time by avoiding unnecessary executions of the application when possible. Results indicate search time reductions of 65% on average, often reducing searches from hours to minutes. The second approach modifies the search so fewer generations are required to achieve the same results. Measurements show that the average number of required generations decreased by 68%. These improvements have the potential for making evolutionary compilation a viable choice for tuning embedded applications.
p2174
aVInstruction scheduling is a compiler optimization that can improve program speed, sometimes by 10% or more, but it can also be expensive. Furthermore, time spent optimizing is more important in a Java justintime (JIT) compiler than in a traditional one because a JIT compiles code at run time, adding to the running time of the program. We found that, on any given block of code, instruction scheduling often does not produce significant benefit and sometimes degrades speed. Thus, we hoped that we could focus scheduling effort on those blocks that benefit from it.Using supervised learning we induced heuristics to predict which blocks benefit from scheduling. The induced function chooses, for each block, between list scheduling and not scheduling the block at all. Using the induced function we obtained over 90% of the improvement of scheduling every block but with less than 25% of the scheduling effort. When used in combination with profilebased adaptive optimization, the induced function remains effective but gives a smaller reduction in scheduling effort. Deciding when to optimize, and which optimization(s) to apply, is an important open problem area in compiler research. We show that supervised learning solves one of these problems well.
p2175
aVRapid exploration of the design space with simulation models is essential for quality hardware systems research and development. Despite striking commonalities across hardware systems, designers routinely fail to achieve high levels of reuse across models constructed in existing generalpurpose and domainspecific languages. This lack of reuse adversely impacts hardware system design by slowing the rate at which ideas are evaluated. This paper presents an examination of existing languages to reveal their fundamental limitations regarding reuse in hardware modeling. With this understanding, a solution is described in the context of the design and implementation of the Liberty Structural Specification Language (LSS), the input language for a publicly available highlevel digitalhardware modeling tool called the Liberty Simulation Environment. LSS is the first language to enable lowoverhead reuse by simultaneously supporting static inference based on hardware structure and flexibility via parameterizable structure. Through LSS, this paper also introduces a new type inference algorithm and a new programming language technique, called usebased specialization, which, in a manner analogous to type inference, customizes reusable components by statically inferring structural properties that otherwise would have had to have been specified manually.
p2176
aVMany program analyses can be reduced to graph reachability problems involving a limited form of contextfree language reachability called DyckCFL reachability. We show a new reduction from DyckCFL reachability to set constraints that can be used in practice to solve these problems. Our reduction is much simpler than the general reduction from contextfree language reachability to set constraints. We have implemented our reduction on top of a set constraints toolkit and tested its performance on a substantial polymorphic flow analysis application.
p2177
aVThe design of concurrent programs is errorprone due to the interaction between concurrently executing threads. Traditional automated techniques for finding errors in concurrent programs, such as model checking, explore all possible thread interleavings. Since the number of thread interleavings increases exponentially with the number of threads, such analyses have high computational complexity. In this paper, we present a novel analysis technique for concurrent programs that avoids this exponential complexity. Our analysis transforms a concurrent program into a sequential program that simulates the execution of a large subset of the behaviors of the concurrent program. The sequential program is then analyzed by a tool that only needs to understand the semantics of sequential execution. Our technique never reports false errors but may miss errors. We have implemented the technique in KISS, an automated checker for multithreaded C programs, and obtained promising initial results by using KISS to detect race conditions in Windows device drivers.
p2178
aVRegular path queries are a way of declaratively expressing queries on graphs as regularexpressionlike patterns that are matched against paths in the graph. There are two kinds of queries: existential queries, which specify properties about individual paths, and universal queries, which specify properties about all paths. They provide a simple and convenient framework for expressing program analyses as queries on graph representations of programs, for expressing verification (modelchecking) problems as queries on transition systems, for querying semistructured data, etc. Parametric regular path queries extend the patterns with variables, called parameters, which significantly increase the expressiveness by allowing additional information along single or multiple paths to be captured and relate.This paper shows how a variety of program analysis and modelchecking problems can be expressed easily and succinctly using parametric regular path queries. The paper describes the specification, design, analysis, and implementation of algorithms and data structures for efficiently solving existential and universal parametric regular path queries. Major contributions include the first complete algorithms and data structures for directly and efficiently solving existential and universal parametric regular path queries, detailed complexity analysis of the algorithms, detailed analytical and experimental performance comparison of variations of the algorithms and data structures, and investigation of efficiency tradeoffs between different formulations of queries.
p2179
aVIn this paper we describe the design and implementation of a static arraybound checker for a family of embedded programs: the flight control software of recent Mars missions. These codes are large (up to 280 KLOC), pointer intensive, heavily multithreaded and written in an objectoriented style, which makes their analysis very challenging. We designed a tool called C Global Surveyor (CGS) that can analyze the largest code in a couple of hours with a precision of 80%. The scalability and precision of the analyzer are achieved by using an incremental framework in which a pointer analysis and a numerical analysis of array indices mutually refine each other. CGS has been designed so that it can distribute the analysis over several processors in a cluster of machines. To the best of our knowledge this is the first distributed implementation of static analysis algorithms. Throughout the paper we will discuss the scalability setbacks that we encountered during the construction of the tool and their impact on the initial design decisions.
p2180
aVRegionbased memory management offers several important potential advantages over garbage collection, including realtime performance, better data locality, and more efficient use of limited memory. Researchers have advocated the use of regions for functional, imperative, and objectoriented languages. Lexically scoped regions are now a core feature of the RealTime Specification for Java (RTSJ)[5].Recent research in regionbased programming for Java has focused on region checking, which requires manual effort to augment the program with region annotations. In this paper, we propose an automatic region inference system for a core subset of Java. To provide an inference method that is both precise and practical, we support classes and methods that are regionpolymorphic, with regionpolymorphic recursion for methods. One challenging aspect is to ensure region safety in the presence of features such as class subtyping, method overriding, and downcast operations. Our region inference rules can handle these objectoriented features safely without creating dangling references.
p2181
aVWhile the memory of most machines is organized as a hierarchy, program data are laid out in a uniform address space. This paper defines a model of reference affinity, which measures how close a group of data are accessed together in a reference trace. It proves that the model gives a hierarchical partition of program data. At the top is the set of all data with the weakest affinity. At the bottom is each data element with the strongest affinity. Based on the theoretical model, the paper presents kdistance analysis, a practical test for the hierarchical affinity of sourcelevel data. When used for array regrouping and structure splitting, kdistance analysis consistently outperforms data organizations given by the programmer, compiler analysis, frequency profiling, statistical clustering, and all other methods we have tried.
p2182
aVCache miss stalls hurt performance because of the large gap between memory and processor speeds  for example, the popular server benchmark SPEC JBB2000 spends 45% of its cycles stalled waiting for memory requests on the Itanium 2 processor. Traversing linked data structures causes a large portion of these stalls. Prefetching for linked data structures remains a major challenge because serial data dependencies between elements in a linked data structure preclude the timely materialization of prefetch addresses. This paper presents Mississippi Delta (MS Delta), a novel technique for prefetching linked data structures that closely integrates the hardware performance monitor (HPM), the garbage collector's global view of heap and object layout, the typelevel metadata inherent in typesafe programs, and JIT compiler analysis. The garbage collector uses the HPM's data cache miss information to identify cache miss intensive traversal paths through linked data structures, and then discovers regular distances (deltas) between these linked objects. JIT compiler analysis injects prefetch instructions using deltas to materialize prefetch addresses.We have implemented MS Delta in a fully dynamic profileguided optimization system: the StarJIT dynamic compiler [1] and the ORP Java virtual machine [9]. We demonstrate a 2829% reduction in stall cycles attributable to the highlatency cache misses targeted by MS Delta and a speedup of 1114% on the cache miss intensive SPEC JBB2000 benchmark.
p2183
aVGraphcoloring register allocation is an elegant and extremely popular optimization for modern machines. But as currently formulated, it does not handle two characteristics commonly found in commercial architectures. First, a single register name may appear in multiple register classes, where a class is a set of register names that are interchangeable in a particular role. Second, multiple register names may be aliases for a single hardware register. We present a generalization of graphcoloring register allocation that handles these problematic characteristics while preserving the elegance and practicality of traditional graph coloring. Our generalization adapts easily to a new target machine, requiring only the sets of names in the register classes and a map of the register aliases. It also drops easily into a wellknown graphcoloring allocator, is efficient at compile time, and produces highquality code.
p2184
aVIn this paper, we show how separation (decomposing a verification problem into a collection of verification subproblems) can be used to improve the efficiency and precision of verification of safety properties. We present a simple language for specifying separation strategies for decomposing a single verification problem into a set of subproblems. (The strategy specification is distinct from the safety property specification and is specified separately.) We present a general framework of heterogeneous abstraction that allows different parts of the heap to be abstracted using different degrees of precision at different points during the analysis. We show how the goals of separation (i.e., more efficient verification) can be realized by first using a separation strategy to transform (instrument) a verification problem instance (consisting of a safety property specification and an input program), and by then utilizing heterogeneous abstraction during the verification of the transformed verification problem.
p2185
aVDynamic memory allocators (malloc/free) rely on mutual exclusion locks for protecting the consistency of their shared data structures under multithreading. The use of locking has many disadvantages with respect to performance, availability, robustness, and programming flexibility. A lockfree memory allocator guarantees progress regardless of whether some threads are delayed or even killed and regardless of scheduling policies. This paper presents a completely lockfree memory allocator. It uses only widelyavailable operating system support and hardware atomic instructions. It offers guaranteed availability even under arbitrary thread termination and crashfailure, and it is immune to deadlock regardless of scheduling policies, and hence it can be used even in interrupt handlers and realtime applications without requiring special scheduler support. Also, by leveraging some highlevel structures from Hoard, our allocator is highly scalable, limits space blowup to a constant factor, and is capable of avoiding false sharing. In addition, our allocator allows finer concurrency and much lower latency than Hoard. We use PowerPC shared memory multiprocessor systems to compare the performance of our allocator with the default AIX 5.1 libc malloc, and two widelyused multithread allocators, Hoard and Ptmalloc. Our allocator outperforms the other allocators in virtually all cases and often by substantial margins, under various levels of parallelism and allocation patterns. Furthermore, our allocator also offers the lowest contentionfree latency among the allocators by significant margins.
p2186
aVWhen an individual task can be forcefully terminated at any time, cooperating tasks must communicate carefully. For example, if two tasks share an object, and if one task is terminated while it manipulates the object, the object may remain in an inconsistent or frozen state that incapacitates the other task. To support communication among terminable tasks, language runtime systems (and operating systems) provide killsafe abstractions for intertask communication. No killsafe guarantee is available, however, for abstractions that are implemented outside the runtime system.In this paper, we show how a runtime system can support new killsafe abstractions without requiring modification to the runtime system, and without requiring the runtime system to trust any new code. Our design frees the runtime implementor to provide only a modest set of synchronization primitives in the trusted computing base, while still allowing tasks to communicate using sophisticated abstractions.
p2187
aVWith billiontransistor chips on the horizon, singlechip multiprocessors (CMPs) are likely to become commodity components. Speculative CMPs use hardware to enforce dependence, allowing the compiler to improve performance by speculating on ambiguous dependences without absolute guarantees of independence. The compiler is responsible for decomposing a sequential program into speculatively parallel threads, while considering multiple performance overheads related to data dependence, load imbalance, and thread prediction. Although the decomposition problem lends itself to a mincutbased approach, the overheads depend on the thread size, requiring the edge weights to be changed as the algorithm progresses. The changing weights make our approach different from graphtheoretic solutions to the general problem of task scheduling. One recent work uses a set of heuristics, each targeting a specific overhead in isolation, and gives precedence to thread prediction, without comparing the performance of the threads resulting from each heuristic. By contrast, our method uses a sequence of balanced mincuts that give equal consideration to all the overheads, and adjusts the edge weights after every cut. This method achieves an (geometric) average speedup of 74% for floatingpoint programs and 23% for integer programs on a fourprocessor chip, improving on the 52% and 13% achieved by the previous heuristics.
p2188
aVThe emerging hardware support for threadlevel speculation opens new opportunities to parallelize sequential programs beyond the traditional limits. By speculating that many data dependences are unlikely during runtime, consecutive iterations of a sequential loop can be executed speculatively in parallel. Runtime parallelism is obtained when the speculation is correct. To take full advantage of this new execution model, a program needs to be programmed or compiled in such a way that it exhibits high degree of speculative threadlevel parallelism. We propose a comprehensive costdriven compilation framework to perform speculative parallelization. Based on a misspeculation cost model, the compiler aggressively transforms loops into optimal speculative parallel loops and selects only those loops whose speculative parallel execution is likely to improve program performance. The framework also supports and uses enabling techniques such as loop unrolling, software value prediction and dependence profiling to expose more speculative parallelism. The proposed framework was implemented on the ORC compiler. Our evaluation showed that the costdriven speculative parallelization was effective. Our compiler was able to generate good speculative parallel loops in ten Spec2000Int benchmarks, which currently achieve an average 8% speedup. We anticipate an average 15.6% speedup when all enabling techniques are in place.
p2189
aVWhen vectorizing for SIMD architectures that are commonly employed by today's multimedia extensions, one of the new challenges that arise is the handling of memory alignment. Prior research has focused primarily on vectorizing loops where all memory references are properly aligned. An important aspect of this problem, namely, how to vectorize misaligned memory references, still remains unaddressed.This paper presents a compilation scheme that systematically vectorizes loops in the presence of misaligned memory references. The core of our technique is to automatically reorganize data in registers to satisfy the alignment requirement imposed by the hardware. To reduce the data reorganization overhead, we propose several techniques to minimize the number of data reorganization operations generated. During the code generation, our algorithm also exploits temporal reuse when aligning references that access contiguous memory across loop iterations. Our code generation scheme guarantees to never load the same data associated with a single static access twice. Experimental results indicate near peak speedup factors, e.g., 3.71 for 4 data per vector and 6.06 for 8 data per vector, respectively, for a set of loops where 75% or more of the static memory references are misaligned.
p2190
aVAlthough dynamic program slicing was first introduced to aid in user level debugging, applications aimed at improving software quality, reliability, security, and performance have since been identified as candidates for using dynamic slicing. However, the dynamic dependence graph constructed to compute dynamic slices can easily cause slicing algorithms to run out of memory for realistic program runs. In this paper we present the design and evaluation of a cost effective dynamic program slicing algorithm. This algorithm is based upon a dynamic dependence graph representation that is highly compact and rapidly traversable. Thus, the graph can be held in memory and dynamic slices can be quickly computed. A compact representation is derived by recognizing that all dynamic dependences (data and control) need not be individually represented. We identify sets of dynamic dependence edges between a pair of statements that can share a single representative edge. We further show that the dependence graph can be transformed in a manner that increases sharing and sharing can be performed even in the presence of aliasing. Experiments show that transformed dynamic dependence graphs explicitly represent only 6% of the dependence edges present in the full dynamic dependence graph. When the full graph sizes range from 0.84 to 1.95 Gigabytes in size, our compacted graphs range from 20 to 210 Megabytes in size. Average slicing times for our algorithm range from 1.74 to 36.25 seconds across several benchmarks from SPECInt2000/95.
p2191
aVWe aim to improve reliability of multithreaded programs by proposing a dynamic detector that detects potentially erroneous program executions and their causes. We design and evaluate a Serializability Violation Detector (SVD) that has two unique goals: (I) triggering automatic recovery from erroneous executions using backward error recovery (BER), or simply alerting users that a software error may have occurred; and (II) helping debug programs by revealing causes of error symptoms.Two properties of SVD help in achieving these goals. First, to detect only erroneous executions, SVD checks serializability of atomic regions, which are code regions that need to be executed atomically. Second, to improve usability, SVD does not require a priori annotations of atomic regions; instead, SVD approximates them using a heuristic. Experimental results on three widelyused multithreaded server programs show that SVD finds real bugs and reports modest false positives. The goal of this paper is to develop a detector suitable for (I) BERbased avoidance of erroneous program executions; and (II) alerting users as software errors occur. We argue that such a detector should have the following two properties.
p2192
aVCode placement techniques have traditionally improved instruction fetch bandwidth by increasing instruction locality and decreasing the number of taken branches. However, traditional code placement techniques have less benefit in the presence of a trace cache that alters the placement of instructions in the instruction cache. Moreover, as pipelines have become deeper to accommodate increasing clock rates, branch misprediction penalties have become a significant impediment to performance. We evaluate pattern history table partitioning, a feedback directed code placement technique that explicitly places conditional branches so that they are less likely to interfere destructively with one another in branch prediction tables. On SPEC CPU benchmarks running on an Intel Pentium 4, branch mispredictions are reduced by up to 22% and 3.5% on average. This reduction yields a speedup of up to 16.0% and 4.5% on average. By contrast, branch alignment, a previous code placement technique, yields only up to a 4.7% speedup and less than 1% on average.
p2193
aVAspectJ, an aspectoriented extension of Java, is becoming increasingly popular. However, not much work has been directed at optimising compilers for AspectJ. Optimising AOP languages provides many new and interesting challenges for compiler writers, and this paper identifies and addresses three such challenges.First, compiling around advice efficiently is particularly challenging. We provide a new code generation strategy for around advice, which (unlike previous implementations) both avoids the use of excessive inlining and the use of closures. We show it leads to more compact code, and can also improve runtime performance. Second, woven code sometimes includes runtime tests to determine whether advice should execute. One important case is the cflow pointcut which uses information about the dynamic calling context. Previous techniques for cflow were very costly in terms of both time and space. We present new techniques to minimise or eliminate the overhead of cflow using both intra and interprocedural analyses. Third, we have addressed the general problem of how to structure an optimising compiler so that traditional analyses can be easily adapted to the AOP setting.We have implemented all of the techniques in this paper in abc, our AspectBench Compiler for AspectJ, and we demonstrate significant speedups with empirical results. Some of our techniques have already been integrated into the production AspectJ compiler, ajc 1.2.1.
p2194
aVThis paper describes Automatic Pool Allocation, a transformation framework that segregates distinct instances of heapbased data structures into seperate memory pools and allows heuristics to be used to partially control the internal layout of those data structures. The primary goal of this work is performance improvement, not automatic memory management, and the paper makes several new contributions. The key contribution is a new compiler algorithm for partitioning heap objects in imperative programs based on a contextsensitive pointer analysis, including a novel strategy for correct handling of indirect (and potentially unsafe) function calls. The transformation does not require type safe programs and works for the full generality of C and C++. Second, the paper describes several optimizations that exploit data structure partitioning to further improve program performance. Third, the paper evaluates how memory hierarchy behavior and overall program performance are impacted by the new transformations. Using a number of benchmarks and a few applications, we find that compilation times are extremely low, and overall running times for heap intensive programs speed up by 1025% in many cases, about 2x in two cases, and more than 10x in two small benchmarks. Overall, we believe this work provides a new framework for optimizing pointer intensive programs by segregating and controlling the layout of heapbased data structures.
p2195
aVGarbage collection offers numerous software engineering advantages, but interacts poorly with virtual memory managers. Existing garbage collectors require far more pages than the application's working set and touch pages without regard to which ones are in memory, especially during fullheap garbage collection. The resulting paging can cause throughput to plummet and pause times to spike up to seconds or even minutes. We present a garbage collector that avoids paging. This bookmarking collector cooperates with the virtual memory manager to guide its eviction decisions. Using summary information ("bookmarks") recorded from evicted pages, the collector can perform inmemory fullheap collections. In the absence of memory pressure, the bookmarking collector matches the throughput of the best collector we tested while running in smaller heaps. In the face of memory pressure, it improves throughput by up to a factor of five and reduces pause times by up to a factor of 45 over the next best collector. Compared to a collector that consistently provides high throughput (generational marksweep), the bookmarking collector reduces pause times by up to 218x and improves throughput by up to 41x. Bookmarking collection thus provides greater utilization of available physical memory than other collectors while matching or exceeding their throughput.
p2196
aVSoftware pipelining of a multidimensional loop is an important optimization that overlaps the execution of successive outermost loop iterations to explore instructionlevel parallelism from the entire ndimensional iteration space. This paper investigates register allocation for software pipelined multidimensional loops.For single loop software pipelining, the lifetime instances of a loop variant in successive iterations of the loop form a repetitive pattern. An effective register allocation method is to represent the pattern as a vector of lifetimes (or a vector lifetime using Rau's terminology) and map it to rotating registers. Unfortunately, the software pipelined schedule of a multidimensional loop is considerably more complex, and so are the vector lifetimes in it.In this paper, we develop a way to normalize and represent vector lifetimes in multidimensional loop software pipelining, which capture their complexity, while exposing their regularity that enables us to develop a simple, yet powerful solution. Our algorithm is based on the development of a metric, called distance, that quantitatively determines the degree of potential overlapping (conflicts) between two vector lifetimes. We show how to calculate and use the distance, conservatively or aggressively, to guide the register allocation of the vector lifetimes under a binpacking algorithm framework. The classical register allocation for software pipelined single loops is subsumed by our method as a special case.The method has been implemented in the ORC compiler and produced code for the Itanium architecture. We report the effectiveness of our method on 134 loop nests with 348 loop levels. Several strategies for register allocation are compared and analyzed.
p2197
aVMicroarchitecture designers are very cautious about expanding the number of architected registers (also the register field), because increasing the register field adds to the code size, raises Icache and memory pressure, complicates processor pipeline. Especially for lowend processors, encoding space could be extremely limited due to area and power considerations. On the other hand, the number of architected registers exposed to the compiler could directly affect the effectiveness of compiler analysis and optimization. For high performance computers, register pressure can be higher than the available registers in some regions, e.g. due to optimizations like aggressive function inlining, software pipelining etc. The compiler cannot effectively perform compilation and optimization if only a small number of registers are exposed through the ISA. Therefore, it is crucial that more architected registers are available at the compiler's disposal without expanding the code size significantly.In this paper, we look at a new register encoding scheme called differential encoding that allows more registers to be addressed in the operand field of instructions than the direct encoding currently being used. We show it can be implemented with very low overhead. Based upon differential encoding, we apply it in several ways such that the extra architected registers can benefit the performance. Three schemes are devised to integrate differential encoding with register allocation. We demonstrate that differential register allocation is helpful in improving the performance of both highend and lowend processors. Moreover, We can combine it with software pipelining to provide more registers and reduce spills.Our results show that differential encoding significantly reduces the number of spills and speeds up program execution. For a lowend configuration, we achieve over 12% speedup while keeping code size almost unaffected. For optimization on loops, it significantly speeds up loops with high register pressure (over 70% speedup).
p2198
aVModulo scheduling is an effective code generation technique that exploits the parallelism in program loops by overlapping iterations. One drawback of this optimization is that register requirements increase significantly because values across different loop iterations can be live concurrently. One possible solution to reduce register pressure is to insert spill code to release registers. Spill code stores values to memory between the producer and consumer instructions.Spilling heuristics can be divided into two classes: 1) a posteriori approaches (spill code is inserted after scheduling the loop) or 2) onthefly approaches (spill code is inserted during loop scheduling). Recent studies have reported obtaining better results for spilling onthefly. In this work, we study both approaches and propose two new techniques, one for each approach. Our new algorithms try to address the drawbacks observed in previous proposals. We show that the new algorithms outperform previous techniques and, at the same time, reduce compilation time. We also show that, much to our surprise, a posteriori spilling can be in fact slitghtly more effective than onthefly spilling.
p2199
aVRobust and powerful software instrumentation tools are essential for program analysis tasks such as profiling, performance evaluation, and bug detection. To meet this need, we have developed a new instrumentation system called Pin. Our goals are to provide easytouse, portable, transparent, and efficient instrumentation. Instrumentation tools (called Pintools) are written in C/C++ using Pin's rich API. Pin follows the model of ATOM, allowing the tool writer to analyze an application at the instruction level without the need for detailed knowledge of the underlying instruction set. The API is designed to be architecture independent whenever possible, making Pintools source compatible across different architectures. However, a Pintool can access architecturespecific details when necessary. Instrumentation with Pin is mostly transparent as the application and Pintool observe the application's original, uninstrumented behavior. Pin uses dynamic compilation to instrument executables while they are running. For efficiency, Pin uses several techniques, including inlining, register reallocation, liveness analysis, and instruction scheduling to optimize instrumentation. This fully automated approach delivers significantly better instrumentation performance than similar tools. For example, Pin is 3.3x faster than Valgrind and 2x faster than DynamoRIO for basicblock counting. To illustrate Pin's versatility, we describe two Pintools in daily use to analyze production software. Pin is publicly available for Linux platforms on four architectures: IA32 (32bit x86), EM64T (64bit x86), Itanium, and ARM. In the ten months since Pin 2 was released in July 2004, there have been over 3000 downloads from its website.
p2200
aVFaults that occur in production systems are the most important faults to fix, but most production systems lack the debugging facilities present in development environments. TraceBack provides debugging information for production systems by providing execution history data about program problems (such as crashes, hangs, and exceptions). TraceBack supports features commonly found in production environments such as multiple threads, dynamically loaded modules, multiple source languages (e.g., Java applications running with JNI modules written in C++), and distributed execution across multiple computers. TraceBack supports first fault diagnosisdiscovering what went wrong the first time a fault is encountered. The user can see how the program reached the fault state without having to rerun the computation; in effect enabling a limited form of a debugger in production code.TraceBack uses static, binary program analysis to inject lowoverhead runtime instrumentation at controlflow block granularity. Postfacto reconstruction of the records written by the instrumentation code produces a sourcestatement trace for user diagnosis. The trace shows the dynamic instruction sequence leading up to the fault state, even when the program took exceptions or terminated abruptly (e.g., kill 9).We have implemented TraceBack on a variety of architectures and operating systems, and present examples from a variety of platforms. Performance overhead is variable, from 5% for Apache running SPECweb99, to 16%25% for the Java SPECJbb benchmark, to 60% average for SPECint2000. We show examples of TraceBack's crosslanguage and crossmachine abilities, and report its use in diagnosing problems in production software.
p2201
aVWe present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static sourcecode parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles   there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and nontermination. Preliminary experiments to unit test several examples of C programs are very encouraging.
p2202
aVWe present a statistical debugging algorithm that isolates bugs in programs containing multiple undiagnosed bugs. Earlier statistical algorithms that focus solely on identifying predictors that correlate with program failure perform poorly when there are multiple bugs. Our new technique separates the effects of different bugs and identifies predictors that are associated with individual bugs. These predictors reveal both the circumstances under which bugs occur as well as the frequencies of failure modes, making it easier to prioritize debugging efforts. Our algorithm is validated using several case studies, including examples in which the algorithm identified previously unknown, significant crashing bugs in widely used systems.
p2203
aVProgramming network processors is challenging. To sustain high line rates, network processors have extremely tight memory access and instruction budgets. Achieving desired performance has traditionally required handcoded assembly. Researchers have recently proposed highlevel programming languages for packet processing, but the challenges of compiling these languages into code that is competitive with handtuned assembly remain unanswered.This paper describes the ShangriLa compiler, which accepts a packet program written in a Clike highlevel language and applies scalar and specialized optimizations to generate a highly optimized binary. Hot code paths identified by profiling are mapped across processing elements to maximize processor utilization. Since our compilation target has no hardware caches, softwarecontrolled caches are generated for frequently accessed application data structures. Packet handling optimizations significantly reduce perpacket memory access and instruction counts. Finally, a custom stack model maps stack frames to the fastest levels of the target processor's heterogeneous memory hierarchy.Binaries generated by the compiler were evaluated on the Intel IXP2400 network processor with eight packet processing cores and eight threads per core. Our results show the importance of both traditional and specialized optimization techniques for achieving the maximum forwarding rates on three network applications, L3Switch, MPLS and Firewall.
p2204
aVModern network processors employs parallel processing engines (PEs) to keep up with explosive internet packet processing demands. Most network processors further allow processing engines to be organized in a pipelined fashion to enable higher processing throughput and flexibility. In this paper, we present a novel program transformation technique to exploit parallel and pipelined computing power of modern network processors. Our proposed method automatically partitions a sequential packet processing application into coordinated pipelined parallel subtasks which can be naturally mapped to contemporary highperformance network processors. Our transformation technique ensures that packet processing tasks are balanced among pipeline stages and that data transmission between pipeline stages is minimized. We have implemented the proposed transformation method in an autopartitioning C compiler product for Intel Network Processors. Experimental results show that our method provides impressive speed up for the commonly used NPF IPv4 forwarding and IP forwarding benchmarks. For a 9stage pipeline, our autopartitioning C compiler obtained more than 4X speedup for the IPv4 forwarding PPS and the IP forwarding PPS (for both the IPv4 traffic and IPv6 traffic).
p2205
aVAdhoc networks of mobile devices such as smart phones and PDAs represent a new and exciting distributed system architecture. Building distributed applications on such an architecture poses new design challenges in programming models, languages, compilers, and runtime systems. This paper discusses SpatialViews, a highlevel language designed for programming mobile devices connected through a wireless adhoc network. SpatialViews allows specification of virtual networks with nodes providing desired services and residing in interesting spaces. These nodes are discovered dynamically with userspecified time constraints and quality of result (QoR). The programming model supports "besteffort" semantics, i.e., different executions of the same program may result in "correct" answers of different quality. It is the responsibility of the compiler and runtime system to produce a highquality answer for the particular network and resource conditions encountered during program execution. Four applications, which exercise different features of the SpatialViews language, are presented to demonstrate the expressiveness of the language and the efficiency of the compiler generated code. The applications are an application that collects and aggregates sensor data in network, an application that performs dynamic service installation, a mobile camera application that supports computation offloading for image understanding, and an augmentedreality (AR) Pacman game. The efficiency of the compiler generated code is verified through simulation and physical measurements. The reported results show that SpatialViews is an expressive and effective language for adhoc networks. In addition, compiler optimizations can significantly improve response times and energy consumption.
p2206
aVIn many environments, multithreaded code is written in a language that was originally designed without thread support (e.g. C), to which a library of threading primitives was subsequently added. There appears to be a general understanding that this is not the right approach. We provide specific arguments that a pure library approach, in which the compiler is designed independently of threading issues, cannot guarantee correctness of the resulting code.We first review why the approach almost works, and then examine some of the surprising behavior it may entail. We further illustrate that there are very simple cases in which a pure librarybased approach seems incapable of expressing an efficient parallel algorithm.Our discussion takes place in the context of C with Pthreads, since it is commonly used, reasonably well specified, and does not attempt to ensure typesafety, which would entail even stronger constraints. The issues we raise are not specific to that context.
p2207
aVSpeculative parallelization can provide significant sources of additional threadlevel parallelism, especially for irregular applications that are hard to parallelize by conventional approaches. In this paper, we present the Mitosis compiler, which partitions applications into speculative threads, with special emphasis on applications for which conventional parallelizing approaches fail.The management of interthread data dependences is crucial for the performance of the system. The Mitosis framework uses a pure software approach to predict/compute the thread's input values. This software approach is based on the use of precomputation slices (pslices), which are built by the Mitosis compiler and added at the beginning of the speculative thread. Pslices must compute thread input values accurately but they do not need to guarantee correctness, since the underlying architecture can detect and recover from misspeculations. This allows the compiler to use aggressive/unsafe optimizations to significantly reduce their overhead. The most important optimizations included in the Mitosis compiler and presented in this paper are branch pruning, memory and register dependence speculation, and early thread squashing.Performance evaluation of Mitosis compiler/architecture shows an average speedup of 2.2.
p2208
aVComputer architecture is about to undergo, if not another revolution, then a vigorous shakingup. The major chip manufacturers have, for the time being, simply given up trying to make processors run faster. Instead, they have recently started shipping "multicore" architectures, in which multiple processors (cores) communicate directly through shared hardware caches, providing increased concurrency instead of increased clock speed.As a result, system designers and software engineers can no longer rely on increasing clock speed to hide software bloat. Instead, they must somehow learn to make effective use of increasing parallelism. This adaptation will not be easy. Conventional synchronization techniques based on locks and conditions are unlikely to be effective in such a demanding environment. Coarsegrained locks, which protect relatively large amounts of data, do not scale, and finegrained locks introduce substantial software engineering problems.Transactional memory is a computational model in which threads synchronize by optimistic, lockfree transactions. This synchronization model promises to alleviate many (perhaps not all) of the problems associated with locking, and there is a growing community of researchers working on both software and hardware support for this approach. This talk will survey the area, with a focus on open research problems.
p2209
aVThis paper introduces the concept of programming with sketches, an approach for the rapid development of highperformance applications. This approach allows a programmer to write clean and portable reference code, and then obtain a highquality implementation by simply sketching the outlines of the desired implementation. Subsequently, a compiler automatically fills in the missing details while also ensuring that a completed sketch is faithful to the input reference code. In this paper, we develop StreamBit as a sketching methodology for the important class of bitstreaming programs (e.g., coding and cryptography).A sketch is a partial specification of the implementation, and as such, it affords several benefits to programmer in terms of productivity and code robustness. First, a sketch is easier to write compared to a complete implementation. Second, sketching allows the programmer to focus on exploiting algorithmic properties rather than on orchestrating lowlevel details. Third, a sketchaware compiler rejects "buggy" sketches, thus improving reliability while allowing the programmer to quickly evaluate sophisticated implementation ideas.We evaluated the productivity and performance benefits of our programming methodology in a userstudy, where a group of novice StreamBit programmers competed with a group of experienced C programmers on implementing a cipher. We learned that, given the same time budget, the ciphers developed in StreamBit ran 2.5x faster than ciphers coded in C. We also produced implementations of DES and Serpent that were competitive with hand optimized implementations available in the public domain.
p2210
aVPADS is a declarative data description language that allows data analysts to describe both the physical layout of ad hoc data sources and semantic properties of that data. From such descriptions, the PADS compiler generates libraries and tools for manipulating the data, including parsing routines, statistical profiling tools, translation programs to produce wellbehaved formats such as Xml or those required for loading relational databases, and tools for running XQueries over raw PADS data sources. The descriptions are concise enough to serve as "living" documentation while flexible enough to describe most of the ASCII, binary, and Cobol formats that we have seen in practice. The generated parsing library provides for robust, applicationspecific error handling.
p2211
aVWe introduce a language and system that supports definition and composition of complex runtime security policies for Java applications. Our policies are comprised of two sorts of methods. The first is query methods that are called whenever an untrusted application tries to execute a securitysensitive action. A query method returns a suggestion indicating how the securitysensitive action should be handled. The second sort of methods are those that perform state updates as the policy's suggestions are followed.The structure of our policies facilitates composition, as policies can query other policies for suggestions. In order to give programmers control over policy composition, we have designed the system so that policies, suggestions, and application events are all firstclass objects that a higherorder policy may manipulate. We show how to use these programming features by developing a library of policy combinators.Our system is fully implemented, and we have defined a formal semantics for an idealized subset of the language containing all of the key features. We demonstrate the effectiveness of our system by implementing a largescale security policy for an email client.
p2212
aVWe present a runtime technique for checking that a concurrentlyaccessed data structure implementation, such as a file system or the storage management module of a database, conforms to an executable specification that contains an atomic method per data structure operation. The specification can be provided separately or a nonconcurrent, "atomized" interpretation of the implementation can serve as the specification. The technique consists of two phases. In the first phase, the implementation is instrumented in order to record information into a log during execution. In the second, a separate verification thread uses the logged information to drive an instance of the specification and to check whether the logged execution conforms to it. We paid special attention to the general applicability and scalability of the techniques and to minimizing their concurrency and performance impact. The result is a lightweight verification method that provides a significant improvement over testing for concurrent programs.We formalize conformance to a specification using the notion of refinement: Each trace of the implementation must be equivalent to some trace of the specification. Among the novel features of our work are two variations on the definition of refinement appropriate for runtime checking: I/O and "view" refinement. These definitions were motivated by our experience with two industrialscale concurrent data structure implementations: the Boxwood project, a Blink tree data structure built on a novel storage infrastructure [10] and the Scan file system [9]. I/O and view refinement checking were implemented as a verification tool named VRYD (VerifYing concurrent programs by Runtime Refinementviolation Detection). VYRD was applied to the verification of Boxwood, Java class libraries, and, previously, to the Scan filesystem. It was able to detect previously unnoticed subtle concurrency bugs in Boxwood and the Scan file system, and the known bugs in the Java class libraries and manually constructed examples. Experimental results indicate that our techniques have modest computational cost.
p2213
aVWe present a new technique, path slicing, that takes as input a possibly infeasible path to a target location, and eliminates all the operations that are irrelevant towards the reachability of the target location. A path slice is a subsequence of the original path whose infeasibility guarantees the infeasibility of the original path, and whose feasibility guarantees the existence of some feasible variant of the given path that reaches the target location even though the given path may itself be infeasible. Our method combines the ability of program slicing to look at several program paths, with the precision that dynamic slicing enjoys by focusing on a single path. We have implemented Path Slicing to analyze possible counterexamples returned by the software model checker Blast. We show its effectiveness in drastically reducing the size of the counterexamples to less than 1% of their original size. This enables the precise verification of application programs (upto 100KLOC), by allowing the analysis to focus on the part of the counterexample that is relevant to the property being checked.
p2214
aVReuse of existing code from class libraries and frameworks is often difficult because APIs are complex and the client code required to use the APIs can be hard to write. We observed that a common scenario is that the programmer knows what type of object he needs, but does not know how to write the code to get the object.In order to help programmers write API client code more easily, we developed techniques for synthesizing jungloid code fragments automatically given a simple query that describes that desired code in terms of input and output types. A jungloid is simply a unary expression; jungloids are simple, enabling synthesis, but are also versatile, covering many coding problems, and composable, combining to form more complex code fragments. We synthesize jungloids using both API method signatures and jungloids mined from a corpus of sample client programs.We implemented a tool, prospector, based on these techniques. prospector is integrated with the Eclipse IDE code assistance feature, and it infers queries from context so there is no need for the programmer to write queries. We tested prospector on a set of real programming problems involving APIs; prospector found the desired solution for 18 of 20 problems. We also evaluated prospector in a user study, finding that programmers solved programming problems more quickly and with more reuse when using prospector than without prospector.
p2215
aVWe present a multilingual type inference system for checking type safety across a foreign function interface. The goal of our system is to prevent foreign function calls from introducing type and memory safety violations into an otherwise safe language. Our system targets OCaml's FFI to C, which is relatively lightweight and illustrates some interesting challenges in multilingual type inference. The type language in our system embeds OCaml types in C types and viceversa, which allows us to track type information accurately even through the foreign language, where the original types are lost. Our system uses representational types that can model multiple OCaml types, because C programs can observe that many OCaml types have the same physical representation. Furthermore, because C has a lowlevel view of OCaml data, our inference system includes a dataflow analysis to track memory offsets and tag information. Finally, our type system includes garbage collection information to ensure that pointers from the FFI to the OCaml heap are tracked properly. We have implemented our inference system and applied it to a small set of benchmarks. Our results show that programmers do misuse these interfaces, and our implementation has found several bugs and questionable coding practices in our benchmarks.
p2216
aVConcepts are an essential language feature for generic programming in the large. Concepts allow for succinct expression of constraints on type parameters of generic algorithms, enable systematic organization of problem domain abstractions, and make generic algorithms easier to use. In this paper we present the design of a type system and semantics for concepts that is suitable for nontypeinferencing languages. Our design shares much in common with the type classes of Haskell, though our primary influence is from best practices in the C++ community, where concepts are used to document type requirements for templates in generic libraries. Concepts include a novel combination of associated types and sametype constraints that do not appear in type classes, but that are similar to nested types and type sharing in ML.
p2217
aVWe present a new approach for supporting userdefined type refinements, which augment existing types to specify and check additional invariants of interest to programmers. We provide an expressive language in which users define new refinements and associated type rules. These rules are automatically incorporated by an extensible typechecker during static typechecking of programs. Separately, a soundness checkerautomatically proves that each refinement's type rules ensure the intended invariant, for all possible programs. We have formalized our approach and have instantiated it as a framework for adding new type qualifiers to C programs. We have used this framework to define and automatically prove sound a host of type qualifiers of different sorts, including pos and neg for integers, tainted and untainted for strings, and nonnull and unique for pointers, and we have applied our qualifiers to ensure important invariants on opensource C programs.
p2218
aVToday's module systems do not effectively support information hiding in the presence of shared mutable objects, causing serious problems in the development and evolution of large software systems. Ownership types have been proposed as a solution to this problem, but current systems have adhoc access restrictions and are limited to Javalike languages.In this paper, we describe System Fown, an extension of System F with references and ownership. Our design shows both how ownership fits into standard type theory and the encapsulation benefits it can provide in languages with firstclass functions, abstract data types, and parametric polymorphism. By looking at ownership in the setting of SystemF, we were able to develop a design that is more principled and flexible than previous ownership type systems, while also providing stronger encapsulation guarantees.
p2219
aVAtomos is the first programming language with implicit transactions, strong atomicity, and a scalable multiprocessor implementation. Atomos is derived from Java, but replaces its synchronization and conditional waiting constructs with simpler transactional alternatives.The Atomos watch statement allows programmers to specify finegrained watch sets used with the Atomos retry conditional waiting statement for efficient transactional conflictdriven wakeup even in transactional memory systems with a limited number of transactional contexts. Atomos supports opennested transactions, which are necessary for building both scalable application programs and virtual machine implementations.The implementation of the Atomos scheduler demonstrates the use of open nesting within the virtual machine and introduces the concept of transactional memory violation handlers that allow programs to recover from data dependency violations without rolling back.Atomos programming examples are given to demonstrate the usefulness of transactional programming primitives. Atomos and Java are compared through the use of several benchmarks. The results demonstrate both the improvements in parallel programming ease and parallel program performance provided by Atomos.
p2220
aVThis paper describes the design and implementation of a scalable runtime system and an optimizing compiler for Unified Parallel C (UPC). An experimental evaluation on BlueGene/L, a distributedmemory machine, demonstrates that the combination of the compiler with the runtime system produces programs with performance comparable to that of efficient MPI programs and good performance scalability up to hundreds of thousands of processors.Our runtime system design solves the problem of maintaining shared object consistency efficiently in a distributed memory machine. Our compiler infrastructure simplifies the code generated for parallel loops in UPC through the elimination of affinity tests, eliminates several levels of indirection for accesses to segments of shared arrays that the compiler can prove to be local, and implements remote update operations through a lowercost asynchronous message. The performance evaluation uses three wellknown benchmarks   HPC RandomAccess, HPC STREAM and NAS CG   to obtain scaling and absolute performance numbers for these benchmarks on up to 131072 processors, the full BlueGene/L machine. These results were used to win the HPC Challenge Competition at SC05 in Seattle WA, demonstrating that PGAS languages support both productivity and performance.
p2221
aVThe widespread presence of SIMD devices in today's microprocessors has made compiler techniques for these devices tremendously important. One of the most important and difficult issues that must be addressed by these techniques is the generation of the data permutation instructions needed for noncontiguous and misaligned memory references. These instructions are expensive and, therefore, it is of crucial importance to minimize their number to improve performance and, in many cases, enable speedups over scalar code.Although it is often difficult to optimize an isolated data reorganization operation, a collection of related data permutations can often be manipulated to reduce the number of operations. This paper presents a strategy to optimize all forms of data permutations. The strategy is organized into three steps. First, all data permutations in the source program are converted into a generic representation. These permutations can originate from vector accesses to noncontiguous and misaligned memory locations or result from compiler transformations. Second, an optimization algorithm is applied to reduce the number of data permutations in a basic block. By propagating permutations across statements and merging consecutive permutations whenever possible, the algorithm can significantly reduce the number of data permutations. Finally, a code generation algorithm translates generic permutation operations into native permutation instructions for the target platform. Experiments were conducted on various kinds of applications. The results show that up to 77% of the permutation instructions are eliminated and, as a result, the average performance improvement is 48% on VMX and 68% on SSE2. For several applications, near perfect speedups have been achieved on both platforms.
p2222
aVMost implementations of the Single Instruction Multiple Data (SIMD) model available today require that data elements be packed in vector registers. Operations on disjoint vector elements are not supported directly and require explicit data reorganization manipulations. Computations on noncontiguous and especially interleaved data appear in important applications, which can greatly benefit from SIMD instructions once the data is reorganized properly. Vectorizing such computations efficiently is therefore an ambitious challenge for both programmers and vectorizing compilers. We demonstrate an automatic compilation scheme that supports effective vectorization in the presence of interleaved data with constant strides that are powers of 2, facilitating data reorganization. We demonstrate how our vectorization scheme applies to dominant SIMD architectures, and present experimental results on a wide range of key kernels, showing speedups in execution time up to 3.7 for interleaving levels (stride) as high as 8.
p2223
aVStatic analysis of programs in weakly typed languages such as C and C++ is generally not sound because of possible memory errors due to dangling pointer references, uninitialized pointers, and array bounds overflow. We describe a compilation strategy for standard C programs that guarantees that aggressive interprocedural pointer analysis (or less precise ones), a call graph, and type information for a subset of memory, are never invalidated by any possible memory errors. We formalize our approach as a new type system with the necessary runtime checks in operational semantics and prove the correctness of our approach for a subset of C. Our semantics provide the foundation for other sophisticated static analyses to be applied to C programs with a guarantee of soundness. Our work builds on a previously published transformation called Automatic Pool Allocation to ensure that hardtodetect memory errors (dangling pointer references and certain array bounds errors) cannot invalidate the call graph, pointsto information or type information. The key insight behind our approach is that pool allocation can be used to create a runtime partitioning of memory that matches the compiletime memory partitioning in a pointsto graph, and efficient checks can be used to isolate the runtime partitions. Furthermore, we show that the sound analysis information enables static checking techniques that eliminate many runtime checks. Our approach requires no source code changes, allows memory to be managedexplicitly, and does not use metadata on pointers or individual tag bits for memory. Using several benchmark s and system codes, we show experimentally that the runtime overheads are low (less than 10% in nearly all cases and 30% in the worst case we have seen).We also show the effectiveness of static analyses in eliminating runtime checks.
p2224
aVApplications written in unsafe languages like C and C++ are vulnerable to memory errors such as buffer overflows, dangling pointers, and reads of uninitialized data. Such errors can lead to program crashes, security vulnerabilities, and unpredictable behavior. We present DieHard, a runtime system that tolerates these errors while probabilistically maintaining soundness. DieHard uses randomization and replication to achieve probabilistic memory safety by approximating an infinitesized heap. DieHard's memory manager randomizes the location of objects in a heap that is at least twice as large as required. This algorithm prevents heap corruption and provides a probabilistic guarantee of avoiding memory errors. For additional safety, DieHard can operate in a replicated mode where multiple replicas of the same application are run simultaneously. By initializing each replica with a different random seed and requiring agreement on output, the replicated version of DieHard increases the likelihood of correct execution because errors are unlikely to have the same effect across all replicas. We present analytical and experimental results that show DieHard's resilience to a wide range of memory errors, including a heapbased buffer overflow in an actual application.
p2225
aVGiven an incorrect value produced during a failed program run (e.g., a wrong output value or a value that causes the program to crash), the backward dynamic slice of the value very frequently captures the faulty code responsible for producing the incorrect value. Although the dynamic slice often contains only a small percentage of the statements executed during the failed program run, the dynamic slice can still be large and thus considerable effort may be required by the programmer to locate the faulty code.In this paper we develop a strategy for pruning the dynamic slice to identify a subset of statements in the dynamic slice that are likely responsible for producing the incorrect value. We observe that some of the statements used in computing the incorrect value may also have been involved in computing correct values (e.g., a value produced by a statement in the dynamic slice of the incorrect value may also have been used in computing a correct output value prior to the incorrect value). For each such executed statement in the dynamic slice, using the value profiles of the executed statements, we compute a confidence value ranging from 0 to 1  a higher confidence value corresponds to greater likelihood that the execution of the statement produced a correct value. Given a failed run involving execution of a single error, we demonstrate that the pruning of a dynamic slice by excluding only the statements with the confidence value of 1 is highly effective in reducing the size of the dynamic slice while retaining the faulty code in the slice. Our experiments show that the number of distinct statements in a pruned dynamic slice are 1.79 to 190.57 times less than the full dynamic slice. Confidence values also prioritize the statements in the dynamic slice according to the likelihood of them being faulty. We show that examining the statements in the order of increasing confidence values is an effective strategy for reducing the effort of fault location.
p2226
aVProgressing beyond the productivity of presentday languages appears to require using domainspecific knowledge. Domainspecific languages and libraries (DSLs) proliferate, but most optimizations and language features have limited portability because each language's semantics are related closely to its domain. We explain how any DSL compiler can use a domainindependent AI planner to implement algorithm composition as a language feature. Our notion of composition addresses a common DSL problem: good library designers tend to minimize redundancy by including only fundamental procedures that users must chain together into call sequences. Novice users are confounded by not knowing an appropriate sequence to achieve their goal. Composition allows the programmer to define and call an abstract algorithm (AA) like a procedure. The compiler replaces an AA call with a sequence of library calls, while considering the calling context. Because AI planners compute a sequence of operations to reach a goal state, the compiler can implement composition by analyzing the calling context to provide the planner's initial state. Nevertheless, mapping composition onto planning is not straightforward because applying planning to software requires extensions to classical planning, and procedure specifications may be incomplete when expressed in a planning language. Compositions may not be provably correct, so our approach mitigates semantic incompleteness with unobtrusive programmercompiler interaction. This tradeoff is key to making composition a practical and natural feature of otherwise imperative languages, whose users eschew complex logical specifications. Compositions satisfying an AA may not be equal in performance, memory usage, or precision and require selection of a preferred solution. We examine language design and implementation issues, and we perform a case study on the BioPerl bioinformatics library.
p2227
aVWhile scalable NoC (NetworkonChip) based communication architectures have clear advantages over long pointtopoint communication channels, their power consumption can be very high. In contrast to most of the existing hardwarebased efforts on NoC power optimization, this paper proposes a compilerdirected approach where the compiler decides the appropriate voltage/frequency levels to be used for each communication channel in the NoC. Our approach builds and operates on a novel graph based representation of a parallel program and has been implemented within an optimizing compiler and tested using 12 embedded benchmarks. Our experiments indicate that the proposed approach behaves better  from both performance and power perspectives  than a hardwarebased scheme and the energy savings it achieves are very close to the savings that could be obtained from an optimal, but hypothetical voltage/frequency scaling scheme.
p2228
aVThis paper describes a global progressive register allocator, a register allocator that uses an expressive model of the register allocation problem to quickly find a good allocation and then progressively find better allocations until a provably optimal solution is found or a preset time limit is reached. The key contributions of this paper are an expressive model of global register allocation based on multicommodity network flows that explicitly represents spill code optimization, register preferences, copy insertion, and constant rematerialization; two fast, but effective, heuristic allocators based on this model; and a more elaborate progressive allocator that uses Lagrangian relaxation to compute the optimality of its allocations. Our progressive allocator demonstrates code size improvements as large as 16.75% compared to a traditional graph allocator. On average, we observe an initial improvement of 3.47%, which increases progressively to 6.84% as more time is permitted for compilation.
p2229
aVLiverange splitting is a technique to split the live range of a given variable into multiple subranges, each of which can be assigned to a different register or spilled out to memory in order to improve results of coloring register allocation. Previous techniques, such as aggressive liverange splitting, tend to produce extra spill code in the frequently executed (called hot) regions of the code, since they don't distinguish hot regions from others. We propose a new liverange splitting algorithm, which can reduce the amount of spill code in hot regions by coalescing the live ranges based on profile information after splitting the live ranges at every join and fork point in the controlflow graph. Our experimental results have shown that our new algorithm improved the performance of SPECjvm98 by up to 33% over aggressive liverange splitting and 7% over the base coloring algorithm without any liverange splitting.
p2230
aVAtomic blocks allow programmers to delimit sections of code as 'atomic', leaving the language's implementation to enforce atomicity. Existing work has shown how to implement atomic blocks over wordbased transactional memory that provides scalable multiprocessor performance without requiring changes to the basic structure of objects in the heap. However, these implementations perform poorly because they interpose on all accesses to shared memory in the atomic block, redirecting updates to a threadprivate log which must be searched by reads in the block and later reconciled with the heap when leaving the block.This paper takes a fourpronged approach to improving performance: (1) we introduce a new 'direct access' implementation that avoids searching threadprivate logs, (2) we develop compiler optimizations to reduce the amount of logging (e.g. when a thread accesses the same data repeatedly in an atomic block), (3) we use runtime filtering to detect duplicate log entries that are missed statically, and (4) we present a series of GCtime techniques to compact the logs generated by longrunning atomic blocks.Our implementation supports shortrunning scalable concurrent benchmarks with less than 50\u005c% overhead over a nonthreadsafe baseline. We support long atomic blocks containing millions of shared memory accesses with a 2.54.5x slowdown.
p2231
aVIn order to generate highquality code for modern processors, a compiler must aggressively schedule instructions, maximizing resource utilization for execution efficiency. For a compiler to produce such code, it must avoid structural hazards by being aware of the processor's available resources and of how these resources are utilized by each instruction. Unfortunately, the most prevalent approach to constructing such a scheduler, manually discovering and specifying this information, is both tedious and errorprone. This paper presents a new approach which, when given a processor or processor model, automatically determines this information. After establishing that the problem of perfectly determining a processor's structural hazards through probing is not solvable, this paper proposes a heuristic algorithm that discovers most of this information in practice. This can be used either to alleviate the problems associated with manual creation or to verify an existing specification. Scheduling with these automatically derived structural hazards yields almost all of the performance gain achieved using perfect hazard information.
p2232
aVAs hardware complexity increases and virtualization is added at more layers of the execution stack, predicting the performance impact of optimizations becomes increasingly difficult. Production compilers and virtual machines invest substantial development effort in performance tuning to achieve good performance for a range of benchmarks. Although optimizations typically perform well on average, they often have unpredictable impact on running time, sometimes degrading performance significantly. Today's VMs perform sophisticated feedbackdirected optimizations, but these techniques do not address performance degradations, and they actually make the situation worse by making the system more unpredictable.This paper presents an online framework for evaluating the effectiveness of optimizations, enabling an online system to automatically identify and correct performance anomalies that occur at runtime. This work opens the door for a fundamental shift in the way optimizations are developed and tuned for online systems, and may allow the body of work in offline empirical optimization search to be applied automatically at runtime. We present our implementation and evaluation of this system in a product Java VM.
p2233
aVThe memory system performance of many programs can be improved by coallocating contemporaneously accessed heap objects in the same cache block. We present a novel profilebased analysis for producing such a layout. The analysis achieves cacheconscious coallocation of a hot data stream H (i.e., a regular data access pattern that frequently repeats) by isolating and combining allocation sites of object instances that appear in H such that intervening allocations coming from other sites are separated. The coallocation solution produced by the analysis is enforced by an automatic tool, cminstr, that redirects a program's heap allocations to a runtime coallocation library comalloc. We also extend the analysis to coallocation at object field granularity. The resulting field coallocation solution generalizes common data restructuring techniques, such as field reordering, object splitting, and object merging, and allows their combination. Furthermore, it provides insight into object restructuring by breaking down the coallocation benefit on a pertechnique basis, which provides the opportunity to pick the "sweet spot" for each program. Experimental results using a set of memoryperformancelimited benchmarks, including a few SPECInt2000 programs, and Microsoft VisualFoxPro, indicate that programs possess significant coallocation opportunities. Automatic object coallocation improves execution time by 13% on average in the presence of hardware prefetching. Handimplemented field coallocation solutions for two of the benchmarks produced additional improvements (12% and 22%) but the effort involved suggests implementing an automated version for typesafe languages, such as Java and C#.
p2234
aVCalling context profiles are used in many interprocedural code optimizations and in overall program understanding. Unfortunately, the collection of profile information is highly intrusive due to the high frequency of method calls in most applications. Previously proposed callingcontext profiling mechanisms consequently suffer from either low accuracy, high overhead, or both. We have developed a new approach for building the calling context tree at runtime, called adaptive bursting. By selectively inhibiting redundant profiling, this approach dramatically reduces overhead while preserving profile accuracy. We first demonstrate the drawbacks of previously proposed calling context profiling mechanisms. We show that a lowoverhead solution using sampled stackwalking alone is less than 50% accurate, based on degree of overlap with a complete callingcontext tree. We also show that a static bursting approach collects a highly accurate profile, but causes an unacceptable application slowdown. Our adaptive solution achieves 85% degree of overlap and provides an 88% hotedge coverage when using a 0.1 hotedge threshold, while dramatically reducing overhead compared to the static bursting approach.
p2235
aVGeneric programming has recently emerged as a paradigm for developing highly reusable software libraries, most notably in C++. We have designed and implemented a constrained generics extension for C++ to support modular type checking of generic algorithms and to address other issues associated with unconstrained generics. To be as broadly applicable as possible, generic algorithms are defined with minimal requirements on their inputs. At the same time, to achieve a high degree of efficiency, generic algorithms may have multiple implementations that exploit features of specific classes of inputs. This process of algorithm specialization relies on nonlocal type information and conflicts directly with the local nature of modular type checking. In this paper, we review the design and implementation of our extensions for generic programming in C++, describe the issues of algorithm specialization and modular type checking in detail, and discuss the important design tradeoffs in trying to accomplish both.We present the particular design that we chose for our implementation, with the goal of hitting the sweet spot in this interesting design space.
p2236
aVWhile realtime garbage collection has achieved worstcase latencies on the order of a millisecond, this technology is approaching its practical limits. For tasks requiring extremely low latency, and especially periodic tasks with frequencies above 1 KHz, Java programmers must currently resort to the NoHeapRealtimeThread construct of the RealTime Specification for Java. This technique requires expensive runtime checks, can result in unpredictable lowlevel exceptions, and inhibits communication with the rest of the garbagecollected application. We present Eventrons, a programming construct that can arbitrarily preempt the garbage collector, yet guarantees safety and allows its data to be visible to the garbagecollected heap. Eventrons are a strict subset of Java, and require no runtime memory access checks. Safety is enforced using a datasensitive analysis and simple runtime support with extremely low overhead. We have implemented Eventrons in IBM's J9 Java virtual machine, and present experimental results in which we ran Eventrons at frequencies up to 22 KHz (a 45 \u03bcs period). Across 10 million periods, 99.997% of the executions ran within 10 \u03bcss of their deadline, compared to 99.999% of the executions of the equivalent program written in C.
p2237
aVOnline transducers are an important class of computational agent; we construct and compose together many software systems using them, such as stream processors, layered network protocols, DSP networks and graphics pipelines. We show an interesting use of continuations, that, when taken in a CPS setting, exposes the control flow of these systems. This enables a CPSbased compiler to optimise systems composed of these transducers, using only standard, known analyses and optimisations. Critically, the analysis permits optimisation across the composition of these transducers, allowing efficient construction of systems in a hierarchical way.
p2238
aVWe present a novel technique for static race detection in Java programs, comprised of a series of stages that employ a combination of static analyses to successively reduce the pairs of memory accesses potentially involved in a race. We have implemented our technique and applied it to a suite of multithreaded Java programs. Our experiments show that it is precise, scalable, and useful, reporting tens to hundreds of serious and previously unknown concurrency bugs in large, widelyused programs with few false alarms.
p2239
aVOne common technique for preventing data races in multithreaded programs is to ensure that all accesses to shared locations are consistently protected by a lock. We present a tool called LOCKSMITH for detecting data races in C programs by looking for violations of this pattern. We call the relationship between locks and the locations they protect consistent correlation, and the core of our technique is a novel constraintbased analysis that infers consistent correlation contextsensitively, using the results to check that locations are properly guarded by locks. We present the core of our algorithm for a simple formal language \u03bb> which we have proven sound, and discuss how we scale it up to an algorithm that aims to be sound for all of C. We develop several techniques to improve the precision and performance of the analysis, including a sharing analysis for inferring thread locality; existential quantification for modeling locks in data structures; and heuristics for modeling unsafe features of C such as type casts. When applied to several benchmarks, including multithreaded servers and Linux device drivers, LOCKSMITH found several races while producing a modest number of false alarm.
p2240
aVMany applications written in garbage collected languages have large dynamic working sets and poor data locality. We present a new system for continuously improving program data locality at run time with low overhead. Our system proactively reorganizes the heap by leveraging the garbage collector and uses profile information collected through a lowoverhead mechanism to guide the reorganization at run time. The key contributions include making a case that garbage collection should be viewed as a proactive technique for improving data locality by triggering garbage collection for locality optimization independently of normal garbage collection for space, combining page and cache locality optimization in the same system, and demonstrating that sampling provides sufficiently detailed data access information to guide both page and cache locality optimization with low runtime overhead. We present experimental results obtained by modifying a commercial, stateoftheart garbage collector to support our claims. Independently triggering garbage collection for locality optimization significantly improved optimizations benefits. Combining page and cache locality optimizations in the same system provided larger average execution time improvements (17%) than either alone (page 8%, cache 7%). Finally, using sampling limited profiling overhead to less than 3%, on average.
p2241
aVProgrammers have traditionally used locks to synchronize concurrent access to shared data. Lockbased synchronization, however, has wellknown pitfalls: using locks for finegrain synchronization and composing code that already uses locks are both difficult and prone to deadlock. Transactional memory provides an alternate concurrency control mechanism that avoids these pitfalls and significantly eases concurrent programming. Transactional memory language constructs have recently been proposed as extensions to existing languages or included in new concurrent language specifications, opening the door for new compiler optimizations that target the overheads of transactional memory.This paper presents compiler and runtime optimizations for transactional memory language constructs. We present a highperformance software transactional memory system (STM) integrated into a managed runtime environment. Our system efficiently implements nested transactions that support both composition of transactions and partial roll back. Our JIT compiler is the first to optimize the overheads of STM, and we show novel techniques for enabling JIT optimizations on STM operations. We measure the performance of our optimizations on a 16way SMP running multithreaded transactional workloads. Our results show that these techniques enable transactional memory's performance to compete with that of welltuned synchronization.
p2242
aVConstructing correct concurrent garbage collection algorithms is notoriously hard. Numerous such algorithms have been proposed, implemented, and deployed  and yet the relationship among them in terms of speed and precision is poorly understood, and the validation of one algorithm does not carry over to others.As programs with low latency requirements written in garbagecollected languages become part of society's missioncritical infrastructure, it is imperative that we raise the level of confidence in the correctness of the underlying system, and that we understand the tradeoffs inherent in our algorithmic choice.In this paper we present correctnesspreserving transformations that can be applied to an initial abstract concurrent garbage collection algorithm which is simpler, more precise, and easier to prove correct than algorithms used in practice but also more expensive and with less concurrency. We then show how both preexisting and new algorithms can be synthesized from the abstract algorithm by a series of our transformations. We relate the algorithms formally using a new definition of precision, and informally with respect to overhead and concurrency.This provides many insights about the nature of concurrent collection, allows the direct synthesis of new and useful algorithms, reduces the burden of proof to a single simple algorithm, and lays the groundwork for the automated synthesis of correct concurrent collectors.
p2243
aVThe widely used MarkandSweep garbage collector has a drawback in that it does not move objects during collection. As a result, large longrunning realistic applications, such as Web application servers, frequently face the fragmentation problem. To eliminate fragmentation, a heap compaction is run periodically. However, compaction typically imposes very long undesirable pauses in the application. While efficient concurrent collectors are ubiquitous in production runtime systems (such as JVMs), an efficient nonintrusive compactor is still missing.In this paper we present the Compressor, a novel compaction algorithm that is concurrent, parallel, and incremental. The Compressor compacts the entire heap to a single condensed area, while preserving the objects' order, but reduces pause times significantly, thereby allowing acceptable runs on large heaps. Furthermore, the Compressor is the first compactor that requires only a single heap pass. As such, it is the most efficient compactors known today, even when run in a parallel StoptheWorld manner (i.e., when the program threads are halted). Thus, to the best of our knowledge, the Compressor is the most efficient compactor known today. The Compressor was implemented on a Jikes Research RVM and we provide measurements demonstrating its qualities.
p2244
aVGarbage collection has proven benefits, including fewer memory related errors and reduced programmer effort. Garbage collection, however, trades space for time. It reclaims memory only when it is invoked: invoking it more frequently reclaims memory quickly, but incurs a significant cost; invoking it less frequently fills memory with dead objects. In contrast, explicit memory management provides prompt low cost reclamation, but at the expense of programmer effort.This work comes closer to the best of both worlds by adding novel compiler and runtime support for compiler inserted frees to a garbagecollected system. The compiler's freeme analysis identifies when objects become unreachable and inserts calls to free. It combines a lightweight pointer analysis with liveness information that detects when shortlived objects die. Our approach differs from stack and region allocation in two crucial ways. First, it frees objects incrementally exactly when they become unreachable, instead of based on program scope. Second, our system does not require allocationsite lifetime homogeneity, and thus frees objects on some paths and not on others. It also handles common patterns: it can free objects in loops and objects created by factory methods.We evaluate free() variations for freelist and bumppointer allocators. Explicit freeing improves performance by promptly reclaiming objects and reducing collection load. Compared to marksweep alone, freeme cuts total time by 22% on average, collector time by 50% to 70%, and allows programs to run in 17% less memory. This combination retains the software engineering benefits of garbage collection while increasing space efficiency and improving performance, and thus is especially appealing for realtime and space constrained systems.
p2245
aVWe present a methodology for automatically combining abstract interpreters over given lattices to construct an abstract interpreter for the combination of those lattices. This lends modularity to the process of design and implementation of abstract interpreters.We define the notion of logical product of lattices. This kind of combination is more precise than the reduced product combination. We give algorithms to obtain the join operator and the existential quantification operator for the combined lattice from the corresponding operators of the individual lattices. We also give a bound on the number of steps required to reach a fixed point across loops during analysis over the combined lattice in terms of the corresponding bounds for the individual lattices. We prove that our combination methodology yields the most precise abstract interpretation operators over the logical product of lattices when the individual lattices are over theories that are convex, stably infinite, and disjoint.We also present an interesting application of logical product wherein some lattices can be reduced to combination of other (unrelated) lattices with known abstract interpreters.
p2246
aVWe present a scalable and precise contextsensitive pointsto analysis with three key properties: (1) filtering out of unrealizable paths, (2) a contextsensitive heap abstraction, and (3) a contextsensitive call graph. Previous work [21] has shown that all three properties are important for precisely analyzing large programs, e.g., to show safety of downcasts. Existing analyses typically give up one or more of the properties for scalability. We have developed a refinementbased analysis that succeeds by simultaneously refining handling of method calls and heap accesses, allowing the analysis to precisely analyze important code while entirely skipping irrelevant code. The analysis is demanddriven and clientdriven, facilitating refinement specific to each queried variable and increasing scalability. In our experimental evaluation, our analysis proved the safety of 61% more casts than one of the most precise existing analyses across a suite of large benchmarks. The analysis checked the casts in under 13 minutes per benchmark (taking less than 1 second per query) and required only 35MB of memory, far less than previous approaches.
p2247
aVRuntime stacks are critical components of any modern software they are used to implement powerful control structures such as function call/return, stack cutting and unwinding, coroutines, and thread context switch. Stack operations, however, are very hard to reason about: there are no known formal specifications for certifying Cstyle setjmp/longjmp, stack cutting and unwinding, or weak continuations (in C ). In many proofcarrying code (PCC) systems, return code pointers and exception handlers are treated as general firstclass functions (as in continuationpassing style) even though both should have more limited scopes.In this paper we show that stackbased control abstractions follow a much simpler pattern than general firstclass code pointers. We present a simple but flexible Hoarestyle framework for modular verification of assembly code with all kinds of stackbased control abstractions, including function call/return, tail call, setjmp/longjmp, weak continuation, stack cutting, stack unwinding, multireturn function call, coroutines, and thread context switch. Instead of presenting a specific logic for each control structure, we develop all reasoning systems as instances of a generic framework. This allows program modules and their proofs developed in different PCC systems to be linked together. Our system is fully mechanized. We give the complete soundness proof and a full verification of several examples in the Coq proof assistant.
p2248
aVWe explore how to make the benefits of modularity available for syntactic specifications and present Rats!, a parser generator for Java that supports easily extensible syntax. Our parser generator builds on recent research on parsing expression grammars (PEGs), which, by being closed under composition, prioritizing choices, supporting unlimited lookahead, and integrating lexing and parsing, offer an attractive alternative to contextfree grammars. PEGs are implemented by socalled packrat parsers, which are recursive descent parsers that memoize all intermediate results (hence their name). Memoization ensures lineartime performance in the presence of unlimited lookahead, but also results in an essentially lazy, functional parsing technique. In this paper, we explore how to leverage PEGs and packrat parsers as the foundation for extensible syntax. In particular, we show how make packrat parsing more widely applicable by implementing this lazy, functional technique in a strict, imperative language, while also generating better performing parsers through aggressive optimizations. Next, we develop a module system for organizing, modifying, and composing largescale syntactic specifications. Finally, we describe a new technique for managing (global) parsing state in functional parsers. Our experimental evaluation demonstrates that the resulting parser generator succeeds at providing extensible syntax. In particular, Rats! enables other grammar writers to realize realworld language extensions in little time and code, and it generates parsers that consistently outperform parsers created by two GLR parser generators.
p2249
aVTree parsing as supported by code generator generators like BEG, burg, iburg, lburg and mlburg is a popular instruction selection method. There are two existing approaches for implementing tree parsing: dynamic programming, and treeparsing automata; each approach has its advantages and disadvantages. We propose a new implementation approach that combines the advantages of both existing approaches: we start out with dynamic programming at compile time, but at every step we generate a state for a treeparsing automaton, which is used the next time a tree matching the state is found, turning the instruction selector into a fast treeparsing automaton. We have implemented this approach in the Gforth code generator. The implementation required little effort and reduced the startup time of Gforth by up to a factor of 2.5.
p2250
aVProcedures have long been the basic units of compilation in conventional optimization frameworks. However, procedures are typically formed to serve software engineering rather than optimization goals, arbitrarily constraining code transformations. Techniques, such as aggressive inlining and interprocedural optimization, have been developed to alleviate this problem, but, due to code growth and compile time issues, these can be applied only sparingly.This paper introduces the Procedure Boundary Elimination (PBE) compilation framework, which allows unrestricted wholeprogram optimization. PBE allows all intraprocedural optimizations and analyses to operate on arbitrary subgraphs of the program, regardless of the original procedure boundaries and without resorting to inlining. In order to control compilation time, PBE also introduces novel extensions of region formation and encapsulation. PBE enables targeted code specialization, which recovers the specialization benefits of inlining while keeping code growth in check. This paper shows that PBE attains better performance than inlining with half the code growth.
p2251
aVSoftware updates typically require stopping and restarting an application, but many systems cannot afford to halt service, or would prefer not to. Dynamic software updating (DSU) addresses this difficulty by permitting programs to be updated while they run. DSU is appealing compared to other approaches for online upgrades because it is quite general and requires no redundant hardware. The challenge is in making DSU practical: it should be flexible, and yet safe, efficient, and easy to use.In this paper, we present Ginseng, a DSU implementation for C that aims to meet this challenge. We compile programs specially so that they can be dynamically patched, and generate most of a dynamic patch automatically. Ginseng performs a series of analyses that when combined with some simple runtime support ensure that an update will not violate typesafety while guaranteeing that data is kept uptodate. We have used Ginseng to construct and dynamically apply patches to three substantial opensource server programs Very Secure FTP daemon, OpenSSH sshd daemon, and GNU Zebra. In total, we dynamically patched each program with three years' worth of releases. Though the programs changed substantially, the majority of updates were easy to generate. Performance experiments show that all patches could be applied in less than 5 ms, and that the overhead on application throughput due to updating support ranged from 0 to at most 32%.
p2252
aVA number of hardware and software techniques have been proposed to detect dynamic program behaviors that may indicate a bug in a program. Because these techniques suffer from high overheads they are useful in finding bugs in programs before they are released, but are significantly less useful in finding bugs in longrunning programs on production systems   the same bugs that are the most difficult to find using traditional techniques. In this paper we propose the Artemis1 is the Greek goddess of the hunt and wild animals. Our framework guides the hunt for wild bugs. compilerbased instrumentation framework that complements many preexisting runtime monitoring techniques. The Artemis framework guides baseline monitoring techniques toward regions of the program where bugs are likely to occur, yielding a low asymptotic monitoring overhead. Artemis also facilitates systemload aware runtime monitoring that allows the monitoring coverage to be dynamically scaled up to take advantage of extra cycles when the system load is low, and dynamically scaled down to monitor only the most suspicious regions when the system load is high. Our experiments show that Artemis' asymptotic overhead can outperform the performance floor overhead of random sampling for many tools, and that Artemis can effectively guide a monitoring tool to the buggy regions of a program. Our experimental results show that Artemis applied to a hardwarebased PCinvariance monitoring scheme and a valuebased invariance detection and checking scheme significantly improves their runtime monitoring overhead (by up to 4.6 times) with moderate impact on their bugdetecting capabilities.
p2253
aVDependence graphs and memoization can be used to efficiently update the output of a program as the input changes dynamically. Recent work has studied techniques for combining these approaches to effectively dynamize a wide range of applications. Toward this end various theoretical results were given. In this paper we describe the implementation of a library based on these ideas, and present experimental results on the efficiency of this library on a variety of applications. The results of the experiments indicate that the approach is effective in practice, often requiring orders of magnitude less time than recomputing the output from scratch. We believe this is the first experimental evidence that incremental computation of any type is effective in practice for a reasonably broad set of applications.
p2254
aVPrograms written in C and C++ are susceptible to memory errors, including buffer overflows and dangling pointers. These errors, whichcan lead to crashes, erroneous execution, and security vulnerabilities, are notoriously costly to repair. Tracking down their location in the source code is difficult, even when the full memory state of the program is available. Once the errors are finally found, fixing them remains challenging: even for critical securitysensitive bugs, the average time between initial reports and the issuance of a patch is nearly one month. We present Exterminator, a system that automatically correct sheapbased memory errors without programmer intervention. Exterminator exploits randomization to pinpoint errors with high precision. From this information, Exterminator derives runtime patches that fix these errors both in current and subsequent executions. In addition, Exterminator enables collaborative bug correction by merging patches generated by multiple users. We present analytical and empirical results that demonstrate Exterminator's effectiveness at detecting and correcting both injected and real faults.
p2255
aVAn error occurs when software cannot complete a requested action as a result of some problem with its input, configuration, or environment. A highquality error report allows a user to understand and correct the problem. Unfortunately, the quality of error reports has been decreasing as software becomes more complex and layered. Endusers take the cryptic error messages given to them by programsand struggle to fix their problems using search engines and support websites. Developers cannot improve their error messages when they receive an ambiguous or otherwise insufficient error indicator from a blackbox software component. We introduce Clarify, a system that improves error reporting by classifying application behavior. Clarify uses minimally invasive monitoring to generate a behavior profile, which is a summary of the program's execution history. A machine learning classifier uses the behavior profile to classify the application's behavior, thereby enabling a more precise error report than the output of the application itself. We evaluate a prototype Clarify system on ambiguous error messages generated by large, modern applications like gcc, LaTeX, and the Linux kernel. For a performance cost of less than 1% on user applications and 4.7% on the Linux kernel, the proto type correctly disambiguates at least 85% of application behaviors that result in ambiguous error reports. This accuracy does not degrade significantly with more behaviors: a Clarify classifier for 81 LaTeX error messages is at most 2.5% less accurate than a classifier for 27 LaTeX error messages. Finally, we show that without any human effort to build a classifier, Clarify can provide nearestneighbor software support, where users who experience a problem are told about 5 other users who might have had the same problem. On average 2.3 of the 5 users that Clarify identifies have experienced the same problem.
p2256
aVProgram slicing systematically identifies parts of a program relevant to a seed statement. Unfortunately, slices of modern programs often grow too large for human consumption. We argue that unwieldy slices arise primarily from an overly broad definition of relevance, rather than from analysis imprecision. While a traditional slice includes all statements that may affect a point of interest, not all such statements appear equally relevant to a human. As an improved method of finding relevant statements, we propose thin slicing. A thin slice consists only of producer statements for the seed, i.e., those statements that help compute and copy avalue to the seed. Statements that explain why producers affect the seed are excluded. For example, for a seed that reads a value from a container object, a thin slice includes statements that store the value into the container, but excludes statements that manipulate pointers to the container itself. Thin slices can also be hierarchically expanded to include statements explaining how producers affect the seed, yielding a traditional slice in the limit. We evaluated thin slicing for a set of debugging and program understanding tasks. The evaluation showed that thin slices usually included the desired statements for the tasks (e.g., the buggy statement for a debugging task). Furthermore, in simulated use of a slicing tool, thin slices revealed desired statements after inspecting 3.3 times fewer statements than traditional slicing for our debugging tasks and 9.4 times fewer statements for our program understanding tasks. Finally, our thin slicing algorithm scales well to relatively large Java benchmarks, suggesting that thin slicing represents an attractive option for practical tools.
p2257
aVThe reliability and correctness of complex software systems can be significantly enhanced through welldefined specifications that dictate the use of various units of abstraction (e.g., modules, or procedures). Often times, however, specifications are either missing, imprecise, or simply too complex to encode within a signature, necessitating specification inference. The process of inferring specifications from complex software systems forms the focus of this paper. We describe a static inference mechanism for identifying the preconditions that must hold whenever a procedure is called. These preconditions may reflect both data flow properties (e.g., whenever p is called, variable x must be nonnull) as well as controlflow properties (e.g., every call to p must bepreceded by a call to q). We derive these preconditions using a ninterprocedural pathsensitive dataflow analysis that gathers predicates at each program point. We apply mining techniques to these predicates to make specification inference robust to errors. This technique also allows us to derive higherlevel specifications that abstract structural similarities among predicates (e.g., procedure p is called immediately after a conditional test that checks whether some variable v is nonnull.) We describe an implementation of these techniques, and validate the effectiveness of the approach on a number of large opensource benchmarks. Experimental results confirm that our mining algorithms are efficient, and that the specifications derived are both precise and usefulthe implementation discovers several critical, yet previously, undocumented preconditions for welltested libraries.
p2258
aVPrevious work presented a language called Rhodium for writing program analyses and transformations, in the form of declarative flow functions that propagate instances of userdefined dataflow fact schemas. Each dataflow fact schema specifies a semantic meaning, which allows the Rhodium system to automatically verify the correctness of the user's flow functions. In this work, we have reversed the roles of the flow functions and semantic meanings: rather than checking the correctness of the userwritten flow functions using the facts' semantic meanings, we automatically infer correct flow functions solely from the meanings of the dataflow fact schemas. We have implemented our algorithm for inferring flow functions from fact schemas in the context of the Whirlwind compiler, and have used this implementation to infer flow functions for a variety of fact schemas. The automatically generated flow functions cover most of the situations covered by an earlier suite of handwritten rules.
p2259
aVDivideandconquer algorithms are suitable for modern parallel machines, tending to have large amounts of inherent parallelism and working well with caches and deep memory hierarchies. Among others, list homomorphisms are a class of recursive functions on lists, which match very well with the divideandconquer paradigm. However, direct programming with list homomorphisms is a challenge for many programmers. In this paper, we propose and implement a novel systemthat can automatically derive costoptimal list homomorphisms from a pair of sequential programs, based on the third homomorphism theorem. Our idea is to reduce extraction of list homomorphisms to derivation of weak right inverses. We show that a weak right inverse always exists and can be automatically generated from a wide class of sequential programs. We demonstrate our system with several nontrivial examples, including the maximum prefix sum problem, the prefix sum computation, the maximum segment sum problem, and the lineofsight problem. The experimental results show practical efficiency of our automatic parallelization algorithm and good speedups of the generated parallel programs.
p2260
aVFuture mainstream microprocessors will likely integrate specialized accelerators, such as GPUs, onto a single die to achieve better performance and power efficiency. However, it remains a keen challenge to program such a heterogeneous multicore platform, since these specialized accelerators feature ISAs and functionality that are significantly different from the general purpose CPU cores. In this paper, we present EXOCHI: (1) Exoskeleton Sequencer(EXO), an architecture to represent heterogeneous acceleratorsas ISAbased MIMD architecture resources, and a shared virtual memory heterogeneous multithreaded program execution model that tightly couples specialized accelerator cores with generalpurpose CPU cores, and (2) C for Heterogeneous Integration(CHI), an integrated C/C++ programming environment that supports acceleratorspecific inline assembly and domainspecific languages. The CHI compiler extends the OpenMP pragma for heterogeneous multithreading programming, and produces a single fat binary with code sections corresponding to different instruction sets. The runtime can judiciously spread parallel computation across the heterogeneous cores to optimize performance and power. We have prototyped the EXO architecture on a physical heterogeneous platform consisting of an Intel Core\u2122 2 Duo processor and an 8core 32thread Intel Graphics Media Accelerator X3000. In addition, we have implemented the CHI integrated programming environment with the Intel C++ Compiler, runtime toolset, and debugger. On the EXO prototype system, we have enhanced a suite of productionquality media kernels for video and image processing to utilize the accelerator through the CHI programming interface, achieving significant speedup (1.41X to10.97X) over execution on the IA32 CPU alone.
p2261
aVPerformance of stencil computations can be significantly improved through smart implementations that improve memory locality, computation reuse, or parallelize the computation. Unfortunately, efficient implementations are hard to obtain because they often involve nontraditional transformations, which means that they cannot be produced by optimizing the reference stencil with a compiler. In fact, many stencils are produced by code generators that were tediously handcrafted. In this paper, we show how stencil implementations can be produced with sketching. Sketching is a software synthesis approach where the programmer develops a partial implementation a sketch and a separate specification of the desired functionality given by a reference (unoptimized) stencil. The synthesizer then completes the sketch to behave like the specification, filling in code fragments that are difficult to develop manually. Existing sketching systems work only for small finite programs, i.e.,, programs that can be represented as small Boolean circuits. In this paper, we develop a sketching synthesizer that works for stencil computations, a large class of programs that, unlike circuits, have unbounded inputs and outputs, as well as an unbounded number of computations. The key contribution is a reduction algorithm that turns a stencil into a circuit, allowing us to synthesize stencils using an existing sketching synthesizer.
p2262
aVBuilding distributed systems is particularly difficult because of the asynchronous, heterogeneous, and failureprone environment where these systemsmust run. Tools for building distributed systems must strike a compromise between reducing programmer effort and increasing system efficiency. We present Mace, a C++ language extension and sourcetosource compiler that translates a concise but expressive distributed system specification into a C++ implementation. Mace overcomes the limitations of lowlevel languages by providing a unified framework for networking and event handling, and the limitations of highlevel languages by allowing programmers to write program components in a controlled and structured manner in C++. By imposing structure and restrictions on how applications can be written, Mace supports debugging at a higher level, including support for efficient model checking and causalpath debugging. Because Mace programs compile to C++, programmers can use existing C++ tools, including optimizers, profilers, and debuggers to analyze their systems.
p2263
aVThis paper proposes to combine two seemingly opposed programming models for building massively concurrent network services: the eventdriven model and the multithreaded model. The result is a hybrid design that offers the best of both worlds the ease of use and expressiveness of threads and the flexibility and performance of events. This paper shows how the hybrid model can be implemented entirely at the application level using concurrency monads in Haskell, which provides typesafe abstractions for both events and threads. This approach simplifies the development of massively concurrent software in a way that scales to realworld network services. The Haskell implementation supports exceptions, symmetrical multiprocessing, software transactional memory, asynchronous I/O mechanisms and applicationlevel network protocol stacks. Experimental results demonstrate that this monadbased approach has good performance: the threads are extremely lightweight (scaling to ten million threads), and the I/O performance compares favorably to that of Linux NPTL. tens of thousands of simultaneous, mostlyidle client connections. Such massivelyconcurrent programs are difficult to implement, especially when other requirements, such as high performance and strong security, must also be met.
p2264
aVIt is currently difficult to build practical and reliable programming systems out of distributed and resourceconstrained sensor devices. The state of the art in today's sensornet programming is centered around a componentbased language called nesC. nesC is a nodelevel languagea program is written for an individual node in the networkand nesC programs use the services of an operating system called TinyOS. We are pursuing an approach to programming sensor networks that significantly raises the level of abstraction over this practice. The critical change is one of perspective: rather than writing programs from the point of view of an individual node, programmers implement a central program that conceptually has access to the entire network. This approach pushes to the compiler the task of producing nodelevel programs that implement the desired behavio. We present the Pleiades programming language, its compiler, and its runtime. The Pleiades language extends the C language with constructs that allow programmers to name and access nodelocal state within the network and to specify simple forms of concurrent execution. The compiler and runtime system cooperate to implement Pleiades programs efficiently and reliably. First, the compiler employs a novel program analysis to translate Pleiades programs into messageefficient units of work implemented in nesC. The Pleiades runtime system orchestrates execution of these units, using TinyOS services, across a network of sensor nodes. Second, the compiler and runtime system employ novel locking, deadlock detection, and deadlock recovery algorithms that guarantee serializability in the face of concurrent execution. We illustrate the readability, reliability and efficiency benefits of the Pleiades language through detailed experiments, and demonstrate that the Pleiades implementation of a realistic application performs similar to a handcoded nesC version that contains more than ten times as much code.
p2265
aVConcurrency libraries can facilitate the development of multithreaded programs by providing concurrent implementations of familiar data types such as queues or sets. There exist many optimized algorithms that can achieve superior performance on multiprocessors by allowing concurrent data accesses without using locks. Unfortunately, such algorithms can harbor subtle concurrency bugs. Moreover, they requirememory ordering fences to function correctly on relaxed memory models. To address these difficulties, we propose a verification approach that can exhaustively check all concurrent executions of a given test program on a relaxed memory model and can verify that they are observationally equivalent to a sequential execution. Our CheckFence prototype automatically translates the C implementation code and the test program into a SAT formula, hands the latter to a standard SAT solver, and constructs counter example traces if there exist incorrect executions. Applying CheckFence to five previously published algorithms, we were able to (1) find several bugs (some not previously known), and (2) determine how to place memory ordering fences for relaxed memory models.
p2266
aVIrregular applications, which manipulate large, pointerbased data structures like graphs, are difficult to parallelize manually. Automatic tools and techniques such as restructuring compilers and runtime speculative execution have failed to uncover much parallelism in these applications, in spite of a lot of effort by the research community. These difficulties have even led some researchers to wonder if there is any coarsegrain parallelism worth exploiting in irregular applications. In this paper, we describe two realworld irregular applications: a Delaunay mesh refinement application and a graphics application thatperforms agglomerative clustering. By studying the algorithms and data structures used in theseapplications, we show that there is substantial coarsegrain, data parallelism in these applications, but that this parallelism is very dependent on the input data and therefore cannot be uncoveredby compiler analysis. In principle, optimistic techniques such asthreadlevel speculation can be used to uncover this parallelism, but we argue that current implementations cannot accomplish thisbecause they do not use the proper abstractions for the data structuresin these programs. These insights have informed our design of the Galois system, an objectbased optimistic parallelization system for irregular applications. There are three main aspects to Galois: (1) a small number of syntactic constructs for packaging optimistic parallelism as iteration over ordered and unordered sets, (2)assertions about methods in class libraries, and (3) a runtime scheme for detecting and recovering from potentially unsafe accesses to shared memory made by an optimistic computation. We show that Delaunay mesh generation and agglomerative clustering can be parallelized in a straightforward way using the Galois approach, and we present experimental measurements to show that this approach is practical. These results suggest that Galois is a practical approach to exploiting data parallelismin irregular programs.
p2267
aVMany sequential applications are difficult to parallelize because of unpredictable control flow, indirect data access, and inputdependent parallelism. These difficulties led us to build a software system for behavior oriented parallelization (BOP), which allows a program to be parallelized based on partial information about program behavior, for example, a user reading just part of the source code, or a profiling tool examining merely one or few executions. The basis of BOP is programmable software speculation, where a user or an analysis tool marks possibly parallel regions in the code, and the runtime system executes these regions speculatively. It is imperative to protect the entire address space during speculation. The main goal of the paper is to demonstrate that the general protection can be made cost effective by three novel techniques: programmable speculation, criticalpath minimization, and valuebased correctness checking. On a recently acquired multicore, multiprocessor PC, the BOP system reduced the endtoend execution time by integer factors for a Lisp interpreter, a data compressor, a language parser, and a scientific library, with no change to the underlying hardware or operating system.
p2268
aVPerformance optimization of stencil computations has been widely studied in the literature, since they occur in many computationally intensive scientific and engineering applications. Compiler frameworks have also been developed that can transform sequential stencil codes for optimization of data locality and parallelism. However, loop skewing is typically required in order to tile stencil codes along the time dimension, resulting in load imbalance in pipelined parallel execution of the tiles. In this paper, we develop an approach for automatic parallelization of stencil codes, that explicitly addresses the issue of loadbalanced execution of tiles. Experimental results are provided that demonstrate the effectiveness of the approach.
p2269
aVData races often result in unexpected and erroneous behavior. In addition to causing data corruption and leading programs to crash, the presence of data races complicates the semantics of an execution which might no longer be sequentially consistent. Motivated by these observations, we have designed and implemented a Java runtime system that monitors program executions and throws a DataRaceException when a data race is about to occur. Analogous to other runtime exceptions, the DataRaceException provides two key benefits. First, accesses causing race conditions are interruptedand handled before they cause errors that may be difficult to diagnose later. Second, if no DataRaceException is thrown in an execution, it is guaranteed to be sequentially consistent. This strong guarantee helps to rule out many concurrencyrelated possibilities as the cause of erroneous behavior. When a DataRaceException is caught, the operation, thread, or program causing it can be terminated gracefully. Alternatively, the DataRaceException can serve as a conflictdetection mechanism inoptimistic uses of concurrency. We start with the definition of dataracefree executions in the Java memory model. We generalize this definition to executions that use transactions in addition to locks and volatile variables for synchronization. We present a precise and efficient algorithm for dynamically verifying that an execution is free of data races. This algorithm generalizes the Goldilocks algorithm for datarace detectionby handling transactions and providing the ability to distinguish between read and write accesses. We have implemented our algorithm and the DataRaceException in the Kaffe Java Virtual Machine. We have evaluated our system on a variety of publicly available Java benchmarks and a few microbenchmarks that combine lockbased and transactionbased synchronization. Our experiments indicate that our implementation has reasonable overhead. Therefore, we believe that inaddition to being a debugging tool, the DataRaceException may be a viable mechanism to enforce the safety of executions of multithreaded Java programs.
p2270
aVSeparation logic with recursively defined predicates allows for concise yet precise description of the shapes of data structures. However, most uses of separation logic for program analysis rely on predefined recursive predicates, limiting the class of programs analyzable to those that manipulate only a priori data structures. This paper describes a general algorithm based on inductive program synthesis that automatically infers recursive shape invariants, yielding a shape analysis based on separation logic that can be applied to any program. A key strength of separation logic is that it facilitates, via explicit expression of structural separation, local reasoning about heap where the effects of altering one part of a data structure are analyzed in isolation from the rest. The interaction between local reasoning and the global invariants given by recursive predicates is a difficult area, especially in the presence of complex internal sharing in the data structures. Existing approaches, using logic rules specifically designed for the list predicate to unfold and fold linkedlists, again require a priori knowledge about the shapes of the data structures and do not easily generalize to more complex data structures. We introduce a notion of "truncation points" in a recursive predicate, which gives rise to generic algorithms for unfolding and folding arbitrary data structures.
p2271
aVWe present the first shape analysis for multithreaded programs that avoids the explicit enumeration of executioninterleavings. Our approach is to automatically infer a resource invariant associated with each lock that describes the part of the heap protected by the lock. This allows us to use a sequential shape analysis on each thread. We show that resource invariants of a certain class can be characterized as least fixed points and computed via repeated applications of shape analysis only on each individual thread. Based on this approach, we have implemented a threadmodular shape analysis tool and applied it to concurrent heapmanipulating code from Windows device drivers.
p2272
aVContextsensitive pointer analysis algorithms with full "heapcloning" are powerful but are widely considered to be too expensive to include in production compilers. This paper shows, for the first time, that a contextsensitive, fieldsensitive algorithm with fullheap cloning (by acyclic call paths) can indeed be both scalable and extremely fast in practice. Overall, the algorithm is able to analyze programs in the range of 100K200K lines of C code in 13 seconds,takes less than 5% of the time it takes for GCC to compile the code (which includes no wholeprogram analysis), and scales well across five orders of magnitude of code size. It is also able to analyze the Linux kernel (about 355K linesof code) in 3.1 seconds. The paper describes the major algorithmic and engineering design choices that are required to achieve these results, including (a) using flowinsensitive and unificationbasedanalysis, which are essential to avoid exponential behavior in practice;(b) sacrificing contextsensitivity within strongly connected components of the call graph; and (c) carefully eliminating several kinds of O(N2) behaviors (largely without affecting precision). The techniques used for (b) and (c) eliminated several major bottlenecks to scalability, and both are generalizable to other contextsensitive algorithms. We show that the engineering choices collectively reduce analysis time by factors of up to 10x15xin our larger programs, and have found that the savings grow strongly with program size. Finally, we briefly summarize results demonstrating the precision of the analysis.
p2273
aVPointer information is a prerequisite for most program analyses, and the quality of this information can greatly affect their precision and performance. Inclusionbased (i.e. Andersenstyle) pointer analysis is an important point in the space of pointer analyses, offering a potential sweetspot in the tradeoff between precision and performance. However, current techniques for inclusionbased pointer analysis can have difficulties delivering on this potential. We introduce and evaluate two novel techniques for inclusionbased pointer analysis one lazy, one eager1 that significantly improve upon the current stateoftheart without impacting precision. These techniques focus on the problem of online cycle detection, a critical optimization for scaling such analyses. Using a suite of six opensource C programs, which range in size from 169K to 2.17M LOC, we compare our techniques against the three best inclusionbased analyses described by Heintze and Tardieu [11], by Pearce et al. [21], and by Berndl et al. [4]. The combination of our two techniques results in an algorithm which is on average 3.2 xfaster than Heintze and Tardieu's algorithm, 6.4 xfaster than Pearce et al.'s algorithm, and 20.6 faster than Berndl et al.'s algorithm. We also investigate the use of different data structures to represent pointsto sets, examining the impact on both performance and memory consumption. We compare a sparsebitmap implementation used in the GCC compiler with a BDDbased implementation, and we find that the BDD implementation is on average 2x slower than using sparse bitmaps but uses 5.5x less memory.
p2274
aVThe success of software verification depends on the ability to find a suitable abstraction of a program automatically. We propose a method for automated abstraction refinement which overcomes some limitations of current predicate discovery schemes. In current schemes, the cause of a false alarm is identified as an infeasible error path, and the abstraction is refined in order to remove that path. By contrast, we view the cause of a false alarm the spurious counterexample as a fullfledged program, namely, a fragment of the original program whose controlflow graph may contain loops and represent unbounded computations. There are two advantages to using such path programs as counterexamples for abstraction refinement. First, we can bring the whole machinery of program analysis to bear on path programs, which are typically small compared to the original program. Specifically, we use constraintbased invariant generation to automatically infer invariants of path programssocalled path invariants. Second, we use path invariants for abstraction refinement in order to remove not one infeasibility at a time, but at once all (possibly infinitely many) infeasible error computations that are represented by a path program. Unlike previous predicate discovery schemes, our method handles loops without unrolling them; it infers abstractions that involve universal quantification and naturally incorporates disjunctive reasoning.
p2275
aVWe present DITTO, an automatic incrementalizer for dynamic, sideeffectfree data structure invariant checks. Incrementalization speeds up the execution of a check by reusing its previous executions, checking the invariant anew only the changed parts of the data structure. DITTO exploits properties specific to the domain of invariant checks to automate and simplify the process without restricting what mutations the program can perform. Our incrementalizer works for modern imperative languages such as Java and C#. It can incrementalize,for example, verification of redblack tree properties and the consistency of the hash code in a hash table bucket. Our sourcetosource implementation for Java is automatic, portable, and efficient. DITTO provides speedups on data structures with as few as 100 elements; on larger data structures, its speedups are characteristic of nonautomatic incrementalizers: roughly 5fold at 5,000 elements,and growing linearly with data structure size.
p2276
aVMany concurrency bugs in multithreaded programs are due to dataraces. There have been many efforts to develop static and dynamic mechanisms to automatically find the data races. Most of the prior work has focused on finding the data races and eliminating the false positives. In this paper, we instead focus on a dynamic analysis technique to automatically classify the data races into two categories  the dataraces that are potentially benign and the data races that are potentially harmful. A harmful data race is a real bug that needs to be fixed. This classification is needed to focus the triaging effort on those data races that are potentially harmful. Without prioritizing the data races we have found that there are too many data races to triage. Our second focus is to automatically provide to the developer a reproducible scenario of the data race, which allows the developer to understand the different effects of a harmful data race on a program's execution. To achieve the above, we record a multithreaded program's execution in a replay log. The replay log is used to replay the multithreaded program, and during replay we find the data races using a happensbefore based algorithm. To automatically classify if a data race that we find is potentially benign or potentially harmful, were play the execution twice for a given data race  one for each possible order between the conflicting memory operations. If the two replays for the two orders produce the same result, then we classify the data race to be potentially benign. We discuss our experiences in using our replay based dynamic data race checker on several Microsoft applications.
p2277
aVConcurrent programs are often designed such that certain functions executing within critical threads must terminate. Examples of such cases can be found in operating systems, web servers, email clients, etc. Unfortunately, no known automatic program termination prover supports a practical method of proving the termination of threads. In this paper we describe such a procedure. The procedure's scalability is achieved through the use of environment models that abstract away the surrounding threads. The procedure's accuracy is due to a novel method of incrementally constructing environment abstractions. Our method finds the conditions that a thread requires of its environment in order to establish termination by looking at the conditions necessary to prove that certain paths through the thread represent wellfounded relations if executed in isolation of the other threads. The paper gives a description of experimental results using an implementation of our procedureon Windows device drivers and adescription of a previously unknown bug found withthe tool.
p2278
aVA general class of program analyses area combination of contextfree and regular language reachability. We define regularly annotated set constraints, a constraint formalism that captures this class. Our results extend the class of reachability problems expressible naturally in a single constraint formalism, including such diverse applications as interprocedural dataflow analysis, precise typebased flow analysis, and pushdown model checking.
p2279
aVWe present an algorithm to solve XPath decision problems under regular tree type constraints and show its use to statically typecheck XPath queries. To this end, we prove the decidability of a logic with converse for finite ordered trees whose time complexity is a simple exponential of the size of a formula. The logic corresponds to the alternation free modal \u03bccalculus without greatest fixpoint, restricted to finite trees, and where formulas are cyclefree. Our proof method is based on two auxiliary results. First, XML regular tree types and XPath expressions have a linear translation to cyclefree formulas. Second, the least and greatest fixpoints are equivalent for finite trees, hence the logic is closed under negation. Building on these results, we describe a practical, effective system for solving the satisfiability of a formula. The system has been experimented with some decision problems such as XPath emptiness, containment, overlap, and coverage, with or without type constraints. The benefit of the approach is that our system can be effectively used in static analyzers for programming languages manipulating both XPath expressions and XML type annotations (as input and output types).
p2280
aVEmbedded systems pose unique challenges to Java application developers and virtual machine designers. Chief among these challenges is the memory footprint of both the virtual machine and the applications that run within it. With the rapidly increasing set of features provided by the Java language, virtual machine designers are often forced to build custom implementations that make various tradeoffs between the footprint of the virtual machine and the subset of the Java language and class libraries that are supported. In this paper, we present the ExoVM, a system in which an application is initialized in a fully featured virtual machine, and then the code, data, and virtual machine features necessary to execute it are packaged into a binary image. Key to this process is feature analysis, a technique for computing the reachable code and data of a Java program and its implementation inside the VM simultaneously. The ExoVM reduces the need to develop customized embedded virtual machines by reusing a single VM infrastructure and automatically eliding the implementation of unused Java features on a perprogram basis. We present a constraintbased instantiation of the analysis technique, an implementation in IBM's J9 Java VM, experiments evaluating our technique for the EEMBC benchmark suite, and some discussion of the individual costs of some of Java's features. Our evaluation shows that our system can reduce the nonheap memory allocation of the virtual machine by as much as 75%. We discuss VM and language design decisions that our work shows are important in targeting embedded systems, supporting the longterm goal of a common VM infrastructure spanning from motes to large servers.
p2281
aVWe present offline RAM compression, an automated sourcetosource transformation that reduces a program's data size. Statically allocated scalars, pointers, structures, and arrays are encoded and packed based on the results of a wholeprogram analysis in the value set and pointer set domains. We target embedded software written in C that relies heavily on static memory allocation and runs on Harvardarchitecture microcontrollers supporting just a few KB of onchip RAM. On a collection of embedded applications for AVR microcontrollers, our transformation reduces RAM usage by an average of 12%, in addition to a 10% reduction through a deaddata elimination pass that is also driven by our wholeprogram analysis, for a total RAM savings of 22%. We also developeda technique for giving developers access to a flexible spectrum of tradeoffs between RAM consumption, ROM consumption, and CPU efficiency. This technique is based on a model for estimating the cost/benefit ratio of compressing each variable and then selectively compressing only those variables that present a good value proposition in terms of the desired tradeoffs.
p2282
aVHardware performance monitors provide detailed direct feedback about application behavior and are an additional source of information that a compiler may use for optimization. A JIT compiler is in a good position to make use of such information because it is running on the same platform as the user applications. As hardware platforms become more and more complex, it becomes more and more difficult to model their behavior. Profile information that captures general program properties (like execution frequency of methods or basic blocks) may be useful, but does not capture sufficient information about the execution platform. Machinelevel performance data obtained from a hardware performance monitor can not only direct the compiler to those parts of the program that deserve its attention but also determine if an optimization step actually improved the performance of the application. This paper presents an infrastructure based on a dynamic compiler+runtime environment for Java that incorporates machinelevel information as an additional kind of feedback for the compiler and runtime environment. The lowoverhead monitoring system provides finegrained performance data that can be tracked back to individual Java bytecode instructions. As an example, the paper presents results for object coallocation in a generational garbage collector that optimizes spatial locality of objects online using measurements about cache misses. In the best case, the execution time is reduced by 14% and L1 cache misses by 28%.
p2283
aVWireless sensor networks (WSN), composed of a large number of lowcost, batterypowered sensors, have recently emerged as promising computing platforms for many nontraditional applications. The preloaded code on remote sensors often needs to be updated after deployment in order for the WSN to adapt to the changing demands from the users. Postdeployment code dissemination is challenging as the data are transmitted via batterypowered wireless communication. Recent studies show that the energy for sending a single bit is about the same as executing 1000 instructions in aWSN. Therefore it is important to achieve energy efficiency in code dissemination. In this paper, we propose novel updateconscious compilation(UCC) techniques for energyefficient code dissemination in WSNs. An updateconscious compiler, when compiling the modified code, includes the compilation decisions that were made when generating the old binary. The compiler employs a detailed energy model and strives to match the old decisions for a more energyefficient result. In most cases, matching the previous decisions improves the binary code similarity, reduces the amount of data to be transmitted to remote sensors, and thus, consumes less energy. In this paper, we develop updateconscious register allocation and data layout algorithms. Our experimental results show that they can achieve great improvements over the traditional, updateoblivious approaches.
p2284
aVReducing energy consumption of a NetworkonChip (NoC) is a critical design goal, especially for powerconstrained embedded systems.In response, prior research has proposed several circuit/architectural level mechanisms to reduce NoC power consumption. This paper considers the problem from a different perspective and demonstrates that compiler analysis can be very helpful for enhancing the effectiveness of a hardwarebased link power management mechanism by increasing the duration of communication links' idle periods. The proposed profilebased approach achieves its goal by maximizing the communication link reuse through compilerdirected, static message rerouting. That is, it clusters the required data communications into a small set of communication links at any given time, which increases the idle periods for the remaining communication links in the network. This helps hardware shut down more communication links and their corresponding buffers to reduce leakage power. The current experimental evaluation, with twelve dataintensive embedded applications, shows that the proposed profiledriven compiler approach reduces leakage energy by more than 35% (on average) as compared to a pure hardwarebased link power management scheme.
p2285
aVParameterized tiled loopswhere the tile sizes are not fixed at compile time, but remain symbolic parameters until later are quite useful for iterative compilers and "autotuners" that produce highly optimized libraries and codes. Tile size parameterization could also enable optimizations such as register tiling to become dynamic optimizations. Although it is easy to generate such loops for (hyper) rectangular iteration spaces tiled with (hyper) rectangular tiles, many important computations do not fall into this restricted domain. Parameterized tile code generation for the general case of convex iteration spaces being tiled by (hyper) rectangular tiles has in the past been solved with bounding box approaches or symbolic Fourier Motzkin approaches. However, both approaches have less than ideal code generation efficiency and resulting code quality. We present the theoretical foundations, implementation, and experimental validation of a simple, unified technique for generating parameterized tiled code. Our code generation efficiency is comparable to all existing code generation techniques including those for fixed tile sizes, and the resulting code is as efficient as, if not more than, all previous techniques. Thus the technique provides parameterized tiled loops for free! Our "onesizefitsall" solution, which is available as open source software can be adapted for use in production compilers.
p2286
aVExecution omission errors are known to be difficult to locate using dynamic analysis. These errors lead to a failure at runtime because of the omission of execution of some statements that would have been executed if the program had no errors. Since dynamic analysis is typically designed to focus on dynamic information arising from executed statements, and statements whose execution is omitted do not produce dynamic information, detection of execution omission errors becomes a challenging task. For example, while dynamic slices are very effective in capturing faulty code for other types of errors, they fail to capture faulty code in presence of execution omission errors. To address this issue relevant slices have been defined to consider certain static dependences (called potential dependences) in addition to dynamic dependences. However, due to the conservative nature of static analysis, overly large slices are produced. In this paper, we propose a fully dynamic solution to locating execution omission errors using dynamic slices. We introduce the notion of implicit dependences which are dependences that are normally invisible to dynamic slicing due to the omission of execution of some statements. We design a dynamic method that forces the execution of the omitted code by switching outcomes of relevant predicates such that those implicit dependences are exposed and become available for dynamic slicing. Dynamic slices can be computed and effectively pruned to produce fault candidate sets containing the execution omission errors. We solve two main problems: verifying the existence of a single implicit dependence through predicate switching, and recovering the implicit dependences in a demand driven manner such that a small number of verifications are required before the root cause is captured. Our experiments show that the proposed technique is highly effective in capturing execution omission errors.
p2287
aVWeb applications are popular targets of security attacks. One common type of such attacks is SQL injection, where an attacker exploits faulty application code to execute maliciously crafted database queries. Bothstatic and dynamic approaches have been proposed to detect or prevent SQL injections; while dynamic approaches provide protection for deployed software, static approaches can detect potential vulnerabilities before software deployment. Previous static approaches are mostly based on tainted information flow tracking and have at least some of the following limitations: (1) they do not model the precise semantics of input sanitization routines; (2) they require manually written specifications, either for each query or for bug patterns; or (3) they are not fully automated and may require user intervention at various points in the analysis. In this paper, we address these limitations by proposing a precise, sound, and fully automated analysis technique for SQL injection. Our technique avoids the need for specifications by consideringas attacks those queries for which user input changes the intended syntactic structure of the generated query. It checks conformance to this policy byconservatively characterizing the values a string variable may assume with a context free grammar, tracking the nonterminals that represent usermodifiable data, and modeling string operations precisely as language transducers. We have implemented the proposed technique for PHP, the most widelyused web scripting language. Our tool successfully discovered previously unknown and sometimes subtle vulnerabilities in realworld programs, has a low false positive rate, and scales to large programs (with approx. 100K loc).
p2288
aVAdvanced type systems often need some form of type inference to reduce the burden of explicit typing, but type inference often leads to poor error messages for illtyped programs. This work pursues a new approach to constructing compilers and presenting typeerror messages in which the typechecker itself does not produce the messages. Instead, it is an oracle for a search procedure that finds similar programs that do typecheck. Our twofold goal is to improve error messages while simplifying compiler construction. Our primary implementation and evaluation is for Caml, a language with full type inference. We also present a prototype for C++ template functions, where type instantiation is implicit. A key extension is making our approach robust even when the program has multiple independent type errors.
p2289
aVInconsistency checking is a method for detecting software errors that relies only on examining multiple uses of a value. We propose that inconsistency inference is best understood as a variant of the older and better understood problem of type inference. Using this insight, we describe a precise and formal framework for discovering inconsistency errors. Unlike previous approaches to the problem, our technique for finding inconsistency errors is purely semantic and can deal with complex aliasing and pathsensitive conditions. We have built a nullde reference analysis of C programs based on semantic inconsistency inference and have used it to find hundreds of previously unknown null dereference errors in widely used C programs.
p2290
aVMultithreaded programs are difficult to get right because of unexpected interaction between concurrently executing threads. Traditional testing methods are inadequate for catching subtle concurrency errors which manifest themselves late in the development cycle and postdeployment. Model checking or systematic exploration of program behavior is a promising alternative to traditional testing methods. However, it is difficult to perform systematic search on large programs as the number of possible program behaviors grows exponentially with the program size. Confronted with this stateexplosion problem, traditional model checkers perform iterative depthbounded search. Although effective for messagepassing software, iterative depthbounding is inadequate for multithreaded software. This paper proposes iterative contextbounding, a new search algorithm that systematically explores the executions of a multithreaded program in an order that prioritizes executions with fewer context switches. We distinguish between preempting and nonpreempting context switches, and show that bounding the number of preempting context switches to a small number significantly alleviates the state explosion, without limiting the depth of explored executions. We show both theoretically and empirically that contextbounded search is an effective method for exploring the behaviors of multithreaded programs. We have implemented our algorithmin two model checkers and applied it to a number of realworld multithreaded programs. Our implementation uncovered 9 previously unknown bugs in our benchmarks, each of which was exposed by an execution with at most 2 preempting context switches. Our initial experience with the technique is encouraging and demonstrates that iterative contextbounding is a significant improvement over existing techniques for testing multithreaded programs.
p2291
aVConcurrent garbage collectors are notoriously hard to design, implement, and verify. We present a framework for the automatic exploration of a space of concurrent markandsweep collectors. In our framework, the designer specifies a set of "building blocks" from which algorithms can be constructed. These blocks reflect the designer's insights about the coordination between the collector and the mutator. Given a set of building blocks, our framework automatically explores a space of algorithms, using model checking with abstraction to verify algorithms in the space. We capture the intuition behind some common markandsweep algorithms using a set of building blocks. We utilize our framework to automatically explore a space of more than 1,600,000 algorithms built from these blocks, and derive over 100 correct finegrained algorithms with various space, synchronization, and precision tradeoffs.
p2292
aVGarbagecollected languages such as Java and C# are becoming more and more widely used in both highend software and realtime embedded applications. The correctness of the GC implementation is essential to the reliability and security of a large portion of the world's missioncritical software. Unfortunately, garbage collectors especially incremental and concurrent ones are extremely hard to implement correctly. In this paper, we present a new uniform approach to verifying the safety of both a mutator and its garbage collector in Hoarestyle logic. We define a formal garbage collector interface general enough to reason about a variety of algorithms while allowing the mutator to ignore implementationspecific details of the collector. Our approach supports collectors that require read and write barriers. We have used our approach to mechanically verify assembly implementations of marksweep, copying and incremental copying GCs in Coq, as well as sample mutator programs that can be linked with any of the GCs to produce a fullyverified garbagecollected program. Our work provides a foundation for reasoning about complex mutatorcollector interaction and makes an important advance toward building fully certified productionquality GCs.
p2293
aVA transient hardware fault occurs when an energetic particle strikes a transistor, causing it to change state. Although transient faults do not permanently damage the hardware, they may corrupt computations by altering stored values and signal transfers. In this paper, we propose a new scheme for provably safe and reliable computing in the presence of transient hardware faults. In our scheme, software computations are replicated to provide redundancy while special instructions compare the independently computed results to detect errors before writing critical data. In stark contrast to any previous efforts in this area, we have analyzed our fault tolerance scheme from a formal, theoretical perspective. To be specific, first, we provide an operational semantics for our assembly language, which includes a precise formal definition of our fault model. Second, we develop an assemblylevel type system designed to detect reliability problems in compiled code. Third, we provide a formal specification for program fault tolerance under the given fault model and prove that all welltyped programs are indeed fault tolerant. In addition to the formal analysis, we evaluate our detection scheme and show that it only takes 34% longer to execute than the unreliable version.
p2294
aVWe present a certified compiler from the simplytyped lambda calculus to assembly language. The compiler is certified in the sense that it comes with a machinechecked proof of semantics preservation, performed with the Coq proof assistant. The compiler and the terms of its several intermediate languages are given dependent types that guarantee that only welltyped programs are representable. Thus, type preservation for each compiler pass follows without any significant "proofs" of the usual kind. Semantics preservation is proved based on denotational semantics assigned to the intermediate languages. We demonstrate how working with a typepreserving compiler enables typedirected proof search to discharge large parts of our proof obligations automatically.
p2295
aVSelfmodifying code (SMC), in this paper, broadly refers to anyprogram that loads, generates, or mutates code at runtime. It is widely used in many of the world's critical software systems tosupport runtime code generation and optimization, dynamic loading and linking, OS boot loader, justintime compilation, binary translation,or dynamic code encryption and obfuscation. Unfortunately, SMC is alsoextremely difficult to reason about: existing formal verification techniquesincluding Hoare logic and type systemconsistentlyassume that program code stored in memory is fixedand immutable; this severely limits their applicability and power. This paper presents a simple but novel Hoarelogiclike framework that supports modular verification of general vonNeumann machine code with runtime code manipulation. By dropping the assumption that code memory is fixed and immutable, we are forced to apply local reasoningand separation logic at the very beginning, and treat program code uniformly as regular data structure. We address the interaction between separation and code memory and show how to establish the frame rules for local reasoning even in the presence of SMC. Our frameworkis realistic, but designed to be highly generic, so that it can support assembly code under all modern CPUs (including both x86 andMIPS). Our system is expressive and fully mechanized. We prove itssoundness in the Coq proof assistant and demonstrate its power by certifying a series of realistic examples and applicationsall of which can directly run on the SPIM simulator or any stock x86 hardware.
p2296
aVTransactional memory provides a new concurrency control mechanism that avoids many of the pitfalls of lockbased synchronization. Highperformance software transactional memory (STM) implementations thus far provide weak atomicity: Accessing shared data both inside and outside a transaction can result in unexpected, implementationdependent behavior. To guarantee isolation and consistent ordering in such a system, programmers are expected to enclose all sharedmemory accesses inside transactions. A system that provides strong atomicity guarantees isolation even in the presence of threads that access shared data outside transactions. A stronglyatomic system also orders transactions with conflicting nontransactional memory operations in a consistent manner. In this paper, we discuss some surprising pitfalls of weak atomicity, and we present an STM system that avoids these problems via strong atomicity. We demonstrate how to implement nontransactional data accesses via efficient read and write barriers, and we present compiler optimizations that further reduce the overheads of these barriers. We introduce a dynamic escape analysis that differentiates private and public data at runtime to make barriers cheaper and a static notaccessedintransaction analysis that removes many barriers completely. Our results on a set of Java programs show that strong atomicity can be implemented efficiently in a highperformance STM system.
p2297
aVDynamic binary instrumentation (DBI) frameworks make it easy to build dynamic binary analysis (DBA) tools such as checkers and profilers. Much of the focus on DBI frameworks has been on performance; little attention has been paid to their capabilities. As a result, we believe the potential of DBI has not been fully exploited. In this paper we describe Valgrind, a DBI framework designed for building heavyweight DBA tools. We focus on its unique support for shadow valuesa powerful but previously littlestudied and difficulttoimplement DBA technique, which requires a tool to shadow every register and memory value with another value that describes it. This support accounts for several crucial design features that distinguish Valgrind from other DBI frameworks. Because of these features, lightweight tools built with Valgrind run comparatively slowly, but Valgrind can be used to build more interesting, heavyweight tools that are difficult or impossible to build with other DBI frameworks such as Pin and DynamoRIO.
p2298
aVWe present a new static analysis for race freedom and race detection. The analysis checks race freedom by reducing the problem to (rational) linear programming. Unlike conventional static analyses for race freedom or race detection, our analysis avoids explicit computation of locksets and lock linearity/mustaliasness. Our analysis can handle a variety of synchronization idioms that more conventional approaches often have difficulties with, such as thread joining, semaphores, and signals. We achieve efficiency by utilizing modern linear programming solvers that can quickly solve large linear programming instances. This paper reports on the formal properties of the analysis and the experience with applying an implementation to real world C programs.
p2299
aVWe present the design and implementation of an automatic polyhedral sourcetosource transformation framework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism and locality simultaneously. Through this work, we show the practicality of analytical modeldriven automatic transformation in the polyhedral model   far beyond what is possible by current production compilers. Unlike previous works, our approach is an endtoend fully automatic one driven by an integer linear optimization framework that takes an explicit view of finding good ways of tiling for parallelism and locality using affine transformations. The framework has been implemented into a tool to automatically generate OpenMP parallel code from C program sections. Experimental results from the tool show very high speedups for local and parallel execution on multicores over stateoftheart compiler frameworks from the research community as well as the best native production compilers. The system also enables the easy use of powerful empirical/iterative optimization for general arbitrarily nested loop sequences.
p2300
aVWhile multicore hardware has become ubiquitous, explicitly parallel programming models and compiler techniques for exploiting parallelism on these systems have noticeably lagged behind. Stream programming is one model that has wide applicability in the multimedia, graphics, and signal processing domains. Streaming models execute as a set of independent actors that explicitly communicate data through channels. This paper presents a compiler technique for planning and orchestrating the execution of streaming applications on multicore platforms. An integrated unfolding and partitioning step based on integer linear programming is presented that unfolds data parallel actors as needed and maximally packs actors onto cores. Next, the actors are assigned to pipeline stages in such a way that all communication is maximally overlapped with computation on the cores. To facilitate experimentation, a generalized code generation template for mapping the software pipeline onto the Cell architecture is presented. For a range of streaming applications, a geometric mean speedup of 14.7x is achieved on a 16core Cell platform compared to a single core.
p2301
aVPractical and efficient algorithms for concurrent data structures are difficult to construct and modify. Algorithms in the literature are often optimized for a specific setting, making it hard to separate the algorithmic insights from implementation details. The goal of this work is to systematically construct algorithms for a concurrent data structure starting from its sequential implementation. Towards that goal, we follow a construction process that combines manual steps corresponding to highlevel insights with automatic exploration of implementation details. To assist us in this process, we built a new tool called Paraglider. The tool quickly explores large spaces of algorithms and uses bounded model checking to check linearizability of algorithms. Starting from a sequential implementation and assisted by the tool, we present the steps that we used to derive various highlyconcurrent algorithms. Among these algorithms is a new finegrained set data structure that provides a waitfree contains operation, and uses only the compareandswap (CAS) primitive for synchronization.
p2302
aVWe describe PSketch, a program synthesizer that helps programmers implement concurrent data structures. The system is based on the concept of sketching, a form of synthesis that allows programmers to express their insight about an implementation as a partial program: a sketch. The synthesizer automatically completes the sketch to produce an implementation that matches a given correctness criteria. PSketch is based on a new counterexampleguided inductive synthesis algorithm (CEGIS) that generalizes the original sketch synthesis algorithm from SolarLezama et.al. to cope efficiently with concurrent programs. The new algorithm produces a correct implementation by iteratively generating candidate implementations, running them through a verifier, and if they fail, learning from the counterexample traces to produce a better candidate; converging to a solution in a handful of iterations. PSketch also extends Sketch with higherlevel sketching constructs that allow the programmer to express her insight as a "soup" of ingredients from which complicated code fragments must be assembled. Such sketches can be viewed as syntactic descriptions of huge spaces of candidate programs (over 108 candidates for some sketches we resolved). We have used the PSketch system to implement several classes of concurrent data structures, including lockfree queues and concurrent sets with finegrained locking. We have also sketched some other concurrent objects including a sensereversing barrier and a protocol for the dining philosophers problem; all these sketches resolved in under an hour.
p2303
aVUnintended or unmediated data sharing is a frequent cause of insidious bugs in multithreaded programs. We present a tool called SharC (short for Sharing Checker) that allows a user to write lightweight annotations to declare how they believe objects are being shared between threads in their program. SharC uses a combination of static and dynamic analyses to check that the program conforms to this specification. SharC allows any type to have one of five "sharing modes"   private to the current thread, readonly, shared under the control of a specified lock, intentionally racy, or checked dynamically. The dynamic mode uses runtime checking to verify that objects are either readonly, or only accessed by one thread. This allows us to check programs that would be difficult to check with a purely static system. If the user does not give a type an explicit annotation, then SharC uses a static typequalifier analysis to infer that it is either private or should be checked dynamically. SharC allows objects to move between different sharing modes at runtime by using reference counting to check that there are no other references to the objects when they change mode. SharC's baseline dynamic analysis can check any C program, but is slow, and will generate false warnings about intentional data sharing. As the user adds more annotations, false warnings are reduced, and performance improves.We have found in practice that very few annotations are needed to describe all sharing and give reasonable performance. We ran SharC on 6 legacy C programs, summing to over 600k lines of code, and found that a total of only 60 simple annotations were needed to remove all false positives and to reduce performance overhead to only 214%.
p2304
aVWe present Logically Qualified Data Types, abbreviated to Liquid Types, a system that combines HindleyMilner type inference with Predicate Abstraction to automatically infer dependent types precise enough to prove a variety of safety properties. Liquid types allow programmers to reap many of the benefits of dependent types, namely static verification of critical properties and the elimination of expensive runtime checks, without the heavy price of manual annotation. We have implemented liquid type inference in DSOLVE, which takes as input an OCAML program and a set of logical qualifiers and infers dependent types for the expressions in the OCAML program. To demonstrate the utility of our approach, we describe experiments using DSOLVE to statically verify the safety of array accesses on a set of OCAML benchmarks that were previously annotated with dependent types as part of the DML project. We show that when used in conjunction with a fixed set of array bounds checking qualifiers, DSOLVE reduces the amount of manual annotation required for proving safety from 31% of program text to under 1%.
p2305
aVHardware interrupts are widely used in the world's critical software systems to support preemptive threads, device drivers, operating system kernels, and hypervisors. Handling interrupts properly is an essential component of lowlevel system programming. Unfortunately, interrupts are also extremely hard to reason about: they dramatically alter the program control flow and complicate the invariants in lowlevel concurrent code (e.g., implementation of synchronization primitives). Existing formal verification techniques including Hoare logic, typed assembly language, concurrent separation logic, and the assumeguarantee method have consistently ignored the issues of interrupts; this severely limits the applicability and power of today's program verification systems. In this paper we present a novel Hoarelogiclike framework for certifying lowlevel system programs involving both hardware interrupts and preemptive threads. We show that enabling and disabling interrupts can be formalized precisely using simple ownershiptransfer semantics, and the same technique also extends to the concurrent setting. By carefully reasoning about the interaction among interrupt handlers, context switching, and synchronization libraries, we are able to for the first time successfully certify a preemptive thread implementation and a large number of common synchronization primitives. Our work provides a foundation for reasoning about interruptbased kernel programs and makes an important advance toward building fully certified operating system kernels and hypervisors.
p2306
aVTypepreserving compilers translate welltyped source code, such as Java or C#, into verifiable target code, such as typed assembly language or proofcarrying code. This paper presents the implementation of typepreserving compilation in a complex, largescale optimizing compiler. Compared to prior work, this implementation supports extensive optimizations, and it verifies a large portion of the interface between the compiler and the runtime system. This paper demonstrates the practicality of typepreserving compilation in complex optimizing compilers: the generated typed assembly language is only 2.3% slower than the base compiler's generated untyped assembly language, and the typepreserving compiler is 82.8% slower than the base compiler.
p2307
aVWe present a new technique for determining how much information about a program's secret inputs is revealed by its public outputs. In contrast to previous techniques based on reachability from secret inputs (tainting), it achieves a more precise quantitative result by computing a maximum flow of information between the inputs and outputs. The technique uses static controlflow regions to soundly account for implicit flows via branches and pointer operations, but operates dynamically by observing one or more program executions and giving numeric flow bounds specific to them (e.g., "17 bits"). The maximum flow in a network also gives a minimum cut (a set of edges that separate the secret input from the output), which can be used to efficiently check that the same policy is satisfied on future executions. We performed case studies on 5 real C, C++, and Objective C programs, 3 of which had more than 250K lines of code. The tool checked multiple security policies, including one that was violated by a previously unknown bug.
p2308
aVWhitebox fuzzing is a form of automatic dynamic test generation, based on symbolic execution and constraint solving, designed for security testing of large applications. Unfortunately, the current effectiveness of whitebox fuzzing is limited when testing applications with highlystructured inputs, such as compilers and interpreters. These applications process their inputs in stages, such as lexing, parsing and evaluation. Due to the enormous number of control paths in early processing stages, whitebox fuzzing rarely reaches parts of the application beyond those first stages. In this paper, we study how to enhance whitebox fuzzing of complex structuredinput applications with a grammarbased specification of their valid inputs. We present a novel dynamic test generation algorithm where symbolic execution directly generates grammarbased constraints whose satisfiability is checked using a custom grammarbased constraint solver. We have implemented this algorithm and evaluated it on a large securitycritical application, the JavaScript interpreter of Internet Explorer 7 (IE7). Results of our experiments show that grammarbased whitebox fuzzing explores deeper program paths and avoids deadends due to nonparsable inputs. Compared to regular whitebox fuzzing, grammarbased whitebox fuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from 53% to 81% while using three times fewer tests.
p2309
aVBugs in multithreaded programs often arise due to data races. Numerous static and dynamic program analysis techniques have been proposed to detect data races. We propose a novel randomized dynamic analysis technique that utilizes potential data race information obtained from an existing analysis tool to separate real races from false races without any need for manual inspection. Specifically, we use potential data race information obtained from an existing dynamic analysis technique to control a random scheduler of threads so that real race conditions get created with very high probability and those races get resolved randomly at runtime. Our approach has several advantages over existing dynamic analysis tools. First, we can create a real race condition and resolve the race randomly to see if an error can occur due to the race. Second, we can replay a race revealing execution efficiently by simply using the same seed for random number generation we do not need to record the execution. Third, our approach has very low overhead compared to other precise dynamic race detection techniques because we only track all synchronization operations and a single pair of memory access statements that are reported to be in a potential race by an existing analysis. We have implemented the technique in a prototype tool for Java and have experimented on a number of large multithreaded Java programs. We report a number of previously known and unknown bugs and real races in these Java programs.
p2310
aVWe show that register allocation can be viewed as solving a collection of puzzles. We model the register file as a puzzle board and the program variables as puzzle pieces; precoloring and register aliasing fit in naturally. For architectures such as PowerPC, x86, and StrongARM, we can solve the puzzles in polynomial time, and we have augmented the puzzle solver with a simple heuristic for spilling. For SPEC CPU2000, the compilation time of our implementation is as fast as that of the extended version of linear scan used by LLVM, which is the JIT compiler in the openGL stack of Mac OS 10.5. Our implementation produces x86 code that is of similar quality to the code produced by the slower, stateoftheart iterated register coalescing of George and Appel with the extensions proposed by Smith, Ramsey, and Holloway in 2004.
p2311
aVRegister allocation is always a tradeoff between liverange splitting and coalescing. Liverange splitting generally leads to less spilling at the cost of inserting shuffle code. Coalescing removes shuffle code while potentially raising the register demand and causing spilling. Recent research showed that the liverange splitting of the SSA form's functions leads to chordal interference graphs. This improves upon two longstanding inconveniences of graph coloring register allocation: First, chordal graphs are optimally colorable in quadratic time. Second, the number of colors needed to color the graph is equal to the maximal register pressure in the program. However, the inserted shuffle code incurred by the functions can slow down the program severely. Hence, to make such an approach work in practice, a coalescing technique is needed that removes most of the shuffle code without causing further spilling. In this paper, we present a coalescing technique designed for, but not limited to, SSAform register allocation. We exploit that a valid coloring can be easily obtained by an SSAbased register allocator. This initial coloring is then improved by recoloring the interference graph and assigning shufflecode related nodes the same color. Thereby, we always keep the coloring of the graph valid. Hence, the coalescing is safe, i. e. no spill code will be caused by coalescing. Comparing to iterated register coalescing, the state of the art in safe coalescing, our method is able to remove 22.5% of the costs and 44.3% of the copies iterated coalescing left over. The best solution possible, found by a colaescer using integer linear programming (ILP), was 35.9% of the costs and 51.9% of the copies iterated coalescing left over. The runtime of programs compiled with our heuristic matches that of the programs compiled with the ILP technique.
p2312
aVExecution indexing uniquely identifies a point in an execution. Desirable execution indices reveal correlations between points in an execution and establish correspondence between points across multiple executions. Therefore, execution indexing is essential for a wide variety of dynamic program analyses, for example, it can be used to organize program profiles; it can precisely identify the point in a reexecution that corresponds to a given point in an original execution and thus facilitate debugging or dynamic instrumentation. In this paper, we formally define the concept of execution index and propose an indexing scheme based on execution structure and program state. We present a highly optimized online implementation of the technique. We also perform a client study, which targets producing a failure inducing schedule for a data race by verifying the two alternative happensbefore orderings of a racing pair. Indexing is used to precisely locate corresponding points across multiple executions in the presence of nondeterminism so that no heavyweight tracing/replay system is needed.
p2313
aVWe propose a framework for improving both the scalability as well as the accuracy of pointer alias analysis, irrespective of its flow or contextsensitivities, by leveraging a threepronged strategy that effectively combines divide and conquer, parallelization and function summarization. A key step in our approach is to first identify small subsets of pointers such that the problem of computing aliases of any pointer can be reduced to computing them in these small subsets instead of the entire program. In order to identify these subsets, we first apply a series of increasingly accurate but highly scalable (context and flowinsensitive) alias analyses in a cascaded fashion such that each analysis Ai works on the subsets generated by the previous one Ai1. Restricting the application of Ai to subsets generated by Ai1, instead of the entire program, improves it scalability, i.e., Ai is bootstrapped by Ai1. Once these small subsets have been computed, in order to make our overall analysis accurate, we employ our new summarizationbased flow and contextsensitive alias analysis. The small size of each subset offsets the higher computational complexity of the contextsensitive analysis. An important feature of our framework is that the analysis for each of the subsets can be carried out independently of others thereby allowing us to leverage parallelization further improving scalability.
p2314
aVWith programs getting larger and often more complex with each new release, programmers need all the help they can get in understanding and transforming programs. Fortunately, modern development environments, such as Eclipse, incorporate tools for understanding, navigating, and transforming programs. These tools typically use program analyses to extract relevant properties of programs. These tools are often invaluable to developers; for example, many programmers use refactoring tools regularly. However, poor results by the underlying analyses can compromise a tool's usefulness. For example, a bug finding tool may produce too many false positives if the underlying analysis is overly conservative, and thus overwhelm the user with too many possible errors in the program. In such cases it would be invaluable for the tool to explain to the user why it believes that each bug exists. Armed with this knowledge, the user can decide which bugs are worth pursing and which are false positives. The contributions of this paper are as follows: (i) We describe requirements on the structure of an analysis so that we can produce reasons when the analysis fails; the user of the analysis determines whether or not an analysis's results constitute failure. We also describe a simple language that enforces these requirements; (ii) We describe how to produce necessary and sufficient reasons for analysis failure; (iii) We evaluate our system with respect to a number of analyses and programs and find that most reasons are small (and thus usable) and that our system is fast enough for interactive use.
p2315
aVWe present a new, precise technique for fully path and contextsensitive program analysis. Our technique exploits two observations: First, using quantified, recursive formulas, path and contextsensitive conditions for many program properties can be expressed exactly. To compute a closed form solution to such recursive constraints, we differentiate between observable and unobservable variables, the latter of which are existentially quantified in our approach. Using the insight that unobservable variables can be eliminated outside a certain scope, our technique computes satisfiability and validitypreserving closedform solutions to the original recursive constraints. We prove the solution is as precise as the original system for answering may and must queries as well as being small in practice, allowing our technique to scale to the entire Linux kernel, a program with over 6 million lines of code.
p2316
aVA constraintbased approach to invariant generation in programs translates a program into constraints that are solved using offtheshelf constraint solvers to yield desired program invariants. In this paper we show how the constraintbased approach can be used to model a wide spectrum of program analyses in an expressive domain containing disjunctions and conjunctions of linear inequalities. In particular, we show how to model the problem of contextsensitive interprocedural program verification. We also present the first constraintbased approach to weakest precondition and strongest postcondition inference. The constraints we generate are boolean combinations of quadratic inequalities over integer variables. We reduce these constraints to SAT formulae using bitvector modeling and use offtheshelf SAT solvers to solve them. Furthermore, we present interesting applications of the above analyses, namely bounds analysis and generation of mostgeneral counterexamples for both safety and termination properties. We also present encouraging preliminary experimental results demonstrating the feasibility of our technique on a variety of challenging examples.
p2317
aVAtomicity is a fundamental correctness property in multithreaded programs, both because atomic code blocks are amenable to sequential reasoning (which significantly simplifies correctness arguments), and because atomicity violations often reveal defects in a program's synchronization structure. Unfortunately, all atomicity analyses developed to date are incomplete in that they may yield false alarms on correctly synchronized programs, which limits their usefulness. We present the first dynamic analysis for atomicity that is both sound and complete. The analysis reasons about the exact dependencies between operations in the observed trace of the target program, and it reports error messages if and only if the observed trace is not conflictserializable. Despite this significant increase in precision, the performance and coverage of our analysis is competitive with earlier incomplete dynamic analyses for atomicity.
p2318
aVAtomic sections are a recent and popular idiom to support the development of concurrent programs. Updates performed within an atomic section should not be visible to other threads until the atomic section has been executed entirely. Traditionally, atomic sections are supported through the use of optimistic concurrency, either using a transactional memory hardware, or an equivalent software emulation (STM). This paper explores automatically supporting atomic sections using pessimistic concurrency. We present a system that combines compiler and runtime techniques to automatically transform programs written with atomic sections into programs that only use locking primitives. To minimize contention in the transformed programs, our compiler chooses from several lock granularities, using finegrain locks whenever it is possible. This paper formally presents our framework, shows that our compiler is sound (i.e., it protects all shared locations accessed within atomic sections), and reports experimental results.
p2319
aVDataflow analyses for concurrent programs differ from their singlethreaded counterparts in that they must account for shared memory locations being overwritten by concurrent threads. Existing dataflow analysis techniques for concurrent programs typically fall at either end of a spectrum: at one end, the analysis conservatively kills facts about all data that might possibly be shared by multiple threads; at the other end, a precise threadinterleaving analysis determines which data may be shared, and thus which dataflow facts must be invalidated. The former approach can suffer from imprecision, whereas the latter does not scale. We present RADAR, a framework that automatically converts a dataflow analysis for sequential programs into one that is correct for concurrent programs. RADAR uses a race detection engine to kill the dataflow facts, generated and propagated by the sequential analysis, that become invalid due to concurrent writes. Our approach of factoring all reasoning about concurrency into a race detection engine yields two benefits. First, to obtain analyses for code using new concurrency constructs, one need only design a suitable race detection engine for the constructs. Second, it gives analysis designers an easy way to tune the scalability and precision of the overall analysis by only modifying the race detection engine. We describe the RADAR framework and its implementation using a preexisting race detection engine. We show how RADAR was used to generate a concurrent version of a nullpointer dereference analysis, and we analyze the result of running the generated concurrent analysis on several benchmarks.
p2320
aVProgrammers are increasingly choosing managed languages for modern applications, which tend to allocate many shorttomedium lived small objects. The garbage collector therefore directly determines program performance by making a classic spacetime tradeoff that seeks to provide space efficiency, fast reclamation, and mutator performance. The three canonical tracing garbage collectors: semispace, marksweep, and markcompact each sacrifice one objective. This paper describes a collector family, called markregion, and introduces opportunistic defragmentation, which mixes copying and marking in a single pass. Combining both, we implement immix, a novel high performance garbage collector that achieves all three performance objectives. The key insight is to allocate and reclaim memory in contiguous regions, at a coarse block grain when possible and otherwise in groups of finer grain lines. We show that immix outperforms existing canonical algorithms, improving total application performance by 7 to 25% on average across 20 benchmarks. As the mature space in a generational collector, immix matches or beats a highly tuned generational collector, e.g. it improves jbb2000 by 5%. These innovations and the identification of a new family of collectors open new opportunities for garbage collector design.
p2321
aVDevelopers commonly build contemporary enterprise applications using typesafe, componentbased platforms, such as J2EE, and architect them to comprise multiple tiers, such as a web container, application server, and database engine. Administrators increasingly execute each tier in its own managed runtime environment (MRE) to improve reliability and to manage system complexity through the fault containment and modularity offered by isolated MRE instances. Such isolation, however, necessitates expensive crosstier communication based on protocols such as object serialization and remote procedure calls. Administrators commonly colocate communicating MREs on a single host to reduce communication overhead and to better exploit increasing numbers of available processing cores. However, stateoftheart MREs offer no support for more efficient communication between colocated MREs, while fast interprocess communication mechanisms, such as shared memory, are widely available as a standard operating system service on most modern platforms. To address this growing need, we present the design and implementation of XMem ? typesafe, transparent, shared memory support for colocated MREs. XMem guarantees typesafety through coordinated, parallel, multiprocess class loading and garbage collection. To avoid introducing any level of indirection, XMem manipulates virtual memory mapping. In addition, object sharing in XMem is fully transparent: shared objects are identical to local objects in terms of field access, synchronization, garbage collection, and method invocation, with the only difference being that sharedtoprivate pointers are disallowed. XMem facilitates easy integration and use by existing communication technologies and software systems, such as RMI, JNDI, JDBC, serialization/XML, and network sockets. We have implemented XMem in the opensource, productionquality HotSpot Java Virtual Machine. Our experimental evaluation, based on core communication technologies underlying J2EE, as well as using opensource server applications, indicates that XMem significantly improves throughput and response time by avoiding the overheads imposed by object serialization and network communication.
p2322
aVArray bound checking and array dependency analysis (for parallelization) have been widely studied. However, there are much less results about analyzing properties of array contents. In this paper, we propose a way of using abstract interpretation for discovering properties about array contents in some restricted cases: onedimensional arrays, traversed by simple "for" loops. The basic idea, borrowed from [GRS05], consists in partitioning arrays into symbolic intervals (e.g., [1,i   1], [i,i], [i + 1,n]), and in associating with each such interval I and each array A an abstract variable AI; the new idea is to consider relational abstract properties \u03c8(AI, BI, ...) about these abstract variables, and to interpret such a property pointwise on the interval I: \u2200l \u2208 I, \u03c8(A[l], B[l],...). The abstract semantics of our simple programs according to these abstract properties has been defined and implemented in a prototype tool. The method is able, for instance, to discover that the result of an insertion sort is a sorted array, or that, in an array traversal guarded by a "sentinel", the index stays within the bounds.
p2323
aVWe present the first verification of full functional correctness for a range of linked data structure implementations, including mutable lists, trees, graphs, and hash tables. Specifically, we present the use of the Jahob verification system to verify formal specifications, written in classical higherorder logic, that completely capture the desired behavior of the Java data structure implementations (with the exception of properties involving execution time and/or memory consumption). Given that the desired correctness properties include intractable constructs such as quantifiers, transitive closure, and lambda abstraction, it is a challenge to successfully prove the generated verification conditions. Our Jahob verification system uses integrated reasoning to split each verification condition into a conjunction of simpler subformulas, then apply a diverse collection of specialized decision procedures, firstorder theorem provers, and, in the worst case, interactive theorem provers to prove each subformula. Techniques such as replacing complex subformulas with stronger but simpler alternatives, exploiting structure inherently present in the verification conditions, and, when necessary, inserting verified lemmas and proof hints into the imperative source code make it possible to seamlessly integrate all of the specialized decision procedures and theorem provers into a single powerful integrated reasoning system. By appropriately applying multiple proof techniques to discharge different subformulas, this reasoning system can effectively prove the complex and challenging verification conditions that arise in this context.
p2324
aVStateless model checking is a useful statespace exploration technique for systematically testing complex realworld software. Existing stateless model checkers are limited to the verification of safety properties on terminating programs. However, realistic concurrent programs are nonterminating, a property that significantly reduces the efficacy of stateless model checking in testing them. Moreover, existing stateless model checkers are unable to verify that a nonterminating program satisfies the important liveness property of livelockfreedom, a property that requires the program to make continuous progress for any input. To address these shortcomings, this paper argues for incorporating a fair scheduler in stateless exploration. The key contribution of this paper is an explicit scheduler that is (strongly) fair and at the same time sufficiently nondeterministic to guarantee full coverage of safety properties.We have implemented the fair scheduler in the CHESS model checker. We show through theoretical arguments and empirical evaluation that our algorithm satisfies two important properties: 1) it visits all states of a finitestate program achieving state coverage at a faster rate than existing techniques, and 2) it finds all livelocks in a finitestate program. Before this work, nonterminating programs had to be manually modified in order to apply CHESS to them. The addition of fairness has allowed CHESS to be effectively applied to realworld nonterminating programs without any modification. For example, we have successfully booted the Singularity operating system under the control of CHESS.
p2325
aVConcurrent garbage collection is highly attractive for realtime systems, because offloading the collection effort from the executing threads allows faster response, allowing for extremely short deadlines at the microseconds level. Concurrent collectors also offer much better scalability over incremental collectors. The main problem with concurrent realtime collectors is their complexity. The first concurrent realtime garbage collector that can support fine synchronization, STOPLESS, has recently been presented by Pizlo et al. In this paper, we propose two additional (and different) algorithms for concurrent realtime garbage collection: CLOVER and CHICKEN. Both collectors obtain reduced complexity over the first collector STOPLESS, but need to trade a benefit for it. We study the algorithmic strengths and weaknesses of CLOVER and CHICKEN and compare them to STOPLESS. Finally, we have implemented all three collectors on the Bartok compiler and runtime for C# and we present measurements to compare their efficiency and responsiveness.
p2326
aVRegionbased memory management is a popular scheme in systems software for better organization and performance. In the scheme, a developer constructs a hierarchy of regions of different lifetimes and allocates objects in regions. When the developer deletes a region, the runtime will recursively delete all its subregions and simultaneously reclaim objects in the regions. The developer must construct a consistent placement of objects in regions; otherwise, if a region that contains pointers to other regions is not always deleted before pointees, an inconsistency will surface and cause dangling pointers, which may lead to either crashes or leaks. This paper presents a static analysis tool RegionWiz that can find such lifetime inconsistencies in large C programs using regions. The tool is based on an analysis framework that generalizes the relations and constraints over regions and objects as conditional correlations. This framework allows a succinct formalization of consistency rules for region lifetimes, preserving memory safety and avoiding dangling pointers. RegionWiz uses these consistency rules to implement an efficient static analysis to compute the conditional correlation and reason about region lifetime consistency; the analysis is based on a contextsensitive, fieldsensitive pointer analysis with heap cloning. Experiments with applying RegionWiz to six realworld software packages (including the RC compiler, Apache web server, and Subversion version control system) with two different regionbased memory management interfaces show that RegionWiz can reason about region lifetime consistency in large C programs. The experiments also show that RegionWiz can find several previously unknown inconsistency bugs in these packages.
p2327
aVMicrofluidics has enabled labonachip technology to miniaturize and integrate biological and chemical analyses to a single chip comprising channels, valves, mixers, heaters, separators, and sensors. Recent papers have proposed programmable labsonachip as an alternative to traditional applicationspecific chips to reduce design effort, time, and cost. While these previous papers provide the basic support for programmability, this paper identifies and addresses a practical issue, namely, fluid volume management. Volume management addresses the problem that the use of a fluid depletes it and unless the given volume of a fluid is distributed carefully among all its uses, execution may run out of the fluid before all its uses are complete. Additionally, fluid volumes should not overflow (i.e., exceed hardware capacity) or underflow (i.e., fall below hardware resolution). We show that the problem can be formulated as a linear programming problem (LP). Because LP's complexity and slow execution times in practice may be a concern, we propose another approach, called DAGSolve, which overconstrains the problem to achieve linear complexity while maintaining good solution quality. We also propose two optimizations, called cascading and static replication, to handle cases involving extreme mix ratios and numerous fluid uses which may defeat both LP and DAGSolve. Using some realworld assays, we show that our techniques produce good solutions while being faster than LP.
p2328
aVCurrently multithreaded C or C++ programs combine a singlethreaded programming language with a separate threads library. This is not entirely sound [7]. We describe an effort, currently nearing completion, to address these issues by explicitly providing semantics for threads in the next revision of the C++ standard. Our approach is similar to that recently followed by Java [25], in that, at least for a welldefined and interesting subset of the language, we give sequentially consistent semantics to programs that do not contain data races. Nonetheless, a number of our decisions are often surprising even to those familiar with the Java effort:We (mostly) insist on sequential consistency for racefree programs, in spite of implementation issues that came to light after the Java work. We give no semantics to programs with data races. There are no benign C++ data races. We use weaker semantics for trylock than existing languages or libraries, allowing us to promise sequential consistency with an intuitive race definition, even for programs with trylock. This paper describes the simple model we would like to be able to provide for C++ threads programmers, and explain how this, together with some practical, but often underappreciated implementation constraints, drives us towards the above decisions.
p2329
aVRecently, language extensions have been proposed for Java and C# to support patternbased reflective declaration. These extensions introduce a disciplined form of metaprogramming and aspectoriented programming to mainstream languages: They allow members of a class (i.e., fields and methods) to be declared by statically iterating over and patternmatching on members of other classes. Such techniques, however, have been unable to safely express simple, but common, idioms such as declaring getter and setter methods for fields. In this paper, we present a mechanism that addresses the lack of expressiveness in past work without sacrificing safety. Our technique is based on the idea of nested patterns that elaborate the outermost pattern with blocking or enabling conditions. We implemented this mechanism in a language, MorphJ. We demonstrate the expressiveness of MorphJ with realworld applications. In particular, the MorphJ reimplementation of DSTM2, a software transactional memory library, reduces 1,107 lines of Java reflection and bytecode engineering library calls to just 374 lines of MorphJ code. At the same time, the MorphJ solution is both high level and safer, as MorphJ can separately type check generic classes and catch errors early. We present and formalize the MorphJ type system, and offer a typechecking algorithm.
p2330
aVHighlevel loop optimizations are necessary to achieve good performance over a wide variety of processors. Their performance impact can be significant because they involve indepth program transformations that aim to sustain a balanced workload over the computational, storage, and communication resources of the target architecture. Therefore, it is mandatory that the compiler accurately models the target architecture as well as the effects of complex code restructuring. However, because optimizing compilers (1) use simplistic performance models that abstract away many of the complexities of modern architectures, (2) rely on inaccurate dependence analysis, and (3) lack frameworks to express complex interactions of transformation sequences, they typically uncover only a fraction of the peak performance available on many applications. We propose a complete iterative framework to address these issues. We rely on the polyhedral model to construct and traverse a large and expressive search space. This space encompasses only legal, distinct versions resulting from the restructuring of any static control loop nest. We first propose a feedbackdriven iterative heuristic tailored to the search space properties of the polyhedral model. Though, it quickly converges to good solutions for small kernels, larger benchmarks containing higher dimensional spaces are more challenging and our heuristic misses opportunities for significant performance improvement. Thus, we introduce the use of a genetic algorithm with specialized operators that leverage the polyhedral representation of program dependences. We provide experimental evidence that the genetic algorithm effectively traverses huge optimization spaces, achieving good performance improvements on large loop nests.
p2331
aVSoftware evolves to fix bugs and add features. Stopping and restarting programs to apply changes is inconvenient and often costly. Dynamic software updating (DSU) addresses this problem by updating programs while they execute, but existing DSU systems for managed languages do not support many updates that occur in practice and are inefficient. This paper presents the design and implementation of Jvolve, a DSUenhanced Java VM. Updated programs may add, delete, and replace fields and methods anywhere within the class hierarchy. Jvolve implements these updates by adding to and coordinating VM classloading, justintime compilation, scheduling, return barriers, onstack replacement, and garbage collection. Jvolve, is safe: its use of bytecode verification and VM thread synchronization ensures that an update will always produce typecorrect executions. Jvolve is flexible: it can support 20 of 22 updates to three opensource programs Jetty web server, JavaEmailServer, and CrossFTP server based on actual releases occurring over 1 to 2 years. Jvolve is efficient: performance experiments show that incurs no overhead during steadystate execution. These results demonstrate that this work is a significant step towards practical support for dynamic updates in virtual machines for managed languages.
p2332
aVWe present a novel dynamic analysis technique that finds real deadlocks in multithreaded programs. Our technique runs in two stages. In the first stage, we use an imprecise dynamic analysis technique to find potential deadlocks in a multithreaded program by observing an execution of the program. In the second stage, we control a random thread scheduler to create the potential deadlocks with high probability. Unlike other dynamic analysis techniques, our approach has the advantage that it does not give any false warnings. We have implemented the technique in a prototype tool for Java, and have experimented on a number of large multithreaded Java programs. We report a number of previously known and unknown real deadlocks that were found in these benchmarks.
p2333
aV\u005cbegin{abstract} Multithreaded programs are notoriously prone to race conditions. Prior work on dynamic race detectors includes fast but imprecise race detectors that report false alarms, as well as slow but precise race detectors that never report false alarms. The latter typically use expensive vector clock operations that require time linear in the number of program threads. This paper exploits the insight that the full generality of vector clocks is unnecessary in most cases. That is, we can replace heavyweight vector clocks with an adaptive lightweight representation that, for almost all operations of the target program, requires only constant space and supports constanttime operations. This representation change significantly improves time and space performance, with no loss in precision. Experimental results on Java benchmarks including the Eclipse development environment show that our FastTrack race detector is an order of magnitude faster than a traditional vectorclock race detector, and roughly twice as fast as the highperformance DJIT+ algorithm. FastTrack is even comparable in speed to Eraser on our Java benchmarks, while never reporting false alarms.
p2334
aVData races are one of the most common and subtle causes of pernicious concurrency bugs. Static techniques for preventing data races are overly conservative and do not scale well to large programs. Past research has produced several dynamic data race detectors that can be applied to large programs. They are precise in the sense that they only report actual data races. However, dynamic data race detectors incur a high performance overhead, slowing down a program's execution by an order of magnitude. In this paper we present LiteRace, a very lightweight data race detector that samples and analyzes only selected portions of a program's execution. We show that it is possible to sample a multithreaded program at a low frequency, and yet, find infrequently occurring data races. We implemented LiteRace using Microsoft's Phoenix compiler. Our experiments with several Microsoft programs, Apache, and Firefox show that LiteRace is able to find more than 70% of data races by sampling less than 2% of memory accesses in a given program execution.
p2335
aVParallel platforms are becoming ubiquitous with modern computing systems. Many parallel applications attempt to avoid locks in order to achieve high responsiveness, aid scalability, and avoid deadlocks and livelocks. However, avoiding the use of system locks does not guarantee that no locks are actually used, because progress inhibitors may occur in subtle ways through various program structures. Notions of progress guarantee such as lockfreedom, waitfreedom, and obstructionfreedom have been proposed in the literature to provide various levels of progress guarantees. In this paper we formalize the notions of progress guarantees using linear temporal logic (LTL). We concentrate on lockfreedom and propose a variant of it denoted bounded lockfreedom, which is more suitable for guaranteeing progress in practical systems. We use this formal definition to build a tool that checks if a concurrent program is bounded lockfree for a given bound. We then study the interaction between programs with progress guarantees and the underlying system (e.g., compilers, runtimes, operating systems, and hardware platforms). We propose a means to argue that an underlying system supports lockfreedom. A composition theorem asserts that bounded lockfree algorithms running on bounded lockfree supporting systems retain bounded lockfreedom for the composed execution.
p2336
aVTransactional memory (TM) is an appealing abstraction for programming multicore systems. Potential target applications for TM, such as business software and video games, are likely to involve complex data structures and large transactions, requiring specific software solutions (STM). So far, however, STMs have been mainly evaluated and optimized for smaller scale benchmarks. We revisit the main STM design choices from the perspective of complex workloads and propose a new STM, which we call SwissTM. In short, SwissTM is lock and wordbased and uses (1) optimistic (committime) conflict detection for read/write conflicts and pessimistic (encountertime) conflict detection for write/write conflicts, as well as (2) a new twophase contention manager that ensures the progress of long transactions while inducing no overhead on short ones. SwissTM outperforms stateoftheart STM implementations, namely RSTM, TL2, and TinySTM, in our experiments on STMBench7, STAMP, LeeTM and redblack tree benchmarks. Beyond SwissTM, we present the most complete evaluation to date of the individual impact of various STM design choices on the ability to support the mixed workloads of large applications.
p2337
aVMulticore designs have emerged as the mainstream design paradigm for the microprocessor industry. Unfortunately, providing multiple cores does not directly translate into performance for most applications. The industry has already fallen short of the decadesold performance trend of doubling performance every 18 months. An attractive approach for exploiting multiple cores is to rely on tools, both compilers and runtime optimizers, to automatically extract threads from sequential applications. However, despite decades of research on automatic parallelization, most techniques are only effective in the scientific and data parallel domains where array dominated codes can be precisely analyzed by the compiler. Threadlevel speculation offers the opportunity to expand parallelization to generalpurpose programs, but at the cost of expensive hardware support. In this paper, we focus on providing lowoverhead software support for exploiting speculative parallelism. We propose STMlite, a lightweight software transactional memory model that is customized to facilitate profileguided automatic loop parallelization. STMlite eliminates a considerable amount of checking and locking overhead in conventional software transactional memory models by decoupling the commit phase from main transaction execution. Further, strong atomicity requirements for generic transactional memories are unnecessary within a stylized automatic parallelization framework. STMlite enables sequential applications to extract meaningful performance gains on commodity multicore hardware.
p2338
aVCompilerbased autoparallelization is a much studied area, yet has still not found widespread application. This is largely due to the poor exploitation of application parallelism, subsequently resulting in performance levels far below those which a skilled expert programmer could achieve. We have identified two weaknesses in traditional parallelizing compilers and propose a novel, integrated approach, resulting in significant performance improvements of the generated parallel code. Using profiledriven parallelism detection we overcome the limitations of static analysis, enabling us to identify more application parallelism and only rely on the user for final approval. In addition, we replace the traditional targetspecific and inflexible mapping heuristics with a machinelearning based prediction mechanism, resulting in better mapping decisions while providing more scope for adaptation to different target architectures. We have evaluated our parallelization strategy against the NAS and SPEC OMP benchmarks and two different multicore platforms (dual quadcore Intel Xeon SMP and dualsocket QS20 Cell blade). We demonstrate that our approach not only yields significant improvements when compared with stateoftheart parallelizing compilers, but comes close to and sometimes exceeds the performance of manually parallelized codes. On average, our methodology achieves 96% of the performance of the handtuned OpenMP NAS and SPEC parallel benchmarks on the Intel Xeon platform and gains a significant speedup for the IBM Cell platform, demonstrating the potential of profileguided and machinelearning based parallelization for complex multicore platforms.
p2339
aVReasoning about string variables, in particular program inputs, is an important aspect of many program analyses and testing frameworks. Program inputs invariably arrive as strings, and are often manipulated using highlevel string operations such as equality checks, regular expression matching, and string concatenation. It is difficult to reason about these operations because they are not wellintegrated into current constraint solvers. We present a decision procedure that solves systems of equations over regular language variables. Given such a system of constraints, our algorithm finds satisfying assignments for the variables in the system. We define this problem formally and render a mechanized correctness proof of the core of the algorithm. We evaluate its scalability and practical utility by applying it to the problem of automatically finding inputs that cause SQL injection vulnerabilities.
p2340
aVThere is an increasing interest in extensible languages, (domainspecific) language extensions, and mechanisms for their specification and implementation. One challenge is to develop tools that allow nonexpert programmers to add an eclectic set of language extensions to a host language. We describe mechanisms for composing and analyzing concrete syntax specifications of a host language and extensions to it. These specifications consist of contextfree grammars with each terminal symbol mapped to a regular expression, from which a slightlymodified LR parser and contextaware scanner are generated. Traditionally, conflicts are detected when a parser is generated from the composed grammar, but this comes too late since it is the nonexpert programmer directing the composition of independently developed extensions with the host language. The primary contribution of this paper is a modular analysis that is performed independently by each extension designer on her extension (composed alone with the host language). If each extension passes this modular analysis, then the language composed later by the programmer will compile with no conflicts or lexical ambiguities. Thus, extension writers can verify that their extension will safely compose with others and, if not, fix the specification so that it will. This is possible due to the contextaware scanner's lexical disambiguation and a set of reasonable restrictions limiting the constructs that can be introduced by an extension. The restrictions ensure that the parse table states can be partitioned so that each state can be attributed to the host language or a single extension.
p2341
aVWe show that recursive programs where variables range over finite domains can be effectively and efficiently analyzed by describing the analysis algorithm using a formula in a fixedpoint calculus. In contrast with programming in traditional languages, a fixedpoint calculus serves as a highlevel programming language to easily, correctly, and succinctly describe modelchecking algorithms While there have been declarative highlevel formalisms that have been proposed earlier for analysis problems (e.g., Datalog the fixedpoint calculus we propose has the salient feature that it also allows algorithmic aspects to be specified. We exhibit two classes of algorithms of symbolic (BDDbased) algorithms written using this framework  one for checking for errors in sequential recursive Boolean programs, and the other to check for errors reachable within a bounded number of contextswitches in a concurrent recursive Boolean program. Our formalization of these otherwise complex algorithms is extremely simple, and spans just a page of fixedpoint formulae. Moreover, we implement these algorithms in a tool called Getafix which expresses algorithms as fixedpoint formulae and evaluates them efficiently using a symbolic fixedpoint solver called Mucke. The resulting modelchecking tools are surprisingly efficient and are competitive in performance with mature existing tools that have been finetuned for these problems.
p2342
aVMany dynamic updating systems have been developed that enable a program to be patched while it runs, to fix bugs or add new features. This paper explores techniques for supporting dynamic updates to multithreaded programs, focusing on the problem of applying an update in a timely fashion while still producing correct behavior. Past work has shown that this tension of safety versus timeliness can be balanced for singlethreaded programs. For multithreaded programs, the task is more difficult because myriad thread interactions complicate understanding the possible program states to which a patch could be applied. Our approach allows the programmer to specify a few program points (e.g., one per thread) at which a patch may be applied, which simplifies reasoning about safety. To improve timeliness, a combination of static analysis and runtime support automatically expands these few points to many more that produce behavior equivalent to the originals. Experiments with thirteen realistic updates to three multithreaded servers show that we can safely perform a dynamic update within milliseconds when more straightforward alternatives would delay some updates indefinitely.
p2343
aVWe address the problem of automatically generating invariants with quantified and boolean structure for proving the validity of given assertions or generating preconditions under which the assertions are valid. We present three novel algorithms, having different strengths, that combine template and predicate abstraction based formalisms to discover required sophisticated program invariants using SMT solvers. Two of these algorithms use an iterative approach to compute fixedpoints (one computes a least fixedpoint and the other computes a greatest fixedpoint), while the third algorithm uses a constraint based approach to encode the fixedpoint. The key idea in all these algorithms is to reduce the problem of invariant discovery to that of finding optimal solutions for unknowns (over conjunctions of some predicates from a given set) in a template formula such that the formula is valid. Preliminary experiments using our implementation of these algorithms show encouraging results over a benchmark of small but complicated programs. Our algorithms can verify program properties that, to our knowledge, have not been automatically verified before. In particular, our algorithms can generate full correctness proofs for sorting algorithms (which requires nested universallyexistentially quantified invariants) and can also generate preconditions required to establish worstcase upper bounds of sorting algorithms. Furthermore, for the case of previously considered properties, in particular sortedness in sorting algorithms, our algorithms take less time than reported by previous techniques.
p2344
aVThis paper introduces GC assertions, a system interface that programmers can use to check for errors, such as data structure invariant violations, and to diagnose performance problems, such as memory leaks. GC assertions are checked by the garbage collector, which is in a unique position to gather information and answer questions about the lifetime and connectivity of objects in the heap. By piggybacking on existing garbage collector computations, our system is able to check heap properties with very low overhead   around 3% of total execution time   low enough for use in a deployed setting. We introduce several kinds of GC assertions and describe how they are implemented in the collector. We also describe our reporting mechanism, which provides a complete path through the heap to the offending objects. We report results on both the performance of our system and the experience of using our assertions to find and repair errors in realworld programs.
p2345
aVThe serious bugs and security vulnerabilities facilitated by C/C++'s lack of bounds checking are well known, yet C and C++ remain in widespread use. Unfortunately, C's arbitrary pointer arithmetic, conflation of pointers and arrays, and programmervisible memory layout make retrofitting C/C++ with spatial safety guarantees extremely challenging. Existing approaches suffer from incompleteness, have high runtime overhead, or require nontrivial changes to the C source code. Thus far, these deficiencies have prevented widespread adoption of such techniques. This paper proposes SoftBound, a compiletime transformation for enforcing spatial safety of C. Inspired by HardBound, a previously proposed hardwareassisted approach, SoftBound similarly records base and bound information for every pointer as disjoint metadata. This decoupling enables SoftBound to provide spatial safety without requiring changes to C source code. Unlike HardBound, SoftBound is a softwareonly approach and performs metadata manipulation only when loading or storing pointer values. A formal proof shows that this is sufficient to provide spatial safety even in the presence of arbitrary casts. SoftBound's full checking mode provides complete spatial violation detection with 67% runtime overhead on average. To further reduce overheads, SoftBound has a storeonly checking mode that successfully detects all the security vulnerabilities in a test suite at the cost of only 22% runtime overhead on average.
p2346
aVThis paper describes a completely memorysafe compiler for C language programs that is fully compatible with the ANSI C specification. Programs written in C often suffer from nasty errors due to dangling pointers and buffer overflow. Such errors in Internet server programs are often exploited by malicious attackers to crack an entire system. The origin of these errors is usually corruption of inmemory data structures caused by outofbound array accesses. Usual C compilers do not provide any protection against such outofbound access, although many other languages such as Java and ML do provide such protection. There have been several proposals for preventing such memory corruption from various aspects: runtime buffer overrun detectors, designs for new Clike languages, and compilers for (subsets of) the C language. However, as far as we know, none of them have achieved full memory protection and full compatibility with the C language specification at the same time. We propose the most powerful solution to this problem ever presented. We have developed FailSafe C, a memorysafe implementation of the full ANSI C language. It detects and disallows all unsafe operations, yet conforms to the full ANSI C standard (including casts and unions). This paper introduces several techniques both compiletime and runtime to reduce the overhead of runtime checks, while still maintaining 100% memory safety. This compiler lets programmers easily make their programs safe without heavy rewriting or porting of their code. It also supports many of the "dirty tricks" commonly used in many existing C programs, which do not strictly conform to the standard specification. In this paper, we demonstrate several realworld server programs that can be processed by our compiler and present technical details and benchmark results for it.
p2347
aVUnchecked errors are especially pernicious in operating system file management code. Transient or permanent hardware failures are inevitable, and errormanagement bugs at the file system layer can cause silent, unrecoverable data corruption. We propose an interprocedural static analysis that tracks errors as they propagate through file system code. Our implementation detects overwritten, outofscope, and unsaved unchecked errors. Analysis of four widelyused Linux file system implementations (CIFS, ext3, IBM JFS and ReiserFS), a relatively new file system implementation (ext4), and shared virtual file system (VFS) code uncovers 312 error propagation bugs. Our flow and contextsensitive approach produces more precise results than related techniques while providing better diagnostic information, including possible execution paths that demonstrate each bug found.
p2348
aVClass sharing is a new language mechanism for building extensible software systems. Recent work has separately explored two different kinds of extensibility: first, family inheritance, in which an entire family of related classes can be inherited, and second, adaptation, in which existing objects are extended in place with new behavior and state. Class sharing integrates these two kinds of extensibility mechanisms. With little programmer effort, objects of one family can be used as members of another, while preserving relationships among objects. Therefore, a family of classes can be adapted in place with new functionality spanning multiple classes. Object graphs can evolve from one family to another, adding or removing functionality even at run time. Several new mechanisms support this flexibility while ensuring type safety. Class sharing has been implemented as an extension to Java, and its utility for evolving and extending software is demonstrated with realistic systems.
p2349
aVSelfrepresentation   the ability to represent programs in their own language   has important applications in reflective languages and many other domains of programming language design. Although approaches to designing typed program representations for sublanguages of some base language have become quite popular recently, the question whether a fully metacircular typed selfrepresentation is possible is still open. This paper makes a big step towards this aim by defining the F\u03c9* calculus, an extension of the higherorder polymorphic lambda calculus F\u03c9 that allows typed selfrepresentations. While the usability of these representations for metaprogramming is still limited, we believe that our approach makes a significant step towards a new generation of reflective languages that are both safe and efficient.
p2350
aVWe present a refinement typebased approach for the static verification of complex data structure invariants. Our approach is based on the observation that complex data structures are typically fashioned from two elements: recursion (e.g., lists and trees), and maps (e.g., arrays and hash tables). We introduce two novel typebased mechanisms targeted towards these elements: recursive refinements and polymorphic refinements. These mechanisms automate the challenging work of generalizing and instantiating rich universal invariants by piggybacking simple refinement predicates on top of types, and carefully dividing the labor of analysis between the type system and an SMT solver. Further, the mechanisms permit the use of the abstract interpretation framework of liquid type inference to automatically synthesize complex invariants from simple logical qualifiers, thereby almost completely automating the verification. We have implemented our approach in dsolve, which uses liquid types to verify ocaml programs. We present experiments that show that our typebased approach reduces the manual annotation required to verify complex properties like sortedness, balancedness, binarysearchordering, and acyclicity by more than an order of magnitude.
p2351
aVTranslation validation establishes a posteriori the correctness of a run of a compilation pass or other program transformation. In this paper, we develop an efficient translation validation algorithm for the Lazy Code Motion (LCM) optimization. LCM is an interesting challenge for validation because it is a global optimization that moves code across loops. Consequently, care must be taken not to move computations that may fail before loops that may not terminate. Our validator includes a specific check for anticipability to rule out such incorrect moves. We present a mechanicallychecked proof of correctness of the validation algorithm, using the Coq proof assistant. Combining our validator with an unverified implementation of LCM, we obtain a LCM pass that is provably semanticspreserving and was integrated in the CompCert formally verified compiler.
p2352
aVTranslation validation is a technique for checking that, after an optimization has run, the input and output of the optimization are equivalent. Traditionally, translation validation has been used to prove concrete, fully specified programs equivalent. In this paper we present Parameterized Equivalence Checking (PEC), a generalization of translation validation that can prove the equivalence of parameterized programs. A parameterized program is a partially specified program that can represent multiple concrete programs. For example, a parameterized program may contain a section of code whose only known property is that it does not modify certain variables. By proving parameterized programs equivalent, PEC can prove the correctness of transformation rules that represent complex optimizations once and for all, before they are ever run. We implemented our PEC technique in a tool that can establish the equivalence of two parameterized programs. To highlight the power of PEC, we designed a language for implementing complex optimizations using manytomany rewrite rules, and used this language to implement a variety of optimizations including software pipelining, loop unrolling, loop unswitching, loop interchange, and loop fusion. Finally, to demonstrate the effectiveness of PEC, we used our PEC implementation to verify that all the optimizations we implemented in our language preserve program behavior.
p2353
aVSelfadjusting computation offers a languagecentric approach to writing programs that can automatically respond to modifications to their data (e.g., inputs). Except for several domainspecific implementations, however, all previous implementations of selfadjusting computation assume mostly functional, higherorder languages such as Standard ML. Prior to this work, it was not known if selfadjusting computation can be made to work with lowlevel, imperative languages such as C without placing undue burden on the programmer. We describe the design and implementation of CEAL: a Cbased language for selfadjusting computation. The language is fully general and extends C with a small number of primitives to enable writing selfadjusting programs in a style similar to conventional C programs. We present efficient compilation techniques for translating CEAL programs into C that can be compiled with existing C compilers using primitives supplied by a runtime library for selfadjusting computation. We implement the proposed compiler and evaluate its effectiveness. Our experiments show that CEAL is effective in practice: compiled selfadjusting programs respond to small modifications to their data by orders of magnitude faster than recomputing from scratch while slowing down a fromscratch run by a moderate constant factor. Compared to previous work, we measure significant space and time improvements.
p2354
aVWe present an integrated proof language for guiding the actions of multiple reasoning systems as they work together to prove complex correctness properties of imperative programs. The language operates in the context of a program verification system that uses multiple reasoning systems to discharge generated proof obligations. It is designed to 1) enable developers to resolve key choice points in complex program correctness proofs, thereby enabling automated reasoning systems to successfully prove the desired correctness properties; 2) allow developers to identify key lemmas for the reasoning systems to prove, thereby guiding the reasoning systems to find an effective proof decomposition; 3) enable multiple reasoning systems to work together productively to prove a single correctness property by providing a mechanism that developers can use to divide the property into lemmas, each of which is suitable for a different reasoning system; and 4) enable developers to identify specific lemmas that the reasoning systems should use when attempting to prove other lemmas or correctness properties, thereby appropriately confining the search space so that the reasoning systems can find a proof in an acceptable amount of time. The language includes a rich set of declarative proof constructs that enables developers to direct the reasoning systems as little or as much as they desire. Because the declarative proof statements are embedded into the program as specialized comments, they also serve as verified documentation and are a natural extension of the assertion mechanism found in most program verification systems. We have implemented our integrated proof language in the context of a program verification system for Java and used the resulting system to verify a collection of linked data structure implementations. Our experience indicates that our proof language makes it possible to successfully prove complex program correctness properties that are otherwise beyond the reach of automated reasoning systems.
p2355
aVHighlevel languages are growing in popularity. However, decades of C software development have produced large libraries of fast, timetested, meritorious code that are impractical to recreate from scratch. Crosslanguage bindings can expose lowlevel C code to highlevel languages. Unfortunately, writing bindings by hand is tedious and errorprone, while mainstream binding generators require extensive manual annotation or fail to offer the language features that users of modern languages have come to expect. We present an improved bindinggeneration strategy based on static analysis of unannotated library source code. We characterize three highlevel idioms that are not uniquely expressible in C's lowlevel type system: array parameters, resource managers, and multiple return values. We describe a suite of interprocedural analyses that recover this highlevel information, and we show how the results can be used in a binding generator for the Python programming language. In experiments with four large C libraries, we find that our approach avoids the mistakes characteristic of handwritten bindings while offering a level of Python integration unmatched by prior automated approaches. Among the thousands of functions in the public interfaces of these libraries, roughly 40% exhibit the behaviors detected by our static analyses.
p2356
aVSymbolic analysis shows promise as a foundation for bugfinding, specification inference, verification, and test generation. This paper addresses demanddriven symbolic analysis for objectoriented programs and frameworks. Many such codes comprise large, partial programs with highly dynamic behaviors polymorphism, reflection, and so on posing significant scalability challenges for any static analysis. We present an approach based on interprocedural backwards propagation of weakest preconditions. We present several novel techniques to improve the efficiency of such analysis. First, we present directed call graph construction, where call graph construction and symbolic analysis are interleaved. With this technique, call graph construction is guided by constraints discovered during symbolic analysis, obviating the need for exhaustively exploring a large, conservative call graph. Second, we describe generalization, a technique that greatly increases the reusability of procedure summaries computed during interprocedural analysis. Instead of tabulating how a procedure transforms a symbolic state in its entirety, our technique tabulates how the procedure transforms only the pertinent portion of the symbolic state. Additionally, we show how integrating an inexpensive, custom logic simplifier with weakest precondition computation dramatically improves performance. We have implemented the analysis in a tool called Snugglebug and evaluated it as a bugreport feasibility checker. Our results show that the algorithmic techniques were critical for successfully analyzing large Java applications.
p2357
aVSymbolic complexity bounds help programmers understand the performance characteristics of their implementations. Existing work provides techniques for statically determining bounds of procedures with simple controlflow. However, procedures with nested loops or multiple paths through a single loop are challenging. In this paper we describe two techniques, controlflow refinement and progress invariants, that together enable estimation of precise bounds for procedures with nested and multipath loops. Controlflow refinement transforms a multipath loop into a semantically equivalent code fragment with simpler loops by making the structure of path interleaving explicit. We show that this enables nondisjunctive invariant generation tools to find a bound on many procedures for which previous techniques were unable to prove termination. Progress invariants characterize relationships between consecutive states that can arise at a program location. We further present an algorithm that uses progress invariants to compute precise bounds for nested loops. The utility of these two techniques goes beyond our application to symbolic bound analysis. In particular, we discuss applications of controlflow refinement to proving safety properties that otherwise require disjunctive invariants. We have applied our methodology to over 670,000 lines of code of a significant Microsoft product and were able to find symbolic bounds for 90% of the loops. We are not aware of any other published results that report experiences running a bound analysis on a real codebase.
p2358
aVMore and more server workloads are becoming Webbased. In these Webbased workloads, most of the memory objects are used only during one transaction. We study the effect of the memory management approaches on the performance of such Webbased applications on two modern multicore processors. In particular, using six PHP applications, we compare a generalpurpose allocator (the default allocator of the PHP runtime) and a regionbased allocator, which can reduce the cost of memory management by not supporting perobject free. The regionbased allocator achieves better performance for all workloads on one processor core due to its smaller memory management cost. However, when using eight cores, the regionbased allocator suffers from hidden costs of increased bus traffics and the performance is reduced for many workloads by as much as 27.2% compared to the default allocator. This is because the memory bandwidth tends to become a bottleneck in systems with multicore processors. We propose a new memory management approach, defragdodging, to maximize the performance of the Webbased workloads on multicore processors. In our approach, we reduce the memory management cost by avoiding defragmentation overhead in the malloc and free functions during a transaction. We found that the transactions in Webbased applications are short enough to ignore heap fragmentation, and hence the costs of the defragmentation activities in existing generalpurpose allocators outweigh their benefits. By comparing our approach against the regionbased approach, we show that a perobject free capability can reduce bus traffic and achieve higher performance on multicore processors. We demonstrate that our defragdodging approach improves the performance of all the evaluated applications on both processors by up to 11.4% and 51.5% over the default allocator and the regionbased allocator, respectively.
p2359
aVInefficient use of memory, including leaks and bloat, remain a significant challenge for C and C++ developers. Applications with these problems become slower over time as their working set grows and can become unresponsive. At the same time, memory leaks and bloat remain notoriously difficult to debug, and comprise a large number of reported bugs in mature applications. Previous tools for diagnosing memory inefficienciesbased on garbage collection, binary rewriting, or code samplingimpose high overheads (up to 100X) or generate many false alarms. This paper presents Hound, a runtime system that helps track down the sources of memory leaks and bloat in C and C++ applications. Hound employs data sampling, a stalenesstracking approach based on a novel heap organization, to make it both precise and efficient. Hound has no false positives, and its runtime and space overhead are low enough that it can be used in deployed applications. We demonstrate Hound's efficacy across a suite of synthetic benchmarks and real applications.
p2360
aVLanguages such as Java and C#, as well as scripting languages like Python, and Ruby, make extensive use of Collection classes. A collection implementation represents a fixed choice in the dimensions of operation time, space utilization, and synchronization. Using the collection in a manner not consistent with this fixed choice can cause significant performance degradation. In this paper, we present CHAMELEON, a lowoverhead automatic tool that assists the programmer in choosing the appropriate collection implementation for her application. During program execution, CHAMELEON computes elaborate trace and heapbased metrics on collection behavior. These metrics are consumed onthefly by a rules engine which outputs a list of suggested collection adaptation strategies. The tool can apply these corrective strategies automatically or present them to the programmer. We have implemented CHAMELEON on top of a IBM's J9 production JVM, and evaluated it over a small set of benchmarks. We show that for some applications, using CHAMELEON leads to a significant improvement of the memory footprint of the application.
p2361
aVMany largescale Java applications suffer from runtime bloat. They execute large volumes of methods, and create many temporary objects, all to execute relatively simple operations. There are large opportunities for performance optimizations in these applications, but most are being missed by existing optimization and tooling technology. While JIT optimizations struggle for a few percent, performance experts analyze deployed applications and regularly find gains of 2x or more. Finding such big gains is difficult, for both humans and compilers, because of the diffuse nature of runtime bloat. Time is spread thinly across calling contexts, making it difficult to judge how to improve performance. Bloat results from a pileup of seemingly harmless decisions. Each adds temporary objects and method calls, and often copies values between those temporary objects. While data copies are not the entirety of bloat, we have observed that they are excellent indicators of regions of excessive activity. By optimizing copies, one is likely to remove the objects that carry copied values, and the method calls that allocate and populate them. We introduce copy profiling, a technique that summarizes runtime activity in terms of chains of data copies. A flat copy profile counts copies by method. We show how flat profiles alone can be helpful. In many cases, diagnosing a problem requires data flow context. Tracking and making sense of raw copy chains does not scale, so we introduce a summarizing abstraction called the copy graph. We implement three clients analyses that, using the copy graph, expose common patterns of bloat, such as finding hot copy chains and discovering temporary data structures. We demonstrate, with examples from a largescale commercial application and several benchmarks, that copy profiling can be used by a programmer to quickly find opportunities for large performance gains.
p2362
aVThe client computing platform is moving towards a heterogeneous architecture consisting of a combination of cores focused on scalar performance, and a set of throughputoriented cores. The throughput oriented cores (e.g. a GPU) may be connected over both coherent and noncoherent interconnects, and have different ISAs. This paper describes a programming model for such heterogeneous platforms. We discuss the language constructs, runtime implementation, and the memory model for such a programming environment. We implemented this programming environment in a x86 heterogeneous platform simulator. We ported a number of workloads to our programming environment, and present the performance of our programming environment on these workloads.
p2363
aVModern programs frequently employ sophisticated modular designs. As a result, performance problems cannot be identified from costs attributed to routines in isolation; understanding code performance requires information about a routine's calling context. Existing performance tools fall short in this respect. Prior strategies for attributing contextsensitive performance at the source level either compromise measurement accuracy, remain too close to the binary, or require custom compilers. To understand the performance of fully optimized modular code, we developed two novel binary analysis techniques: 1) onthefly analysis of optimized machine code to enable minimally intrusive and accurate attribution of costs to dynamic calling contexts; and 2) postmortem analysis of optimized machine code and its debugging sections to recover its program structure and reconstruct a mapping back to its source code. By combining the recovered static program structure with dynamic calling context information, we can accurately attribute performance metrics to calling contexts, procedures, loops, and inlined instances of procedures. We demonstrate that the fusion of this information provides unique insight into the performance of complex modular codes. This work is implemented in the HPCToolkit performance tools (http://hpctoolkit.org).
p2364
aVIt is often impossible to obtain a onesizefitsall solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarsegrained, library level selections or hand coded cutoffs between base cases and recursive cases. We present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice a first class construct of the language. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both finegrained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking. Additionally, we introduce novel techniques to autotune algorithms for different convergence criteria. When choosing between various direct and iterative methods, the PetaBricks compiler is able to tune a program in such a way that delivers nearoptimal efficiency for any desired level of accuracy. The compiler has the flexibility of utilizing different convergence criteria for the various components within a single algorithm, providing the user with accuracy choice alongside algorithmic choice.
p2365
aVAs computer systems continue to become more powerful and complex, so do programs. Highlevel abstractions introduced to deal with complexity in large programs, while simplifying human reasoning, can often obfuscate salient program properties gleaned from automated sourcelevel analysis through subtle (often nonlocal) interactions. Consequently, understanding the effects of program changes and whether these changes violate intended protocols become difficult to infer. Refactorings, and feature additions, modifications, or removals can introduce hardtocatch bugs that often go undetected until many revisions later. To address these issues, this paper presents a novel dynamic program analysis that builds a semantic view of program executions. These views reflect program abstractions and aspects; however, views are not simply projections of execution traces, but are linked to each other to capture semantic interactions among abstractions at different levels of granularity in a scalable manner. We describe our approach in the context of Java and demonstrate its utility to improve regression analysis. We first formalize a subset of Java and a grammar for traces generated at program execution. We then introduce several types of views used to analyze regression bugs along with a novel, scalable technique for semantic differencing of traces from different versions of the same program. Benchmark results on large opensource Java programs demonstrate that semanticaware trace differencing can identify precise and useful details about the underlying cause for a regression, even in programs that use reflection, multithreading, or dynamic code generation, features that typically confound other analysis techniques.
p2366
aVModern websites are powered by JavaScript, a flexible dynamic scripting language that executes in client browsers. A common paradigm in such websites is to include thirdparty JavaScript code in the form of libraries or advertisements. If this code were malicious, it could read sensitive information from the page or write to the location bar, thus redirecting the user to a malicious page, from which the entire machine could be compromised. We present an informationflow based approach for inferring the effects that a piece of JavaScript has on the website in order to ensure that key security properties are not violated. To handle dynamically loaded and generated JavaScript, we propose a framework for staging information flow properties. Our framework propagates information flow through the currently known code in order to compute a minimal set of syntactic residual checks that are performed on the remaining code when it is dynamically loaded. We have implemented a prototype framework for staging information flow. We describe our techniques for handling some difficult features of JavaScript and evaluate our system's performance on a variety of large realworld websites. Our experiments show that static information flow is feasible and efficient for JavaScript, and that our technique allows the enforcement of informationflow policies with almost no runtime overhead.
p2367
aVDecentralized information flow control (DIFC) is a promising model for writing programs with powerful, endtoend security guarantees. Current DIFC systems that run on commodity hardware can be broadly categorized into two types: languagelevel and operating systemlevel DIFC. Language level solutions provide no guarantees against security violations on system resources, like files and sockets. Operating system solutions can mediate accesses to system resources, but are inefficient at monitoring the flow of information through finegrained program data structures. This paper describes Laminar, the first system to implement decentralized information flow control using a single set of abstractions for OS resources and heapallocated objects. Programmers express security policies by labeling data with secrecy and integrity labels, and then access the labeled data in lexically scoped security regions. Laminar enforces the security policies specified by the labels at runtime. Laminar is implemented using a modified Java virtual machine and a new Linux security module. This paper shows that security regions ease incremental deployment and limit dynamic security checks, allowing us to retrofit DIFC policies on four application case studies. Replacing the applications' adhoc security policies changes less than 10% of the code, and incurs performance overheads from 1% to 56%. Whereas prior DIFC systems only support limited types of multithreaded programs, Laminar supports a more general class of multithreaded DIFC programs that can access heterogeneously labeled data.
p2368
aVThe last several years have seen a proliferation of static and runtime analysis tools for finding security violations that are caused by explicit information flow in programs. Much of this interest has been caused by the increase in the number of vulnerabilities such as crosssite scripting and SQL injection. In fact, these explicit information flow vulnerabilities commonly found in Web applications now outnumber vulnerabilities such as buffer overruns common in typeunsafe languages such as C and C++. Tools checking for these vulnerabilities require a specification to operate. In most cases the task of providing such a specification is delegated to the user. Moreover, the efficacy of these tools is only as good as the specification. Unfortunately, writing a comprehensive specification presents a major challenge: parts of the specification are easy to miss, leading to missed vulnerabilities; similarly, incorrect specifications may lead to false positives. This paper proposes Merlin, a new approach for automatically inferring explicit information flow specifications from program code. Such specifications greatly reduce manual labor, and enhance the quality of results, while using tools that check for security violations caused by explicit information flow. Beginning with a data propagation graph, which represents interprocedural flow of information in the program, Merlin aims to automatically infer an information flow specification. Merlin models information flow paths in the propagation graph using probabilistic constraints. A naive modeling requires an exponential number of constraints, one per path in the propagation graph. For scalability, we approximate these path constraints using constraints on chosen triples of nodes, resulting in a cubic number of constraints. We characterize this approximation as a probabilistic abstraction, using the theory of probabilistic refinement developed by McIver and Morgan. We solve the resulting system of probabilistic constraints using factor graphs, which are a wellknown structure for performing probabilistic inference. We experimentally validate the Merlin approach by applying it to 10 large businesscritical Web applications that have been analyzed with CAT.NET, a stateoftheart static analysis tool for .NET. We find a total of 167 new confirmed specifications, which result in a total of 322 additional vulnerabilities across the 10 benchmarks. More accurate specifications also reduce the false positive rate: in our experiments, Merlininferred specifications result in 13 false positives being removed; this constitutes a 15% reduction in the CAT.NET false positive rate on these 10 programs. The final false positive rate for CAT.NET after applying Merlin in our experiments drops to under 1%.
p2369
aVTaint analysis, a form of informationflow analysis, establishes whether values from untrusted methods and parameters may flow into securitysensitive operations. Taint analysis can detect many common vulnerabilities in Web applications, and so has attracted much attention from both the research community and industry. However, most static taintanalysis tools do not address critical requirements for an industrialstrength tool. Specifically, an industrialstrength tool must scale to large industrial Web applications, model essential Webapplication code artifacts, and generate consumable reports for a wide range of attack vectors. We have designed and implemented a static Taint Analysis for Java (TAJ) that meets the requirements of industrylevel applications. TAJ can analyze applications of virtually any size, as it employs a set of techniques designed to produce useful answers given limited time and space. TAJ addresses a wide variety of attack vectors, with techniques to handle reflective calls, flow through containers, nested taint, and issues in generating useful reports. This paper provides a description of the algorithms comprising TAJ, evaluates TAJ against productionlevel benchmarks, and compares it with alternative solutions.
p2370
aVSharC is a recently developed system for checking datasharing in multithreaded programs. Programmers specify sharing rules (readonly, protected by a lock, etc.) for individual objects, and the SharC compiler enforces these rules using static and dynamic checks. Violations of these rules indicate unintended data sharing, which is the underlying cause of harmful dataraces. Additionally, SharC allows programmers to change the sharing rules for a specific object using a sharing cast, to capture the fact that sharing rules for an object often change during the object's lifetime. SharC was successfully applied to a number of multithreaded C programs. However, many programs are not readily checkable using SharC because their sharing rules, and changes to sharing rules, effectively apply to whole data structures rather than to individual objects. We have developed a system called Shoal to address this shortcoming. In addition to the sharing rules and sharing cast of SharC, our system includes a new concept that we call groups. A group is a collection of objects all having the same sharing mode. Each group has a distinguished member called the group leader. When the sharing mode of the group leader changes by way of a sharing cast, the sharing mode of all members of the group also changes. This operation is made sound by maintaining the invariant that at the point of a sharing cast, the only external pointer into the group is the pointer to the group leader. The addition of groups allows checking safe concurrency at the level of data structures rather than at the level of individual objects. We demonstrate the necessity and practicality of groups by applying Shoal to a wide range of concurrent C programs (the largest approaching a million lines of code). In all benchmarks groups entail low annotation burden and no significant additional performance overhead.
p2371
aVThe JavaScript programming language is widely used for web programming and, increasingly, for general purpose computing. As such, improving the correctness, security and performance of JavaScript applications has been the driving force for research in type systems, static analysis and compiler techniques for this language. Many of these techniques aim to reign in some of the most dynamic features of the language, yet little seems to be known about how programmers actually utilize the language or these features. In this paper we perform an empirical study of the dynamic behavior of a corpus of widelyused JavaScript programs, and analyze how and why the dynamic features are used. We report on the degree of dynamism that is exhibited by these JavaScript programs and compare that with assumptions commonly made in the literature and accepted industry benchmark suites.
p2372
aVTyped assembly language (TAL) and Hoare logic can verify the absence of many kinds of errors in lowlevel code. We use TAL and Hoare logic to achieve highly automated, static verification of the safety of a new operating system called Verve. Our techniques and tools mechanically verify the safety of every assembly language instruction in the operating system, runtime system, drivers, and applications (in fact, every part of the system software except the boot loader). Verve consists of a "Nucleus" that provides primitive access to hardware and memory, a kernel that builds services on top of the Nucleus, and applications that run on top of the kernel. The Nucleus, written in verified assembly language, implements allocation, garbage collection, multiple stacks, interrupt handling, and device access. The kernel, written in C# and compiled to TAL, builds higherlevel services, such as preemptive threads, on top of the Nucleus. A TAL checker verifies the safety of the kernel and applications. A Hoarestyle verifier with an automated theorem prover verifies both the safety and correctness of the Nucleus. Verve is, to the best of our knowledge, the first operating system mechanically verified to guarantee both type and memory safety. More generally, Verve's approach demonstrates a practical way to mix highlevel typed code with lowlevel untyped code in a verifiably safe manner.
p2373
aVVerified compilers, such as Leroy's CompCert, are accompanied by a fully checked correctness proof. Both the compiler and proof are often constructed with an interactive proof assistant. This technique provides a strong, endtoend correctness guarantee on top of a small trusted computing base. Unfortunately, these compilers are also challenging to extend since each additional transformation must be proven correct in full formal detail. At the other end of the spectrum, techniques for compiler correctness based on a domainspecific language for writing optimizations, such as Lerner's Rhodium and Cobalt, make the compiler easy to extend: the correctness of additional transformations can be checked completely automatically. Unfortunately, these systems provide a weaker guarantee since their endtoend correctness has not been proven fully formally. We present an approach for compiler correctness that provides the best of both worlds by bridging the gap between compiler verification and compiler extensibility. In particular, we have extended Leroy's CompCert compiler with an execution engine for optimizations written in a domain specific and proved that this execution engine preserves program semantics, using the Coq proof assistant. We present our CompCert extension, XCert, including the details of its execution engine and proof of correctness in Coq. Furthermore, we report on the important lessons learned for making the proof development manageable.
p2374
aVDependent types provide a strong foundation for specifying and verifying rich properties of programs through typechecking. The earliest implementations combined dependency, which allows types to mention program variables; with typelevel computation, which facilitates expressive specifications that compute with recursive functions over types. While many recent applications of dependent types omit the latter facility, we argue in this paper that it deserves more attention, even when implemented without dependency. In particular, the ability to use functional programs as specifications enables staticallytyped metaprogramming: programs write programs, and static typechecking guarantees that the generating process never produces invalid code. Since our focus is on generic validity properties rather than full correctness verification, it is possible to engineer type inference systems that are very effective in narrow domains. As a demonstration, we present Ur, a programming language designed to facilitate metaprogramming with firstclass records and names. On top of Ur, we implement Ur/Web, a special standard library that enables the development of modern Web applications. Adhoc code generation is already in wide use in the popular Web application frameworks, and we show how that generation may be tamed using types, without forcing metaprogram authors to write proofs or forcing metaprogram users to write any fancy types.
p2375
aVWe describe an automatic verification method to check whether transactional memories ensure strict serializability a key property assumed of the transactional interface. Our main contribution is a technique for effectively verifying parameterized systems. The technique merges ideas from parameterized hardware and protocol verification verification by invisible invariants and symmetry reduction with ideas from software verification templatebased invariant generation and satisfiability checking for quantified formul (modulo theories). The combination enables us to precisely model and analyze unbounded systems while taming state explosion. Our technique enables automated proofs that twophase locking (TPL), dynamic software transactional memory (DSTM), and transactional locking II (TL2) systems ensure strict serializability. The verification is challenging since the systems are unbounded in several dimensions: the number and length of concurrently executing transactions, and the size of the shared memory they access, have no finite limit. In contrast, stateoftheart software model checking tools such as BLAST and TVLA are unable to validate either system, due to inherent expressiveness limitations or state explosion.
p2376
aVManaged languages such as Java and C# are being considered for use in hard realtime systems. A hurdle to their widespread adoption is the lack of garbage collection algorithms that offer predictable spaceandtime performance in the face of fragmentation. We introduce SCHISM/CMR, a new concurrent and realtime garbage collector that is fragmentation tolerant and guarantees timeandspace worstcase bounds while providing good throughput. SCHISM/CMR combines markregion collection of fragmented objects and arrays (arraylets) with separate replicationcopying collection of immutable arraylet spines, so as to cope with external fragmentation when running in small heaps. We present an implementation of SCHISM/CMR in the Fiji VM, a highperformance Java virtual machine for missioncritical systems, along with a thorough experimental evaluation on a wide variety of architectures, including serverclass and embedded systems. The results show that SCHISM/CMR tolerates fragmentation better than previous schemes, with a much more acceptable throughput penalty.
p2377
aVRuntime bloat degrades significantly the performance and scalability of software systems. An important source of bloat is the inefficient use of containers. It is expensive to create inefficientlyused containers and to invoke their associated methods, as this may ultimately execute large volumes of code, with call stacks dozens deep, and allocate many temporary objects. This paper presents practical static and dynamic tools that can find inappropriate use of containers in Java programs. At the core of these tools is a base static analysis that identifies, for each container, the objects that are added to this container and the key statements (i.e., heap loads and stores) that achieve the semantics of common container operations such as ADD and GET. The static tool finds problematic uses of containers by considering the nesting relationships among the loops where these semanticsachieving statements are located, while the dynamic tool can instrument these statements and find inefficiencies by profiling their execution frequencies. The high precision of the base analysis is achieved by taking advantage of a contextfree language (CFL)reachability formulation of pointsto analysis and by accounting for containerspecific properties. It is demanddriven and clientdriven, facilitating refinement specific to each queried container object and increasing scalability. The tools built with the help of this analysis can be used both to avoid the creation of containerrelated performance problems early during development, and to help with diagnosis when problems are observed during tuning. Our experimental results show that the static tool has a low false positive rate and produces more relevant information than its dynamic counterpart. Further case studies suggest that significant optimization opportunities can be found by focusing on staticallyidentified containers for which high allocation frequency is observed at run time.
p2378
aVMany opportunities for easy, bigwin, program optimizations are missed by compilers. This is especially true in highly layered Java applications. Often at the heart of these missed optimization opportunities lie computations that, with great expense, produce data values that have little impact on the program's final output. Constructing a new date formatter to format every date, or populating a large set full of expensively constructed structures only to check its size: these involve costs that are out of line with the benefits gained. This disparity between the formation costs and accrued benefits of data structures is at the heart of much runtime bloat. We introduce a runtime analysis to discover these lowutility data structures. The analysis employs dynamic thin slicing, which naturally associates costs with value flows rather than raw data flows. It constructs a model of the incremental, hoptohop, costs and benefits of each data structure. The analysis then identifies suspicious structures based on imbalances of its incremental costs and benefits. To decrease the memory requirements of slicing, we introduce abstract dynamic thin slicing, which performs thin slicing over bounded abstract domains. We have modified the IBM J9 commercial JVM to implement this approach. We demonstrate two client analyses: one that finds objects that are expensive to construct but are not necessary for the forward execution, and second that pinpoints ultimatelydead values. We have successfully applied them to largescale and longrunning Java applications. We show that these analyses are effective at detecting operations that have unbalanced costs and benefits.
p2379
aVPerformance analysts profile their programs to find methods that are worth optimizing: the "hot" methods. This paper shows that four commonlyused Java profilers (xprof , hprof , jprofile, and yourkit) often disagree on the identity of the hot methods. If two profilers disagree, at least one must be incorrect. Thus, there is a good chance that a profiler will mislead a performance analyst into wasting time optimizing a cold method with little or no performance improvement. This paper uses causality analysis to evaluate profilers and to gain insight into the source of their incorrectness. It shows that these profilers all violate a fundamental requirement for sampling based profilers: to be correct, a samplingbased profilermust collect samples randomly. We show that a proofofconcept profiler, which collects samples randomly, does not suffer from the above problems. Specifically, we show, using a number of case studies, that our profiler correctly identifies methods that are important to optimize; in some cases other profilers report that these methods are cold and thus not worth optimizing.
p2380
aVEnergyefficient computing is important in several systems ranging from embedded devices to large scale data centers. Several application domains offer the opportunity to tradeoff quality of service/solution (QoS) for improvements in performance and reduction in energy consumption. Programmers sometimes take advantage of such opportunities, albeit in an adhoc manner and often without providing any QoS guarantees. We propose a system called Green that provides a simple and flexible framework that allows programmers to take advantage of such approximation opportunities in a systematic manner while providing statistical QoS guarantees. Green enables programmers to approximate expensive functions and loops and operates in two phases. In the calibration phase, it builds a model of the QoS loss produced by the approximation. This model is used in the operational phase to make approximation decisions based on the QoS constraints specified by the programmer. The operational phase also includes an adaptation function that occasionally monitors the runtime behavior and changes the approximation decisions and QoS model to provide strong statistical QoS guarantees. To evaluate the effectiveness of Green, we implemented our system and language extensions using the Phoenix compiler framework. Our experiments using benchmarks from domains such as graphics, machine learning, signal processing, and finance, and an inproduction, realworld web search engine, indicate that Green can produce significant improvements in performance and energy consumption with small and controlled QoS degradation.
p2381
aVWe present a new programming model GUEESSTIMATE for developing collaborative distributed systems. The model allows atomic, isolated operations that transform a system from consistent state to consistent state, and provides a shared transactional store for a collection of such operations executed by various machines in a distributed system. In addition to "committed state" which is identical in all machines in the distributed system, GUESSTIMATE allows each machine to have a replicated local copy of the state (called "guesstimated state") so that operations on shared state can be executed locally without any blocking, while also guaranteeing that eventually all machines agree on the sequences of operations executed. Thus, each operation is executed multiple times, once at the time of issue when it updates the guesstimated state of the issuing machine, once when the operation is committed (atomically) to the committed state of all machines, and several times in between as the guesstimated state converges toward the committed state. While we expect the results of these executions of the operation to be identical most of the time in the class of applications we study, it is possible for an operation to succeed the first time when it is executed on the guesstimated state, and fail when it is committed. GUESSTIMATE provides facilities that allow the programmer to deal with this potential discrepancy. This paper presents our programming model, its operational semantics, its realization as an API in C#, and our experience building collaborative distributed applications with this model.
p2382
aVCalling context the set of active methods on the stack is critical for understanding the dynamic behavior of large programs. Dynamic program analysis tools, however, are almost exclusively context insensitive because of the prohibitive cost of representing calling contexts at run time. Deployable dynamic analyses, in particular, have been limited to reporting only static program locations. This paper presents Breadcrumbs, an efficient technique for recording and reporting dynamic calling contexts. It builds on an existing technique for computing a compact (one word) encoding of each calling context that client analyses can use in place of a program location. The key feature of our system is a search algorithm that can reconstruct a calling context from its encoding using only a static call graph and a small amount of dynamic information collected at cold (infrequently executed) callsites. Breadcrumbs requires no offline training or program modifications, and handles all language features, including dynamic class loading. We use Breadcrumbs to add context sensitivity to two dynamic analyses: a datarace detector and an analysis for diagnosing null pointer exceptions. On average, it adds 10% to 20% runtime overhead, depending on a tunable parameter that controls how much dynamic information is collected. Collecting less information lowers the overhead, but can result in a search space explosion. In some cases this causes reconstruction to fail, but in most cases Breadcrumbs >produces nontrivial calling contexts that have the potential to significantly improve both the precision of the analyses and the quality of the bug reports.
p2383
aVAn ad hoc data format is any nonstandard, semistructured data format for which robust data processing tools are not easily available. In this paper, we present ANNE, a new kind of markup language designed to help users generate documentation and data processing tools for ad hoc text data. More specifically, given a new ad hoc data source, an ANNE programmer edits the document to add a number of simple annotations, which serve to specify its syntactic structure. Annotations include elements that specify constants, optional data, alternatives, enumerations, sequences, tabular data, and recursive patterns. The ANNE system uses a combination of user annotations and the raw data itself to extract a contextfree grammar from the document. This contextfree grammar can then be used to parse the data and transform it into an XML parse tree, which may be viewed through a browser for analysis or debugging purposes. In addition, the ANNE system generates a PADS/ML description, which may be saved as lasting documentation of the data format or compiled into a host of useful data processing tools. In addition to designing and implementing ANNE, we have devised a semantic theory for the core elements of the language. This semantic theory describes the editing process, which translates a raw, unannotated text document into an annotated document, and the grammar extraction process, which generates a contextfree grammar from an annotated document. We also present an alternative characterization of system behavior by drawing upon ideas from the field of relevance logic. This secondary characterization, which we call relevance analysis, specifies a direct relationship between unannotated documents and the contextfree grammars that our system can generate from them. Relevance analysis allows us to prove important theorems concerning the expressiveness and utility of our system.
p2384
aVWe present algorithms for accurately converting floatingpoint numbers to decimal representation. They are fast (up to 4 times faster than commonly used algorithms that use highprecision integers) and correct: any printed number will evaluate to the same number, when read again. Our algorithms are fast, because they require only fixedsize integer arithmetic. The sole requirement for the integer type is that it has at least two more bits than the significand of the floatingpoint number. Hence, for IEEE 754 doubleprecision numbers (having a 53bit significand) an integer type with 55 bits is sufficient. Moreover we show how to exploit additional bits to improve the generated output. We present three algorithms with different properties: the first algorithm is the most basic one, and does not take advantage of any extra bits. It simply shows how to perform the binarytodecimal transformation with the minimal number of bits. Our second algorithm improves on the first one by using the additional bits to produce a shorter (often the shortest) result. Finally we propose a third version that can be used when the shortest output is a requirement. The last algorithm either produces optimal decimal representations (with respect to shortness and rounding) or rejects its input. For IEEE 754 doubleprecision numbers and 64bit integers roughly 99.4% of all numbers can be processed efficiently. The remaining 0.6% are rejected and need to be printed by a slower complete algorithm.
p2385
aVMultithreaded programs are notoriously prone to race conditions, a problem exacerbated by the widespread adoption of multicore processors with complex memory models and cache coherence protocols. Much prior work has focused on static and dynamic analyses for race detection, but these algorithms typically are unable to distinguish destructive races that cause erroneous behavior from benign races that do not. Performing this classification manually is difficult, time consuming, and error prone. This paper presents a new dynamic analysis technique that uses adversarial memory to classify race conditions as destructive or benign on systems with relaxed memory models. Unlike a typical language implementation, which may only infrequently exhibit nonsequentially consistent behavior, our adversarial memory implementation exploits the full freedom of the memory model to return older, unexpected, or stale values for memory reads whenever possible, in an attempt to crash the target program (that is, to force the program to behave erroneously). A crashing execution provides concrete evidence of a destructive bug, and this bug can be strongly correlated with a specific race condition in the target program. Experimental results with our Jumble prototype for Java demonstrate that adversarial memory is highly effective at identifying destructive race conditions, and in distinguishing them from race conditions that are real but benign. Adversarial memory can also reveal destructive races that would not be detected by traditional testing (even after thousands of runs) or by model checkers that assume sequential consistency.
p2386
aVData races indicate serious concurrency bugs such as order, atomicity, and sequential consistency violations. Races are difficult to find and fix, often manifesting only after deployment. The frequency and unpredictability of these bugs will only increase as software adds parallelism to exploit multicore hardware. Unfortunately, sound and precise race detectors slow programs by factors of eight or more and do not scale to large numbers of threads. This paper presents a precise, lowoverhead samplingbased data race detector called Pacer. PACER makes a proportionality guarantee: it detects any race at a rate equal to the sampling rate, by finding races whose first access occurs during a global sampling period. During sampling, PACER tracks all accesses using the dynamically sound and precise FastTrack algorithm. In nonsampling periods, Pacer discards sampled access information that cannot be part of a reported race, and Pacer simplifies tracking of the happensbefore relationship, yielding nearconstant, instead of linear, overheads. Experimental results confirm our theoretical guarantees. PACER reports races in proportion to the sampling rate. Its time and space overheads scale with the sampling rate, and sampling rates of 13% yield overheads low enough to consider in production software. The resulting system provides a "get what you pay for" approach that is suitable for identifying real, hardtoreproduce races in deployed systems.
p2387
aVIt is not uncommon in parallel workloads to encounter shared data structures with readmostly access patterns, where operations that update data are infrequent and most operations are readonly. Typically, data consistency is guaranteed using mutual exclusion or readwrite locks. The cost of atomic update of lock variables result in high overheads and high cache coherence traffic under active sharing, thus slowing down single thread performance and limiting scalability. In this paper, we present SOLERO (Software Optimistic Lock Elision for ReadOnly critical sections), a new lock implementation called for optimizing readonly critical sections in Java based on sequential locks. SOLERO is compatible with the conventional lock implementation of Java. However, unlike the conventional implementation, only critical sections that may write data or have side effects need to update lock variables, while readonly critical sections need only read lock variables without writing them. Each writing critical section changes the lock value to a new value. Hence, a readonly critical section is guaranteed to be consistent if the lock is free and its value does not change from the beginning to the end of the readonly critical section. Using Java workloads including SPECjbb2005 and the HashMap and TreeMap Java classes, we evaluate the performance impact of applying SOLERO to readmostly locks. Our experimental results show performance improvements across the board, often substantial, in both single thread speed and scalability over the conventional lock implementation (mutual exclusion) and readwrite locks. SOLERO improves the performance of SPECjbb2005 by 35% on single and multiple threads. The results using the HashMap and TreeMap benchmarks show that SOLERO outperforms the conventional lock implementation and readwrite locks by substantial multiples on multithreads.
p2388
aVWe present smooth interpretation, a method to systematically approximate numerical imperative programs by smooth mathematical functions. This approximation facilitates the use of numerical search techniques like gradient descent for program analysis and synthesis. The method extends to programs the notion of Gaussian smoothing, a popular signalprocessing technique that filters out noise and discontinuities from a signal by taking its convolution with a Gaussian function. In our setting, Gaussian smoothing executes a program according to a probabilistic semantics; the execution of program P on an input x after Gaussian smoothing can be summarized as follows: (1) Apply a Gaussian perturbation to x   the perturbed input is a random variable following a normal distribution with mean x. (2) Compute and return the expected output of P on this perturbed input. Computing the expectation explicitly would require the execution of P on all possible inputs, but smooth interpretation bypasses this requirement by using a form of symbolic execution to approximate the effect of Gaussian smoothing on P. The result is an efficient but approximate implementation of Gaussian smoothing of programs. Smooth interpretation has the effect of attenuating features of a program that impede numerical searches of its input space   for example, discontinuities resulting from conditional branches are replaced by continuous transitions. We apply smooth interpretation to the problem of synthesizing values of numerical control parameters in embedded control applications. This problem is naturally formulated as one of numerical optimization: the goal is to find parameter values that minimize the error between the resulting program and a programmerprovided behavioral specification. Solving this problem by directly applying numerical optimization techniques is often impractical due to the discontinuities in the error function. By eliminating these discontinuities, smooth interpretation makes it possible to search the parameter space efficiently by means of simple gradient descent. Our experiments demonstrate the value of this strategy in synthesizing parameters for several challenging programs, including models of an automated gear shift and a PID controller.
p2389
aVWe define the reachabilitybound problem to be the problem of finding a symbolic worstcase bound on the number of times a given control location inside a procedure is visited in terms of the inputs to that procedure. This has applications in bounding resources consumed by a program such as time, memory, networktraffic, power, as well as estimating quantitative properties (as opposed to boolean properties) of data in programs, such as information leakage or uncertainty propagation. Our approach to solving the reachabilitybound problem brings together two different techniques for reasoning about loops in an effective manner. One of these techniques is an abstractinterpretation based iterative technique for computing precise disjunctive invariants (to summarize nested loops). The other technique is a noniterative proofrules based technique (for loop bound computation) that takes over the role of doing inductive reasoning, while deriving its power from the use of SMT solvers to reason about abstract loopfree fragments. Our solution to the reachabilitybound problem allows us to compute precise symbolic complexity bounds for several loops in .Net baseclass libraries for which earlier techniques fail. We also illustrate the precision of our algorithm for disjunctive invariant computation (which has a more general applicability beyond the reachabilitybound problem) on a set of benchmark examples.
p2390
aVLowlevel program analysis is a fundamental problem, taking the shape of "flow analysis" in functional languages and "pointsto" analysis in imperative and objectoriented languages. Despite the similarities, the vocabulary and results in the two communities remain largely distinct, with limited crossunderstanding. One of the few links is Shivers's kCFA work, which has advanced the concept of "contextsensitive analysis" and is widely known in both communities. Recent results indicate that the relationship between the functional and objectoriented incarnations of kCFA is not as well understood as thought. Van Horn and Mairson proved kCFA for k \u2265 1 to be EXPTIMEcomplete; hence, no polynomialtime algorithm can exist. Yet, there are several polynomialtime formulations of contextsensitive pointsto analyses in objectoriented languages. Thus, it seems that functional kCFA may actually be a profoundly different analysis from objectoriented kCFA. We resolve this paradox by showing that the exact same specification of kCFA is polynomialtime for objectoriented languages yet exponentialtime for functional ones: objects and closures are subtly different, in a way that interacts crucially with contextsensitivity and complexity. This illumination leads to an immediate payoff: by projecting the objectoriented treatment of objects onto closures, we derive a polynomialtime hierarchy of contextsensitive CFAs for functional programs.
p2391
aVSynthesis of program fragments from specifications can make programs easier to write and easier to reason about. To integrate synthesis into programming languages, synthesis algorithms should behave in a predictable way  they should succeed for a welldefined class of specifications. They should also support unbounded data types such as numbers and data structures. We propose to generalize decision procedures into predictable and complete synthesis procedures. Such procedures are guaranteed to find code that satisfies the specification if such code exists. Moreover, we identify conditions under which synthesis will statically decide whether the solution is guaranteed to exist, and whether it is unique. We demonstrate our approach by starting from decision procedures for linear arithmetic and data structures and transforming them into synthesis procedures. We establish results on the size and the efficiency of the synthesized code. We show that such procedures are useful as a language extension with implicit value definitions, and we show how to extend a compiler to support such definitions. Our constructs provide the benefits of synthesis to programmers, without requiring them to learn new concepts or give up a deterministic execution model.
p2392
aVModular development of concurrent applications requires threadsafe components that behave correctly when called concurrently by multiple client threads. This paper focuses on linearizability, a specific formalization of thread safety, where all operations of a concurrent component appear to take effect instantaneously at some point between their call and return. The key insight of this paper is that if a component is intended to be deterministic, then it is possible to build an automatic linearizability checker by systematically enumerating the sequential behaviors of the component and then checking if each its concurrent behavior is equivalent to some sequential behavior. We develop this insight into a tool called LineUp, the first complete and automatic checker for deterministic linearizability. It is complete, because any reported violation proves that the implementation is not linearizable with respect to any sequential deterministic specification. It is automatic, requiring no manual abstraction, no manual specification of semantics or commit points, no manually written test suites, no access to source code. We evaluate LineUp by analyzing 13 classes with a total of 90 methods in two versions of the .NET Framework 4.0. The violations of deterministic linearizability reported by LineUp exposed seven errors in the implementation that were fixed by the development team.
p2393
aVDynamic correctness checking tools (a.k.a. lifeguards) can detect a wide array of correctness issues, such as memory, security, and concurrency misbehavior, in unmodified executables at run time. However, lifeguards that are implemented using dynamic binary instrumentation (DBI) often slow down the monitored application by 1050X, while proposals that replace DBI with hardware still see 38X slowdowns. The remaining overhead is the cost of performing the lifeguard analysis itself. In this paper, we explore compiler optimization techniques to reduce this overhead. The lifeguard software is typically structured as a set of eventdriven handlers, where the events are individual instructions in the monitored application's dynamic instruction stream. We propose to decouple the lifeguard checking code from the application that it is monitoring so that the lifeguard analysis can be invoked at the granularity of hot paths in the monitored application. In this way, we are able to find many more opportunities for eliminating redundant work in the lifeguard analysis, even starting with welloptimized applications and handtuned lifeguard handlers. Experimental results with two lifeguard frameworks  one DBIbased and one hardwareassisted  show significant reduction in monitoring overhead.
p2394
aVMemory models are hard to reason about due to their complexity, which stems from the need to strike a balance between easeofprogramming and allowing compiler and hardware optimizations. In this paper, we present an automated tool, MemSAT, that helps in debugging and reasoning about memory models. Given an axiomatic specification of a memory model and a multithreaded test program containing assertions, MemSAT outputs a trace of the program in which both the assertions and the memory model axioms are satisfied, if one can be found. The tool is fully automatic and is based on a SAT solver. If it cannot find a trace, it outputs a minimal subset of the memory model and program constraints that are unsatisfiable. We used MemSAT to check several existing memory models against their published test cases, including the current Java Memory Model by Manson et al. and a revised version of it by Sevcik and Aspinall. We found subtle discrepancies between what was expected and the actual results of test programs.
p2395
aVThe most intuitive memory model for sharedmemory multithreaded programming is sequential consistency(SC), but it disallows the use of many compiler and hardware optimizations thereby impacting performance. Dataracefree (DRF) models, such as the proposed C++0x memory model, guarantee SC execution for dataracefree programs. But these models provide no guarantee at all for racy programs, compromising the safety and debuggability of such programs. To address the safety issue, the Java memory model, which is also based on the DRF model, provides a weak semantics for racy executions. However, this semantics is subtle and complex, making it difficult for programmers to reason about their programs and for compiler writers to ensure the correctness of compiler optimizations. We present the DRFx memory model, which is simple for programmers to understand and use while still supporting many common optimizations. We introduce a memory model (MM) exception which can be signaled to halt execution. If a program executes without throwing this exception, then DRFx guarantees that the execution is SC. If a program throws an MM exception during an execution, then DRFx guarantees that the program has a data race. We observe that SC violations can be detected in hardware through a lightweight form of conflict detection. Furthermore, our model safely allows aggressive compiler and hardware optimizations within compilerdesignated program regions. We formalize our memory model, prove several properties about this model, describe a compiler and hardware design suitable for DRFx, and evaluate the performance overhead due to our compiler and hardware requirements.
p2396
aVMapReduce and similar systems significantly ease the task of writing dataparallel code. However, many realworld computations require a pipeline of MapReduces, and programming and managing such pipelines can be difficult. We present FlumeJava, a Java library that makes it easy to develop, test, and run efficient dataparallel pipelines. At the core of the FlumeJava library are a couple of classes that represent immutable parallel collections, each supporting a modest number of operations for processing them in parallel. Parallel collections and their operations present a simple, highlevel, uniform abstraction over different data representations and execution strategies. To enable parallel operations to run efficiently, FlumeJava defers their evaluation, instead internally constructing an execution plan dataflow graph. When the final results of the parallel operations are eventually needed, FlumeJava first optimizes the execution plan, and then executes the optimized operations on appropriate underlying primitives (e.g., MapReduces). The combination of highlevel abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easytouse system that approaches the efficiency of handoptimized pipelines. FlumeJava is in active use by hundreds of pipeline developers within Google.
p2397
aVApplications composed of multiple parallel libraries perform poorly when those libraries interfere with one another by obliviously using the same physical cores, leading to destructive resource oversubscription. This paper presents the design and implementation of Lithe, a lowlevel substrate that provides the basic primitives and a standard interface for composing parallel codes efficiently. Lithe can be inserted underneath the runtimes of legacy parallel libraries to provide bolton composability without needing to change existing application code. Lithe can also serve as the foundation for building new parallel abstractions and libraries that automatically interoperate with one another. In this paper, we show versions of Threading Building Blocks (TBB) and OpenMP perform competitively with their original implementations when ported to Lithe. Furthermore, for two applications composed of multiple parallel libraries, we show that leveraging our substrate outperforms their original, even expertly tuned, implementations.
p2398
aVTraditional dataoriented programming languages such as dataflow languages and stream languages provide a natural abstraction for parallel programming. In these languages, a developer focuses on the flow of data through the computation and these systems free the developer from the complexities of lowlevel, threadoriented concurrency primitives. This simplification comes at a cost   traditional dataoriented approaches restrict the mutation of state and, in practice, the types of data structures a program can effectively use. Bamboo borrows from work in typestate and software transactions to relax the traditional restrictions of dataoriented programming models to support mutation of arbitrary data structures. We have implemented a compiler for Bamboo which generates code for the TILEPro64 manycore processor. We have evaluated this implementation on six benchmarks: Tracking, a feature tracking algorithm from computer vision; KMeans, a Kmeans clustering algorithm; MonteCarlo, a Monte Carlo simulation; FilterBank, a multichannel filter bank; Fractal, a Mandelbrot set computation; and Series, a Fourier series computation. We found that our compiler generated implementations that obtained speedups ranging from 26.2x to 61.6x when executed on 62 cores.
p2399
aVMultistage programming (MSP) provides a disciplined approach to runtime code generation. In the purely functional setting, it has been shown how MSP can be used to reduce the overhead of abstractions, allowing clean, maintainable code without paying performance penalties. Unfortunately, MSP is difficult to combine with imperative features, which are prevalent in mainstream languages. The central difficulty is scope extrusion, wherein free variables can inadvertently be moved outside the scopes of their binders. This paper proposes a new approach to combining MSP with imperative features that occupies a "sweet spot" in the design space in terms of how well useful MSP applications can be expressed and how easy it is for programmers to understand. The key insight is that escapes (or "antiquotes") must be weakly separable from the rest of the code, i.e. the computational effects occurring inside an escape that are visible outside the escape are guaranteed to not contain code. To demonstrate the feasibility of this approach, we formalize a type system based on Lightweight Java which we prove sound, and we also provide an implementation, called Mint, to validate both the expressivity of the type system and the effect of staging on the performance of Java programs.
p2400
aVA number of programming languages use rich type systems to verify security properties of code. Some of these languages are meant for source programming, but programs written in these languages are compiled without explicit security proofs, limiting their utility in settings where proofs are necessary, e.g., proofcarrying authorization. Others languages do include explicit proofs, but these are generally lambda calculi not intended for source programming, that must be further compiled to an executable form. A language suitable for source programming backed by a compiler that enables endtoend verification is missing. In this paper, we present a typepreserving compiler that translates programs written in FINE, a sourcelevel functional language with dependent refinements and affine types, to DCIL, a new extension of the .NET Common Intermediate Language. FINE is type checked using an external SMT solver to reduce the proof burden on source programmers. We extract explicit LCFstyle proof terms from the solver and carry these proof terms in the compilation to DCIL, thereby removing the solver from the trusted computing base. Explicit proofs enable DCIL to be used in a number of important scenarios, including the verification of mobile code, proofcarrying authorization, and evidencebased auditing. We report on our experience using FINE to build reference monitors for several applications, ranging from a pluginbased email client to a conference management server.
p2401
aVA certifying compiler preserves type information through compilation to assembly language programs, producing typed assembly language (TAL) programs that can be verified for safety independently so that the compiler does not need to be trusted. There are two challenges for adopting certifying compilation in practice. First, requiring every compiler transformation and optimization to preserve types is a large burden on compilers, especially when adopting certifying compilation into existing optimizing noncertifying compilers. Second, type annotations significantly increase the size of assembly language programs. This paper proposes an alternative to traditional certifying compilers. It presents iTalX, the first inferable TAL type system that supports existential types, arrays, interfaces, and stacks. We have proved our inference algorithm is complete, meaning if an assembly language program is typeable with iTalX then our algorithm will infer an iTalX typing for that program. Furthermore, our algorithm is guaranteed to terminate even if the assembly language program is untypeable. We demonstrate that it is practical to infer such an expressive TAL by showing a prototype implementation of type inference for code compiled by Bartok, an optimizing C# compiler. Our prototype implementation infers complete type annotations for 98% of functions in a suite of realistic C# benchmarks. The typeinference time is about 8% of the compilation time. We needed to change only 2.5% of the compiler code, mostly adding new code for defining types and for writing types to object files. Most transformations are untouched. Typeannotation size is only 17% of the size of pure code and data, reducing type annotations in our previous certifying compiler [4] by 60%. The compiler needs to preserve only essential type information such as method signatures, objectlayout information, and types for static data and external labels. Even noncertifying compilers have most of this information available.
p2402
aVStatic analysis designers must carefully balance precision and efficiency. In our experience, many static analysis tools are built around an elegant, core algorithm, but that algorithm is then extensively tweaked to add just enough precision for the coding idioms seen in practice, without sacrificing too much efficiency. There are several downsides to adding precision in this way: the tool's implementation becomes much more complicated; it can be hard for an enduser to interpret the tool's results; and as software systems vary tremendously in their coding styles, it may require significant algorithmic engineering to enhance a tool to perform well in a particular software domain. In this paper, we present Mix, a novel system that mixes type checking and symbolic execution. The key aspect of our approach is that these analyses are applied independently on disjoint parts of the program, in an offtheshelf manner. At the boundaries between nested type checked and symbolically executed code regions, we use special mix rules to communicate information between the offtheshelf systems. The resulting mixture is a provably sound analysis that is more precise than type checking alone and more efficient than exclusive symbolic execution. In addition, we also describe a prototype implementation, Mixy, for C. Mixy checks for potential null dereferences by mixing a null/nonnull type qualifier inference system with a symbolic executor.
p2403
aVWhile iterative optimization has become a popular compiler optimization approach, it is based on a premise which has never been truly evaluated: that it is possible to learn the best compiler optimizations across data sets. Up to now, most iterative optimization studies find the best optimizations through repeated runs on the same data set. Only a handful of studies have attempted to exercise iterative optimization on a few tens of data sets. In this paper, we truly put iterative compilation to the test for the first time by evaluating its effectiveness across a large number of data sets. We therefore compose KDataSets, a data set suite with 1000 data sets for 32 programs, which we release to the public. We characterize the diversity of KDataSets, and subsequently use it to evaluate iterative optimization.We demonstrate that it is possible to derive a robust iterative optimization strategy across data sets: for all 32 programs, we find that there exists at least one combination of compiler optimizations that achieves 86% or more of the best possible speedup across all data sets using Intel's ICC (83% for GNU's GCC). This optimal combination is programspecific and yields speedups up to 1.71 on ICC and 2.23 on GCC over the highest optimization level (fast and O3, respectively). This finding makes the task of optimizing programs across data sets much easier than previously anticipated, and it paves the way for the practical and reliable usage of iterative optimization. Finally, we derive preshipping and postshipping optimization strategies for software vendors.
p2404
aVProgramming language specifications mandate static and dynamic analyses to preclude syntactic and semantic errors. Although individual languages are usually wellspecified, composing languages is not, and this poor specification is a source of many errors in multilingual programs. For example, virtually all Java programs compose Java and C using the Java Native Interface (JNI). Since JNI is informally specified, developers have difficulty using it correctly, and current Java compilers and virtual machines (VMs) inconsistently check only a subset of JNI constraints. This paper's most significant contribution is to show how to synthesize dynamic analyses from state machines to detect foreign function interface (FFI) violations. We identify three classes of FFI constraints encoded by eleven state machines that capture thousands of JNI and Python/C FFI rules. We use a mapping function to specify which state machines, transitions, and program entities (threads, objects, references) to check at each FFI call and return. From this function, we synthesize a contextspecific dynamic analysis to find FFI bugs. We build bug detection tools for JNI and Python/C using this approach. For JNI, we dynamically and transparently interpose the analysis on Java and C language transitions through the JVM tools interface. The resulting tool, called Jinn, is compiler and virtual machine independent. It detects and diagnoses a wide variety of FFI bugs that other tools miss. This approach greatly reduces the annotation burden by exploiting common FFI constraints: whereas the generated Jinn code is 22,000+ lines, we wrote only 1,400 lines of state machine and mapping code. Overall, this paper lays the foundation for a more principled approach to developing correct multilingual software and a more concise and automated approach to FFI specification.
p2405
aVSingle thread performance remains an important consideration even for multicore, multiprocessor systems. As a result, techniques for improving single thread performance using multiple cores have received considerable attention. This work describes a technique, software data spreading, that leverages the cache capacity of extra cores and extra sockets rather than their computational resources. Software data spreading is a softwareonly technique that uses compilerdirected thread migration to aggregate cache capacity across cores and chips and improve performance. This paper describes an automated scheme that applies data spreading to various types of loops. Experiments with a set of SPEC2000, SPEC2006, NAS, and microbenchmark workloads show that data spreading can provide speedup of over 2, averaging 17% for the SPEC and NAS applications on two systems. In addition, despite using more cores for the same computation, data spreading actually saves power since it reduces access to DRAM.
p2406
aVArrays are the ubiquitous organization for indexed data. Throughout programming language evolution, implementations have laid out arrays contiguously in memory. This layout is problematic in space and time. It causes heap fragmentation, garbage collection pauses in proportion to array size, and wasted memory for sparse and overprovisioned arrays. Because of array virtualization in managed languages, an array layout that consists of indirection pointers to fixedsize discontiguous memory blocks can mitigate these problems transparently. This design however incurs significant overhead, but is justified when realtime deadlines and space constraints trump performance. This paper proposes zrays, a discontiguous array design with flexibility and efficiency. A zray has a spine with indirection pointers to fixedsize memory blocks called arraylets, and uses five optimizations: (1) inlining the first N array bytes into the spine, (2) lazy allocation, (3) zero compression, (4) fast array copy, and (5) arraylet copyonwrite. Whereas discontiguous arrays in prior work improve responsiveness and space efficiency, zrays combine time efficiency and flexibility. On average, the best zray configuration performs within 12.7% of an unmodified Java Virtual Machine on 19 benchmarks, whereas previous designs have two to three times higher overheads. Furthermore, language implementers can configure zray optimizations for various design goals. This combination of performance and flexibility creates a better building block for past and future array optimization.
p2407
aVExecution order constraints imposed by dependences can serialize computation, preventing parallelization of code and algorithms. Speculating on the value(s) carried by dependences is one way to break such critical dependences. Value speculation has been used effectively at a low level, by compilers and hardware. In this paper, we focus on the use of speculation by programmers as an algorithmic paradigm to parallelize seemingly sequential code. We propose two new language constructs, speculative composition and speculative iteration. These constructs enable programmers to declaratively express speculative parallelism in programs: to indicate when and how to speculate, increasing the parallelism in the program, without concerning themselves with mundane implementation details. We present a core language with speculation constructs and mutable state and present a formal operational semantics for the language. We use the semantics to define the notion of a correct speculative execution as one that is equivalent to a nonspeculative execution. In general, speculation requires a runtime mechanism to undo the effects of speculative computation in the case of mis predictions. We describe a set of conditions under which such rollback can be avoided. We present a static analysis that checks if a given program satisfies these conditions. This allows us to implement speculation efficiently, without the overhead required for rollbacks. We have implemented the speculation constructs as a C# library, along with the static checker for safety. We present an empirical evaluation of the efficacy of this approach to parallelization.
p2408
aVThe availability of multicore processors has led to significant interest in compiler techniques for speculative parallelization of sequential programs. Isolation of speculative state from nonspeculative state forms the basis of such speculative techniques as this separation enables recovery from misspeculations. In our prior work on CorD [35,36] we showed that for array and scalar variable based programs copying of data between speculative and nonspeculative memory can be highly optimized to support state separation that yields significant speedups on multicore machines available today. However, we observe that in context of heapintensive programs that operate on linked dynamic data structures, state separation based speculative parallelization poses many challenges. The copying of data structures from nonspeculative to speculative state (copyin operation) can be very expensive due to the large sizes of dynamic data structures. The copying of updated data structures from speculative state to nonspeculative state (copyout operation) is made complex due to the changes in the shape and size of the dynamic data structure made by the speculative computation. In addition, we must contend with the need to translate pointers internal to dynamic data structures between their nonspeculative and speculative memory addresses. In this paper we develop an augmented design for the representation of dynamic data structures such that all of the above operations can be performed efficiently. Our experiments demonstrate significant speedups on a real machine for a set of programs that make extensive use of heap based dynamic data structures.
p2409
aVThe main contribution of this paper is a compiler based, cache topology aware code optimization scheme for emerging multicore systems. This scheme distributes the iterations of a loop to be executed in parallel across the cores of a target multicore machine and schedules the iterations assigned to each core. Our goal is to improve the utilization of the onchip multilayer cache hierarchy and to maximize overall application performance. We evaluate our cache topology aware approach using a set of twelve applications and three different commercial multicore machines. In addition, to study some of our experimental parameters in detail and to explore future multicore machines (with higher core counts and deeper onchip cache hierarchies), we also conduct a simulation based study. The results collected from our experiments with three Intel multicore machines show that the proposed compilerbased approach is very effective in enhancing performance. In addition, our simulation results indicate that optimizing for the onchip cache hierarchy will be even more important in future multicores with increasing numbers of cores and cache levels.
p2410
aVThis paper presents a novel optimizing compiler for general purpose computation on graphics processing units (GPGPU). It addresses two major challenges of developing high performance GPGPU programs: effective utilization of GPU memory hierarchy and judicious management of parallelism. The input to our compiler is a nave GPU kernel function, which is functionally correct but without any consideration for performance optimization. The compiler analyzes the code, identifies its memory access patterns, and generates both the optimized kernel and the kernel invocation parameters. Our optimization process includes vectorization and memory coalescing for memory bandwidth enhancement, tiling and unrolling for data reuse and parallelism management, and thread block remapping or addressoffset insertion for partitioncamping elimination. The experiments on a set of scientific and media processing algorithms show that our optimized code achieves very high performance, either superior or very close to the highly finetuned library, NVIDIA CUBLAS 2.2, and up to 128 times speedups over the naive versions. Another distinguishing feature of our compiler is the understandability of the optimized code, which is useful for performance analysis and algorithm refinement.
p2411
aVSusan Eggers, a Professor of Computer Science and Engineering at the University of Washington, joined her department in 1989. She received a B.A. in 1965 from Connecticut College and a Ph.D. in 1989 from the University of California, Berkeley. Her research interests are in computer architecture and backend compiler optimization, with an emphasis on experimental performance analysis. With her colleague Hank Levy and their students, she developed the first commercially viable multithreaded architecture, Simultaneous Multithreading, adopted by Intel (as Hyperthreading), IBM, Sun and others. Her current research is in the areas of distributed dataflow machines, FPGAs and chip multiprocessors. In 1989 Professor Eggers was awarded an IBM Faculty Development Award, in 1990 an NSF Presidential Young Investigator Award, in 1994 the Microsoft Professorship in Computer Science and Engineering, and in 2009 the ACMW Athena Lecturer. She is a Fellow of the ACM and IEEE, a Fellow of the AAAS, and a member of the National Academy of Engineering.
p2412
aVSequential programming models express a total program order, of which a partial order must be respected. This inhibits parallelizing tools from extracting scalable performance. Programmer written semantic commutativity assertions provide a natural way of relaxing this partial order, thereby exposing parallelism implicitly in a program. Existing implicit parallel programming models based on semantic commutativity either require additional programming extensions, or have limited expressiveness. This paper presents a generalized semantic commutativity based programming extension, called Commutative Set (COMMSET), and associated compiler technology that enables multiple forms of parallelism. COMMSET expressions are syntactically succinct and enable the programmer to specify commutativity relations between groups of arbitrary structured code blocks. Using only this construct, serializing constraints that inhibit parallelization can be relaxed, independent of any particular parallelization strategy or concurrency control mechanism. COMMSET enables well performing parallelizations in cases where they were inapplicable or nonperforming before. By extending eight sequential programs with only 8 annotations per program on average, COMMSET and the associated compiler technology produced a geomean speedup of 5.7x on eight cores compared to 1.5x for the best nonCOMMSET parallelization.
p2413
aVInformation flow is an important security property that must be incorporated from the ground up, including at hardware design time, to provide a formal basis for a system's root of trust. We incorporate insights and techniques from designing informationflow secure programming languages to provide a new perspective on designing secure hardware. We describe a new hardware description language, Caisson, that combines domainspecific abstractions common to hardware design with insights from typebased techniques used in secure programming languages. The proper combination of these elements allows for an expressive, provablysecure HDL that operates at a familiar level of abstraction to the target audience of the language, hardware architects. We have implemented a compiler for Caisson that translates designs into Verilog and then synthesizes the designs using existing tools. As an example of Caisson's usefulness we have addressed an open problem in secure hardware by creating the firstever provably informationflow secure processor with microarchitectural features including pipelining and cache. We synthesize the secure processor and empirically compare it in terms of chip area, power consumption, and clock frequency with both a standard (insecure) commercial processor and also a processor augmented at the gate level to dynamically track information flow. Our processor is competitive with the insecure processor and significantly better than dynamic tracking.
p2414
aVDeclarative queries enable programmers to write data manipulation code without being aware of the underlying data structure implementation. By increasing the level of abstraction over imperative code, they improve program readability and, crucially, create opportunities for automatic parallelization and optimization. For example, the Language Integrated Query (LINQ) extensions to C# allow the same declarative query to process inmemory collections, and datasets that are distributed across a compute cluster. However, our experiments show that the serial performance of declarative code is several times slower than the equivalent handoptimized code, because it is implemented using runtime abstractions such as iterators that incur overhead due to virtual function calls and superfluous instructions. To address this problem, we have developed Steno, which uses a combination of novel and wellknown techniques to generate code for declarative queries that is almost as efficient as handoptimized code. Steno translates a declarative LINQ query into typespecialized, inlined and loopbased imperative code. It eliminates chains of iterators from query execution, and optimizes nested queries. We have implemented Steno for uniprocessor, multiprocessor and distributed computing platforms, and show that, for a realworld distributed job, it can almost double the speed of endtoend execution.
p2415
aVProgramming language design benefits from constructs for extending the syntax and semantics of a host language. While C's stringbased macros empower programmers to introduce notational shorthands, the parserlevel macros of Lisp encourage experimentation with domainspecific languages. The Scheme programming language improves on Lisp with macros that respect lexical scope.  The design of Racket a descendant of Scheme goes even further with the introduction of a fullfledged interface to the static semantics of the language. A Racket extension programmer can thus add constructs that are indistinguishable from "native" notation, large and complex embedded domainspecific languages, and even optimizing transformations for the compiler backend. This power to experiment with language design has been used to create a series of sublanguages for programming with firstclass classes and modules, numerous languages for implementing the Racket system, and the creation of a complete and fully integrated typed sister language to Racket's untyped base language. This paper explains Racket's language extension API via an implementation of a small typed sister language. The new language provides a rich type system that accommodates the idioms of untyped Racket. Furthermore, modules in this typed language can safely exchange values with untyped modules. Last but not least, the implementation includes a typebased optimizer that achieves promising speedups. Although these extensions are complex, their Racket implementation is just a library, like any other library, requiring no changes to the Racket implementation.
p2416
aVThe performance benefits of GPU parallelism can be enormous, but unlocking this performance potential is challenging. The applicability and performance of GPU parallelizations is limited by the complexities of CPUGPU communication. To address these communications problems, this paper presents the first fully automatic system for managing and optimizing CPUGPU communcation. This system, called the CPUGPU Communication Manager (CGCM), consists of a runtime library and a set of compiler transformations that work together to manage and optimize CPUGPU communication without depending on the strength of static compiletime analyses or on programmersupplied annotations. CGCM eases manual GPU parallelizations and improves the applicability and performance of automatic GPU parallelizations. For 24 programs, CGCMenabled automatic GPU parallelization yields a whole program geomean speedup of 5.36x over the best sequential CPUonly execution.
p2417
aVMATLAB is an array language, initially popular for rapid prototyping, but is now being increasingly used to develop production code for numerical and scientific applications. Typical MATLAB programs have abundant data parallelism. These programs also have control flow dominated scalar regions that have an impact on the program's execution time. Today's computer systems have tremendous computing power in the form of traditional CPU cores and throughput oriented accelerators such as graphics processing units(GPUs). Thus, an approach that maps the control flow dominated regions to the CPU and the data parallel regions to the GPU can significantly improve program performance. In this paper, we present the design and implementation of MEGHA, a compiler that automatically compiles MATLAB programs to enable synergistic execution on heterogeneous processors. Our solution is fully automated and does not require programmer input for identifying data parallel regions. We propose a set of compiler optimizations tailored for MATLAB. Our compiler identifies data parallel regions of the program and composes them into kernels. The problem of combining statements into kernels is formulated as a constrained graph clustering problem. Heuristics are presented to map identified kernels to either the CPU or GPU so that kernel execution on the CPU and the GPU happens synergistically and the amount of data transfer needed is minimized. In order to ensure required data movement for dependencies across basic blocks, we propose a data flow analysis and edge splitting strategy. Thus our compiler automatically handles composition of kernels, mapping of kernels to CPU and GPU, scheduling and insertion of required data transfer. The proposed compiler was implemented and experimental evaluation using a set of MATLAB benchmarks shows that our approach achieves a geometric mean speedup of 19.8X for data parallel benchmarks over native execution of MATLAB.
p2418
aVEnergy is increasingly a firstorder concern in computer systems. Exploiting energyaccuracy tradeoffs is an attractive choice in applications that can tolerate inaccuracies. Recent work has explored exposing this tradeoff in programming models. A key challenge, though, is how to isolate parts of the program that must be precise from those that can be approximated so that a program functions correctly even as quality of service degrades. We propose using type qualifiers to declare data that may be subject to approximate computation. Using these types, the system automatically maps approximate variables to lowpower storage, uses lowpower operations, and even applies more energyefficient algorithms provided by the programmer. In addition, the system can statically guarantee isolation of the precise program component from the approximate component. This allows a programmer to control explicitly how information flows from approximate data to precise data. Importantly, employing static analysis eliminates the need for dynamic checks, further improving energy savings. As a proof of concept, we develop EnerJ, an extension to Java that adds approximate data types. We also propose a hardware architecture that offers explicit approximate storage and computation. We port several applications to EnerJ and show that our extensions are expressive and effective; a small number of annotations lead to significant potential energy savings (10%50%) at very little accuracy cost.
p2419
aVExploiting today's multiprocessors requires highperformance and correct concurrent systems code (optimising compilers, language runtimes, OS kernels, etc.), which in turn requires a good understanding of the observable processor behaviour that can be relied on. Unfortunately this critical hardware/software interface is not at all clear for several current multiprocessors. In this paper we characterise the behaviour of IBM POWER multiprocessors, which have a subtle and highly relaxed memory model (ARM multiprocessors have a very similar architecture in this respect). We have conducted extensive experiments on several generations of processors: POWER G5, 5, 6, and 7. Based on these, on published details of the microarchitectures, and on discussions with IBM staff, we give an abstractmachine semantics that abstracts from most of the implementation detail but explains the behaviour of a range of subtle examples. Our semantics is explained in prose but defined in rigorous machineprocessed mathematics; we also confirm that it captures the observable processor behaviour, or the architectural intent, for our examples with an executable checker. While not officially sanctioned by the vendor, we believe that this model gives a reasonable basis for reasoning about current POWER multiprocessors. Our work should bring new clarity to concurrent systems programming for these architectures, and is a necessary precondition for any analysis or verification. It should also inform the design of languages such as C and C++, where the language memory model is constrained by what can be efficiently compiled to such multiprocessors.
p2420
aVWe present an approach for automatic verification and fence inference in concurrent programs running under relaxed memory models. Verification under relaxed memory models is a hard problem. Given a finite state program and a safety specification, verifying that the program satisfies the specification under a sufficiently relaxed memory model is undecidable. For stronger models, the problem is decidable but has nonprimitive recursive complexity. In this paper, we focus on models that have storebuffer based semantics, e.g., SPARC TSO and PSO. We use abstract interpretation to provide an effective verification procedure for programs running under this type of models. Our main contribution is a family of novel partialcoherence abstractions, specialized for relaxed memory models, which partially preserve information required for memory coherence and consistency. We use our abstractions to automatically verify programs under relaxed memory models. In addition, when a program violates its specification but can be fixed by adding fences, our approach can automatically infer a correct fence placement that is optimal under the abstraction. We implemented our approach in a tool called BLENDER and applied it to verify and infer fences in several concurrent algorithms.
p2421
aVThe most intuitive memory consistency model for sharedmemory multithreaded programming is sequential consistency (SC). However, current concurrent programming languages support a relaxed model, as such relaxations are deemed necessary for enabling important optimizations. This paper demonstrates that an SCpreserving compiler, one that ensures that every SC behavior of a compilergenerated binary is an SC behavior of the source program, retains most of the performance benefits of an optimizing compiler. The key observation is that a large class of optimizations crucial for performance are either already SCpreserving or can be modified to preserve SC while retaining much of their effectiveness. An SCpreserving compiler, obtained by restricting the optimization phases in LLVM, a stateoftheart C/C++ compiler, incurs an average slowdown of 3.8% and a maximum slowdown of 34% on a set of 30 programs from the SPLASH2, PARSEC, and SPEC CINT2006 benchmark suites. While the performance overhead of preserving SC in the compiler is much less than previously assumed, it might still be unacceptable for certain applications. We believe there are several avenues for improving performance without giving up SCpreservation. In this vein, we observe that the overhead of our SCpreserving compiler arises mainly from its inability to aggressively perform a class of optimizations we identify as eagerload optimizations. This class includes commonsubexpression elimination, constant propagation, global value numbering, and common cases of loopinvariant code motion. We propose a notion of interference checks in order to enable eagerload optimizations while preserving SC. Interference checks expose to the compiler a commonly used hardware speculation mechanism that can efficiently detect whether a particular variable has changed its value since last read.
p2422
aVStatic analysis tools aim to find bugs in software that correspond to violations of specifications. Unfortunately, for large and complex software, these specifications are usually either unavailable or sophisticated, and hard to write. This paper presents ANEK, a tool and accompanying methodology for inferring specifications useful for modular typestate checking of programs. In particular, these specifications consist of pre and postconditions along with aliasing annotations known as access permissions. A novel feature of ANEK is that it can generate program specifications even when the code under analysis gives rise to conflicting constraints, a situation that typically occurs when there are bugs. The design of ANEK also makes it easy to add heuristic constraints that encode intuitions gleaned from several years of experience writing such specifications, and this allows it to infer specifications that are better in a subjective sense. The ANEK algorithm is based on a modular analysis that makes it fast and scalable, while producing reliable specifications. All of these features are enabled by its underlying probabilistic analysis that produces specifications that are very likely. Our implementation of ANEK infers access permissions specifications used by the PLURAL [5] modular typestate checker for Java programs. We have run ANEK on a number of Java benchmark programs, including one large opensource program(approximately 38K lines of code), to infer specifications that were then checked using PLURAL. The results for the large benchmark show that ANEK can quickly infer specifications that are both accurate and qualitatively similar to those written by hand, and at 5% of the time taken to manually discover and handcode the specifications.
p2423
aVFor more than thirty years, the parallel programming community has used the dependence graph as the main abstraction for reasoning about and exploiting parallelism in "regular" algorithms that use dense arrays, such as finitedifferences and FFTs. In this paper, we argue that the dependence graph is not a suitable abstraction for algorithms in new application areas like machine learning and network analysis in which the key data structures are "irregular" data structures like graphs, trees, and sets. To address the need for better abstractions, we introduce a datacentric formulation of algorithms called the operator formulation in which an algorithm is expressed in terms of its action on data structures. This formulation is the basis for a structural analysis of algorithms that we call taoanalysis. Taoanalysis can be viewed as an abstraction of algorithms that distills out algorithmic properties important for parallelization. It reveals that a generalized form of dataparallelism called amorphous dataparallelism is ubiquitous in algorithms, and that, depending on the taostructure of the algorithm, this parallelism may be exploited by compiletime, inspectorexecutor or optimistic parallelization, thereby unifying these seemingly unrelated parallelization techniques. Regular algorithms emerge as a special case of irregular algorithms, and many applicationspecific optimization techniques can be generalized to a broader context. These results suggest that the operator formulation and taoanalysis of algorithms can be the foundation of a systematic approach to parallel programming.
p2424
aVHigherorder model checking (more precisely, the model checking of higherorder recursion schemes) has been extensively studied recently, which can automatically decide properties of programs written in the simplytyped \u03bbcalculus with recursion and finite data domains. This paper formalizes predicate abstraction and counterexampleguided abstraction refinement (CEGAR) for higherorder model checking, enabling automatic verification of programs that use infinite data domains such as integers. A prototype verifier for higherorder functional programs based on the formalization has been implemented and tested for several programs.
p2425
aVSeveral recent projects have shown the feasibility of verifying lowlevel systems software. Verifications based on automated theoremproving have omitted reasoning about firstclass code pointers, which is critical for tasks like certifying implementations of threads and processes. Conversely, verifications that deal with firstclass code pointers have featured long, complex, manual proofs. In this paper, we introduce the Bedrock framework, which supports mostlyautomated proofs about programs with the full range of features needed to implement, e.g., language runtime systems. The heart of our approach is in mostlyautomated discharge of verification conditions inspired by separation logic. Our take on separation logic is computational, in the sense that function specifications are usually written in terms of reference implementations in a purely functional language. Logical quantifiers are the most challenging feature for most automated verifiers; by relying on functional programs (written in the expressive language of the Coq proof assistant), we are able to avoid quantifiers almost entirely. This leads to some dramatic improvements compared to both past work in classical verification, which we compare against with implementations of data structures like binary search trees and hash tables; and past work in verified programming with code pointers, which we compare against with examples like function memoization and a cooperative threading library.
p2426
aVLogging and replay is important to reproducing software failures and recovering from failures. Replaying a long execution is time consuming, especially when replay is further integrated with runtime techniques that require expensive instrumentation, such as dependence detection. In this paper, we propose a technique to reduce a replay log while retaining its ability to reproduce a failure. While traditional logging records only system calls and signals, our technique leverages the compiler to selectively collect additional information on the fly. Upon a failure, the log can be reduced by analyzing itself. The collection is highly optimized. The additional runtime overhead of our technique, compared to a plain logging tool, is trivial (2.61% average) and the size of additional log is comparable to the original log. Substantial reduction can be costeffectively achieved through a search based algorithm. The reduced log is guaranteed to reproduce the failure.
p2427
aVSymbolic reasoning about large programs is bound to be imprecise. How to deal with this imprecision is a fundamental problem in program analysis. Imprecision forces approximation. Traditional static program verification builds "may" overapproximations of the program behaviors to check universal "forallpaths" properties, while automatic test generation requires "must" underapproximations to check existential "forsomepath" properties. In this paper, we introduce a new approach to test generation where tests are derived from validity proofs of firstorder logic formulas, rather than satisfying assignments of quantifierfree firstorder logic formulas as usual. Two key ingredients of this higherorder test generation are to (1) represent complex/unknown program functions/instructions causing imprecision in symbolic execution by uninterpreted functions, and (2) record uninterpreted function samples capturing inputoutput pairs observed at execution time for those functions. We show that higherorder test generation generalizes and is more precise than simplifying complex symbolic expressions using their concrete runtime values. We present several program examples where our approach can exercise program paths and find bugs missed by previous techniques. We discuss the implementability and applications of this approach. We also explain in what sense dynamic test generation is more powerful than static test generation.
p2428
aVIn large programs written in managed languages such as Java and C#, holding unnecessary references often results in memory leaks and bloat, degrading significantly their runtime performance and scalability. Despite the existence of many leak detectors for such languages, these detectors often target lowlevel objects; as a result, their reports contain many false warnings and lack sufficient semantic information to help diagnose problems. This paper introduces a specificationbased technique called LeakChaser that can not only capture precisely the unnecessary references leading to leaks, but also explain, with highlevel semantics, why these references become unnecessary. At the heart of LeakChaser is a threetier approach that uses varying levels of abstraction to assist programmers with different skill levels and code familiarity to find leaks. At the highest tier of the approach, the programmer only needs to specify the boundaries of coarsegrained activities, referred to as transactions. The tool automatically infers liveness properties of these transactions, by monitoring the execution, in order to find unnecessary references. Diagnosis at this tier can be performed by any programmer after inspecting the APIs and basic modules of a program, without understanding of the detailed implementation of these APIs. At the middle tier, the programmer can introduce applicationspecific semantic information by specifying properties for the transactions. At the lowest tier of the approach is a liveness checker that does not rely on higherlevel semantic information, but rather allows a programmer to assert lifetime relationships for pairs of objects. This task could only be performed by skillful programmers who have a clear understanding of data structures and algorithms in the program. We have implemented LeakChaser in Jikes RVM and used it to help us diagnose several realworld leaks. The implementation incurs a reasonable overhead for debugging and tuning. Our case studies indicate that the implementation is powerful in guiding programmers with varying code familiarity to find the root causes of several memory leaks even someone who had not studied a leaking program can quickly find the cause after using LeakChaser's iterative process that infers and checks properties with different levels of semantic information.
p2429
aVCompilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized testcase generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compilertesting tool and the results of our bughunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrongcode bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in opensource C compilers.
p2430
aVTranslation validators are static analyzers that attempt to verify that program transformations preserve semantics. Normalizing translation validators do so by trying to match the valuegraphs of an original function and its transformed counterpart. In this paper, we present the design of such a validator for LLVM's intraprocedural optimizations, a design that does not require any instrumentation of the optimizer, nor any rewriting of the source code to compile, and needs to run only once to validate a pipeline of optimizations. We present the results of our preliminary experiments on a set of benchmarks that include GCC, a perl interpreter, SQLite3, and other C programs.
p2431
aVCurrent proposals for concurrent sharedmemory languages, including C++ and C, provide sequential consistency only for programs without data races (the DRF guarantee). While the implications of such a contract for hardware optimisations are relatively wellunderstood, the correctness of compiler optimisations under the DRF guarantee is less clear, and experience with Java shows that this area is errorprone. In this paper we give a rigorous study of optimisations that involve both reordering and elimination of memory reads and writes, covering many practically important optimisations. We first define powerful classes of transformations semantically, in a languageindependent trace semantics. We prove that any composition of these transformations is sound with respect to the DRF guarantee, and moreover that they provide basic security guarantees (no thinair reads) even for programs with data races. To give a concrete example, we apply our semantic results to a simple imperative language and prove that several syntactic transformations are safe for that language. We also discuss some surprising limitations of the DRF guarantee.
p2432
aVEvery day, millions of computer endusers need to perform tasks over large, tabular data, yet lack the programming knowledge to do such tasks automatically. In this work, we present an automatic technique that takes from a user an example of how the user needs to transform a table of data, and provides to the user a program that implements the transformation described by the example. In particular, we present a language of programs TableProg that can describe transformations that real users require.We then present an algorithm ProgFromEx that takes an example input and output table, and infers a program in TableProg that implements the transformation described by the example. When the program is applied to the example input, it reproduces the example output. When the program is applied to another, potentially larger, table with a 'similar' layout as the example input table, then the program produces a corresponding table with a layout that is similar to the example output table. A user can apply ProgFromEx interactively, providing multiple small examples to obtain a program that implements the transformation that the user desires. Moreover, ProgFromEx can help identify 'noisy' examples that contain errors. To evaluate the practicality of TableProg and ProgFromEx, we implemented ProgFromEx as a module for the Microsoft Excel spreadsheet program. We applied the module to automatically implement over 50 table transformations specified by endusers through examples on online Excel help forums. In seconds, ProgFromEx found programs that satisfied the examples and could be applied to larger input tables. This experience demonstrates that TableProg and ProgFromEx can significantly automate the tasks over tabular data that users need to perform.
p2433
aVSoftware modifications are often systematic  they consist of similar, but not identical, program changes to multiple contexts. Existing tools for systematic program transformation are limited because they require programmers to manually prescribe edits or only suggest a location to edit with a related example. This paper presents the design and implementation of a program transformation tool called SYDIT. Given an example edit, SYDIT generates a contextaware, abstract edit script, and then applies the edit script to new program locations. To correctly encode a relative position of the edits in a new location, the derived edit script includes unchanged statements on which the edits are control and data dependent. Furthermore, to make the edit script applicable to a new context using different identifier names, the derived edit script abstracts variable, method, and type names. The evaluation uses 56 systematic edit pairs from five large software projects as an oracle. SYDIT has high coverage and accuracy. For 82% of the edits (46/56), SYDIT matches the context and applies an edit, producing code that is 96% similar to the oracle. Overall, SYDIT mimics human programmers correctly on 70% (39/56) of the edits. Generation of edit scripts seeks to improve programmer productivity by relieving developers from tedious, errorprone, manual code updates. It also has the potential to guide automated program repair by creating program transformations applicable to similar contexts.
p2434
aVIn writing parallel programs, programmers expose parallelism and optimize it to meet a particular performance goal on a single platform under an assumed set of workload characteristics. In the field, changing workload characteristics, new parallel platforms, and deployments with different performance goals make the programmer's developmenttime choices suboptimal. To address this problem, this paper presents the Degree of Parallelism Executive (DoPE), an API and runtime system that separates the concern of exposing parallelism from that of optimizing it. Using the DoPE API, the application developer expresses parallelism options. During program execution, DoPE's runtime system uses this information to dynamically optimize the parallelism options in response to the facts on the ground. We easily port several emerging parallel applications to DoPE's API and demonstrate the DoPE runtime system's effectiveness in dynamically optimizing the parallelism for a variety of performance goals.
p2435
aVEven experienced developers struggle to implement security policies correctly. For example, despite 15 years of development, standard Java libraries still suffer from missing and incorrectly applied permission checks, which enable untrusted applications to execute native calls or modify private class variables without authorization. Previous techniques for static verification of authorization enforcement rely on manually specified policies or attempt to infer the policy by codemining. Neither approach guarantees that the policy used for verification is correct. In this paper, we exploit the fact that many modern APIs have multiple, independent implementations. Our flow and contextsensitive analysis takes as input an API, multiple implementations thereof, and the definitions of security checks and securitysensitive events. For each API entry point, the analysis computes the security policies enforced by the checks before securitysensitive events such as native method calls and API returns, compares these policies across implementations, and reports the differences. Unlike codemining, this technique finds missing checks even if they are part of a rare pattern. Securitypolicy differencing has no intrinsic false positives: implementations of the same API must enforce the same policy, or at least one of them is wrong! Our analysis finds 20 new, confirmed security vulnerabilities and 11 interoperability bugs in the Sun, Harmony, and Classpath implementations of the Java Class Library, many of which were missed by prior analyses. These problems manifest in 499 entry points in these mature, wellstudied libraries. Multiple API implementations are proliferating due to cloudbased software services and standardization of library interfaces. Comparing software implementations for consistency is a new approach to discovering "deep" bugs in them.
p2436
aVWhen dealing with dynamic, untrusted content, such as on the Web, software behavior must be sandboxed, typically through use of a language like JavaScript. However, even for such speciallydesigned languages, it is difficult to ensure the safety of highlyoptimized, dynamic language runtimes which, for efficiency, rely on advanced techniques such as JustInTime (JIT) compilation, large libraries of nativecode support routines, and intricate mechanisms for multithreading and garbage collection. Each new runtime provides a new potential attack surface and this security risk raises a barrier to the adoption of new languages for creating untrusted content. Removing this limitation, this paper introduces general mechanisms for safely and efficiently sandboxing software, such as dynamic language runtimes, that make use of advanced, lowlevel techniques like runtime code modification. Our languageindependent sandboxing builds on Softwarebased Fault Isolation (SFI), a traditionally static technique. We provide a more flexible form of SFI by adding new constraints and mechanisms that allow safety to be guaranteed despite runtime code modifications. We have added our extensions to both the x8632 and x8664 variants of a productionquality, SFIbased sandboxing platform; on those two architectures SFI mechanisms face different challenges. We have also ported two representative language platforms to our extended sandbox: the Mono common language runtime and the V8 JavaScript engine. In detailed evaluations, we find that sandboxing slowdown varies between different benchmarks, languages, and hardware platforms. Overheads are generally moderate and they are close to zero for some important benchmark/platform combinations.
p2437
aVSecurity enforcement inlined into user threads often delays the protected programs; inlined resource reclamation may interrupt program execution and defer resource release. We propose software cruising, a novel technique that migrates security enforcement and resource reclamation from user threads to a concurrent monitor thread. The technique leverages the increasingly popular multicore and multiprocessor architectures and uses lockfree data structures to achieve nonblocking and efficient synchronization between the monitor and user threads. As a case study, software cruising is applied to the heap buffer overflow problem. Previous mitigation and detection techniques for this problem suffer from high performance overhead, legacy code compatibility, semantics loyalty, or tedious manual program transformation. We present a concurrent heap buffer overflow detector, Cruiser, in which a concurrent thread is added to the user program to monitor heap integrity, and custom lockfree data structures and algorithms are designed to achieve high efficiency and scalability. The experiments show that our approach is practical: it imposes an average of 5% performance overhead on SPEC CPU2006, and the throughput slowdown on Apache is negligible on average.
p2438
aVIn this paper we propose Recon, a new general approach to concurrency debugging. Recon goes beyond just detecting bugs, it also presents to the programmer short fragments of buggy execution schedules that illustrate how and why bugs happened. These fragments, called reconstructions, are inferred from interthread communication surrounding the root cause of a bug and significantly simplify the process of understanding bugs. The key idea in Recon is to monitor executions and build graphs that encode interthread communication with enough context information to build reconstructions. Recon leverages reconstructions built from multiple application executions and uses machine learning to identify which ones illustrate the root cause of a bug. Recon's approach is general because it does not rely on heuristics specific to any type of bug, application, or programming model. Therefore, it is able to deal with single and multiplevariable concurrency bugs regardless of their type (e.g., atomicity violation, ordering, etc). To make graph collection efficient, Recon employs selective monitoring and allows metadata information to be imprecise without compromising accuracy. With these optimizations, Recon's graph collection imposes overheads typically between 5x and 20x for both C/C++ and Java programs, with overheads as low as 13% in our experiments. We evaluate Recon with buggy applications, and show it produces reconstructions that include all code points involved in bugs' causes, and presents them in an accurate order. We include a case study of understanding and fixing a previously unresolved bug to showcase Recon's effectiveness.
p2439
aVFixing software bugs has always been an important and timeconsuming process in software development. Fixing concurrency bugs has become especially critical in the multicore era. However, fixing concurrency bugs is challenging, in part due to nondeterministic failures and tricky parallel reasoning. Beyond correctly fixing the original problem in the software, a good patch should also avoid introducing new bugs, degrading performance unnecessarily, or damaging software readability. Existing tools cannot automate the whole fixing process and provide goodquality patches. We present AFix, a tool that automates the whole process of fixing one common type of concurrency bug: singlevariable atomicity violations. AFix starts from the bug reports of existing bugdetection tools. It augments these with static analysis to construct a suitable patch for each bug report. It further tries to combine the patches of multiple bugs for better performance and code readability. Finally, AFix's runtime component provides testing customized for each patch. Our evaluation shows that patches automatically generated by AFix correctly eliminate six out of eight realworld bugs and significantly decrease the failure probability in the other two cases. AFix patches never introduce new bugs and usually have similar performance to manuallydesigned patches.
p2440
aVWe propose to specify the correctness of a program's parallelism using a sequential version of the program with controlled nondeterminism. Such a nondeterministic sequential specification allows (1) the correctness of parallel interference to be verified independently of the program's functional correctness, and (2) the functional correctness of a program to be understood and verified on a sequential version of the program, one with controlled nondeterminism but no interleaving of parallel threads. We identify a number of common patterns for writing nondeterministic sequential specifications. We apply these patterns to specify the parallelism correctness for a variety of parallel Java benchmarks, even in cases when the functional correctness is far too complex to feasibly specify. We describe a sound runtime checking technique to validate that an execution of a parallel program conforms to its nondeterministic sequential specification. The technique uses a novel form of conflictserializability checking to identify, for a given interleaved execution of a parallel program, an equivalent nondeterministic sequential execution. Our experiments show a significant reduction in the number of false positives versus traditional conflictserializability in checking for parallelization bugs.
p2441
aVParametric properties are behavioral properties over program events that depend on one or more parameters. Parameters are bound to concrete data or objects at runtime, which makes parametric properties particularly suitable for stating multiobject relationships or protocols. Monitoring parametric properties independently of the employed formalism involves slicing traces with respect to parameter instances and sending these slices to appropriate nonparametric monitor instances. The number of such instances is theoretically unbounded and tends to be enormous in practice, to an extent that how to efficiently manage monitor instances has become one of the most challenging problems in runtime verification. The previous formalismindependent approach was only able to do the obvious, namely to garbage collect monitor instances when all bound parameter objects were garbage collected. This led to pathological behaviors where unnecessary monitor instances were kept for the entire length of a program. This paper proposes a new approach to garbage collecting monitor instances. Unnecessary monitor instances are collected lazily to avoid creating undue overhead. This lazy collection, along with some careful engineering, has resulted in RV, the most efficient parametric monitoring system to date. Our evaluation shows that the average overhead of RV in the DaCapo benchmark is 15%, which is two times lower than that of JavaMOP and orders of magnitude lower than that of Tracematches.
p2442
aVDespite the power of Parser Expression Grammars (PEGs) and GLR, parsing is not a solved problem. Adding nondeterminism (parser speculation) to traditional LL and LR parsers can lead to unexpected parsetime behavior and introduces practical issues with error handling, singlestep debugging, and sideeffecting embedded grammar actions. This paper introduces the LL(*) parsing strategy and an associated grammar analysis algorithm that constructs LL(*) parsing decisions from ANTLR grammars. At parsetime, decisions gracefully throttle up from conventional fixed k>=1 lookahead to arbitrary lookahead and, finally, fail over to backtracking depending on the complexity of the parsing decision and the input symbols. LL(*) parsing strength reaches into the contextsensitive languages, in some cases beyond what GLR and PEGs can express. By statically removing as much speculation as possible, LL(*) provides the expressivity of PEGs while retaining LL's good error handling and unrestricted grammar actions. Widespread use of ANTLR (over 70,000 downloads/year) shows that it is effective for a wide variety of applications.
p2443
aVMuch effort is spent by programmers everyday in trying to reduce long, failing execution traces to the cause of the error. We present an algorithm for error cause localization based on a reduction to the maximal satisfiability problem (MAXSAT), which asks what is the maximum number of clauses of a Boolean formula that can be simultaneously satisfied by an assignment. At an intuitive level, our algorithm takes as input a program and a failing test, and comprises the following three steps. First, using bounded model checking, and a bound obtained from the execution of the test, we encode the semantics of a bounded unrolling of the program as a Boolean trace formula. Second, for a failing program execution (e.g., one that violates an assertion or a postcondition), we construct an unsatisfiable formula by taking the formula and additionally asserting that the input is the failing test and that the assertion condition does hold at the end. Third, using MAXSAT, we find a maximal set of clauses in this formula that can be satisfied together, and output the complement set as a potential cause of the error. We have implemented our algorithm in a tool called BugAssist that performs error localization for C programs. We demonstrate the effectiveness of BugAssist on a set of benchmark examples with injected faults, and show that in most cases, BugAssist can quickly and precisely isolate a few lines of code whose change eliminates the error. We also demonstrate how our algorithm can be modified to automatically suggest fixes for common classes of errors such as offbyone.We have implemented our algorithm in a tool called BugAssist that performs error localization for C programs. We demonstrate the effectiveness of BugAssist on a set of benchmark examples with injected faults, and show that in most cases, BugAssist can quickly and precisely isolate a few lines of code whose change eliminates the error. We also demonstrate how our algorithm can be modified to automatically suggest fixes for common classes of errors such as offbyone.
p2444
aVIt is often very expensive and practically infeasible to generate test cases that can exercise all possible program states in a program. This is especially true for a medium or large industrial system. In practice, industrial clients of the system often have a set of input data collected either before the system is built or after the deployment of a previous version of the system. Such data are highly valuable as they represent the operations that matter in a client's daily business and may be used to extensively test the system. However, such data often carries sensitive information and cannot be released to thirdparty development houses. For example, a healthcare provider may have a set of patient records that are strictly confidential and cannot be used by any third party. Simply masking sensitive values alone may not be sufficient, as the correlation among fields in the data can reveal the masked information. Also, masked data may exhibit different behavior in the system and become less useful than the original data for testing and debugging. For the purpose of releasing private data for testing and debugging, this paper proposes the kbanonymity model, which combines the kanonymity model commonly used in the data mining and database areas with the concept of program behavior preservation. Like kanonymity, kbanonymity replaces some information in the original data to ensure privacy preservation so that the replaced data can be released to thirdparty developers. Unlike kanonymity, kbanonymity ensures that the replaced data exhibits the same kind of program behavior exhibited by the original data so that the replaced data may still be useful for the purposes of testing and debugging. We also provide a concrete version of the model under three particular configurations and have successfully applied our prototype implementation to three open source programs, demonstrating the utility and scalability of our prototype.
p2445
aVWe consider the problem of specifying combinations of data structures with complex sharing in a manner that is both declarative and results in provably correct code. In our approach, abstract data types are specified using relational algebra and functional dependencies. We describe a language of decompositions that permit the user to specify different concrete representations for relations, and show that operations on concrete representations soundly implement their relational specification. It is easy to incorporate data representations synthesized by our compiler into existing systems, leading to code that is simpler, correct by construction, and comparable in performance to the code it replaces.
p2446
aVMany recent parallelization tools lower the barrier for parallelizing a program, but overlook one of the first questions that a programmer needs to answer: which parts of the program should I spend time parallelizing? This paper examines Kremlin, an automatic tool that, given a serial version of a program, will make recommendations to the user as to what regions (e.g. loops or functions) of the program to attack first. Kremlin introduces a novel hierarchical critical path analysis and develops a new metric for estimating the potential of parallelizing a region: selfparallelism. We further introduce the concept of a parallelism planner, which provides a ranked order of specific regions to the programmer that are likely to have the largest performance impact when parallelized. Kremlin supports multiple planner personalities, which allow the planner to more effectively target a particular programming environment or class of machine. We demonstrate the effectiveness of one such personality, an OpenMP planner, by comparing versions of programs that are parallelized according to Kremlin's plan against thirdparty manually parallelized versions. The results show that Kremlin's OpenMP planner is highly effective, producing plans whose performance is typically comparable to, and sometimes much better than, manual parallelization. At the same time, these plans would require that the user parallelize significantly fewer regions of the program.
p2447
aVExisting work that deals with parallelization of complicated reductions and scans focuses only on formalism and hardly dealt with implementation. To bridge the gap between formalism and implementation, we have integrated parallelization via matrix multiplication into compiler construction. Our framework can deal with complicated loops that existing techniques in compilers cannot parallelize. Moreover, we have sophisticated our framework by developing two sets of techniques. One enhances its capability for parallelization by extracting maxoperators automatically, and the other improves the performance of parallelized programs by eliminating redundancy. We have also implemented our framework and techniques as a parallelizer in a compiler. Experiments on examples that existing compilers cannot parallelize have demonstrated the scalability of programs parallelized by our implementation.
p2448
aVFor decades, compilers have relied on dependence analysis to determine the legality of their transformations. While this conservative approach has enabled many robust optimizations, when it comes to parallelization there are many opportunities that can only be exploited by changing or reordering the dependences in the program. This paper presents Alter: a system for identifying and enforcing parallelism that violates certain dependences while preserving overall program functionality. Based on programmer annotations, Alter exploits new parallelism in loops by reordering iterations or allowing stale reads. Alter can also infer which annotations are likely to benefit the program by using a testdriven framework. Our evaluation of Alter demonstrates that it uncovers parallelism that is beyond the reach of existing static and dynamic tools. Across a selection of 12 performanceintensive loops, 9 of which have loopcarried dependences, Alter obtains an average speedup of 2.0x on 4 cores.
p2449
aVIn this paper, we investigate the problem of semiautomated inversion of imperative programs, which has the potential to make it much easier and less error prone to write programs that naturally pair as inverses, such as insert/delete operations, compressors/decompressors, and so on. Viewing inversion as a subproblem of program synthesis, we propose a novel synthesis technique called Pathbased inductive synthesis (PINS) and apply it to inversion. PINS starts from a program P and a template T for its inverse. PINS then iteratively refines the space of template instantiations by exploring paths in the composition of P and T with symbolic execution. PINS uses an SMT solver to intelligently guide the refinement process, based on the paths explored so far. The key idea motivating this approach is the small pathbound hypothesis: that the behavior of a program can be summarized with a small, carefully chosen set of its program paths. We evaluated PINS by using it to invert 14 programs such as compressors (e.g., LempelZivWelch), encoders (e.g., UUEncode), and arithmetic operations (e.g., vector rotation). Most of these examples are difficult or impossible to invert using prior techniques, but PINS was able to invert all of them. We also found that a semiautomated technique we developed to mine a template from the program to be inverted worked well. In our experiments, PINS takes between one second to thirty minutes to synthesize inverses. We believe this proofofconcept implementation demonstrates the viability of the PINS approach to program synthesis.
p2450
aVThe last few years have seen a resurgence of interest in the use of symbolic execution   a program analysis technique developed more than three decades ago to analyze program execution paths. Scaling symbolic execution and other pathsensitive analysis techniques to large systems remains challenging despite recent algorithmic and technological advances. An alternative to solving the problem of scalability is to reduce the scope of the analysis. One approach that is widely studied in the context of regression analysis is to analyze the differences between two related program versions. While such an approach is intuitive in theory, finding efficient and precise ways to identify program differences, and characterize their effects on how the program executes has proved challenging in practice. In this paper, we present Directed Incremental Symbolic Execution (DiSE), a novel technique for detecting and characterizing the effects of program changes. The novelty of DiSE is to combine the efficiencies of static analysis techniques to compute program difference information with the precision of symbolic execution to explore program execution paths and generate path conditions affected by the differences. DiSE is a complementary technique to other reduction or bounding techniques developed to improve symbolic execution. Furthermore, DiSE does not require analysis results to be carried forward as the software evolves   only the source code for two related program versions is required. A casestudy of our implementation of DiSE illustrates its effectiveness at detecting and characterizing the effects of program changes.
p2451
aVCalling context trees (CCTs) associate performance metrics with paths through a program's call graph, providing valuable information for program understanding and performance analysis. Although CCTs are typically much smaller than call trees, in real applications they might easily consist of tens of millions of distinct calling contexts: this sheer size makes them difficult to analyze and might hurt execution times due to poor access locality. For performance analysis, accurately collecting information about hot calling contexts may be more useful than constructing an entire CCT that includes millions of uninteresting paths. As we show for a variety of prominent Linux applications, the distribution of calling context frequencies is typically very skewed. In this paper we show how to exploit this property to reduce the CCT size considerably. We introduce a novel runtime data structure, called Hot Calling Context Tree (HCCT), that offers an additional intermediate point in the spectrum of data structures for representing interprocedural control flow. The HCCT is a subtree of the CCT that includes only hot nodes and their ancestors. We show how to compute the HCCT without storing the exact frequency of all calling contexts, by using fast and spaceefficient algorithms for mining frequent items in data streams. With this approach, we can distinguish between hot and cold contexts on the fly, while obtaining very accurate frequency counts. We show both theoretically and experimentally that the HCCT achieves a similar precision as the CCT in a much smaller space, roughly proportional to the number of distinct hot contexts: this is typically several orders of magnitude smaller than the total number of calling contexts encountered during a program's execution. Our spaceefficient approach can be effectively combined with previous contextsensitive profiling techniques, such as sampling and bursting.
p2452
aVWe present a new technique for verifying commutativity conditions, which are logical formulas that characterize when operations commute. Because our technique reasons with the abstract state of verified linked data structure implementations, it can verify commuting operations that produce semantically equivalent (but not necessarily identical) data structure states in different execution orders. We have used this technique to verify sound and complete commutativity conditions for all pairs of operations on a collection of linked data structure implementations, including data structures that export a set interface (ListSet and HashSet) as well as data structures that export a map interface (AssociationList, HashTable, and ArrayList). This effort involved the specification and verification of 765 commutativity conditions. Many speculative parallel systems need to undo the effects of speculatively executed operations. Inverse operations, which undo these effects, are often more efficient than alternate approaches (such as saving and restoring data structure state). We present a new technique for verifying such inverse operations. We have specified and verified, for all of our linked data structure implementations, an inverse operation for every operation that changes the data structure state. Together, the commutativity conditions and inverse operations provide a key resource that language designers, developers of program analysis systems, and implementors of software systems can draw on to build languages, program analyses, and systems with strong correctness guarantees.
p2453
aVSpeculative execution is a promising approach for exploiting parallelism in many programs, but it requires efficient schemes for detecting conflicts between concurrently executing threads. Prior work has argued that checking semantic commutativity of method invocations is the right way to detect conflicts for complex data structures such as kdtrees. Several ad hoc ways of checking commutativity have been proposed in the literature, but there is no systematic approach for producing implementations. In this paper, we describe a novel framework for reasoning about commutativity conditions: the commutativity lattice. We show how commutativity specifications from this lattice can be systematically implemented in one of three different schemes: abstract locking, forward gatekeeping and general gatekeeping. We also discuss a disciplined approach to exploiting the lattice to find different implementations that trade off precision in conflict detection for performance. Finally, we show that our novel conflict detection schemes are practical and can deliver speedup on three realworld applications.
p2454
aVProgram analysis and verification tools crucially depend on the ability to symbolically describe and reason about sets of program behaviors. Separation logic provides a promising foundation for dealing with heap manipulating programs, while the development of practical automated deduction/satisfiability checking tools for separation logic is a challenging problem. In this paper, we present an efficient, sound and complete automated theorem prover for checking validity of entailments between separation logic formulas with list segment predicates. Our theorem prover integrates separation logic inference rules that deal with list segments and a superposition calculus to deal with equality/aliasing between memory locations. The integration follows a modular combination approach that allows one to directly incorporate existing advanced techniques for firstorder reasoning with equality, as well as account for additional theories, e.g., linear arithmetic, using extensions of superposition. An experimental evaluation of our entailment prover indicates speedups of several orders of magnitude with respect to the available stateoftheart tools.
p2455
aVWe present a strictly bottomup, summarybased, and precise heap analysis targeted for program verification that performs strong updates to heap locations at call sites. We first present a theory of heap decompositions that forms the basis of our approach; we then describe a full analysis algorithm that is fully symbolic and efficient. We demonstrate the precision and scalability of our approach for verification of real C and C++ programs.
p2456
aVIn this paper, we study the problem of automatically solving ruler/compass based geometry construction problems. We first introduce a logic and a programming language for describing such constructions and then phrase the automation problem as a program synthesis problem. We then describe a new program synthesis technique based on three key insights: (i) reduction of symbolic reasoning to concrete reasoning (based on a deep theoretical result that reduces verification to random testing), (ii) extending the instruction set of the programming language with higher level primitives (representing basic constructions found in textbook chapters, inspired by how humans use their experience and knowledge gained from chapters to perform complicated constructions), and (iii) pruning the forward exhaustive search using a goaldirected heuristic (simulating backward reasoning performed by humans). Our tool can successfully synthesize constructions for various geometry problems picked up from highschool textbooks and examination papers in a reasonable amount of time. This opens up an amazing set of possibilities in the context of making classroom teaching interactive.
p2457
aVWe address the problem of automatic synthesis of assertions on sequential programs with singlylinked lists containing data over infinite domains such as integers or reals. Our approach is based on an accurate abstract interprocedural analysis. Program configurations are represented by graphs where nodes represent list segments without sharing. The data in these list segments are characterized by constraints in abstract domains. We consider a domain where constraints are in a universally quantified fragment of the firstorder logic over sequences, as well as a domain constraining the multisets of data in sequences. Our analysis computes the effect of each procedure in a local manner, by considering only the reachable part of the heap from its actual parameters. In order to avoid losses of information, we introduce a mechanism based on unfolding/folding operations allowing to strengthen the analysis in the domain of firstorder formulas by the analysis in the multisets domain. The same mechanism is used for strengthening the sound (but incomplete) entailment operator of the domain of firstorder formulas. We have implemented our techniques in a prototype tool and we have shown that our approach is powerful enough for automatic (1) generation of nontrivial procedure summaries, (2) pre/postcondition reasoning, and (3) procedure equivalence checking.
p2458
aVMany static analyses do not scale as they are made more precise. For example, increasing the amount of context sensitivity in a klimited pointer analysis causes the number of contexts to grow exponentially with k. Iterative refinement techniques can mitigate this growth by starting with a coarse abstraction and only refining parts of the abstraction that are deemed relevant with respect to a given client. In this paper, we introduce a new technique called pruning that uses client feedback in a different way. The basic idea is to use coarse abstractions to prune away parts of the program analysis deemed irrelevant for proving a client query, and then using finer abstractions on the sliced program analysis. For a klimited pointer analysis, this approach amounts to adaptively refining and pruning a set of prefix patterns representing the contexts relevant for the client. By pruning, we are able to scale up to much more expensive abstractions than before. We also prove that the pruned analysis is both sound and complete, that is, it yields the same results as an analysis that uses a more expensive abstraction directly without pruning.
p2459
aVVariance allows the safe integration of parametric and subtype polymorphism. Two flavors of variance, definitionsite versus usesite variance, have been studied and have had their merits hotly debated. Definitionsite variance (as in Scala and C#) offers simple typeinstantiation rules, but causes fractured definitions of naturally invariant classes; Usesite variance (as in Java) offers simplicity in class definitions, yet complex typeinstantiation rules that elude most programmers. We present a unifying framework for reasoning about variance. Our framework is quite simple and entirely denotational, that is, it evokes directly the definition of variance with a small core calculus that does not depend on specific type systems. This general framework can have multiple applications to combine the best of both worlds: for instance, it can be used to add usesite variance annotations to the Scala type system. We show one such application in detail: we extend the Java type system with a mechanism that modularly infers the definitionsite variance of type parameters, while allowing usesite variance annotations on any typeinstantiation. Applying our technique to six Java generic libraries (including the Java core library) shows that 2058 (depending on the library) of generic definitions are inferred to have singlevariance; 863% of method signatures can be relaxed through this inference, and up to 91% of existing wildcard annotations are unnecessary and can be elided.
p2460
aVWildcards have become an important part of Java's type system since their introduction 7 years ago. Yet there are still many open problems with Java's wildcards. For example, there are no known sound and complete algorithms for subtyping (and consequently type checking) Java wildcards, and in fact subtyping is suspected to be undecidable because wildcards are a form of bounded existential types. Furthermore, some Java types with wildcards have no joins, making inference of type arguments for generic methods particularly difficult. Although there has been progress on these fronts, we have identified significant shortcomings of the current state of the art, along with new problems that have not been addressed. In this paper, we illustrate how these shortcomings reflect the subtle complexity of the problem domain, and then present major improvements to the current algorithms for wildcards by making slight restrictions on the usage of wildcards. Our survey of existing Java programs suggests that realistic code should already satisfy our restrictions without any modifications. We present a simple algorithm for subtyping which is both sound and complete with our restrictions, an algorithm for lazily joining types with wildcards which addresses some of the shortcomings of prior work, and techniques for improving the Java type system as a whole. Lastly, we describe various extensions to wildcards that would be compatible with our algorithms.
p2461
aVAlthough asynchronous communication is an important feature of many concurrent systems, building composable abstractions that leverage asynchrony is challenging. This is because an asynchronous operation necessarily involves two distinct threads of control   the thread that initiates the operation, and the thread that discharges it. Existing attempts to marry composability with asynchrony either entail sacrificing performance (by limiting the degree of asynchrony permitted), or modularity (by forcing natural abstraction boundaries to be broken). In this paper, we present the design and rationale for asynchronous events, an abstraction that enables composable construction of complex asynchronous protocols without sacrificing the benefits of abstraction or performance. Asynchronous events are realized in the context of Concurrent ML's firstclass event abstraction. We discuss the definition of a number of useful asynchronous abstractions that can be built on top of asynchronous events (e.g., composable callbacks) and provide a detailed case study of how asynchronous events can be used to substantially improve the modularity and performance of an I/Ointensive highly concurrent server application.
p2462
aVWe consider the problem of synthesizing loopfree programs that implement a desired functionality using components from a given library. Specifications of the desired functionality and the library components are provided as logical relations between their respective input and output variables. The library components can be used at most once, and hence the library is required to contain a reasonable overapproximation of the multiset of the components required. We solve the above componentbased synthesis problem using a constraintbased approach that involves first generating a synthesis constraint, and then solving the constraint. The synthesis constraint is a firstorder \u2203\u2200 logic formula whose size is quadratic in the number of components. We present a novel algorithm for solving such constraints. Our algorithm is based on counterexample guided iterative synthesis paradigm and uses offtheshelf SMT solvers. We present experimental results that show that our tool Brahma can efficiently synthesize highly nontrivial 1020 line loopfree bitvector programs. These programs represent a state space of approximately 2010 programs, and are beyond the reach of the other tools based on sketching and superoptimization.
p2463
aVDynamic Binary Translation (DBT) is the key technology behind crossplatform virtualization and allows software compiled for one Instruction Set Architecture (ISA) to be executed on a processor supporting a different ISA. Under the hood, DBT is typically implemented using JustInTime (JIT) compilation of frequently executed program regions, also called traces. The main challenge is translating frequently executed program regions as fast as possible into highly efficient native code. As time for JIT compilation adds to the overall execution time, the JIT compiler is often decoupled and operates in a separate thread independent from the main simulation loop to reduce the overhead of JIT compilation. In this paper we present two innovative contributions. The first contribution is a generalized trace compilation approach that considers all frequently executed paths in a program for JIT compilation, as opposed to previous approaches where trace compilation is restricted to paths through loops. The second contribution reduces JIT compilation cost by compiling several hot traces in a concurrent task farm. Altogether we combine generalized lightweight tracing, large translation units, parallel JIT compilation and dynamic work scheduling to ensure timely and efficient processing of hot traces. We have evaluated our industrystrength, LLVMbased parallel DBT implementing the ARCompact ISA against three benchmark suites (EEMBC, BioPerf and SPEC CPU2006) and demonstrate speedups of up to 2.08 on a standard quadcore Intel Xeon machine. Across short and longrunning benchmarks our scheme is robust and never results in a slowdown. In fact, using four processors total execution time can be reduced by on average 11.5% over stateoftheart decoupled, parallel (or asynchronous) JIT compilation.
p2464
aVData structure selection is one of the most critical aspects of developing effective applications. By analyzing data structures' behavior and their interaction with the rest of the application on the underlying architecture, tools can make suggestions for alternative data structures better suited for the program input on which the application runs. Consequently, developers can optimize their data structure usage to make the application conscious of an underlying architecture and a particular program input. This paper presents the design and evaluation of Brainy, a new program analysis tool that automatically selects the best data structure for a given program and its input on a specific microarchitecture. The data structure's interface functions are instrumented to dynamically monitor how the data structure interacts with the application for a given input. The instrumentation records traces of various runtime characteristics including underlying architecturespecific events. These generated traces are analyzed and fed into an offline model, constructed using machine learning, to select the best data structure. That is, Brainy exploits runtime feedback of data structures to model the situation an application runs on, and selects the best data structure for a given application/input/architecture combination based on the constructed model. The empirical evaluation shows that this technique is highly accurate across several realworld applications with various program input sets on two different stateoftheart microarchitectures. Consequently, Brainy achieved an average performance improvement of 27% and 33% on both microarchitectures, respectively.
p2465
aVTo derive maximum optimization benefits from partial redundancy elimination (PRE),it is necessary to go beyond its safety constraint. Algorithms for optimal speculative code motion have been developed based on the application of minimum cut to flow networks formed out of the control flow graph. These previous techniques did not take advantage of the SSA form, which is a popular program representation widely used in modernday compilers. We have developed the MCSSAPRE algorithm that enables an SSAbased compiler to take full advantage of SSA to perform optimal speculative code motion efficiently when an execution profile is available. Our work shows that it is possible to form flow networks out of SSA graphs, and the mincut technique can be applied equally well on these flow networks to find the optimal code placement. We provide proofs of the correctness and computational and lifetime optimality of MCSSAPRE. We analyze its time complexity to show its efficiency advantage. We have implemented MCSSAPRE in the opensourced Path64 compiler. Our experimental data based on the full SPEC CPU2006 Benchmark Suite show that MCSSAPRE can further improve program performance over traditional SSAPRE, and that our sparse approach to the problem does result in smaller problem sizes.
p2466
aVLanguages such as OpenCL and CUDA offer a standard interface for generalpurpose programming of GPUs. However, with these languages, programmers must explicitly manage numerous lowlevel details involving communication and synchronization. This burden makes programming GPUs difficult and errorprone, rendering these powerful devices inaccessible to most programmers. We desire a higherlevel programming model that makes GPUs more accessible while also effectively exploiting their computational power. This paper presents features of Lime, a new Javacompatible language targeting heterogeneous systems, that allow an optimizing compiler to generate high quality GPU code. The key insight is that the language type system enforces isolation and immutability invariants that allow the compiler to optimize for a GPU without heroic compiler analysis. Our compiler attains GPU speedups between 75% and 140% of the performance of native OpenCL code.
p2467
aVWe propose a new languagebased approach to mitigating timing channels. In this language, welltyped programs provably leak only a bounded amount of information over time through external timing channels. By incorporating mechanisms for predictive mitigation of timing channels, this approach also permits a more expressive programming model. Timing channels arising from interaction with underlying hardware features such as instruction caches are controlled. Assumptions about the underlying hardware are explicitly formalized, supporting the design of hardware that efficiently controls timing channels. One such hardware design is modeled and used to show that timing channels can be controlled in some simple programs of realworld significance.
p2468
aVResearch scientists and medical professionals use imaging technology, such as computed tomography (CT) and magnetic resonance imaging (MRI) to measure a wide variety of biological and physical objects. The increasing sophistication of imaging technology creates demand for equally sophisticated computational techniques to analyze and visualize the image data. Analysis and visualization codes are often crafted for a specific experiment or set of images, thus imaging scientists need support for quickly developing codes that are reliable, robust, and efficient. In this paper, we present the design and implementation of Diderot, which is a parallel domainspecific language for biomedical image analysis and visualization. Diderot supports a highlevel model of computation that is based on continuous tensor fields. These tensor fields are reconstructed from discrete image data using separable convolution kernels, but may also be defined by applying higherorder operations, such as differentiation (\u2207). Early experiments demonstrate that Diderot provides both a highlevel concise notation for image analysis and visualization algorithms, as well as high sequential and parallel performance.
p2469
aVOver the last five years, graphics cards have become a tempting target for scientific computing, thanks to unrivaled peak performance, often producing a runtime speedup of x10 to x25 over comparable CPU solutions. However, this increase can be difficult to achieve, and doing so often requires a fundamental rethink. This is especially problematic in scientific computing, where experts do not want to learn yet another architecture. In this paper we develop a method for automatically parallelising recursive functions of the sort found in scientific papers. Using a static analysis of the function dependencies we identify sets  partitions  of independent elements, which we use to synthesise an efficient GPU implementation using polyhedral code generation techniques. We then augment our language with DSL extensions to support a wider variety of applications, and demonstrate the effectiveness of this with three case studies, showing significant performance improvement over equivalent CPU methods, and similar efficiency to handtuned GPU implementations.
p2470
aVWorkload, platform, and available resources constitute a parallel program's execution environment. Most parallelization efforts statically target an anticipated range of environments, but performance generally degrades outside that range. Existing approaches address this problem with dynamic tuning but do not optimize a multiprogrammed system holistically. Further, they either require manual programming effort or are limited to arraybased dataparallel programs. This paper presents Parcae, a generally applicable automatic system for platformwide dynamic tuning. Parcae includes (i) the Nona compiler, which creates flexible parallel programs whose tasks can be efficiently reconfigured during execution; (ii) the Decima monitor, which measures resource availability and system performance to detect change in the environment; and (iii) the Morta executor, which cuts short the life of executing tasks, replacing them with other functionally equivalent tasks better suited to the current environment. Parallel programs made flexible by Parcae outperform original parallel implementations in many interesting scenarios.
p2471
aVThis paper addresses the problem of reducing unnecessary conflicts in optimistic synchronization. Optimistic synchronization must ensure that any two concurrently executing transactions that commit are properly synchronized. Conflict detection is an approximate check for this condition. For efficiency, the traditional approach to conflict detection conservatively checks that the memory locations mutually accessed by two concurrent transactions are accessed only for reading. We present JANUS, a parallelization system that performs conflict detection by considering sequences of operations and their composite effect on the system's state. This is done efficiently, such that the runtime overhead due to conflict detection is on a par with that of writeconflictbased detection. In certain common scenarios, this mode of refinement dramatically improves the precision of conflict detection, thereby reducing the number of false conflicts. Our empirical evaluation of JANUS shows that this precision gain reduces the abort rate by an order of magnitude (22x on average), and achieves a speedup of up to 2.5x, on a suite of realworld benchmarks where no parallelism is exploited by the standard approach.
p2472
aVEfficient communication and synchronization is crucial for fine grained parallelism. Libraries providing such features, while indispensable, are difficult to write, and often cannot be tailored or composed to meet the needs of specific users. We introduce reagents, a set of combinators for concisely expressing concurrency algorithms. Reagents scale as well as their handcoded counterparts, while providing the composability existing libraries lack.
p2473
aVApproximate program transformations such as skipping tasks [29, 30], loop perforation [21, 22, 35], reduction sampling [38], multiple selectable implementations [3, 4, 16, 38], dynamic knobs [16], synchronization elimination [20, 32], approximate function memoization [11],and approximate data types [34] produce programs that can execute at a variety of points in an underlying performance versus accuracy tradeoff space. These transformed programs have the ability to trade accuracy of their results for increased performance by dynamically and nondeterministically modifying variables that control their execution. We call such transformed programs relaxed programs because they have been extended with additional nondeterminism to relax their semantics and enable greater flexibility in their execution. We present language constructs for developing and specifying relaxed programs. We also present proof rules for reasoning about properties [28] which the program must satisfy to be acceptable. Our proof rules work with two kinds of acceptability properties: acceptability properties [28], which characterize desired relationships between the values of variables in the original and relaxed programs, and unary acceptability properties, which involve values only from a single (original or relaxed) program. The proof rules support a staged reasoning approach in which the majority of the reasoning effort works with the original program. Exploiting the common structure that the original and relaxed programs share, relational reasoning transfers reasoning effort from the original program to prove properties of the relaxed program. We have formalized the dynamic semantics of our target programming language and the proof rules in Coq and verified that the proof rules are sound with respect to the dynamic semantics. Our Coq implementation enables developers to obtain fully machinechecked verifications of their relaxed programs.
p2474
aVWhen program verification tools fail to verify a program, either the program is buggy or the report is a false alarm. In this situation, the burden is on the user to manually classify the report, but this task is timeconsuming, errorprone, and does not utilize facts already proven by the analysis. We present a new technique for assisting users in classifying error reports. Our technique computes small, relevant queries presented to a user that capture exactly the information the analysis is missing to either discharge or validate the error. Our insight is that identifying these missing facts is an instance of the abductive inference problem in logic, and we present a new algorithm for computing the smallest and most general abductions in this setting. We perform the first user study to rigorously evaluate the accuracy and effort involved in manual classification of error reports. Our study demonstrates that our new technique is very useful for improving both the speed and accuracy of error report classification. Specifically, our approach improves classification accuracy from 33% to 90% and reduces the time programmers take to classify error reports from approximately 5 minutes to under 1 minute.
p2475
aVSymbolic execution has proven to be a practical technique for building automated test case generation and bug finding tools. Nevertheless, due to state explosion, these tools still struggle to achieve scalability. Given a program, one way to reduce the number of states that the tools need to explore is to merge states obtained on different paths. Alas, doing so increases the size of symbolic path conditions (thereby stressing the underlying constraint solver) and interferes with optimizations of the exploration process (also referred to as search strategies). The net effect is that state merging may actually lower performance rather than increase it. We present a way to automatically choose when and how to merge states such that the performance of symbolic execution is significantly increased. First, we present query count estimation, a method for statically estimating the impact that each symbolic variable has on solver queries that follow a potential merge point; states are then merged only when doing so promises to be advantageous. Second, we present dynamic state merging, a technique for merging states that interacts favorably with search strategies in automated test case generation and bug finding tools. Experiments on the 96 GNU Coreutils show that our approach consistently achieves several orders of magnitude speedup over previously published results. Our code and experimental data are publicly available at http://cloud9.epfl.ch.
p2476
aVParallel programs are known to be difficult to analyze. A key reason is that they typically have an enormous number of execution interleavings, or schedules. Static analysis over all schedules requires overapproximations, resulting in poor precision; dynamic analysis rarely covers more than a tiny fraction of all schedules. We propose an approach called schedule specialization to analyze a parallel program over only a small set of schedules for precision, and then enforce these schedules at runtime for soundness of the static analysis results. We build a schedule specialization framework for C/C++ multithreaded programs that use Pthreads. Our framework avoids the need to modify every analysis to be scheduleaware by specializing a program into a simpler program based on a schedule, so that the resultant program can be analyzed with stock analyses for improved precision. Moreover, our framework provides a precise scheduleaware defuse analysis on memory locations, enabling us to build three highly precise analyses: an alias analyzer, a datarace detector, and a path slicer. Evaluation on 17 programs, including 2 realworld programs and 15 popular benchmarks, shows that analyses using our framework reduced mayaliases by 61.9%, false race reports by 69%, and path slices by 48.7%; and detected 7 unknown bugs in wellchecked programs.
p2477
aVWhile graphics processing units (GPUs) provide lowcost and efficient platforms for accelerating high performance computations, the tedious process of performance tuning required to optimize applications is an obstacle to wider adoption of GPUs. In addition to the programmability challenges posed by GPU's complex memory hierarchy and parallelism model, a wellknown application design problem is target portability across different GPUs. However, even for a single GPU target, changing a program's input characteristics can make an alreadyoptimized implementation of a program perform poorly. In this work, we propose Adaptic, an adaptive inputaware compilation system to tackle this important, yet overlooked, input portability problem. Using this system, programmers develop their applications in a highlevel streaming language and let Adaptic undertake the difficult task of input portable optimizations and code generation. Several inputaware optimizations are introduced to make efficient use of the memory hierarchy and customize thread composition. At runtime, a properly optimized version of the application is executed based on the actual program input. We perform a headtohead comparison between the Adaptic generated and handoptimized CUDA programs. The results show that Adaptic is capable of generating codes that can perform on par with their handoptimized counterparts over certain input ranges and outperform them when the input falls out of the handoptimized programs' "comfort zone". Furthermore, we show that inputaware results are sustainable across different GPU targets making it possible to write and optimize applications once and run them anywhere.
p2478
aVModularity is a central theme in any scalable program analysis. The core idea in a modular analysis is to build summaries at procedure boundaries, and use the summary of a procedure to analyze the effect of calling it at its calling context. There are two ways to perform a modular program analysis: (1) topdown and (2) bottomup. A bottomup analysis proceeds upwards from the leaves of the call graph, and analyzes each procedure in the most general calling context and builds its summary. In contrast, a topdown analysis starts from the root of the call graph, and proceeds downward, analyzing each procedure in its calling context. Topdown analyses have several applications in verification and software model checking. However, traditionally, bottomup analyses have been easier to scale and parallelize than topdown analyses. In this paper, we propose a generic framework, BOLT, which uses MapReduce style parallelism to scale topdown analyses. In particular, we consider topdown analyses that are demand driven, such as the ones used for software model checking. In such analyses, each intraprocedural analysis happens in the context of a reachability query. A query Q over a procedure P results in query tree that consists of subqueries over the procedures called by P. The key insight in BOLT is that the query tree can be explored in parallel using MapReduce style parallelism   the map stage can be used to run a set of enabled queries in parallel, and the reduce stage can be used to manage interdependencies between queries. Iterating the map and reduce stages alternately, we can exploit the parallelism inherent in topdown analyses. Another unique feature of BOLT is that it is parameterized by the algorithm used for intraprocedural analysis. Several kinds of analyses, including may analyses, mustanalyses, and maymustanalyses can be parallelized using BOLT. We have implemented the BOLT framework and instantiated the intraprocedural parameter with a maymustanalysis. We have run BOLT on a test suite consisting of 45 Microsoft Windows device drivers and 150 safety properties. Our results demonstrate an average speedup of 3.71x and a maximum speedup of 7.4x (with 8 cores) over a sequential analysis. Moreover, in several checks where a sequential analysis fails, BOLT is able to successfully complete its analysis.
p2479
aVIn this article we present a general method for achieving global static analyzers that are precise, sound, yet also scalable. Our method generalizes the sparse analysis techniques on top of the abstract interpretation framework to support relational as well as nonrelational semantics properties for Clike languages. We first use the abstract interpretation framework to have a global static analyzer whose scalability is unattended. Upon this underlying sound static analyzer, we add our generalized sparse analysis techniques to improve its scalability while preserving the precision of the underlying analysis. Our framework determines what to prove to guarantee that the resulting sparse version should preserve the precision of the underlying analyzer. We formally present our framework; we present that existing sparse analyses are all restricted instances of our framework; we show more semantically elaborate design examples of sparse nonrelational and relational static analyses; we present their implemen tation results that scale to analyze up to one million lines of C programs. We also show a set of implementation techniques that turn out to be critical to economically support the sparse analysis process.
p2480
aVJavaScript performance is often bound by its dynamically typed nature. Compilers do not have access to static type information, making generation of efficient, typespecialized machine code difficult. We seek to solve this problem by inferring types. In this paper we present a hybrid type inference algorithm for JavaScript based on pointsto analysis. Our algorithm is fast, in that it pays for itself in the optimizations it enables. Our algorithm is also precise, generating information that closely reflects the program's actual behavior even when analyzing polymorphic code, by augmenting static analysis with runtime type barriers. We showcase an implementation for Mozilla Firefox's JavaScript engine, demonstrating both performance gains and viability. Through integration with the justintime (JIT) compiler in Firefox, we have improved performance on major benchmarks and JavaScriptheavy websites by up to 50%. Inferenceenabled compilation is the default compilation mode as of Firefox 9.
p2481
aVModern web pages are becoming increasingly fullfeatured, and this additional functionality often requires greater use of asynchrony. Unfortunately, this asynchrony can trigger unexpected concurrency errors, even though web page scripts are executed sequentially. We present the first formulation of a happensbefore relation for common web platform features. Developing this relation was a nontrivial task, due to complex feature interactions and browser differences. We also present a logical memory access model for web applications that abstracts away browser implementation details. Based on the above, we implemented WebRacer, the first dynamic race detector for web applications. WebRacer is implemented atop the productionquality WebKit engine, enabling testing of fullfeatured web sites. WebRacer can also simulate certain user actions, exposing more races. We evaluated WebRacer by testing a large set of Fortune 100 company web sites. We discovered many harmful races, and also gained insights into how developers handle asynchrony in practice.
p2482
aVMany modern applications are built by combining independently developed packages and services that are distributed over many machines with complex interdependencies. The assembly, installation, and management of such applications is hard, and usually performed either manually or by writing customized scripts. We present Engage, a system for configuring, installing, and managing complex application stacks. Engage consists of three components: a domainspecific model to describe component metadata and intercomponent dependencies; a constraintbased algorithm that takes a partial installation specification and computes a full installation plan; and a runtime system that coordinates the deployment of the application across multiple machines and manages the deployed system. By explicitly modeling configuration metadata and intercomponent dependencies, Engage enables static checking of application configurations and automated, constraintdriven, generation of installation plans across multiple machines. This reduces the tedious manual process of application configuration, installation, and management. We have implemented Engage and we have used it to successfully host a number of applications. We describe our experiences in using Engage to manage a generic platform that hosts Django applications in the cloud or on premises.
p2483
aVModern programming frameworks provide enormous libraries arranged in complex structures, so much so that a large part of modern programming is searching for APIs that surely exist" somewhere in an unfamiliar part of the framework. We present a novel way of phrasing a search for an unknown API: the programmer simply writes an expression leaving holes for the parts they do not know. We call these expressions partial expressions. We present an efficient algorithm that produces likely completions ordered by a ranking scheme based primarily on the similarity of the types of the APIs suggested to the types of the known expressions. This gives a powerful language for both API discovery and code completion with a small impedance mismatch from writing code. In an automated experiment on mature C# projects, we show our algorithm can place the intended expression in the top 10 choices over 80% of the time.
p2484
aVSelfstabilizing programs automatically recover from state corruption caused by software bugs and other sources to reach the correct state. A number of applications are inherently selfstabilizing such programs typically overwrite all nonconstant data with new input data. We present a type system and static analyses that together check whether a program is selfstabilizing. We combine this with a code generation strategy that ensures that a program continues executing long enough to selfstabilize. Our experience using SJava indicates that (1) SJava annotations are easy to write once one understands a program and (2) SJava successfully checked that several benchmarks were selfstabilizing.
p2485
aVApplication data often changes slowly or incrementally over time. Since incremental changes to input often result in only small changes in output, it is often feasible to respond to such changes asymptotically more efficiently than by rerunning the whole computation. Traditionally, realizing such asymptotic efficiency improvements requires designing problemspecific algorithms known as dynamic or incremental algorithms, which are often significantly more complicated than conventional algorithms to design, analyze, implement, and use. A longstanding open problem is to develop techniques that automatically transform conventional programs so that they correctly and efficiently respond to incremental changes. In this paper, we describe a significant step towards solving the problem of automatic incrementalization: a programming language and a compiler that can, given a few type annotations describing what can change over time, compile a conventional program that assumes its data to be static (unchanging over time) to an incremental program. Based on recent advances in selfadjusting computation, including a theoretical proposal for translating purely functional programs to selfadjusting programs, we develop techniques for translating conventional Standard ML programs to selfadjusting programs. By extending the Standard ML language, we design a fully featured programming language with higherorder features, a module system, and a powerful type system, and implement a compiler for this language. The resulting programming language, LML, enables translating conventional programs decorated with simple type annotations into incremental programs that can respond to changes in their data correctly and efficiently. We evaluate the effectiveness of our approach by considering a range of benchmarks involving lists, vectors, and matrices, as well as a ray tracer. For these benchmarks, our compiler incrementalizes existing code with only trivial amounts of annotation. The resulting programs are often asymptotically more efficient, leading to orders of magnitude speedups in practice.
p2486
aVShared memory concurrency relies on synchronisation primitives: compareandswap, loadreserve/storeconditional (aka LL/SC), languagelevel mutexes, and so on. In a sequentially consistent setting, or even in the TSO setting of x86 and Sparc, these have wellunderstood semantics. But in the very relaxed settings of IBM, POWER, ARM, or C/C++, it remains surprisingly unclear exactly what the programmer can depend on. This paper studies relaxedmemory synchronisation. On the hardware side, we give a clear semantic characterisation of the loadreserve/storeconditional primitives as provided by POWER multiprocessors, for the first time since they were introduced 20 years ago; we cover their interaction with relaxed loads, stores, barriers, and dependencies. Our model, while not officially sanctioned by the vendor, is validated by extensive testing, comparing actual implementation behaviour against an oracle generated from the model, and by detailed discussion with IBM staff. We believe the ARM semantics to be similar. On the software side, we prove sound a proposed compilation scheme of the C/C++ synchronisation constructs to POWER, including C/C++ spinlock mutexes, fences, and readmodifywrite operations, together with the simpler atomic operations for which soundness is already known from our previous work; this is a first step in verifying concurrent algorithms that use loadreserve/storeconditional with respect to a realistic semantics. We also build confidence in the C/C++ model in its own terms, fixing some omissions and contributing to the C standards committee adoption of the C++11 concurrency model.
p2487
aVC tools, such as source browsers, bug finders, and automated refactorings, need to process two languages: C itself and the preprocessor. The latter improves expressivity through file includes, macros, and static conditionals. But it operates only on tokens, making it hard to even parse both languages. This paper presents a complete, performant solution to this problem. First, a configurationpreserving preprocessor resolves includes and macros yet leaves static conditionals intact, thus preserving a program's variability. To ensure completeness, we analyze all interactions between preprocessor features and identify techniques for correctly handling them. Second, a configurationpreserving parser generates a wellformed AST with static choice nodes for conditionals. It forks new subparsers when encountering static conditionals and merges them again after the conditionals. To ensure performance, we present a simple algorithm for tabledriven ForkMerge LR parsing and four novel optimizations. We demonstrate the effectiveness of our approach on the x86 Linux kernel.
p2488
aVProgrammers are turning to radical architectures such as reconfigurable hardware (FPGAs) to achieve performance. But such systems, programmed at a very low level in languages with impoverished abstractions, are orders of magnitude more complex to use than conventional CPUs. The continued exponential increase in transistors, combined with the desire to implement ever more sophisticated algorithms, makes it imperative that such systems be programmed at much higher levels of abstraction. One of the fundamental highlevel language features is automatic memory management in the form of garbage collection. We present the first implementation of a complete garbage collector in hardware (as opposed to previous "hardwareassist" techniques), using an FPGA and its onchip memory. Using a completely concurrent snapshot algorithm, it provides singlecycle access to the heap, and never stalls the mutator for even a single cycle, achieving a deterministic mutator utilization (MMU) of 100%. We have synthesized the collector to hardware and show that it never consumes more than 1% of the logic resources of a highend FPGA. For comparison we also implemented explicit (malloc/free) memory management, and show that realtime collection is about 4% to 17% slower than malloc, with comparable energy consumption. Surprisingly, in hardware realtime collection is superior to stoptheworld collection on every performance axis, and even for stressful microbenchmarks can achieve 100% MMU with heaps as small as 1.01 to 1.4 times the absolute minimum.
p2489
aVTo report a compiler bug, one must often find a small test case that triggers the bug. The existing approach to automated testcase reduction, delta debugging, works by removing substrings of the original input; the result is a concatenation of substrings that delta cannot remove. We have found this approach less than ideal for reducing C programs because it typically yields test cases that are too large or even invalid (relying on undefined behavior). To obtain small and valid test cases consistently, we designed and implemented three new, domainspecific testcase reducers. The best of these is based on a novel framework in which a generic fixpoint computation invokes modular transformations that perform reduction operations. This reducer produces outputs that are, on average, more than 25 times smaller than those produced by our other reducers or by the existing reducer that is most commonly used by compiler developers. We conclude that effective program reduction requires more than straightforward delta debugging.
p2490
aVSIMD (singleinstruction multipledata) instruction set extensions are quite common today in both high performance and embedded microprocessors, and enable the exploitation of a specific type of data parallelism called SLP (Superword Level Parallelism). While prior research shows that significant performance savings are possible when SLP is exploited, placing SIMD instructions in an application code manually can be very difficult and error prone. In this paper, we propose a novel automated compiler framework for improving superword level parallelism exploitation. The key part of our framework consists of two stages: superword statement generation and data layout optimization. The first stage is our main contribution and has two phases, statement grouping and statement scheduling, of which the primary goals are to increase SIMD parallelism and, more importantly, capture more superword reuses among the superword statements through global data access and reuse pattern analysis. Further, as a complementary optimization, our data layout optimization organizes data in memory space such that the price of memory operations for SLP is minimized. The results from our compiler implementation and tests on two systems indicate performance improvements as high as 15.2% over a stateoftheart SLP optimization algorithm.
p2491
aVAutomatic parallelization is a promising strategy to improve application performance in the multicore era. However, common programming practices such as the reuse of data structures introduce artificial constraints that obstruct automatic parallelization. Privatization relieves these constraints by replicating data structures, thus enabling scalable parallelization. Prior privatization schemes are limited to arrays and scalar variables because they are sensitive to the layout of dynamic data structures. This work presents Privateer, the first fully automatic privatization system to handle dynamic and recursive data structures, even in languages with unrestricted pointers. To reduce sensitivity to memory layout, Privateer speculatively separates memory objects. Privateer's lightweight runtime system validates speculative separation and speculative privatization to ensure correct parallel execution. Privateer enables automatic parallelization of generalpurpose C/C++ applications, yielding a geomean wholeprogram speedup of 11.4x over best sequential execution on 24 cores, while nonspeculative parallelization yields only 0.93x.
p2492
aVRecent hardware trends with GPUs and the increasing vector lengths of SSElike ISA extensions for multicore CPUs imply that effective exploitation of SIMD parallelism is critical for achieving high performance on emerging and future architectures. A vast majority of existing applications were developed without any attention by their developers towards effective vectorizability of the codes. While developers of production compilers such as GNU gcc, Intel icc, PGI pgcc, and IBM xlc have invested considerable effort and made significant advances in enhancing automatic vectorization capabilities, these compilers still cannot effectively vectorize many existing scientific and engineering codes. It is therefore of considerable interest to analyze existing applications to assess the inherent latent potential for SIMD parallelism, exploitable through further compiler advances and/or via manual code changes. In this paper we develop an approach to infer a program's SIMD parallelization potential by analyzing the dynamic datadependence graph derived from a sequential execution trace. By considering only the observed runtime data dependences for the trace, and by relaxing the execution order of operations to allow any dependencepreserving reordering, we can detect potential SIMD parallelism that may otherwise be missed by more conservative compiletime analyses. We show that for several benchmarks our tool discovers regions of code within computationallyintensive loops that exhibit high potential for SIMD parallelism but are not vectorized by stateoftheart compilers. We present several case studies of the use of the tool, both in identifying opportunities to enhance the transformation capabilities of vectorizing compilers, as well as in pointing to code regions to manually modify in order to enable autovectorization and performance improvement by existing compilers.
p2493
aVWe present a novel technique for verifying properties of data parallel GPU programs via test amplification. The key insight behind our work is that we can use the technique of static information flow to amplify the result of a single test execution over the set of all inputs and interleavings that affect the property being verified. We empirically demonstrate the effectiveness of test amplification for verifying racefreedom and determinism over a large number of standard GPU kernels, by showing that the result of verifying a single dynamic execution can be amplified over the massive space of possible data inputs and thread interleavings.
p2494
aVSoftwarebased fault isolation (SFI), as used in Google's Native Client (NaCl), relies upon a conceptually simple machinecode analysis to enforce a security policy. But for complicated architectures such as the x86, it is all too easy to get the details of the analysis wrong. We have built a new checker that is smaller, faster, and has a much reduced trusted computing base when compared to Google's original analysis. The key to our approach is automatically generating the bulk of the analysis from a declarative description which we relate to a formal model of a subset of the x86 instruction set architecture. The x86 model, developed in Coq, is of independent interest and should be usable for a wide range of machinelevel verification tasks.
p2495
aVAutomatically generated tools can significantly improve programmer productivity. For example, parsers and dataflow analyzers can be automatically generated from declarative specifications in the form of grammars, which tremendously simplifies the task of implementing a compiler. In this paper, we present a method for the automatic synthesis of software verification tools. Our synthesis procedure takes as input a description of the employed proof rule, e.g., program safety checking via inductive invariants, and produces a tool that automatically discovers the auxiliary assertions required by the proof rule, e.g., inductive loop invariants and procedure summaries. We rely on a (standard) representation of proof rules using recursive equations over the auxiliary assertions. The discovery of auxiliary assertions, i.e., solving the equations, is based on an iterative process that extrapolates solutions obtained for finitary unrollings of equations. We show how our method synthesizes automatic safety and liveness verifiers for programs with procedures, multithreaded programs, and functional programs. Our experimental comparison of the resulting verifiers with existing stateoftheart verification tools confirms the practicality of the approach.
p2496
aVWe describe an approach for synthesizing data representations for concurrent programs. Our compiler takes as input a program written using concurrent relations and synthesizes a representation of the relations as sets of cooperating data structures as well as the placement and acquisition of locks to synchronize concurrent access to those data structures. The resulting code is correct by construction: individual relational operations are implemented correctly and the aggregate set of operations is serializable and deadlock free. The relational specification also permits a highlevel optimizer to choose the best performing of many possible legal data representations and locking strategies, which we demonstrate with an experiment autotuning a graph benchmark.
p2497
aVModern architectures implement relaxed memory models which may reorder memory operations or execute them nonatomically. Special instructions called memory fences are provided, allowing control of this behavior. To implement a concurrent algorithm for a modern architecture, the programmer is forced to manually reason about subtle relaxed behaviors and figure out ways to control these behaviors by adding fences to the program. Not only is this process time consuming and errorprone, but it has to be repeated every time the implementation is ported to a different architecture. In this paper, we present the first scalable framework for handling realworld concurrent algorithms running on relaxed architectures. Given a concurrent C program, a safety specification, and a description of the memory model, our framework tests the program on the memory model to expose violations of the specification, and synthesizes a set of necessary ordering constraints that prevent these violations. The ordering constraints are then realized as additional fences in the program. We implemented our approach in a tool called DFence based on LLVM and used it to infer fences in a number of concurrent algorithms. Using DFence, we perform the first indepth study of the interaction between fences in realworld concurrent C programs, correctness criteria such as sequential consistency and linearizability, and memory models such as TSO and PSO, yielding many interesting observations. We believe that this is the first tool that can handle programs at the scale and complexity of a lockfree memory allocator.
p2498
aVSymbolic execution is a key component of precise binary program analysis tools. We discuss how to automatically bootstrap the construction of a symbolic execution engine for a processor instruction set such as x86, x64 or ARM. We show how to automatically synthesize symbolic representations of individual processor instructions from input/output examples and express them as bitvector constraints. We present and compare various synthesis algorithms and instruction sampling strategies. We introduce a new synthesis algorithm based on smart sampling which we show is one to two orders of magnitude faster than previous synthesis algorithms in our context. With this new algorithm, we can automatically synthesize bitvector circuits for over 500 x86 instructions (8/16/32bits, outputs, EFLAGS) using only 6 synthesis templates and in less than two hours using the Z3 SMT solver on a regular machine. During this work, we also discovered several inconsistencies across x86 processors, errors in the x86 Intel spec, and several bugs in previous manuallywritten x86 instruction handlers.
p2499
aVGeneric programming (GP) is an increasingly important trend in programming languages. Wellknown GP mechanisms, such as type classes and the C++0x concepts proposal, usually combine two features: 1) a special type of interfaces; and 2) implicit instantiation of implementations of those interfaces. Scala implicits are a GP language mechanism, inspired by type classes, that break with the tradition of coupling implicit instantiation with a special type of interface. Instead, implicits provide only implicit instantiation, which is generalized to work for any types. This turns out to be quite powerful and useful to address many limitations that show up in other GP mechanisms. This paper synthesizes the key ideas of implicits formally in a minimal and general core calculus called the implicit calculus (\u03bb\u21d2), and it shows how to build source languages supporting implicit instantiation on top of it. A novelty of the calculus is its support for partial resolution and higherorder rules (a feature that has been proposed before, but was never formalized or implemented). Ultimately, the implicit calculus provides a formal model of implicits, which can be used by language designers to study and inform implementations of similar mechanisms in their own languages.
p2500
aVPrograms using floatingpoint arithmetic are prone to accuracy problems caused by rounding and catastrophic cancellation. These phenomena provoke bugs that are notoriously hard to track down: the program does not necessarily crash and the results are not necessarily obviously wrong, but often subtly inaccurate. Further use of these values can lead to catastrophic errors. In this paper, we present a dynamic program analysis that supports the programmer in finding accuracy problems. Our analysis uses binary translation to perform every floatingpoint computation side by side in higher precision. Furthermore, we use a lightweight slicing approach to track the evolution of errors. We evaluate our analysis by demonstrating that it catches wellknown floatingpoint accuracy problems and by analyzing the Spec CFP2006 floatingpoint benchmark. In the latter, we show how our tool tracks down a catastrophic cancellation that causes a complete loss of accuracy leading to a meaningless program result. Finally, we apply our program to a complex, realworld bioinformatics application in which our program detected a serious cancellation. Correcting the instability led not only to improved quality of the result, but also to an improvement of the program's run time.In this paper, we present a dynamic program analysis that supports the programmer in finding accuracy problems. Our analysis uses binary translation to perform every floatingpoint computation side by side in higher precision. Furthermore, we use a lightweight slicing approach to track the evolution of errors. We evaluate our analysis by demonstrating that it catches wellknown floatingpoint accuracy problems and by analyzing the SpecfiCFP2006 floatingpoint benchmark. In the latter, we show how our tool tracks down a catastrophic cancellation that causes a complete loss of accuracy leading to a meaningless program result. Finally, we apply our program to a complex, realworld bioinformatics application in which our program detected a serious cancellation. Correcting the instability led not only to improved quality of the result, but also to an improvement of the program's run time.
p2501
aVChimera uses a new hybrid program analysis to provide deterministic replay for commodity multiprocessor systems. Chimera leverages the insight that it is easy to provide deterministic multiprocessor replay for dataracefree programs (one can just record nondeterministic inputs and the order of synchronization operations), so if we can somehow transform an arbitrary program to be dataracefree, then we can provide deterministic replay cheaply for that program. To perform this transformation, Chimera uses a sound static datarace detector to find all potential dataraces. It then instruments pairs of potentially racing instructions with a weaklock, which provides sufficient guarantees to allow deterministic replay but does not guarantee mutual exclusion. Unsurprisingly, a large fraction of dataraces found by the static tool are false dataraces, and instrumenting them each of them with a weaklock results in prohibitively high overhead. Chimera drastically reduces this cost from 53x to 1.39x by increasing the granularity of weaklocks without significantly compromising on parallelism. This is achieved by employing a combination of profiling and symbolic analysis techniques that target the sources of imprecision in the static datarace detector. We find that performance overhead for deterministic recording is 2.4% on average for Apache and desktop applications and about 86% for scientific applications.
p2502
aVRecovery functionality has many applications in computing systems, from speculation recovery in modern microprocessors to fault recovery in highreliability systems. Modern systems commonly recover using checkpoints. However, checkpoints introduce overheads, add complexity, and often save more state than necessary. This paper develops a novel compiler technique to recover program state without the overheads of explicit checkpoints. The technique breaks programs into idempotent regions regions that can be freely reexecuted which allows recovery without checkpointed state. Leveraging the property of idempotence, recovery can be obtained by simple reexecution. We develop static analysis techniques to construct these regions and demonstrate low overheads and large region sizes for an LLVMbased implementation. Across a set of diverse benchmark suites, we construct idempotent regions close in size to those that could be obtained with perfect runtime information. Although the resulting code runs more slowly, typical performance overheads are in the range of just 212%. The paradigm of executing entire programs as a series of idempotent regions we call idempotent processing, and it has many applications in computer systems. As a concrete example, we demonstrate it applied to the problem of compilerautomated hardware fault recovery. In comparison to two other stateoftheart techniques, redundant execution and checkpointlogging, our idempotent processing technique outperforms both by over 15%.
p2503
aVSoftwarebased threadlevel parallelization has been widely studied for exploiting data parallelism in purely computational loops to improve program performance on multiprocessors. However, none of the previous efforts deal with efficient parallelization of hybrid loops, i.e., loops that contain a mix of computation and I/O operations. In this paper, we propose a set of techniques for efficiently parallelizing hybrid loops. Our techniques apply DOALL parallelism to hybrid loops by breaking the crossiteration dependences caused by I/O operations. We also support speculative execution of I/O operations to enable speculative parallelization of hybrid loops. Helper threading is used to reduce the I/O bus contention caused by the improved parallelism. We provide an easytouse programming model for exploiting parallelism in loops with I/O operations. Parallelizing hybrid loops using our model requires few modifications to the code. We have developed a prototype implementation of our programming model. We have evaluated our implementation on a 24core machine using eight applications, including a widelyused genomic sequence assembler and a multiplayer game server, and others from PARSEC and SPEC CPU2000 benchmark suites. The hybrid loops in these applications take 23%99% of the total execution time on our 24core machine. The parallelized applications achieve speedups of 3.0x12.8x with hybrid loop parallelization over the sequential versions of the same applications. Compared to the versions of applications where only computation loops are parallelized, hybrid loop parallelization improves the application performance by 68% on average.
p2504
aVThis paper presents a new polyhedra scanning system called CodeGen+ to address the challenge of generating highperformance code for complex iteration spaces resulting from compiler optimization and autotuning systems. The strength of our approach lies in two new algorithms. First, a loop overhead removal algorithm provides precise control of tradeoffs between loop overhead and code size based on actual loop nesting depth. Second, an ifstatement simplification algorithm further reduces the number of comparisons in the code. These algorithms combined with the expressive power of Presburger arithmetic enable CodeGen+ to support complex optimization strategies expressed in iteration spaces. We compare with the stateoftheart polyhedra scanning tool CLooG on five loop nest computations, demonstrating that CodeGen+ generates code that is simpler and up to 1.15x faster.
p2505
aVThis paper presents a fully automatic approach to loop parallelization that integrates the use of static and runtime analysis and thus overcomes many known difficulties such as nonlinear and indirect array indexing and complex control flow. Our hybrid analysis framework validates the parallelization transformation by verifying the independence of the loop's memory references. To this end it represents array references using the USR (uniform set representation) language and expresses the independence condition as an equation, S=0, where S is a set expression representing array indexes. Using a language instead of an arrayabstraction representation for S results in a smaller number of conservative approximations but exhibits a potentiallyhigh runtime cost. To alleviate this cost we introduce a language translation F from the USR setexpression language to an equally rich language of predicates (F(S) ==> S = 0). Loop parallelization is then validated using a novel logic inference algorithm that factorizes the obtained complex predicates (F(S)) into a sequence of sufficient independence conditions that are evaluated first statically and, when needed, dynamically, in increasing order of their estimated complexities. We evaluate our automated solution on 26 benchmarks from PERFECTClub and SPEC suites and show that our approach is effective in parallelizing large, complex loops and obtains much better full program speedups than the Intel and IBM Fortran compilers.
p2506
aVConcurrent, objectoriented programs often use threadsafe library classes. Existing techniques for testing a threadsafe class either rely on tests using the class, on formal specifications, or on both. Unfortunately, these techniques often are not fully automatic as they involve the user in analyzing the output. This paper presents an automatic testing technique that reveals concurrency bugs in supposedly threadsafe classes. The analysis requires as input only the class under test and reports only true positives. The key idea is to generate tests in which multiple threads call methods on a shared instance of the tested class. If a concurrent test exhibits an exception or a deadlock that cannot be triggered in any linearized execution of the test, the analysis reports a thread safety violation. The approach is easily applicable, because it is independent of handwritten tests and explicit specifications. The analysis finds 15 concurrency bugs in popular Java libraries, including two previously unknown bugs in the Java standard library.
p2507
aVExisting dynamic race detectors suffer from at least one of the following three limitations: (i)space overhead per memory location grows linearly with the number of parallel threads [13], severely limiting the parallelism that the algorithm can handle; (ii)sequentialization: the parallel program must be processed in a sequential order, usually depthfirst [12, 24]. This prevents the analysis from scaling with available hardware parallelism, inherently limiting its performance; (iii) inefficiency: even though race detectors with good theoretical complexity exist, they do not admit efficient implem entations and are unsuitable for practical use [4, 18]. We present a new precise dynamic race detector that leverages structured parallelism in order to address these limitations. Our algorithm requires constant space per memory location, works in parallel, and is efficient in practice. We implemented and evaluated our algorithm on a set of 15 benchmarks. Our experimental results indicate an average (geometric mean) slowdown of 2.78x on a 16core SMP system.
p2508
aVShared memory multithreading is a popular approach to parallel programming, but also fiendishly hard to get right. We present Liquid Effects, a typeandeffect system based on refinement types which allows for finegrained, lowlevel, shared memory multithreading while statically guaranteeing that a program is deterministic. Liquid Effects records the effect of an expression as a for mula in firstorder logic, making our typeandeffect system highly expressive. Further, effects like Read and Write are recorded in Liquid Effects as ordinary uninterpreted predicates, leaving the effect system open to extension by the user. By building our system as an extension to an existing dependent refinement type system, our system gains precise value and branchsensitive reasoning about effects. Finally, our system exploits the Liquid Types refinement type inference technique to automatically infer refinement types and effects. We have implemented our typeandeffect checking techniques in CSOLVE, a refinement type inference system for C programs. We demonstrate how CSOLVE uses Liquid Effects to prove the determinism of a variety of benchmarks.
p2509
aVLinearizability is a key design methodology for reasoning about implementations of concurrent abstract data types in both shared memory and message passing systems. It provides the illusion that operations execute sequentially and faultfree, despite the asynchrony and faults inherent to a concurrent system, especially a distributed one. A key property of linearizability is interobject composability: a system composed of linearizable objects is itself linearizable. However, devising linearizable objects is very difficult, requiring complex algorithms to work correctly under general circumstances, and often resulting in bad averagecase behavior. Concurrent algorithm designers therefore resort to speculation: optimizing algorithms to handle common scenarios more efficiently. The outcome are even more complex protocols, for which it is no longer tractable to prove their correctness. To simplify the design of efficient yet robust linearizable protocols, we propose a new notion: speculative linearizability. This property is as general as linearizability, yet it allows intraobject composability: the correctness of independent protocol phases implies the correctness of their composition. In particular, it allows the designer to focus solely on the proof of an optimization and derive the correctness of the overall protocol from the correctness of the existing, nonoptimized one. Our notion of protocol phases allows processes to independently switch from one phase to another, without requiring them to reach agreement to determine the change of a phase. To illustrate the applicability of our methodology, we show how examples of speculative algorithms for shared memory and asynchronous message passing naturally fit into our framework. We rigorously define speculative linearizability and prove our intraobject composition theorem in a tracebased as well as an automatonbased model. To obtain a further degree of confidence, we also formalize and mechanically check the theorem in the automatonbased model, using the I/O automata framework within the Isabelle interactive proof assistant. We expect our framework to enable, for the first time, scalable specifications and mechanical proofs of speculative implementations of linearizable objects.
p2510
aVTraditional profilers identify where a program spends most of its resources. They do not provide information about why the program spends those resources or about how resource consumption would change for different program inputs. In this paper we introduce the idea of algorithmic profiling. While a traditional profiler determines a set of measured cost values, an algorithmic profiler determines a cost function. It does that by automatically determining the "inputs" of a program, by measuring the program's "cost" for any given input, and by inferring an empirical cost function.
p2511
aVDevelopers frequently use inefficient code sequences that could be fixed by simple patches. These inefficient code sequences can cause significant performance degradation and resource waste, referred to as performance bugs. Meager increases in single threaded performance in the multicore era and increasing emphasis on energy efficiency call for more effort in tackling performance bugs. This paper conducts a comprehensive study of 110 realworld performance bugs that are randomly sampled from five representative software suites (Apache, Chrome, GCC, Mozilla, and MySQL). The findings of this study provide guidance for future work to avoid, expose, detect, and fix performance bugs. Guided by our characteristics study, efficiency rules are extracted from 25 patches and are used to detect performance bugs. 332 previously unknown performance problems are found in the latest versions of MySQL, Apache, and Mozilla applications, including 219 performance problems found by applying rules across applications.
p2512
aVIn this paper we present a profiling methodology and toolkit for helping developers discover hidden asymptotic inefficiencies in the code. From one or more runs of a program, our profiler automatically measures how the performance of individual routines scales as a function of the input size, yielding clues to their growth rate. The output of the profiler is, for each executed routine of the program, a set of tuples that aggregate performance costs by input size. The collected profiles can be used to produce performance plots and derive trend functions by statistical curve fitting or bounding techniques. A key feature of our method is the ability to automatically measure the size of the input given to a generic code fragment: to this aim, we propose an effective metric for estimating the input size of a routine and show how to compute it efficiently. We discuss several case studies, showing that our approach can reveal asymptotic bottlenecks that other profilers may fail to detect and characterize the workload and behavior of individual routines in the context of real applications. To prove the feasibility of our techniques, we implemented a Valgrind tool called aprof and performed an extensive experimental evaluation on the SPEC CPU2006 benchmarks. Our experiments show that aprof delivers comparable performance to other prominent Valgrind tools, and can generate informative plots even from single runs on typical workloads for most algorithmicallycritical routines.
p2513
aVThe "Cloud" is a wonderfully expansive phrase used to denote computation and data storage centralized in a large datacenter and elastically accessed across a network. The concept is not new; web sites and business servers have run in datacenters for a long time. These, however, were specialized applications, outside of the mainstream of desktop programs. The past few years has seen enormous change as the mainstream shifts from a single computer to mobile devices and clusters of computers. Three factors are driving this change. 1) Mobile computing, where apps run on a size and powerconstrained device and would be far less interesting without backend systems to augment computation and storage capacity. 2) Big data, which uses clusters of computers to extract valuable information from vast amounts of unstructured data. 3) Inexpensive, elastic computing, pioneered by Amazon Web Services, which enables everyone to rapidly obtain and use many servers. As a researcher from the language and compiler community, I firmly believe this sea change is at heart a programming problem. Cloud computing is far different from the environment in which most of today's languages and tools were developed, and few programmers have mastered its complexity. New challenges include pervasive parallelism, partial failure, high and variable communication latency, and replication for reliability and throughput.
p2514
aVLive programming allows programmers to edit the code of a running program and immediately see the effect of the code changes. This tightening of the traditional editcompilerun cycle reduces the cognitive gap between program code and execution, improving the learning experience of beginning programmers while boosting the productivity of seasoned ones. Unfortunately, live programming is difficult to realize in practice as imperative languages lack welldefined abstraction boundaries that make live programming responsive or its feedback comprehensible. This paper enables live programming for user interface programming by cleanly separating the rendering and nonrendering aspects of a UI program, allowing the display to be refreshed on a code change without restarting the program. A type and effect system formalizes this separation and provides an evaluation model that incorporates the code update step. By putting live programming on a more formal footing, we hope to enable critical and technical discussion of live programming systems.
p2515
aVHighperformance computing applications, such as autotuners and domainspecific languages, rely on generative programming techniques to achieve high performance and portability. However, these systems are often implemented in multiple disparate languages and perform code generation in a separate process from program execution, making certain optimizations difficult to engineer. We leverage a popular scripting language, Lua, to stage the execution of a novel lowlevel language, Terra. Users can implement optimizations in the highlevel language, and use builtin constructs to generate and execute highperformance Terra code. To simplify metaprogramming, Lua and Terra share the same lexical environment, but, to ensure performance, Terra code can execute independently of Lua's runtime. We evaluate our design by reimplementing existing multilanguage systems entirely in Terra. Our Terrabased autotuner for BLAS routines performs within 20% of ATLAS, and our DSL for stencil computations runs 2.3x faster than handwritten C.
p2516
aVSparse Matrix Vector multiplication (SpMV) is an important kernel in both traditional high performance computing and emerging dataintensive applications. By far, SpMV libraries are optimized by either applicationspecific or architecturespecific approaches, making the libraries become too complicated to be used extensively in real applications. In this work we develop a Sparse Matrixvector multiplication AutoTuning system (SMAT) to bridge the gap between specific optimizations and generalpurpose usage. SMAT provides users with a unified programming interface in compressed sparse row (CSR) format and automatically determines the optimal format and implementation for any input sparse matrix at runtime. For this purpose, SMAT leverages a learning model, which is generated in an offline stage by a machine learning method with a training set of more than 2000 matrices from the UF sparse matrix collection, to quickly predict the best combination of the matrix feature parameters. Our experiments show that SMAT achieves impressive performance of up to 51GFLOPS in singleprecision and 37GFLOPS in doubleprecision on mainstream x86 multicore processors, which are both more than 3 times faster than the Intel MKL library. We also demonstrate its adaptability in an algebraic multigrid solver from Hypre library with above 20% performance improvement reported.
p2517
aVData locality and parallelism are critical optimization objectives for performance on modern multicore machines. Both coarsegrain parallelism (e.g., multicore) and finegrain parallelism (e.g., vector SIMD) must be effectively exploited, but despite decades of progress at both ends, current compiler optimization schemes that attempt to address data locality and both kinds of parallelism often fail at one of the three objectives. We address this problem by proposing a 3step framework, which aims for integrated data locality, multicore parallelism and SIMD execution of programs. We define the concept of vectorizable codelets, with properties tailored to achieve effective SIMD code generation for the codelets. We leverage the power of a modern highlevel transformation framework to restructure a program to expose good ISAindependent vectorizable codelets, exploiting multidimensional data reuse. Then, we generate ISAspecific customized code for the codelets, using a collection of lowerlevel SIMDfocused optimizations. We demonstrate our approach on a collection of numerical kernels that we automatically tile, parallelize and vectorize, exhibiting significant performance improvements over existing compilers.
p2518
aVWe present CLAP, a new technique to reproduce concurrency bugs. CLAP has two key steps. First, it logs thread local execution paths at runtime. Second, offline, it computes memory dependencies that accord with the logged execution and are able to reproduce the observed bug. The second step works by combining constraints from the thread paths and constraints based on a memory model, and computing an execution with a constraint solver. CLAP has four major advantages. First, logging purely local execution of each thread is substantially cheaper than logging memory interactions, which enables CLAP to be efficient compared to previous approaches. Second, our logging does not require any synchronization and hence with no added memory barriers or fences; this minimizes perturbation and missed bugs due to extra synchronizations foreclosing certain racy behaviors. Third, since it uses no synchronization, we extend CLAP to work on a range of relaxed memory models, such as TSO and PSO, in addition to sequential consistency. Fourth, CLAP can compute a much simpler execution than the original one, that reveals the bug with minimal thread context switches. To mitigate the scalability issues, we also present an approach to parallelize constraint solving, which theoretically scales our technique to programs with arbitrary execution length. Experimental results on a variety of multithreaded benchmarks and real world concurrent applications validate these advantages by showing that our technique is effective in reproducing concurrency bugs even under relaxed memory models; furthermore, it is significantly more efficient than a stateoftheart technique that records shared memory dependencies, reducing execution time overhead by 45% and log size by 88% on average.
p2519
aVWe present CONCURRIT, a domainspecific language (DSL) for reproducing concurrency bugs. Given some partial information about the nature of a bug in an application, a programmer can write a CONCURRIT script to formally and concisely specify a set of thread schedules to explore in order to find a schedule exhibiting the bug. Further, the programmer can specify how these thread schedules should be searched to find a schedule that reproduces the bug. We implemented CONCURRIT as an embedded DSL in C++, which uses manual or automatic source instrumentation to partially control the scheduling of the software under test. Using CONCURRIT, we were able to write concise tests to reproduce concurrency bugs in a variety of benchmarks, including the Mozilla's SpiderMonkey JavaScript engine, Memcached, Apache's HTTP server, and MySQL.
p2520
aVWe present an analysis for identifying determinate variables and expressions that always have the same value at a given program point. This information can be exploited by client analyses and tools to, e.g., identify dead code or specialize uses of dynamic language constructs such as eval, replacing them with equivalent static constructs. Our analysis is completely dynamic and only needs to observe a single execution of the program, yet the determinacy facts it infers hold for any execution. We present a formal soundness proof of the analysis for a simple imperative language, and a prototype implementation that handles full JavaScript. Finally, we report on two case studies that explored how static analysis for JavaScript could leverage the information gathered by dynamic determinacy analysis. We found that in some cases scalability of static pointer analysis was improved dramatically, and that many uses of runtime code generation could be eliminated.
p2521
aVModern compilers, such as LLVM and GCC, use a static single assignment(SSA) intermediate representation (IR) to simplify and enable many advanced optimizations. However, formally verifying the correctness of SSAbased optimizations is challenging because SSA properties depend on a function's entire controlflow graph. This paper addresses this challenge by developing a proof technique for proving SSAbased program invariants and compiler optimizations. We use this technique in the Coq proof assistant to create mechanized correctness proofs of several "micro" transformations that form the building blocks for larger SSA optimizations. To demonstrate the utility of this approach, we formally verify a variant of LLVM's mem2reg transformation in Vellvm, a Coqbased formal semantics of the LLVM IR. The extracted implementation generates code with performance comparable to that of LLVM's unverified implementation.
p2522
aVCompilers sometimes generate correct sequential code but break the concurrency memory model of the programming language: these subtle compiler bugs are observable only when the miscompiled functions interact with concurrent contexts, making them particularly hard to detect. In this work we design a strategy to reduce the hard problem of hunting concurrency compiler bugs to differential testing of sequential code and build a tool that puts this strategy to work. Our first contribution is a theory of sound optimisations in the C11/C++11 memory model, covering most of the optimisations we have observed in real compilers and validating the claim that common compiler optimisations are sound in the C11/C++11 memory model. Our second contribution is to show how, building on this theory, concurrency compiler bugs can be identified by comparing the memory trace of compiled code against a reference memory trace for the source code. Our tool identified several mistaken write introductions and other unexpected behaviours in the latest release of the gcc compiler.
p2523
aVObjectrelational mapping libraries are a popular way for applications to interact with databases because they provide transparent access to the database using the same language as the application. Unfortunately, using such frameworks often leads to poor performance, as modularity concerns encourage developers to implement relational operations in application code. Such application code does not take advantage of the optimized relational implementations that database systems provide, such as efficient implementations of joins or push down of selection predicates. In this paper we present QBS, a system that automatically transforms fragments of application logic into SQL queries. QBS differs from traditional compiler optimizations as it relies on synthesis technology to generate invariants and postconditions for a code fragment. The postconditions and invariants are expressed using a new theory of ordered relations that allows us to reason precisely about both the contents and order of the records produced complex code fragments that compute joins and aggregates. The theory is close in expressiveness to SQL, so the synthesized postconditions can be readily translated to SQL queries. Using 75 code fragments automatically extracted from over 120k lines of opensource code written using the Java Hibernate ORM, we demonstrate that our approach can convert a variety of imperative constructs into relational specifications and significantly improve application performance asymptotically by orders of magnitude.
p2524
aVAggressive random testing tools ("fuzzers") are impressively effective at finding compiler bugs. For example, a single testcase generator has resulted in more than 1,700 bugs reported for a single JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately and repeatedly find bugs that may not be severe enough to fix right away. Currently, users filter out undesirable test cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. This paper formulates and addresses the fuzzer taming problem: given a potentially large number of random test cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. Our evaluation shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering 46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine.
p2525
aVModular assertion checkers are plagued with false alarms due to the need for precise environment specifications (preconditions and callee postconditions). Even the fully precise checkers report assertion failures under the most demonic environments allowed by unconstrained or partial specifications. The inability to preclude overly adversarial environments makes such checkers less attractive to developers and severely limits the adoption of such tools in the development cycle. In this work, we propose a parameterized framework for prioritizing the assertion failures reported by a modular verifier, with the goal of suppressing warnings from overly demonic environments. We formalize it almostcorrect specifications as the minimal weakening of an angelic specification (over a set of predicates) that precludes any dead code intraprocedurally. Our work is inspired by and generalizes some aspects of semantic inconsistency detection. Our formulation allows us to lift this idea to a general class of warnings. We have developed a prototype acspec, which we use to explore a few instantiations of the framework and report preliminary findings on a diverse set of C benchmarks.
p2526
aVBranchingtime temporal logics (e.g. CTL, CTL*, modal mucalculus) allow us to ask sophisticated questions about the nondeterminism that appears in systems. Applications of this type of reasoning include planning, games, security analysis, disproving, precondition synthesis, environment synthesis, etc. Unfortunately, existing automatic branchingtime verification tools have limitations that have traditionally restricted their applicability (e.g. pushdown systems only, universal path quantifiers only, etc). In this paper we introduce an automation strategy that lifts many of these previous restrictions. Our method works reliably for properties with nontrivial mixtures of universal and existential modal operators. Furthermore, our approach is designed to support (possibly infinitestate) programs. The basis of our approach is the observation that existential reasoning can be reduced to universal reasoning if the system's statespace is appropriately restricted. This restriction on the statespace must meet a constraint derived from recent work on proving nontermination. The observation leads to a new route for implementation based on existing tools. To demonstrate the practical viability of our approach, we report on the results applying our preliminary implementation to a set of benchmarks drawn from the Windows operating system, the PostgreSQL database server, SoftUpdates patching system, as well as other handcrafted examples.
p2527
aVWe propose natural proofs for reasoning with programs that manipulate datastructures against specifications that describe the structure of the heap, the data stored within it, and separation and framing of substructures. Natural proofs are a subclass of proofs that are amenable to completely automated reasoning, that provide sound but incomplete procedures, and that capture common reasoning tactics in program verification. We develop a dialect of separation logic over heaps, called Dryad, with recursive definitions that avoids explicit quantification. We develop ways to reason with heaplets using classical logic over the theory of sets, and develop natural proofs for reasoning using proof tactics involving disciplined unfoldings and formula abstractions. Natural proofs are encoded into decidable theories of firstorder logic so as to be discharged using SMT solvers. We also implement the technique and show that a large class of more than 100 correct programs that manipulate datastructures are amenable to full functional correctness using the proposed natural proof method. These programs are drawn from a variety of sources including standard datastructures, the SchorrWaite algorithm for garbage collection, a large number of lowlevel C routines from the Glib library and OpenBSD library, the Linux kernel, and routines from a secure verified OSbrowser project. Our work is the first that we know of that can handle such a wide range of full functional verification properties of heaps automatically, given pre/post and loop invariant annotations. We believe that this work paves the way for deductive verification technology to be used by programmers who do not (and need not) understand the internals of the underlying logic solvers, significantly increasing their applicability in building reliable systems.
p2528
aVAmong techniques for parallelizing sequential codes, privatization is a common and significant transformation performed by both compilers and runtime parallelizing systems. Without privatization, repetitive updates to the same data structures often introduce spurious data dependencies that hide the inherent parallelism. Unfortunately, it remains a significant challenge to compilers to automatically privatize dynamic and recursive data structures which appear frequently in real applications written in languages such as C/C++. This is because such languages lack a naming mechanism to define the address range of a pointerbased data structure, in contrast to arrays with explicitly declared bounds. In this paper we present a novel solution to this difficult problem by expanding general data structures such that memory accesses issued from different threads to contentious data structures are directed to different data fields. Based on compiletime type checking and a data dependence graph, this aggressive extension to the traditional scalar and array expansion isolates the address ranges among different threads, without struggling with privatization based on threadprivate stacks, such that the targeted loop can be effectively parallelized. With this method fully implemented in GCC, experiments are conducted on a set of programs from wellknown benchmark suites such as Mibench, MediaBench II and SPECint. Results show that the new approach can lead to a high speedup when executing the transformed code on multiple cores.
p2529
aVMost programming languages use monitors with explicit signals for synchronization in sharedmemory programs. Requiring programmers to signal threads explicitly results in many concurrency bugs due to missed notifications, or notifications on wrong condition variables. In this paper, we describe an implementation of an automatic signaling monitor in Java called AutoSynch that eliminates such concurrency bugs by removing the burden of signaling from the programmer. We show that the belief that automatic signaling monitors are prohibitively expensive is wrong. For most problems, programs based on AutoSynch are almost as fast as those based on explicit signaling. For some, AutoSynch is even faster than explicit signaling because it never uses signalAll, whereas the programmers end up using signalAll with the explicit signal mechanism. AutoSynch} achieves efficiency in synchronization based on three novel ideas. We introduce an operation called closure that enables the predicate evaluation in every thread, thereby reducing context switches during the execution of the program. Secondly, AutoSynch avoids signalAll by using a property called relay invariance that guarantees that whenever possible there is always at least one thread whose condition is true which has been signaled. Finally, AutoSynch uses a technique called predicate tagging to efficiently determine a thread that should be signaled. To evaluate the efficiency of AutoSynch, we have implemented many different wellknown synchronization problems such as the producers/consumers problem, the readers/writers problems, and the dining philosophers problem. The results show that AutoSynch is almost as efficient as the explicitsignal monitor and even more efficient for some cases.
p2530
aVLinearizable libraries provide operations that appear to execute atomically. Clients, however, may need to execute a sequence of operations (a composite operation) atomically. We consider the problem of extending a linearizable library to support arbitrary atomic composite operations by clients. We introduce a novel approach in which the concurrent library ensures atomicity of composite operations by exploiting information (foresight) provided by its clients. We use a correctness condition, based on a notion of dynamic rightmovers, that guarantees that composite operations execute atomically without deadlocks, and without using rollbacks. We present a static analysis to infer the foresight information required by our approach, allowing a compiler to automatically insert the foresight information into the client. This relieves the client programmer of this burden and simplifies writing client code. We present a generic technique for extending the library implementation to realize foresightbased synchronization. This technique is used to implement a generalpurpose Java library for Map data structures   the library permits composite operations to simultaneously work with multiple instances of Map data structures. We use the Maps library and the static analysis to enforce atomicity of a wide selection of reallife Java composite operations. Our experiments indicate that our approach enables realizing efficient and scalable synchronization for reallife composite operations.
p2531
aVWe present a precise, pathsensitive static analysis for reasoning about heap reachability, that is, whether an object can be reached from another variable or object via pointer dereferences. Precise reachability information is useful for a number of clients, including static detection of a class of Android memory leaks. For this client, we found the heap reachability information computed by a stateoftheart pointsto analysis was too imprecise, leading to numerous falsepositive leak reports. Our analysis combines a symbolic execution capable of pathsensitivity and strong updates with abstract heap information computed by an initial flowinsensitive pointsto analysis. This novel mixed representation allows us to achieve both precision and scalability by leveraging the precomputed pointsto facts to guide execution and prune infeasible paths. We have evaluated our techniques in the Thresher tool, which we used to find several developerconfirmed leaks in Android applications.
p2532
aVWith the maturing of technology for model checking and constraint solving, there is an emerging opportunity to develop programming tools that can transform the way systems are specified. In this paper, we propose a new way to program distributed protocols using concolic snippets. Concolic snippets are sample execution fragments that contain both concrete and symbolic values. The proposed approach allows the programmer to describe the desired system partially using the traditional model of communicating extended finitestatemachines (EFSM), along with highlevel invariants and concrete execution fragments. Our synthesis engine completes an EFSM skeleton by inferring guards and updates from the given fragments which is then automatically analyzed using a model checker with respect to the desired invariants. The counterexamples produced by the model checker can then be used by the programmer to add new concrete execution fragments that describe the correct behavior in the specific scenario corresponding to the counterexample. We describe TRANSIT, a language and prototype implementation of the proposed specification methodology for distributed protocols. Experimental evaluations of TRANSIT to specify cache coherence protocols show that (1) the algorithm for expression inference from concolic snippets can synthesize expressions of size 15 involving typical operators over commonly occurring types, (2) for a classical directorybased protocol, TRANSIT automatically generates, in a few seconds, a complete implementation from a specification consisting of the EFSM structure and a few concrete examples for every transition, and (3) a published partial description of the SGI Origin cache coherence protocol maps directly to symbolic examples and leads to a complete implementation in a few iterations, with the programmer correcting counterexamples resulting from underspecified transitions by adding concrete examples in each iteration.
p2533
aVNew memory technologies, such as phasechange memory (PCM), promise denser and cheaper main memory, and are expected to displace DRAM. However, many of them experience permanent failures far more quickly than DRAM. DRAM mechanisms that handle permanent failures rely on very low failure rates and, if directly applied to PCM, are extremely inefficient: Discarding a page when the first line fails wastes 98% of the memory. This paper proposes low complexity cooperative software and hardware that handle failure rates as high as 50%. Our approach makes error handling transparent to the application by using the memory abstraction offered by managed languages. Once hardware error correction for a memory line is exhausted, rather than discarding the entire page, the hardware communicates the failed line to a failureaware OS and runtime. The runtime ensures memory allocations never use failed lines and moves data when lines fail during program execution. This paper describes minimal extensions to an Immix markregion garbage collector, which correctly utilizes pages with failed physical lines by skipping over failures. This paper also proposes hardware support that clusters failed lines at one end of a memory region to reduce fragmentation and improve performance under failures. Contrary to accepted hardware wisdom that advocates for wearleveling, we show that with software support nonuniform failures delay the impact of memory failure. Together, these mechanisms incur no performance overhead when there are no failures and at failure levels of 10% to 50% suffer only an average overhead of 4% and 12%}, respectively. These results indicate that hardware and software cooperation can greatly extend the life of wearable memories.
p2534
aVWe present a new method for automatically providing feedback for introductory programming problems. In order to use this method, we need a reference implementation of the assignment, and an error model consisting of potential corrections to errors that students might make. Using this information, the system automatically derives minimal corrections to student's incorrect solutions, providing them with a measure of exactly how incorrect a given solution was, as well as feedback about what they did wrong. We introduce a simple language for describing error models in terms of correction rules, and formally define a ruledirected translation strategy that reduces the problem of finding minimal corrections in an incorrect program to the problem of synthesizing a correct program from a sketch. We have evaluated our system on thousands of real student attempts obtained from the Introduction to Programming course at MIT (6.00) and MITx (6.00x). Our results show that relatively simple error models can correct on average 64% of all incorrect submissions in our benchmark set.
p2535
aVCompaction of a managed heap is considered a costly operation, and is avoided as much as possible in commercial runtimes. Instead, partial compaction is often used to defragment parts of the heap and avoid space blow up. Previous study of compaction limitation provided some initial asymptotic bounds but no implications for practical systems. In this work, we extend the theory to obtain better bounds and make them strong enough to become meaningful for modern systems.
p2536
aVWe describe the design and implementation of P, a domainspecific language to write asynchronous event driven code. P allows the programmer to specify the system as a collection of interacting state machines, which communicate with each other using events. P unifies modeling and programming into one activity for the programmer. Not only can a P program be compiled into executable code, but it can also be tested using model checking techniques. P allows the programmer to specify the environment, used to "close" the system during testing, as nondeterministic ghost machines. Ghost machines are erased during compilation to executable code; a type system ensures that the erasure is semantics preserving. The P language is designed so that a P program can be checked for responsiveness the ability to handle every event in a timely manner. By default, a machine needs to handle every event that arrives in every state. But handling every event in every state is impractical. The language provides a notion of deferred events where the programmer can annotate when she wants to delay processing an event. The default safety checker looks for presence of unhandled events. The language also provides default liveness checks that an event cannot be potentially deferred forever. P was used to implement and verify the core of the USB device driver stack that ships with Microsoft Windows 8. The resulting driver is more reliable and performs better than its prior incarnation (which did not use P); we have more confidence in the robustness of its design due to the language abstractions and verification provided by P.
p2537
aVThe field of quantum algorithms is vibrant. Still, there is currently a lack of programming languages for describing quantum computation on a practical scale, i.e., not just at the level of toy problems. We address this issue by introducing Quipper, a scalable, expressive, functional, higherorder quantum programming language. Quipper has been used to program a diverse set of nontrivial quantum algorithms, and can generate quantum gate representations using trillions of gates. It is geared towards a model of computation that uses a classical computer to control a quantum device, but is not dependent on any particular model of quantum hardware. Quipper has proven effective and easy to use, and opens the door towards using formal methods to analyze quantum algorithms.
p2538
aVPattern matching, an important feature of functional languages, is in conflict with data abstraction and extensibility, which are central to objectoriented languages. Modal abstraction offers an integration of deep pattern matching and convenient iteration abstractions into an objectoriented setting; however, because of data abstraction, it is challenging for a compiler to statically verify properties such as exhaustiveness. In this work, we extend modal abstraction in the JMatch language to support static, modular reasoning about exhaustiveness and redundancy. New matching specifications allow these properties to be checked using an SMT solver. We also introduce expressive patternmatching constructs. Our evaluation shows that these new features enable more concise code and that the performance of checking exhaustiveness and redundancy is acceptable.
p2539
aVA software product line (SPL) encodes a potentially large variety of software products as variants of some common code base. Up until now, reusing traditional static analyses for SPLs was virtually intractable, as it required programmers to generate and analyze all products individually. In this work, however, we show how an important class of existing interprocedural static analyses can be transparently lifted to SPLs. Without requiring programmers to change a single line of code, our approach SPLLIFT automatically converts any analysis formulated for traditional programs within the popular IFDS framework for interprocedural, finite, distributive, subset problems to an SPLaware analysis formulated in the IDE framework, a wellknown extension to IFDS. Using a full implementation based on Heros, Soot, CIDE and JavaBDD, we show that with SPLLIFT one can reuse IFDSbased analyses without changing a single line of code. Through experiments using three static analyses applied to four Javabased product lines, we were able to show that our approach produces correct results and outperforms the traditional approach by several orders of magnitude.
p2540
aVWe propose a technique to efficiently search a large family of abstractions in order to prove a query using a parametric dataflow analysis. Our technique either finds the cheapest such abstraction or shows that none exists. It is based on counterexampleguided abstraction refinement but applies a novel metaanalysis on abstract counterexample traces to efficiently find abstractions that are incapable of proving the query. We formalize the technique in a generic framework and apply it to two analyses: a typestate analysis and a threadescape analysis. We demonstrate the effectiveness of the technique on a suite of Java benchmark programs.
p2541
aVNontrivial analysis problems require complete lattices with infinite ascending and descending chains. In order to compute reasonably precise postfixpoints of the resulting systems of equations, Cousot and Cousot have suggested accelerated fixpoint iteration by means of widening and narrowing. The strict separation into phases, however, may unnecessarily give up precision that cannot be recovered later. While widening is also applicable if equations are nonmonotonic, this is no longer the case for narrowing. A narrowing iteration to improve a given postfixpoint, additionally, must assume that all righthand sides are monotonic. The latter assumption, though, is not met in presence of widening. It is also not met by equation systems corresponding to contextsensitive interprocedural analysis, possibly combining contextsensitive analysis of local information with flowinsensitive analysis of globals. As a remedy, we present a novel operator that combines a given widening operator with a given narrowing operator. We present adapted versions of roundrobin as well as of worklist iteration, local, and sideeffecting solving algorithms for the combined operator and prove that the resulting solvers always return sound results and are guaranteed to terminate for monotonic systems whenever only finitely many unknowns (constraint variables) are encountered.
p2542
aVModern programming languages, ranging from Haskell and ML, to JavaScript, C# and Java, all make extensive use of higherorder state. This paper advocates a new verification methodology for higherorder stateful programs, based on a new monad of predicate transformers called the Dijkstra monad. Using the Dijkstra monad has a number of benefits. First, the monad naturally yields a weakest precondition calculus. Second, the computed specifications are structurally simpler in several ways, e.g., singlestate postconditions are sufficient (rather than the more complex twostate postconditions). Finally, the monad can easily be varied to handle features like exceptions and heap invariants, while retaining the same type inference algorithm. We implement the Dijkstra monad and its type inference algorithm for the F* programming language. Our most extensive case study evaluates the Dijkstra monad and its F* implementation by using it to verify JavaScript programs. Specifically, we describe a tool chain that translates programs in a subset of JavaScript decorated with assertions and loop invariants to F*. Once in F*, our type inference algorithm computes verification conditions and automatically discharges their proofs using an SMT solver. We use our tools to prove that a core model of the JavaScript runtime in F* respects various invariants and that a suite of JavaScript source programs are free of runtime errors.
p2543
aVRecent developments in the systematic construction of abstract interpreters hinted at the possibility of a broad unification of concepts in static analysis. We deliver that unification by showing contextsensitivity, polyvariance, flowsensitivity, reachabilitypruning, heapcloning and cardinalitybounding to be independent of any particular semantics. Monads become the unifying agent between these concepts and between semantics. For instance, by plugging the same "contextinsensitivity monad" into a monadicallyparameterized semantics for Java or for the lambda calculus, it yields the expected contextinsensitive analysis. To achieve this unification, we develop a systematic method for transforming a concrete semantics into a monadicallyparameterized abstract machine. Changing the monad changes the behavior of the machine. By changing the monad, we recover a spectrum of machines from the original concrete semantics to a monovariant, flow and contextinsensitive static analysis with a singlythreaded heap and weak updates. The monadic parameterization also suggests an abstraction over the ubiquitous monotone fixedpoint computation found in static analysis. This abstraction makes it straightforward to instrument an analysis with highlevel strategies for improving precision and performance, such as abstract garbage collection and widening. While the paper itself runs the development for continuationpassing style, our generic implementation replays it for directstyle lambdacalculus and Featherweight Java to support generality.
p2544
aVGraphical user interfaces (GUIs) mediate many of our interactions with computers. Functional Reactive Programming (FRP) is a promising approach to GUI design, providing highlevel, declarative, compositional abstractions to describe user interactions and timedependent computations. We present Elm, a practical FRP language focused on easy creation of responsive GUIs. Elm has two major features: simple declarative support for Asynchronous FRP; and purely functional graphical layout. Asynchronous FRP allows the programmer to specify when the global ordering of event processing can be violated, and thus enables efficient concurrent execution of FRP programs; longrunning computation can be executed asynchronously and not adversely affect the responsiveness of the user interface. Layout in Elm is achieved using a purely functional declarative framework that makes it simple to create and combine text, images, and video into rich multimedia displays. Together, Elm's two major features simplify the complicated task of creating responsive and usable GUIs.
p2545
aVDeveloping modern software typically involves composing functionality from existing libraries. This task is difficult because libraries may expose many methods to the developer. To help developers in such scenarios, we present a technique that synthesizes and suggests valid expressions of a given type at a given program point. As the basis of our technique we use type inhabitation for lambda calculus terms in long normal form. We introduce a succinct representation for type judgements that merges types into equivalence classes to reduce the search space, then reconstructs any desired number of solutions on demand. Furthermore, we introduce a method to rank solutions based on weights derived from a corpus of code. We implemented the algorithm and deployed it as a plugin for the Eclipse IDE for Scala. We show that the techniques we incorporated greatly increase the effectiveness of the approach. Our evaluation benchmarks are code examples from programming practice; we make them available for future comparisons.
p2546
aVContextsensitive pointsto analysis is valuable for achieving high precision with good performance. The standard flavors of contextsensitivity are callsitesensitivity (kCFA) and objectsensitivity. Combining both flavors of contextsensitivity increases precision but at an infeasibly high cost. We show that a selective combination of callsite and objectsensitivity for Java pointsto analysis is highly profitable. Namely, by keeping a combined context only when analyzing selected language features, we can closely approximate the precision of an analysis that keeps both contexts at all times. In terms of speed, the selective combination of both kinds of context not only vastly outperforms nonselective combinations but is also faster than a mere objectsensitive analysis. This result holds for a large array of analyses (e.g., 1objectsensitive, 2objectsensitive with a contextsensitive heap, typesensitive) establishing a new set of performance/precision sweet spots.
p2547
aVThe contextfree language (CFL) reachability problem is a wellknown fundamental formulation in program analysis. In practice, many program analyses, especially pointer analyses, adopt a restricted version of CFLreachability, DyckCFLreachability, and compute on edgelabeled bidirected graphs. Solving the allpairs DyckCFLreachability on such bidirected graphs is expensive. For a bidirected graph with n nodes and m edges, the traditional dynamic programming style algorithm exhibits a subcubic time complexity for the Dyck language with k kinds of parentheses. When the underlying graphs are restricted to bidirected trees, an algorithm with O(n log n log k) time complexity was proposed recently. This paper studies the DyckCFLreachability problems on bidirected trees and graphs. In particular, it presents two fast algorithms with O(n) and O(n + m log m) time complexities on trees and graphs respectively. We have implemented and evaluated our algorithms on a stateoftheart alias analysis for Java. Results on standard benchmarks show that our algorithms achieve orders of magnitude speedup and consume less memory.
p2548
aVWe propose an approach for the static analysis of probabilistic programs that sense, manipulate, and control based on uncertain data. Examples include programs used in risk analysis, medical decision making and cyberphysical systems. Correctness properties of such programs take the form of queries that seek the probabilities of assertions over program variables. We present a static analysis approach that provides guaranteed interval bounds on the values (assertion probabilities) of such queries. First, we observe that for probabilistic programs, it is possible to conclude facts about the behavior of the entire program by choosing a finite, adequate set of its paths. We provide strategies for choosing such a set of paths and verifying its adequacy. The queries are evaluated over each path by a combination of symbolic execution and probabilistic volumebound computations. Each path yields interval bounds that can be summed up with a "coverage" bound to yield an interval that encloses the probability of assertion for the program as a whole. We demonstrate promising results on a suite of benchmarks from many different sources including robotic manipulators and medical decision making programs.
p2549
aVLocating linearization points (LPs) is an intuitive approach for proving linearizability, but it is difficult to apply the idea in Hoarestyle logic for formal program verification, especially for verifying algorithms whose LPs cannot be statically located in the code. In this paper, we propose a program logic with a lightweight instrumentation mechanism which can verify algorithms with nonfixed LPs, including the most challenging ones that use the helping mechanism to achieve lockfreedom (as in HSY eliminationbased stack), or have LPs depending on unpredictable future executions (as in the lazy set algorithm), or involve both features. We also develop a threadlocal simulation as the metatheory of our logic, and show it implies contextual refinement, which is equivalent to linearizability. Using our logic we have successfully verified various classic algorithms, some of which are used in the java.util.concurrent package.
p2550
aVWe extend the existing formal verification of the seL4 operating system microkernel from 9500 lines of C source code to the binary level. We handle all functions that were part of the previous verification. Like the original verification, we currently omit the assembly routines and volatile accesses used to control system hardware. More generally, we present an approach for proving refinement between the formal semantics of a program on the C source level and its formal semantics on the binary level, thus checking the validity of compilation, including some optimisations, and linking, and extending static properties proved of the source code to the executable. We make use of recent improvements in SMT solvers to almost fully automate this process. We handle binaries generated by unmodified gcc 4.5.1 at optimisation level 1, and can handle most of seL4 even at optimisation level 2.
p2551
aVIn many areas of computing, techniques ranging from testing to formal modeling to fullblown verification have been successfully used to help programmers build reliable systems. But although networks are critical infrastructure, they have largely resisted analysis using formal techniques. Softwaredefined networking (SDN) is a new network architecture that has the potential to provide a foundation for network reasoning, by standardizing the interfaces used to express network programs and giving them a precise semantics. This paper describes the design and implementation of the first machineverified SDN controller. Starting from the foundations, we develop a detailed operational model for OpenFlow (the most popular SDN platform) and formalize it in the Coq proof assistant. We then use this model to develop a verified compiler and runtime system for a highlevel network programming language. We identify bugs in existing languages and tools built without formal foundations, and prove that these bugs are absent from our system. Finally, we describe our prototype implementation and our experiences using it to build practical applications.
p2552
aVSpecialized execution using spatial architectures provides energy efficient computation, but requires effective algorithms for spatially scheduling the computation. Generally, this has been solved with architecturespecific heuristics, an approach which suffers from poor compiler/architect productivity, lack of insight on optimality, and inhibits migration of techniques between architectures. Our goal is to develop a scheduling framework usable for all spatial architectures. To this end, we expresses spatial scheduling as a constraint satisfaction problem using Integer Linear Programming (ILP). We observe that architecture primitives and scheduler responsibilities can be related through five abstractions: placement of computation, routing of data, managing event timing, managing resource utilization, and forming the optimization objectives. We encode these responsibilities as 20 general ILP constraints, which are used to create schedulers for the disparate TRIPS, DySER, and PLUG architectures. Our results show that a general declarative approach using ILP is implementable, practical, and typically matches or outperforms specialized schedulers.
p2553
aVWork stealing is a popular approach to scheduling taskparallel programs. The flexibility inherent in work stealing when dealing with load imbalance results in seemingly irregular computation structures, complicating the study of its runtime behavior. In this paper, we present an approach to efficiently trace asyncfinish parallel programs scheduled using work stealing. We identify key properties that allow us to trace the execution of tasks with low time and space overheads. We also study the usefulness of the proposed schemes in supporting algorithms for datarace detection and retentive stealing presented in the literature. We demonstrate that the perturbation due to tracing is within the variation in the execution time with 99% confidence and the traces are concise, amounting to a few tens of kilobytes per thread in most cases. We also demonstrate that the traces enable significant reductions in the cost of detecting data races and result in low, stable space overheads in supporting retentive stealing for asyncfinish programs.
p2554
aVAggressive compiler optimizations are formulated around the Program Dependence Graph (PDG). Many techniques, including loop fission and parallelization are concerned primarily with dependence cycles in the PDG. The Directed Acyclic Graph of Strongly Connected Components (DAGSCC) represents these cycles directly. The naive method to construct the DAGSCC first computes the full PDG. This approach limits adoption of aggressive optimizations because the number of analysis queries grows quadratically with program size, making DAGSCC construction expensive. Consequently, compilers optimize small scopes with weaker but faster analyses. We observe that many PDG edges do not affect the DAGSCC and that ignoring them cannot affect clients of the DAGSCC. Exploiting this insight, we present an algorithm to omit those analysis queries to compute the DAGSCC efficiently. Across 366 hot loops from 20 SPEC2006 benchmarks, this method computes the DAGSCC in half of the time using half as many queries.
p2555
aVWe present scalable static analyses to recover variables, data types, and function prototypes from stripped x86 executables (without symbol or debug information) and obtain a functional intermediate representation (IR) for analysis and rewriting purposes. Our techniques on average run 352X faster than current techniques and still have the same precision. This enables analyzing executables as large as millions of instructions in minutes which is not possible using existing techniques. Our techniques can recover variables allocated to the floating point stack unlike current techniques. We have integrated our techniques to obtain a compiler level IR that works correctly if recompiled and produces the same output as the input executable. We demonstrate scalability, precision and correctness of our proposed techniques by evaluating them on the complete SPEC2006 benchmarks suite.
p2556
aVReadModifyWrite (RMW) instructions are widely used as the building blocks of a variety of higher level synchronization constructs, including locks, barriers, and lockfree data structures. Unfortunately, they are expensive in architectures such as x86 and SPARC which enforce (variants of) TotalStoreOrder (TSO). A key reason is that RMWs in these architectures are ordered like a memory barrier, incurring the cost of a writebuffer drain in the critical path. Such strong ordering semantics are dictated by the requirements of the strict atomicity definition (type1) that existing TSO RMWs use. Programmers often do not need such strong semantics. Besides, weakening the atomicity definition of TSO RMWs, would also weaken their ordering   thereby leading to more efficient hardware implementations. In this paper we argue for TSO RMWs to use weaker atomicity definitions   we consider two weaker definitions: type2 and type3, with different relaxed ordering differences. We formally specify how such weaker RMWs would be ordered, and show that type2 RMWs, in particular, can seamlessly replace existing type1 RMWs in common synchronization idioms   except in situations where a type1 RMW is used as a memory barrier. Recent work has shown that the new C/C++11 concurrency model can be realized by generating conventional (type1) RMWs for C/C++11 SCatomicwrites and/or SCatomicreads. We formally prove that this is equally valid using the proposed type2 RMWs; type3 RMWs, on the other hand, could be used for SCatomicreads (and optionally SCatomicwrites). We further propose efficient microarchitectural implementations for type2 (type3) RMWs   simulation results show that our implementation reduces the cost of an RMW by up to 58.9% (64.3%), which translates into an overall performance improvement of up to 9.0% (9.2%) on a set of parallel programs, including those from the SPLASH2, PARSEC, and STAMP benchmarks.
p2557
aVReasoning about side effects and aliasing is the heart of verifying imperative programs. Unrestricted side effects through one reference can invalidate assumptions about an alias. We present a new type system approach to reasoning about safe assumptions in the presence of aliasing and side effects, unifying ideas from reference immutability type systems and relyguarantee program logics. Our approach, relyguarantee references, treats multiple references to shared objects similarly to multiple threads in relyguarantee program logics. We propose statically associating rely and guarantee conditions with individual references to shared objects. Multiple aliases to a given object may coexist only if the guarantee condition of each alias implies the rely condition for all other aliases. We demonstrate that existing reference immutability type systems are special cases of relyguarantee references. In addition to allowing precise control over state modification, relyguarantee references allow types to depend on mutable data while still permitting flexible aliasing. Dependent types whose denotation is stable over the actions of the rely and guarantee conditions for a reference and its data will not be invalidated by any action through any alias. We demonstrate this with refinement (subset) types that may depend on mutable data. As a special case, we derive the first reference immutability type system with dependent types over immutable data. We show soundness for our approach and describe experience using relyguarantee references in a dependentlytyped monadic DSL in Coq.
p2558
aVLanguages are becoming increasingly multiparadigm. Subtype polymorphism in staticallytyped objectoriented languages is being supplemented with parametric polymorphism in the form of generics. Features like firstclass functions and lambdas are appearing everywhere. Yet existing languages like Java, C#, C++, D, and Scala seem to accrete ever more complexity when they reach beyond their original paradigm into another; inevitably older features have some rough edges that lead to nonuniformity and pitfalls. Given a fresh start, a new language designer is faced with a daunting array of potential features. Where to start? What is important to get right first, and what can be added later? What features must work together, and what features are orthogonal? We report on our experience with Virgil III, a practical language with a careful balance of classes, functions, tuples and type parameters. Virgil intentionally lacks many advanced features, yet we find its core feature set enables new species of design patterns that bridge multiple paradigms and emulate features not directly supported such as interfaces, abstract data types, ad hoc polymorphism, and variant types. Surprisingly, we find variance for function types and tuple types often replaces the need for other kinds of type variance when libraries are designed in a more functional style.
p2559
aVOur willingness to deliberately trade accuracy of computing systems for significant resource savings, notably energy consumption, got a boost from two directions. First, energy (or power, the more popularly used measure) consumption started emerging as a serious hurdle to our ability to continue scaling the complexity of processors, and thus enable ever richer computing applications. This "energy hurdle" spanned the gamut from large datacenters to portable embedded computing systems. Second, many believed that an engine of growth that supported scaling, captured by Gordon Moore's remarkable prophecy (Moore's law), was headed towards an irrevocable cliff edge  when this happens, our ability to produce computing systems whose hardware would support precise or exact computing would diminish greatly. In this talk which emphasizes the physical and hardware layers of abstraction where all of these troubles start (after all energy is rooted in thermodynamics), I will first review reasons that compelled and encouraged us to consider trading accuracy for energy savings deliberately resulting in inexact computing.
p2560
aVThe freedom to reorder computations involving associative operators has been widely recognized and exploited in designing parallel algorithms and to a more limited extent in optimizing compilers. In this paper, we develop a novel framework utilizing the associativity and commutativity of operations in regular loop computations to enhance register reuse. Stencils represent a particular class of important computations where the optimization framework can be applied to enhance performance. We show how stencil operations can be implemented to better exploit register reuse and reduce load/stores. We develop a multidimensional retiming formalism to characterize the space of valid implementations in conjunction with other program transformations. Experimental results demonstrate the effectiveness of the framework on a collection of highorder stencils.
p2561
aVWe introduce exotypes, userdefined types that combine the flexibility of metaobject protocols in dynamicallytyped languages with the performance control of lowlevel languages. Like objects in dynamic languages, exotypes are defined programmatically at runtime, allowing behavior based on external data such as a database schema. To achieve high performance, we use staged programming to define the behavior of an exotype during a runtime compilation step and implement exotypes in Terra, a lowlevel staged programming language. We show how exotype constructors compose, and use exotypes to implement highperformance libraries for serialization, dynamic assembly, automatic differentiation, and probabilistic programming. Each exotype achieves expressiveness similar to libraries written in dynamicallytyped languages but implements optimizations that exceed the performance of existing libraries written in lowlevel staticallytyped languages. Though each implementation is significantly shorter, our serialization library is 11 times faster than Kryo, and our dynamic assembler is 3 20 times faster than Google's Chrome assembler.
p2562
aVWe present a way to restrict recursive inheritance without sacrificing the benefits of Fbounded polymorphism. In particular, we distinguish two new concepts, materials and shapes, and demonstrate through a survey of 13.5 million lines of opensource genericJava code that these two concepts never actually overlap in practice. With this MaterialShape Separation, we prove that even nave typechecking algorithms are sound and complete, some of which address problems that were unsolvable even under the existing proposals for restricting inheritance. We illustrate how the simplicity of our design reflects the design intuitions employed by programmers and potentially enables new features coming into demand for upcoming programming languages.
p2563
aVFlexible records are a powerful concept in type systems that form the basis of, for instance, objects in dynamically typed languages. One caveat of using flexible records is that a program may try to access a record field that does not exist. We present a type inference algorithm that checks for these runtime errors. The novelty of our algorithm is that it satisfies a clear notion of completeness: The inferred types are optimal in the sense that type annotations cannot increase the set of typeable programs. Under certain assumptions, our algorithm guarantees the following stronger property: it rejects a program if and only if it contains a path from an empty record to a field access on which the field has not been added. We derive this optimal algorithm by abstracting a semantics to types. The derived inference rules use a novel combination of type terms and Boolean functions that retains the simplicity of unificationbased type inference but adds the ability of Boolean functions to express implications, thereby addressing the challenge of combining implications and types. By following our derivation method, we show how various operations such as record concatenation and branching if a field exists lead to Boolean satisfiability problems of different complexity. Analogously, we show that more expressive type systems give rise to SMT problems. On the practical side, we present an implementation of the select and update operations and give practical evidence that these are sufficient in realworld applications.
p2564
aVTraditional assertions express correctness properties that must hold on every program execution. However, many applications have probabilistic outcomes and consequently their correctness properties are also probabilistic (e.g., they identify faces in images, consume sensor data, or run on unreliable hardware). Traditional assertions do not capture these correctness properties. This paper proposes that programmers express probabilistic correctness properties with probabilistic assertions and describes a new probabilistic evaluation approach to efficiently verify these assertions. Probabilistic assertions are Boolean expressions that express the probability that a property will be true in a given execution rather than asserting that the property must always be true. Given either specific inputs or distributions on the input space, probabilistic evaluation verifies probabilistic assertions by first performing distribution extraction to represent the program as a Bayesian network. Probabilistic evaluation then uses statistical properties to simplify this representation to efficiently compute assertion probabilities directly or with sampling. Our approach is a mix of both static and dynamic analysis: distribution extraction statically builds and optimizes the Bayesian network representation and sampling dynamically interprets this representation. We implement our approach in a tool called Mayhap for C and C++ programs. We evaluate expressiveness, correctness, and performance of Mayhap on programs that use sensors, perform approximate computation, and obfuscate data for privacy. Our case studies demonstrate that probabilistic assertions describe useful correctness properties and that Mayhap efficiently verifies them.
p2565
aVProbabilistic software analysis aims at quantifying how likely a target event is to occur during program execution. Current approaches rely on symbolic execution to identify the conditions to reach the target event and try to quantify the fraction of the input domain satisfying these conditions. Precise quantification is usually limited to linear constraints, while only approximate solutions can be provided in general through statistical approaches. However, statistical approaches may fail to converge to an acceptable accuracy within a reasonable time. We present a compositional statistical approach for the efficient quantification of solution spaces for arbitrarily complex constraints over bounded floatingpoint domains. The approach leverages interval constraint propagation to improve the accuracy of the estimation by focusing the sampling on the regions of the input domain containing the sought solutions. Preliminary experiments show significant improvement on previous approaches both in results accuracy and analysis time.
p2566
aVProbabilistic programs use familiar notation of programming languages to specify probabilistic models. Suppose we are interested in estimating the distribution of the return expression r of a probabilistic program P. We are interested in slicing the probabilistic program P and obtaining a simpler program Sli(P) which retains only those parts of P that are relevant to estimating r, and elides those parts of P that are not relevant to estimating r. We desire that the Sli transformation be both correct and efficient. By correct, we mean that P and Sli(P) have identical estimates on r. By efficient, we mean that estimation over Sli(P) be as fast as possible. We show that the usual notion of program slicing, which traverses control and data dependencies backward from the return expression r, is unsatisfactory for probabilistic programs, since it produces incorrect slices on some programs and suboptimal ones on others. Our key insight is that in addition to the usual notions of control dependence and data dependence that are used to slice nonprobabilistic programs, a new kind of dependence called observe dependence arises naturally due to observe statements in probabilistic programs. We propose a new definition of Sli(P) which is both correct and efficient for probabilistic programs, by including observe dependence in addition to control and data dependences for computing slices. We prove correctness mathematically, and we demonstrate efficiency empirically. We show that by applying the Sli transformation as a prepass, we can improve the efficiency of probabilistic inference, not only in our own inference tool R2, but also in other systems for performing inference such as Church and Infer.NET.
p2567
aVIf the result of an expensive computation is invalidated by a small change to the input, the old result should be updated incrementally instead of reexecuting the whole computation. We incrementalize programs through their derivative. A derivative maps changes in the program's input directly to changes in the program's output, without reexecuting the original program. We present a program transformation taking programs to their derivatives, which is fully static and automatic, supports firstclass functions, and produces derivatives amenable to standard optimization. We prove the program transformation correct in Agda for a family of simplytyped \u03bbcalculi, parameterized by base types and primitives. A precise interface specifies what is required to incrementalize the chosen primitives. We investigate performance by a case study: We implement in Scala the program transformation, a plugin and improve performance of a nontrivial program by orders of magnitude.
p2568
aVMany researchers have proposed programming languages that support incremental computation (IC), which allows programs to be efficiently reexecuted after a small change to the input. However, existing implementations of such languages have two important drawbacks. First, recomputation is oblivious to specific demands on the program output; that is, if a program input changes, all dependencies will be recomputed, even if an observer no longer requires certain outputs. Second, programs are made incremental as a unit, with little or no support for reusing results outside of their original context, e.g., when reordered. To address these problems, we present \u03bbiccdd, a core calculus that applies a demanddriven semantics to incremental computation, tracking changes in a hierarchical fashion in a novel demanded computation graph. \u03bbiccdd also formalizes an explicit separation between inner, incremental computations and outer observers. This combination ensures \u03bbiccdd programs only recompute computations as demanded by observers, and allows inner computations to be reused more liberally. We present Adapton, an OCaml library implementing \u03bbiccdd. We evaluated Adapton on a range of benchmarks, and found that it provides reliable speedups, and in many cases dramatically outperforms stateoftheart IC approaches.
p2569
aVIn this paper, we investigate opportunities to be gained from broadening the definition of program slicing. A major inspiration for our work comes from the field of partial evaluation, in which a wide repertoire of techniques have been developed for specializing programs. While slicing can also be harnessed for specializing programs, the kind of specialization obtainable via slicing has heretofore been quite restricted, compared to the kind of specialization allowed in partial evaluation. In particular, most slicing algorithms are what the partialevaluation community calls monovariant: each program element of the original program generates at most one element in the answer. In contrast, partialevaluation algorithms can be polyvariant, i.e., one program element in the original program may correspond to more than one element in the specialized program. The full paper appears in ACM TOPLAS 36(2), 2014.
p2570
aVA fundamental challenge of parallel programming is to ensure that the observable outcome of a program remains deterministic in spite of parallel execution. Languagelevel enforcement of determinism is possible, but existing deterministicbyconstruction parallel programming models tend to lack features that would make them applicable to a broad range of problems. Moreover, they lack extensibility: it is difficult to add or change language features without breaking the determinism guarantee. The recently proposed LVars programming model, and the accompanying LVish Haskell library, took a step toward broadlyapplicable guaranteeddeterministic parallel programming. The LVars model allows communication through shared monotonic data structures to which information can only be added, never removed, and for which the order in which information is added is not observable. LVish provides a Par monad for parallel computation that encapsulates determinismpreserving effects while allowing a more flexible form of communication between parallel tasks than previous guaranteeddeterministic models provided. While applying LVarbased programming to real problems using LVish, we have identified and implemented three capabilities that extend its reach: inflationary updates other than leastupperbound writes; transitive task cancellation; and parallel mutation of nonoverlapping memory locations. The unifying abstraction we use to add these capabilities to LVish without suffering added complexity or cost in the core LVish implementation, or compromising determinism is a form of monad transformer, extended to handle the Par monad. With our extensions, LVish provides the most broadly applicable guaranteeddeterministic parallel programming interface available to date. We demonstrate the viability of our approach both with traditional parallel benchmarks and with results from a realworld case study: a bioinformatics application that we parallelized using our extended version of LVish.
p2571
aVThe talk extends the Laws of Programming [1] by four laws governing concurrent composition of programs. This operator is associative and commutative and distributive through union; and it has the same unit (do nothing) as sequential composition. Furthermore, sequential and concurrent composition distribute through each other, in accordance with an exchange law; this permits an implementation of concurrency by partial interleaving.
p2572
aVFuture multicore processors will be heterogeneous, be increasingly less reliable, and operate in dynamically changing operating conditions. Such environments will result in a constantly varying pool of hardware resources which can greatly complicate the task of efficiently exposing a program's parallelism onto these resources. Coupled with this uncertainty is the diverse set of efficiency metrics that users may desire. This paper proposes Varuna, a system that dynamically, continuously, rapidly and transparently adapts a program's parallelism to best match the instantaneous capabilities of the hardware resources while satisfying different efficiency metrics. Varuna is applicable to both multithreaded and taskbased programs and can be seamlessly inserted between the program and the operating system without needing to change the source code of either. We demonstrate Varuna's effectiveness in diverse execution environments using unaltered C/C++ parallel programs from various benchmark suites. Regardless of the execution environment, Varuna always outperformed the stateoftheart approaches for the efficiency metrics considered.
p2573
aVEmerging trends in computer design and use are likely to make exceptions, once rare, the norm, especially as the system size grows. Due to exceptions, arising from hardware faults, approximate computing, dynamic resource management, etc., successful and errorfree execution of programs may no longer be assured. Yet, designers will want to tolerate the exceptions so that the programs execute completely, efficiently and without external intervention. Modern computers easily handle exceptions in sequential programs, using precise interrupts. But they are illequipped to handle exceptions in parallel programs, which are growing in prevalence. In this work we introduce the notion of globally preciserestartable execution of parallel programs, analogous to preciseinterruptible execution of sequential programs. We present a software runtime recovery system based on the approach to handle exceptions in suitablywritten parallel programs. Qualitative and quantitative analyses show that the proposed system scales with the system size, especially when exceptions are frequent, unlike the conventional checkpointandrecovery method.
p2574
aVDebugging largescale parallel applications is challenging. In most HPC applications, parallel tasks progress in a coordinated fashion, and thus a fault in one task can quickly propagate to other tasks, making it difficult to debug. Finding the leastprogressed tasks can significantly reduce the effort to identify the task where the fault originated. However, existing approaches for detecting them suffer low accuracy and large overheads; either they use imprecise static analysis or are unable to infer progress dependence inside loops. We present a loopaware progressdependence analysis tool, Prodometer, which determines relative progress among parallel tasks via dynamic analysis. Our faultinjection experiments suggest that its accuracy and precision are over 90% for most cases and that it scales well up to 16,384 MPI tasks. Further, our case study shows that it significantly helped diagnosing a perplexing error in MPI, which only manifested at large scale.
p2575
aVThe probability of bit flips in hardware memory systems is projected to increase significantly as memory systems continue to scale in size and complexity. Effective hardwarebased error detection and correction require that the complete data path, involving all parts of the memory system, be protected with sufficient redundancy. First, this may be costly to employ on commodity computing platforms, and second, even on highend systems, protection against multibit errors may be lacking. Therefore, augmenting hardware error detection schemes with software techniques is of considerable interest. In this paper, we consider softwarelevel mechanisms to comprehensively detect transient memory faults. We develop novel compiletime algorithms to instrument application programs with checksum computation codes to detect memory errors. Unlike prior approaches that employ checksums on computational and architectural states, our scheme verifies every data access and works by tracking variables as they are produced and consumed. Experimental evaluation demonstrates that the proposed comprehensive error detection solution is viable as a completely softwareonly scheme. We also demonstrate that with limited hardware support, overheads of error detection can be further reduced.
p2576
aVWe introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficulttofind miscompilations. To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed. Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction.
p2577
aVWe present a system, RCV, for enabling software applications to survive dividebyzero and nulldereference errors. RCV operates directly on offtheshelf, production, stripped x86 binary executables. RCV implements recovery shepherding, which attaches to the application process when an error occurs, repairs the execution, tracks the repair effects as the execution continues, contains the repair effects within the application process, and detaches from the process after all repair effects are flushed from the process state. RCV therefore incurs negligible overhead during the normal execution of the application. We evaluate RCV on all dividebyzero and nulldereference errors available in the CVE database [2] from January 2011 to March 2013 that 1) provide publiclyavailable inputs that trigger the error which 2) we were able to use to trigger the reported error in our experimental environment. We collected a total of 18 errors in seven real world applications, Wireshark, the FreeType library, Claws Mail, LibreOffice, GIMP, the PHP interpreter, and Chromium. For 17 of the 18 errors, RCV enables the application to continue to execute to provide acceptable output and service to its users on the errortriggering inputs. For 13 of the 18 errors, the continued RCV execution eventually flushes all of the repair effects and RCV detaches to restore the application to full clean functionality. We perform a manual analysis of the source code relevant to our benchmark errors, which indicates that for 11 of the 18 errors the RCV and later patched versions produce identical or equivalent results on all inputs.
p2578
aVA central task for a program analysis concerns how to efficiently find a program abstraction that keeps only information relevant for proving properties of interest. We present a new approach for finding such abstractions for program analyses written in Datalog. Our approach is based on counterexampleguided abstraction refinement: when a Datalog analysis run fails using an abstraction, it seeks to generalize the cause of the failure to other abstractions, and pick a new abstraction that avoids a similar failure. Our solution uses a boolean satisfiability formulation that is general, complete, and optimal: it is independent of the Datalog solver, it generalizes the failure of an abstraction to as many other abstractions as possible, and it identifies the cheapest refined abstraction to try next. We show the performance of our approach on a pointer analysis and a typestate analysis, on eight realworld Java benchmark programs.
p2579
aVInterprocedural static analyses are broadly classified into topdown and bottomup, depending upon how they compute, instantiate, and reuse procedure summaries. Both kinds of analyses are challenging to scale: topdown analyses are hindered by ineffective reuse of summaries whereas bottomup analyses are hindered by inefficient computation and instantiation of summaries. This paper presents a hybrid approach Swift that combines topdown and bottomup analyses in a manner that gains their benefits without suffering their drawbacks. Swift is general in that it is parametrized by the topdown and bottomup analyses it combines. We show an instantiation of Swift on a typestate analysis and evaluate it on a suite of 12 Java programs of size 60250 KLOC each. Swift outperforms both conventional approaches, finishing on all the programs while both of those approaches fail on the larger programs.
p2580
aVToday's smartphones are a ubiquitous source of private and confidential data. At the same time, smartphone users are plagued by carelessly programmed apps that leak important data by accident, and by malicious apps that exploit their given privileges to copy such data intentionally. While existing static taintanalysis approaches have the potential of detecting such data leaks ahead of time, all approaches for Android use a number of coarsegrain approximations that can yield high numbers of missed leaks and false alarms. In this work we thus present FlowDroid, a novel and highly precise static taint analysis for Android applications. A precise model of Android's lifecycle allows the analysis to properly handle callbacks invoked by the Android framework, while context, flow, field and objectsensitivity allows the analysis to reduce the number of false alarms. Novel ondemand algorithms help FlowDroid maintain high efficiency and precision at the same time. We also propose DroidBench, an open test suite for evaluating the effectiveness and accuracy of taintanalysis tools specifically for Android apps. As we show through a set of experiments using SecuriBench Micro, DroidBench, and a set of wellknown Android test applications, FlowDroid finds a very high fraction of data leaks while keeping the rate of false positives low. On DroidBench, FlowDroid achieves 93% recall and 86% precision, greatly outperforming the commercial tools IBM AppScan Source and Fortify SCA. FlowDroid successfully finds leaks in a subset of 500 apps from Google Play and about 1,000 malware apps from the VirusShare project.
p2581
aVA common workflow for developing parallel software is as follows: 1) start with a sequential program, 2) identify subcomputations that should be converted to parallel tasks, 3) insert synchronization to achieve the same semantics as the sequential program, and repeat steps 2) and 3) as needed to improve performance. Though this is not the only approach to developing parallel software, it is sufficiently common to warrant special attention as parallel programming becomes ubiquitous. This paper focuses on automating step 3), which is usually the hardest step for developers who lack expertise in parallel programming. Past solutions to the problem of repairing parallel programs have used staticonly or dynamiconly approaches, both of which incur significant limitations in practice. Static approaches can guarantee soundness in many cases but are limited in precision when analyzing medium or largescale software with accesses to pointerbased data structures in multiple procedures. Dynamic approaches are more precise, but their proposed repairs are limited to a single input and are not reflected back in the original source program. In this paper, we introduce a hybrid static+dynamic testdriven approach to repairing data races in structured parallel programs. Our approach includes a novel coupling between static and dynamic analyses. First, we execute the program on a concrete test input and determine the set of data races for this input dynamically. Next, we compute a set of "finish" placements that prevent these races and also respects the static scoping rules of the program while maximizing parallelism. Empirical results on standard benchmarks and student homework submissions from a parallel computing course establish the effectiveness of our approach with respect to compiletime overhead, precision, and performance of the repaired code.
p2582
aVVerified compilers guarantee the preservation of semantic properties and thus enable formal verification of programs at the source level. However, important quantitative properties such as memory and time usage still have to be verified at the machine level where interactive proofs tend to be more tedious and automation is more challenging. This article describes a framework that enables the formal verification of stackspace bounds of compiled machine code at the C level. It consists of a verified CompCertbased compiler that preserves quantitative properties, a verified quantitative program logic for interactive stackbound development, and a verified stack analyzer that automatically derives stack bounds during compilation. The framework is based on event traces that record function calls and returns. The source language is CompCert Clight and the target language is x86 assembly. The compiler is implemented in the Coq Proof Assistant and it is proved that crucial properties of event traces are preserved during compilation. A novel quantitative Hoare logic is developed to verify stackspace bounds at the CompCert Clight level. The quantitative logic is implemented in Coq and proved sound with respect to event traces generated by the smallstep semantics of CompCert Clight. Stackspace bounds can be proved at the source level without taking into account lowlevel details that depend on the implementation of the compiler. The compiler fills in these lowlevel details during compilation and generates a concrete stackspace bound that applies to the produced machine code. The verified stack analyzer is guaranteed to automatically derive bounds for code with nonrecursive functions. It generates a derivation in the quantitative logic to ensure soundness as well as interoperability with interactively developed stack bounds. In an experimental evaluation, the developed framework is used to obtain verified stackspace bounds for micro benchmarks as well as real system code. The examples include the verified operatingsystem kernel CertiKOS, parts of the MiBench embedded benchmark suite, and programs from the CompCert benchmarks. The derived bounds are close to the measured stackspace usage of executions of the compiled programs on a Linux x86 system.
p2583
aVSoftwaredefined networking (SDN) is a new paradigm for operating and managing computer networks. SDN enables logicallycentralized control over network devices through a "controller" software that operates independently from the network hardware, and can be viewed as the network operating system. Network operators can run both inhouse and thirdparty SDN programs (often called applications) on top of the controller, e.g., to specify routing and access control policies. SDN opens up the possibility of applying formal methods to prove the correctness of computer networks. Indeed, recently much effort has been invested in applying finite state model checking to check that SDN programs behave correctly. However, in general, scaling these methods to large networks is challenging and, moreover, they cannot guarantee the absence of errors. We present VeriCon, the first system for verifying that an SDN program is correct on all admissible topologies and for all possible (infinite) sequences of network events. VeriCon either confirms the correctness of the controller program on all admissible network topologies or outputs a concrete counterexample. VeriCon uses firstorder logic to specify admissible network topologies and desired networkwide invariants, and then implements classical FloydHoareDijkstra deductive verification using Z3. Our preliminary experience indicates that VeriCon is able to rapidly verify correctness, or identify bugs, for a large repertoire of simple core SDN programs. VeriCon is compositional, in the sense that it verifies the correctness of execution of any single network event w.r.t. the specified invariant, and can thus scale to handle large programs. To relieve the burden of specifying inductive invariants from the programmer, VeriCon includes a separate procedure for inferring invariants, which is shown to be effective on simple controller programs. We view VeriCon as a first step en route to practical mechanisms for verifying networkwide invariants of SDN programs.
p2584
aVWe introduce Verification Modulo Versions (VMV), a new static analysis technique for reducing the number of alarms reported by static verifiers while providing sound semantic guarantees. First, VMV extracts semantic environment conditions from a base program P. Environmental conditions can either be sufficient conditions (implying the safety of P) or necessary conditions (implied by the safety of P). Then, VMV instruments a new version of the program, P', with the inferred conditions. We prove that we can use (i) sufficient conditions to identify abstract regressions of P' w.r.t. P; and (ii) necessary conditions to prove the relative correctness of P' w.r.t. P. We show that the extraction of environmental conditions can be performed at a hierarchy of abstraction levels (history, state, or call conditions) with each subsequent level requiring a less sophisticated matching of the syntactic changes between P' and P. Call conditions are particularly useful because they only require the syntactic matching of entry points and callee names across program versions. We have implemented VMV in a widely used static analysis and verification tool. We report our experience on two large code bases and demonstrate a substantial reduction in alarms while additionally providing relative correctness guarantees.
p2585
aVThis paper introduces the concept of a commutativity race. A commutativity race occurs in a given execution when two library method invocations can happen concurrently yet they do not commute. Commutativity races are an elegant concept enabling reasoning about concurrent interaction at the library interface. We present a dynamic commutativity race detector. Our technique is based on a novel combination of vector clocks and a structural representation automatically obtained from a commutativity specification. Conceptually, our work can be seen as generalizing classical readwrite race detection. We also present a new logical fragment for specifying commutativity conditions. This fragment is expressive, yet guarantees a constant number of comparisons per method invocation rather than linear with unrestricted specifications. We implemented our analyzer and evaluated it on realworld applications. Experimental results indicate that our analysis is practical: it discovered harmful commutativity races with overhead comparable to stateoftheart, lowlevel race detectors.
p2586
aVProgramming environments for smartphones expose a concurrency model that combines multithreading and asynchronous eventbased dispatch. While this enables the development of efficient and featurerich applications, unforeseen thread interleavings coupled with nondeterministic reorderings of asynchronous tasks can lead to subtle concurrency errors in the applications. In this paper, we formalize the concurrency semantics of the Android programming model. We further define the happensbefore relation for Android applications, and develop a dynamic race detection technique based on this relation. Our relation generalizes the so far independently studied happensbefore relations for multithreaded programs and singlethreaded eventdriven programs. Additionally, our race detection technique uses a model of the Android runtime environment to reduce false positives. We have implemented a tool called DroidRacer. It generates execution traces by systematically testing Android applications and detects data races by computing the happensbefore relation on the traces. We analyzed 15 Android applications including popular applications such as Facebook, Twitter and K9 Mail. Our results indicate that data races are prevalent in Android applications, and that DroidRacer is an effective tool to identify data races.
p2587
aVMobile systems commonly support an eventbased model of concurrent programming. This model, used in popular platforms such as Android, naturally supports mobile devices that have a rich array of sensors and user input modalities. Unfortunately, most existing tools for detecting concurrency errors of parallel programs focus on a threadbased model of concurrency. If one applies such tools directly to an eventbased program, they work poorly because they infer false dependencies between unrelated events handled sequentially by the same thread. In this paper we present a race detection tool named CAFA for eventdriven mobile systems. CAFA uses the causality model that we have developed for the Android eventdriven system. A novel contribution of our model is that it accounts for the causal order due to the event queues, which are not accounted for in past data race detectors. Detecting races based on lowlevel races between memory accesses leads to a large number of false positives. CAFA overcomes this problem by checking for races between highlevel operations. We discuss our experience in using CAFA for finding and understanding a number of known and unknown harmful races in opensource Android applications.
p2588
aVDespite the numerous static and dynamic program analysis techniques in the literature, data races remain one of the most common bugs in modern concurrent software. Further, the techniques that do exist either have limited detection capability or are unsound, meaning that they report false positives. We present a sound race detection technique that achieves a provably higher detection capability than existing sound techniques. A key insight of our technique is the inclusion of abstracted control flow information into the execution model, which increases the space of the causal model permitted by classical happensbefore or causallyprecedes based detectors. By encoding the control flow and a minimal set of feasibility constraints as a group of firstorder logic formulae, we formulate race detection as a constraint solving problem. Moreover, we formally prove that our formulation achieves the maximal possible detection capability for any sound dynamic race detector with respect to the same input trace under the sequential consistency memory model. We demonstrate via extensive experimentation that our technique detects more races than the other stateoftheart sound race detection techniques, and that it is scalable to executions of real world concurrent applications with tens of millions of critical events. These experiments also revealed several previously unknown races in real systems (e.g., Eclipse) that have been confirmed or fixed by the developers. Our tool is also adopted by Eclipse developers.
p2589
aVWe address the problem of code search in executables. Given a function in binary form and a large code base, our goal is to statically find similar functions in the code base. Towards this end, we present a novel technique for computing similarity between functions. Our notion of similarity is based on decomposition of functions into tracelets: continuous, short, partial traces of an execution. To establish tracelet similarity in the face of lowlevel compiler transformations, we employ a simple rewriting engine. This engine uses constraint solving over alignment constraints and data dependencies to match registers and memory addresses between tracelets, bridging the gap between tracelets that are otherwise similar. We have implemented our approach and applied it to find matches in over a million binary functions. We compare tracelet matching to approaches based on ngrams and graphlets and show that tracelet matching obtains dramatically better precision and recall.
p2590
aVSyntactic sugar is pervasive in language technology. It is used to shrink the size of a core language; to define domainspecific languages; and even to let programmers extend their language. Unfortunately, syntactic sugar is eliminated by transformation, so the resulting programs become unfamiliar to authors. Thus, it comes at a price: it obscures the relationship between the user's source program and the program being evaluated. We address this problem by showing how to compute reduction steps in terms of the surface syntax. Each step in the surface language emulates one or more steps in the core language. The computed steps hide the transformation, thus maintaining the abstraction provided by the surface language. We make these statements about emulation and abstraction precise, prove that they hold in our formalism, and verify part of the system in Coq. We have implemented this work and applied it to three very different languages.
p2591
aVWe present a new visual language, SCCharts, designed for specifying safetycritical reactive systems. SCCharts use a statechart notation and provide determinate concurrency based on a synchronous model of computation (MoC), without restrictions common to previous synchronous MoCs. Specifically, we lift earlier limitations on sequential accesses to shared variables, by leveraging the sequentially constructive MoC. The semantics and key features of SCCharts are defined by a very small set of elements, the Core SCCharts, consisting of state machines plus fork/join concurrency. We also present a compilation chain that allows efficient synthesis of software and hardware.
p2592
aVThe aim of MINIUM is to study the implications of having a concurrentbydefault programming language. This includes language design, runtime system, performance and software engineering considerations. We conduct our study through the design of the concurrentbydefault MINIUM programming language. MINIUM leverages the permission flow of object and group permissions through the program to validate the program's correctness and to automatically infer a possible parallelization strategy via a dataflow graph. MINIUM supports not only forkjoin parallelism but more general dataflow patterns of parallelism. In this paper we present a formal system, called \u03bcMINIUM, modeling the core concepts of MINIUM. \u03bcMINIUM's static type system is based on Featherweight Java with MINIUMspecific extensions. Besides checking for correctness MINIUM's type system it also uses the permission flow to compute a potential parallel execution strategy for the program. \u03bcMINIUM's dynamic semantics use a concurrentbydefault evaluation approach. Along with the formal system we present its soundness proof. We provide a full description of the implementation along with the description of various optimization techniques we used. We implemented MINIUM as an extension of the Plaid programming language, which has firstclass support for permissions builtin. The MINIUM implementation and all case studies are publicly available under the General Public License. We use various case studies to evaluate MINIUM's applicability and to demonstrate that MINIUM parallelized code has performance improvements compared to its sequential counterpart. We chose to use case studies from common domains or problems that are known to benefit from parallelization, to show that MINIUM is powerful enough to encode them. We demonstrate through a webserver application, which evaluates MINIUM's impact on latencybound applications, that MINIUM can achieve a 70% performance improvement over the sequential counterpart. In another case study we chose to implement a dictionary function to evaluate MINIUM's capabilities to express essential data structures. Our evaluation demonstrates that MINIUM can be used to express parallelism in such datastructures and that the performance benefits scale with the amount of annotation effort which is put into the implementation. We chose an integral computationally example to evaluate pure functional programming and computational intensive use cases. Our experiments show that MINIUM is capable of extracting parallelism from functional code and achieving performance improvements up to the limits of Plaid's inherent performance bounds. Overall, we hope that the work helps to advance concurrent programming in modern programming environments.
p2593
aVTree automata and tree transducers are used in a wide range of applications in software engineering, from XML processing to language typechecking. While these formalisms are of immense practical use, they can only model finite alphabets, and since many realworld applications operate over infinite domains such as integers, this is often a limitation. To overcome this problem we augment tree automata and transducers with symbolic alphabets represented as parametric theories. Admitting infinite alphabets makes these models more general and succinct than their classical counterparts. Despite this, we show how the main operations, such as composition and language equivalence, remain computable given a decision procedure for the alphabet theory. We introduce a highlevel language called Fast that acts as a frontend for the above formalisms. Fast supports symbolic alphabets through tight integration with stateoftheart satisfiability modulo theory (SMT) solvers. We demonstrate our techniques on practical case studies, covering a wide range of applications.
p2594
aVThis talk will describe a view of concurrency, the author's own, as it has evolved since the late 1970s. Early notions of concurrency were intimately tied with physical hardware and speeding up of computations, which proved to be an impediment to the development of a logical theory of concurrency. In collaboration with K. Mani Chandy, the author developed a theory called UNITY that combined a programming notation with a verification logic to describe a large class of fundamental concurrent algorithms arising in operating systems, communication protocols and distributed systems. Several model checkers, including Murphi, developed by David Dill, are based on UNITY. A limitation of UNITY was a lack of adequate structuring mechanism. While this was not a major problem in lowlevel applications, the current widespread use of concurrency requires theories that go beyond managing infrastructure to the level of massive applications. Our current research, a programming model called Orc, introduces mechanisms to organize the communication, synchronization and coordination in programs that run on widearea networks. Orc includes constructs to orchestrate the concurrent invocation of services to achieve a goal   while managing timeouts, priorities, and failure of sites or communication.
p2595
aVWe developed Chlorophyll, a synthesisaided programming model and compiler for the GreenArrays GA144, an extremely minimalist lowpower spatial architecture that requires partitioning the program into fragments of no more than 256 instructions and 64 words of data. This processor is 100times more energy efficient than its competitors, but currently can only be programmed using a lowlevel stackbased language. The Chlorophyll programming model allows programmers to provide human insight by specifying partial partitioning of data and computation. The Chlorophyll compiler relies on synthesis, sidestepping the need to develop classical optimizations, which may be challenging given the unusual architecture. To scale synthesis to real problems, we decompose the compilation into smaller synthesis subproblems partitioning, layout, and code generation. We show that the synthesized programs are no more than 65% slower than highly optimized expertwritten programs and are faster than programs produced by a heuristic, nonsynthesizing version of our compiler.
p2596
aVProgrammingbyexample technologies empower endusers to create simple programs merely by providing input/output examples. Existing systems are designed around solvers specialized for a specific set of data types or domainspecific language (DSL). We present a program synthesizer which can be parameterized by an arbitrary DSL that may contain conditionals and loops and therefore is able to synthesize programs in any domain. In order to use our synthesizer, the user provides a sequence of increasingly sophisticated input/output examples along with an expertwritten DSL definition. These two inputs correspond to the two key ideas that allow our synthesizer to work in arbitrary domains. First, we developed a novel iterative synthesis technique inspired by testdriven development which also gives our technique the name of testdriven synthesis where the input/output examples are consumed one at a time as the program is refined. Second, the DSL allows our system to take an efficient componentbased approach to enumerating possible programs. We present applications of our synthesis methodology to enduser programming for transformations over strings, XML, and table layouts. We compare our synthesizer on these applications to stateoftheart DSLspecific synthesizers as well to the general purpose synthesizer Sketch.
p2597
aVWe address the problem of synthesizing code completions for programs using APIs. Given a program with holes, we synthesize completions for holes with the most likely sequences of method calls. Our main idea is to reduce the problem of code completion to a naturallanguage processing problem of predicting probabilities of sentences. We design a simple and scalable static analysis that extracts sequences of method calls from a large codebase, and index these into a statistical language model. We then employ the language model to find the highest ranked sentences, and use them to synthesize a code completion. Our approach is able to synthesize sequences of calls across multiple objects together with their arguments. Experiments show that our approach is fast and effective. Virtually all computed completions typecheck, and the desired completion appears in the top 3 results in 90% of the cases.
p2598
aVWe present an approach for automatically generating provably correct abstractions from C source code that are useful for practical implementation verification. The abstractions are easier for a human verification engineer to reason about than the implementation and increase the productivity of interactive code proof. We guarantee soundness by automatically generating proofs that the abstractions are correct. In particular, we show two key abstractions that are critical for verifying systemslevel C code: automatically turning potentially overflowing machineword arithmetic into ideal integers, and transforming lowlevel C pointer reasoning into separate abstract heaps. Previous work carrying out such transformations has either done so using unverified translations, or required significant proof engineering effort. We implement these abstractions in an existing proofproducing specification transformation framework named AutoCorres, developed in Isabelle/HOL, and demonstrate its effectiveness in a number of case studies. We show scalability on multiple OS microkernels, and we show how our changes to AutoCorres improve productivity for total correctness by porting an existing highlevel verification of the SchorrWaite algorithm to a lowlevel C implementation with minimal effort.
p2599
aVThe natural proof technique for heap verification developed by Qiu et al. [32] provides a platform for powerful sound reasoning for specifications written in a dialect of separation logic called Dryad. Natural proofs are proof tactics that enable automated reasoning exploiting recursion, mimicking common patterns found in human proofs. However, these proofs are known to work only for a simple toy language [32]. In this work, we develop a framework called VCDryad that extends the Vcc framework [9] to provide an automated deductive framework against separation logic specifications for C programs based on natural proofs. We develop several new techniques to build this framework, including (a) a novel tool architecture that allows encoding natural proofs at a higher level in order to use the existing Vcc framework (including its intricate memory model, the underlying typechecker, and the SMTbased verification infrastructure), and (b) a synthesis of ghostcode annotations that captures natural proof tactics, in essence forcing Vcc to find natural proofs using primarily decidable theories. We evaluate our tool extensively, on more than 150 programs, ranging from code manipulating standard data structures, wellknown open source library routines (Glib, OpenBSD), Linux kernel routines, customized OS data structures, etc. We show that all these C programs can be fully automatically verified using natural proofs (given pre/post conditions and loop invariants) without any userprovided proof tactics. VCDryad is perhaps the first deductive verification framework for heapmanipulating programs in a real language that can prove such a wide variety of programs automatically.
p2600
aVImplementing systems in proof assistants like Coq and proving their correctness in full formal detail has consistently demonstrated promise for making extremely strong guarantees about critical software, ranging from compilers and operating systems to databases and web browsers. Unfortunately, these verifications demand such heroic manual proof effort, even for a single system, that the approach has not been widely adopted. We demonstrate a technique to eliminate the manual proof burden for verifying many properties within an entire class of applications, in our case reactive systems, while only expending effort comparable to the manual verification of a single system. A crucial insight of our approach is simultaneously designing both (1) a domainspecific language (DSL) for expressing reactive systems and their correctness properties and (2) proof automation which exploits the constrained language of both programs and properties to enable fully automatic, pushbutton verification. We apply this insight in a deeply embedded Coq DSL, dubbed Reflex, and illustrate Reflex's expressiveness by implementing and automatically verifying realistic systems including a modern web browser, an SSH server, and a web server. Using Reflex radically reduced the proof burden: in previous, similar versions of our benchmarks written in Coq by experts, proofs accounted for over 80% of the code base; our versions require no manual proofs.
p2601
aVPointer information, indispensable for static analysis tools, is expensive to compute and query. We provide a queryefficient persistence technique, Pestrie, to mitigate the costly computation and slow querying of precise pointer information. Leveraging equivalence and hub properties, Pestrie can compress pointer information and answers pointer related queries very efficiently. The experiment shows that Pestrie produces 10.5X and 17.5X smaller persistent files than the traditional bitmap and BDD encodings. Meanwhile, Pestrie is 2.9X to 123.6X faster than traditional demanddriven approaches for serving pointsto related queries.
p2602
aVWe present a method for selectively applying contextsensitivity during interprocedural program analysis. Our method applies contextsensitivity only when and where doing so is likely to improve the precision that matters for resolving given queries. The idea is to use a preanalysis to estimate the impact of contextsensitivity on the main analysis's precision, and to use this information to find out when and where the main analysis should turn on or off its contextsensitivity. We formalize this approach and prove that the analysis always benefits from the preanalysisguided contextsensitivity. We implemented this selective method for an existing industrialstrength interval analyzer for full C. The method reduced the number of (false) alarms by 24.4%, while increasing the analysis cost by 27.8% on average. The use of the selective method is not limited to contextsensitivity. We demonstrate this generality by following the same principle and developing a selective relational analysis.
p2603
aVWe consider the verified compilation of highlevel managed languages like Java or C# whose intermediate representations provide support for sharedmemory synchronization and automatic memory management. In this environment, the interactions between application threads and the language runtime (e.g., the garbage collector) are regulated by compilerinjected code snippets. Example of snippets include allocation fast paths among others. In our TOPLAS paper we propose a refinementbased proof methodology that precisely relates concurrent code expressed at different abstraction levels, cognizant throughout of the relaxed memory semantics of the underlying processor. Our technique allows the compiler writer to reason compositionally about the atomicity of lowlevel concurrent code used to implement managed services. We illustrate our approach with examples taken from the verification of a concurrent garbage collector.
p2604
aVContextsensitivity is the primary approach for adding more precision to a pointsto analysis, while hopefully also maintaining scalability. An oftreported problem with contextsensitive analyses, however, is that they are bimodal: either the analysis is precise enough that it manipulates only manageable sets of data, and thus scales impressively well, or the analysis gets quickly derailed at the first sign of imprecision and becomes ordersofmagnitude more expensive than would be expected given the program's size. There is currently no approach that makes precise contextsensitive analyses (of any flavor: callsite, object, or typesensitive) scale across the board at a level comparable to that of a contextinsensitive analysis. To address this issue, we propose introspective analysis: a technique for uniformly scaling contextsensitive analysis by eliminating its performancedetrimental behavior, at a small precision expense. Introspective analysis consists of a common adaptivity pattern: first perform a contextinsensitive analysis, then use the results to selectively refine (i.e., analyze contextsensitively) program elements that will not cause explosion in the running time or space. The technical challenge is to appropriately identify such program elements. We show that a simple but principled approach can be remarkably effective, achieving scalability (often with dramatic speedup) for benchmarks previously completely outofreach for deep contextsensitive analyses.
p2605
aVIncreased focus on JavaScript performance has resulted in vast performance improvements for many benchmarks. However, for actual code used in websites, the attained improvements often lag far behind those for popular benchmarks. This paper shows that the main reason behind this shortfall is how the compiler understands types. JavaScript has no concept of types, but the compiler assigns types to objects anyway for ease of code generation. We examine the way that the Chrome V8 compiler defines types, and identify two design decisions that are the main reasons for the lack of improvement: (1) the inherited prototype object is part of the current object's type definition, and (2) method bindings are also part of the type definition. These requirements make types very unpredictable, which hinders type specialization by the compiler. Hence, we modify V8 to remove these requirements, and use it to compile the JavaScript code assembled by JSBench from real websites. On average, we reduce the execution time of JSBench by 36%, and the dynamic instruction count by 49%.
p2606
aVWeb browsers have become a de facto universal operating system, and JavaScript its instruction set. Unfortunately, running other languages in the browser is not generally possible. Translation to JavaScript is not enough because browsers are a hostile environment for other languages. Previous approaches are either nonportable or require extensive modifications for programs to work in a browser. This paper presents Doppio, a JavaScriptbased runtime system that makes it possible to run unaltered applications written in generalpurpose languages directly inside the browser. Doppio provides a wide range of runtime services, including a file system that enables local and external (cloudbased) storage, an unmanaged heap, sockets, blocking I/O, and multiple threads. We demonstrate DOPPIO's usefulness with two case studies: we extend Emscripten with Doppio, letting it run an unmodified C++ application in the browser with full functionality, and present DoppioJVM, an interpreter that runs unmodified JVM programs directly in the browser. While substantially slower than a native JVM (between 24X and 42X slower on CPUintensive benchmarks in Google Chrome), DoppioJVM makes it feasible to directly reuse existing, non computeintensive code.
p2607
aVDeterminism is an appealing property for parallel programs, as it simplifies understanding, reasoning and debugging. It is particularly appealing in dynamic (scripting) languages, where ease of programming is a dominant design goal. Some existing parallel languages use the type system to enforce determinism statically, but this is not generally practical for dynamic languages. In this paper, we describe how determinism can be obtained and dynamically enforced/verified for appropriate extensions to a parallel scripting language. Specifically, we introduce the constructs of Deterministic Parallel Ruby (DPR), together with a runtime system (Tardis) that verifies properties required for determinism, including correct usage of reductions and commutative operators, and the mutual independence (datarace freedom) of concurrent tasks. Experimental results confirm that DPR can provide scalable performance on multicore machines and that the overhead of Tardis is low enough for practical testing. In particular, Tardis significantly outperforms alternative datarace detectors with comparable functionality. We conclude with a discussion of future directions in the dynamic enforcement of determinism.
p2608
aVSolveraided domainspecific languages (SDSLs) are an emerging class of computeraided programming systems. They ease the construction of programs by using satisfiability solvers to automate tasks such as verification, debugging, synthesis, and nondeterministic execution. But reducing programming tasks to satisfiability problems involves translating programs to logical constraints, which is an engineering challenge even for domainspecific languages. We have previously shown that translation to constraints can be avoided if SDSLs are implemented by (traditional) embedding into a host language that is itself solveraided. This paper describes how to implement a symbolic virtual machine (SVM) for such a host language. Our symbolic virtual machine is lightweight because it compiles to constraints only a small subset of the host's constructs, while allowing SDSL designers to use the entire language, including constructs for DSL embedding. This lightweight compilation employs a novel symbolic execution technique with two key properties: it produces compact encodings, and it enables concrete evaluation to strip away host constructs that are outside the subset compilable to constraints. Our symbolic virtual machine architecture is at the heart of Rosette, a solveraided language that is host to several new SDSLs.
p2609
aVVarious document types that combine model and view (e.g., text files, webpages, spreadsheets) make it easy to organize (possibly hierarchical) data, but make it difficult to extract raw data for any further manipulation or querying. We present a general framework FlashExtract to extract relevant data from semistructured documents using examples. It includes: (a) an interaction model that allows endusers to give examples to extract various fields and to relate them in a hierarchical organization using structure and sequence constructs. (b) an inductive synthesis algorithm to synthesize the intended program from few examples in any underlying domainspecific language for data extraction that has been built using our specified algebra of few core operators (map, filter, merge, and pair). We describe instantiation of our framework to three different domains: text files, webpages, and spreadsheets. On our benchmark comprising 75 documents, FlashExtract is able to extract intended data using an average of 2.36 examples in 0.84 seconds per field.
p2610
aVMotivated by streaming and data analytics scenarios where many queries operate on the same data and perform similar computations, we propose program consolidation for merging multiple userdefined functions (UDFs) that operate on the same input. Program consolidation exploits common computations between UDFs to generate an equivalent optimized function whose execution cost is often much smaller (and never greater) than the sum of the costs of executing each function individually. We present a sound consolidation calculus and an effective algorithm for consolidating multiple UDFs. Our approach is purely static and uses symbolic SMTbased techniques to identify shared or redundant computations. We have implemented the proposed technique on top of the Naiad data processing system. Our experiments show that our algorithm dramatically improves overall job completion time when executing userdefined filters that operate on the same data and perform similar computations.
p2611
aVModel counting is the problem of determining the number of solutions that satisfy a given set of constraints. Model counting has numerous applications in the quantitative analyses of program execution time, information flow, combinatorial circuit designs as well as probabilistic reasoning. We present a new approach to model counting for structured data types, specifically strings in this work. The key ingredient is a new technique that leverages generating functions as a basic primitive for combinatorial counting. Our tool SMC which embodies this approach can model count for constraints specified in an expressive string language efficiently and precisely, thereby outperforming previous finitesize analysis tools. SMC is expressive enough to model constraints arising in realworld JavaScript applications and UNIX C utilities. We demonstrate the practical feasibility of performing quantitative analyses arising in security applications, such as determining the comparative strengths of password strength meters and determining the information leakage via side channels.
p2612
aVControlFlow Integrity (CFI) is a softwarehardening technique. It inlines checks into a program so that its execution always follows a predetermined ControlFlow Graph (CFG). As a result, CFI is effective at preventing controlflow hijacking attacks. However, past finegrained CFI implementations do not support separate compilation, which hinders its adoption. We present Modular ControlFlow Integrity (MCFI), a new CFI technique that supports separate compilation. MCFI allows modules to be independently instrumented and linked statically or dynamically. The combined module enforces a CFG that is a combination of the individual modules' CFGs. One challenge in supporting dynamic linking in multithreaded code is how to ensure a safe transition from the old CFG to the new CFG when libraries are dynamically linked. The key technique we use is to have the CFG represented in a runtime data structure and have reads and updates of the data structure wrapped in transactions to ensure thread safety. Our evaluation on SPECCPU2006 benchmarks shows that MCFI supports separate compilation, incurs low overhead of around 5%, and enhances security.
p2613
aVAtomicity is a key correctness property that allows programmers to reason about code regions in isolation. However, programs often fail to enforce atomicity correctly, leading to atomicity violations that are difficult to detect. Dynamic program analysis can detect atomicity violations based on an atomicity specification, but existing approaches slow programs substantially. This paper presents DoubleChecker, a novel sound and precise atomicity checker whose key insight lies in its use of two new cooperating dynamic analyses. Its imprecise analysis tracks crossthread dependences soundly but imprecisely with significantly better performance than a fully precise analysis. Its precise analysis is more expensive but only needs to process a subset of the execution identified as potentially involved in atomicity violations by the imprecise analysis. If DoubleChecker operates in singlerun mode, the two analyses execute in the same program run, which guarantees soundness and precision but requires logging program accesses to pass from the imprecise to the precise analysis. In multirun mode, the first program run executes only the imprecise analysis, and a second run executes both analyses. Multirun mode trades accuracy for performance; each run of multirun mode outperforms singlerun mode, but can potentially miss violations. We have implemented DoubleChecker and an existing stateoftheart atomicity checker called Velodrome in a highperformance Java virtual machine. DoubleChecker's singlerun mode significantly outperforms Velodrome, while still providing full soundness and precision. DoubleChecker's multirun mode improves performance further, without significantly impacting soundness in practice. These results suggest that DoubleChecker's approach is a promising direction for improving the performance of dynamic atomicity checking over prior work.
p2614
aVThere is a joke where a physicist and a mathematician are asked to herd cats. The physicist starts with an infinitely large pen which he reduces until it is of reasonable diameter yet contains all the cats. The mathematician builds a fence around himself and declares the outside to be the inside. Defining memory models is akin to herding cats: both the physicist's or mathematician's attitudes are tempting, but neither can go without the other.
p2615
aVJustintime (JIT) compilation of running programs provides more optimization opportunities than offline compilation. Modern JIT compilers, such as those in virtual machines like Oracle's HotSpot for Java or Google's V8 for JavaScript, rely on dynamic profiling as their key mechanism to guide optimizations. While these JIT compilers offer good average performance, their behavior is a black box and the achieved performance is highly unpredictable. In this paper, we propose to turn JIT compilation into a precision tool by adding two essential and generic metaprogramming facilities: First, allow programs to invoke JIT compilation explicitly. This enables controlled specialization of arbitrary code at runtime, in the style of partial evaluation. It also enables the JIT compiler to report warnings and errors to the program when it is unable to compile a code path in the demanded way. Second, allow the JIT compiler to call back into the program to perform compiletime computation. This lets the program itself define the translation strategy for certain constructs on the fly and gives rise to a powerful JIT macro facility that enables "smart" libraries to supply domainspecific compiler optimizations or safety checks. We present Lancet, a JIT compiler framework for Java bytecode that enables such a tight, twoway integration with the running program. Lancet itself was derived from a highlevel Java bytecode interpreter: staging the interpreter using LMS (Lightweight Modular Staging) produced a simple bytecode compiler. Adding abstract interpretation turned the simple compiler into an optimizing compiler. This fact provides compelling evidence for the scalability of the stagedinterpreter approach to compiler construction. In the case of Lancet, JIT macros also provide a natural interface to existing LMSbased toolchains such as the Delite parallelism and DSL framework, which can now serve as accelerator macros for arbitrary JVM bytecode.
p2616
aVThe aggressive optimization of floatingpoint computations is an important problem in highperformance computing. Unfortunately, floatingpoint instruction sets have complicated semantics that often force compilers to preserve programs as written. We present a method that treats floatingpoint optimization as a stochastic search problem. We demonstrate the ability to generate reduced precision implementations of Intel's handwritten C numeric library which are up to 6 times faster than the original code, and achieve endtoend speedups of over 30% on a direct numeric simulation and a ray tracer by optimizing kernels that can tolerate a loss of precision while still remaining correct. Because these optimizations are mostly not amenable to formal verification using the current state of the art, we present a stochastic search technique for characterizing maximum error. The technique comes with an asymptotic guarantee and provides strong evidence of correctness.
p2617
aVWe consider methods of describing the syntax of programming languages in ways that are more flexible and natural than conventional BNF descriptions. These methods involve the use of ambiguous contextfree grammars together with rules to resolve syntactic ambiguities. We show how efficient LL and LR parsers can be constructed directly from certain classes of these specifications.
p2618
aVThe title is not a statement of fact, of course, but an opinion about how language designers should think about types. There has been a natural tendency to look to mathematics for a consistent, precise notion of what types are. The point of view there is extensional: a type is a subset of the universe of values. While this approach may have served its purpose quite adequately in mathematics, defining programming language types in this way ignores some vital ideas. Some interesting developments following the extensional approach are the ALGOL68 type system [vW], Scott's theory [S], and Reynolds' system [R]. While each of these lend valuable insight to programming languages, I feel they miss an important aspect of types.Rather than worry about what types are I shall focus on the role of type checking. Type checking seems to serve two distinct purposes: authentication and secrecy. Both are useful when a programmer undertakes to implement a class of abstract objects to be used by many other programmers. He usually proceeds by choosing a representation for the objects in terms of other objects and then writes the required operations to manipulate them.
p2619
aVHigh level programming languages tend to free a programmer from concern about the underlying machine structure and permit him to talk about his problem domain in more direct terms. Thus, he may imagine that objects such as real numbers, character strings, and linear arrays really exist in the machine as atomic entities and he need not understand the details of how they are actually represented in the machine. Of course what distinguishes various kinds of objects are the operations that may be performed on them, so when talking about the domain of real numbers, we should include the basic arithmetic operations and constants, and for strings, operations such as length, concatenation and indexing.Unfortunately, existing languages do not permit a programmer to ignore completely questions of representation, for if his problem domain does not happen to be included already in the repertoire of domains supported by the language, the programmer must figure out a representation himself and remember it throughout his programming effort. For example, a FORTRAN programmer may know that it is not meaningful to multiply two integers that happen to represent insurance policy numbers, but he has no way of informing the compiler of this fact.Many extensible languages do provide a wide range of data types including structured types, enabling a programmer to choose a more natural representation of his external objects, but the structured data types reflect only the structure of the data, not its meaning. They still provide only one natural representation for both integers and dates, mass and speed, or planar vectors in cartesian or polar coordinates.Many domains that arise in practice have a great deal of similarity between them which one must employ in his programs. For example, the domain of length 3 vectors and the domain of length 4 vectors have the "same" rule of addition, that is, add componentwise, and it would be onerous to have to repeat this information for each new length vector. Thus, one needs both the ability to define brand new domains and also a method for expressing relations among them.We present here some mechanisms, collectively called Aleph1, which allow the expansion of a programming language's repertoire of internal domains. Many of the ideas embodied in Aleph1 have been described previously in the literature, and we acknowledge their influence on our thinking, in particular the work of Balzer [1] and Reynolds [5] suggesting the utility of separating out the abstract behavior of an object from its representation, the languages Pascal [10] and Simula67 [3] which associate functions with data types and types with alternate representations, modular generic functions in Basel [2], Standish's wide variety of structured data [6], and the systematic (though not extensible) treatment of coercion in Algol68 [8]. Our domain mechanisms bear certain strong similarities to those developed by Morris for the purposes of protection [4].
p2620
aVThe purpose of this paper is to advise an approach (and to support that advice by discussion of an example) towards achieving a goal first announced by John McCarthy: that compilers for higherlevel programming languages should be made completely trustworthy by proving their correctness. The author believes that the compilercorrectness problem can be made much less general and betterstructured than the unrestricted programcorrectness problem; to do so will of course entail restricting what a compiler may be.
p2621
aVThe PLANNER project is continuing research in natural and effective means for embedding knowledge in procedures. In the course of this work we have succeeded in unifying the formalism around one fundamental concept: the ACTOR. Intuitively, an ACTOR is an active agent which plays a role on cue according to a script. We use the ACTOR metaphor to emphasize the inseparability of control and data flow in our model. Data structures, functions, semaphores, monitors, ports, descriptions, Quillian nets, logical formulae, numbers, identifiers, demons, processes, contexts, and data bases can all be shown to be special cases of actors. All of the above are objects with certain useful modes of behavior. Our formalism shows how all of these modes of behavior can be defined in terms of one kind of behavior: sending messages to actors. An actor is always invoked uniformly in exactly the same way regardless of whether it behaves as a recursive function, data structure, or process.
p2622
aVThis paper describes a theorem prover that embodies knowledge about programming constructs, such as numbers, arrays, lists, and expressions. The program can reason about these concepts and is used as part of a program verification system that uses the FloydNaur explication of program semantics. It is implemented in the QA4 language; the QA4 system allows many bits of strategic knowledge, each expressed as a small program, to be coordinated so that a program stands forward when it is relevant to the problem at hand. The language allows clear, concise representation of this sort of knowledge. The QA4 system also has special facilities for dealing with commutative functions, ordering relations, and equivalence relations; these features are heavily used in this deductive system. The program interrogates the user and asks his advice in the course of a proof. Verifications have been found for Hoare's FIND program, a realnumber division algorithm, and some sort programs, as well as for many simpler algorithms. Additional theorems have been proved about a pattern matcher and a version of Robinson's unification algorithm.
p2623
aVThis paper discusses the desirability of procedure linkage optimization and sketches a general theory of interpretive semantics which is motivated by technical problems in specifying and validating program transformations that optimize procedure linkages. One particular transformation is treated in detail.Recursive ALGOL 60 procedures sometimes pass parameters by name in such a way that the general thunk mechanism is unnecessary and inefficient. We present an optimization which detects this kind of callbyname and implements it thunklessly. We prove that the transformation preserves semantics and we discuss the effect on running time and memory management.
p2624
aVA technique is presented for global analysis of program structure in order to perform compile time optimization of object code generated for expressions. The global expression optimization presented includes constant propagation, common subexpression elimination, elimination of redundant register load operations, and live expression analysis. A general purpose program flow analysis algorithm is developed which depends upon the existence of an "optimizing function." The algorithm is defined formally using a directed graph model of program flow structure, and is shown to be correct. Several optimizing functions are defined which, when used in conjunction with the flow analysis algorithm, provide the various forms of code optimization. The flow analysis algorithm is sufficiently general that additional functions can easily be defined for other forms of global code optimization.
p2625
aVThere is an ordering of the nodes of a flow graph G which topologically sorts the dominance relation and can be found in 0(edges) steps. This ordering is the reverse of the order in which a node is last visited while growing any depthfirst spanning tree of G. Moreover, if G is reducible, then this ordering topologically sorts the "dag" of G. Thus, for a reducible flow graph (rfg) there is a simple algorithm to compute the dominators of each node in 0(edges) bit vector steps.The main result of this paper relates two parameters of an rfg. If G is reducible, d is the largest number of back edges found in any cyclefree path in G, and k is the length of the interval derived sequence of G, then k\u2265d. From this result it follows that there is a very simple bit propagation algorithm (indeed, the obvious one) which also uses the above ordering, and is at least as good as the interval algorithm for solving all known global data flow problems such as "available expressions" and "live variables."
p2626
aVRecently strict deterministic grammars and languages have been introduced [9,10,11]. This family of languages is quite fundamental in the study of the mathematical properties of deterministic languages and in dealing with some classical families of grammars such as LR(k) and bounded right context grammars [7,8,14]. These grammars are closely related to LR(0) grammars [1,12,13,14], in fact each strict deterministic grammar is also LR(0). In the present paper, we consider the question of how to parse strict deterministic grammars. We introduce part of a more general theory called "characteristic LR(k) parsing". This actually produces parsers which are tuned to the characteristics of a particular family of grammars. We apply the theory to the family of strict deterministic grammars and we get parsers which are as fast as canonical LR(k) parsers but are substantially smaller. They are not necessarily minimal but we postpone any discussion of minimality to the sequel.The techniques used in the present discussion are quite general [5]. For instance, the study in [3] is similar in spirit to our technique. The optimization techniques for LR(k) parsers in [2] do not work in the case k = 0 without modification. After modifying those methods for k = 0, it can be shown that our parsers cannot be produced by those techniques. This fact has its positive aspect in that our parsers may be smaller and its negative aspect in that error detection may be delayed.The present paper is divided into the present introduction and three sections. In the remainder of this introduction, some basic definitions of strict deterministic and LR(k) parsing are given. We have tried to follow [1] as much as possible in order to minimize the amount of new material to be absorbed. The order of the results is the order needed to prove that the characteristic parser works. In Section III, we apply the theory of Section II to strict deterministic parsing. A simple example shows that the new parsers can be unboundedly smaller than the canonical LR(0) parser.The present paper is meant to be an extended abstract and no proofs are included.The rest of the introduction is concerned with notational conventions for the technical concepts needed.
p2627
aVArrays are among the best understood and most widely used data structures. Yet even now, there are no satisfactory techniques for handling algorithms involving extendible arrays (where, e.g., rows and/or columns can be added dynamically). In this paper, the problem of allocating storage for extendible arrays is examined in the light of our earlier work on data graphs and addressing schemes. A formal analog of the assertion that simplicity of array extension precludes simplicity of transition (marching along rows/columns) is proved.
p2628
aVPrecendece techniques have been widely used in the past in the construction of parsers. However, the restrictions imposed by them on the grammars were hard to meet. Thus, alteration of the rules of the grammar was necessary in order to make them acceptable to the parser. We have shown that, by keeping track of the possible set of rules that could be applied at any one time, one can enlarge the class of grammars considered. The possible set of rules to be considered is obtained directly from the information given by a labelled set of precedence relations. Thus, the parsers are easily obtained. Compared to the precedence parsers, this new method gives a considerable increase in the class of parsable grammars, as well as an improvement in error detection. An interesting consequence of this approach is a new decomposition technique for LR parsers.
p2629
aVA substantial portion of any programmer's time is spent in debugging. One of the major services of any compiler ought to be to provide as much information as possible about compiletime errors in order to minimize the time required for debugging. A good error detection and recovery scheme should maximize the number of errors detected but minimize the number of times it reports an error when there is none. These spurious error detections and their associated error messages are usually engendered by an inappropriate recovery action.In this paper we describe a recovery scheme for syntax errors which provides high quality recovery with good diagnostic information at relatively low cost. In addition, implementation of the recovery scheme can be automated  that is, the recovery routine can be created by a parsergenerator. Therefore, the compiler designer need not be burdened with the difficulties of error recovery and the programming effort necessary to design and debug a myriad of ad hoc recovery routines.
p2630
aVThis paper proposes axioms to define a sequence of languageclasses; the most general is that of "programming language", the most restricted has some simple and attractive properties. Here "language" is used in its traditional sense as referring to a set of interpreted expressions. We are concerned with the syntax of an expression only to the degree needed to relate its structure to its "meaning". A clear distinction is drawn between a "language" and the many possible "realizations" of that language.This introduction comprises a survey and opinionated discussion of the contents of the paper, therefore the reader who wishes to get on with the technical exposition can skip to the next section.
p2631
aVIn order to define the semantics of PL/I in a form which is both precise and readable, a method of definition has been developed which employs an abstract machine operating on treestructured data. The classes of trees involved are defined by formal grammars, while the algorithms governing the behaviour of the machine are expressed semiformally in prose. This algorithmic metalanguage is itself intended to have largely intuitive semantics, but these are made more precise by an indication of how they could be supported at a lower level of detail within the abstract machine.
p2632
aVThis paper analyzes the semantics of the programming language SNOBOL4, following the mathematical approach proposed by D. Scott and C. Strachey. The study aims at clarifying a rather unusual semantic structure, and at demonstrating that the mathematical approach can provide a natural and usable formal specification of a practical programming language.
p2633
aVThis paper presents a semantic model for parallel systems with a scheduling mechanism that is useful for expressing and proving a wider range of properties than semantic models that do not consider scheduling.We formally describe a number of properties related to scheduling and deadlock, including "Fairness" and "Fullness", and show that schedulers with these properties behave in desirable ways.Lastly, we prove and conjecture some proof rules for scheduled systems and outline briefly the relation of this work to modelling protection in parallel systems.
p2634
aVPetri nets are used to define a path and process notation which is more general in its ability to express synchronization than previous path notations. The Petri net classes corresponding to the path notation prove to be interesting in their own right and have demonstrable properties such as liveness and safeness.
p2635
aVIt has long been known that most questions of interest about the behavior of programs are recursively undecidable. These questions include whether a program will halt, whether two programs are equivalent, whether one is an optimized form of another, and so on. On the other hand, it is possible to make some or all of these questions decidable by suitably restricting the computational ability of the programming language under consideration. The Loop language of Meyer and Ritchie [MR], for example, has a decidable halting problem, but undecidable equivalence. Restricting the computational ability still further, virtually all of these questions are decidable for finite automata and generalized sequential machines (except that Griffiths [Gri] has shown equivalence undecidable for nondeterministic gsms).A natural question to ask is how hard it is to solve these problems for programming languages for which they are decidable, and it is with this area that we are concerned in this paper. In particular we describe a programming language modeled on current higherlevel languages which has exactly the computational power of deterministic finite state transducers with final states, and analyze the space and time required to decide various questions of programming interest about the language. We find that questions about halting, equivalence, and optimization are already intractable for this very simple language. We also study extensions to the language such as simple arithmetic capabilities, arrays, and recursive subroutines with both callbyvalue and callbyname parameter passing mechanisms, some of which extend the capabilities of the language and/or increase the complexity of its decidable problems. In one case, that of recursion with callbyname, the previously decidable questions are seen to become undecidable.
p2636
aVIt is shown that both the upper and the lower bounds on the time complexity of the circularity test for Knuth's attribute grammars are exponential functions of the size of the grammar description. This result implies the "intractability" of the circularity test in the sense that the implementation of a general algorithm is not feasible. Another significance of this result is that this is one of the first problems actually arising in practice which has been proven to be of exponential time complexity.
p2637
aVIn this paper we derive upper bounds on the complexity of LR(k) testing both when k is considered to be a fixed integer and also when k is considered to be a parameter of the problem. In the latter case, we show that the lower bounds on the running time of such algorithms depend very strongly on the representation chosen for k. Thus LR(k) testing is NPcomplete when k is expressed in unary and complete for nondeterministic exponential time when k is expressed in binary.These results carry over to many other parameterized classes of grammars, such as the LL(k), strong LL(k), SLR(k), LC(k), strong LC(k), BRC(l,k), BC(l,k) and extended precedence (l,k) grammars.
p2638
aVSome social aspects of programming are illuminated through analogies with similar aspects of mathematics and natural languages. The split between pure and applied mathematics is found similarly in programming. The development of natural languages toward flexionless, wordorder based language types speaks for programming language design based on general, abstract constructs. By analogy with incidents of the history of artificial, auxiliary languages it is suggested that Fortran and Cobol will remain dominant for a long time to come. The most promising avenues for further work of wide influence is seen to be high quality program literature (i.e. programs) of general utility and studies of questions related to program style.
p2639
aVIn this paper we wish to consider the problem of proving assertions about programs that construct and alter arbitrarily complex data structures. In recent years several papers have been written on the subject of proving assertions about such programs; however, the class of data structures considered has generally been a proper subclass of the class of all data structures, such as the classes of linear lists or trees. [Burstall 1972] discusses the problem of what he calls Distinct Nonrepeating Lists and Distinct Nonrepeating Trees. [Kowaltowski 1973] extends Burstall's approach. His approach is likewise basically treeoriented but is applicable to more general data structures. [Laventhal 1974] restricts his attention to 'simple singlylinked lists', noting the problem of providing 'a complete framework for correctness proofs' if one attempts to handle very general data structures. [Morris 1972] discusses the question of designing a programming language for general data structures in order to facilitate verification of programs written in such a language. [Standish 1973] provides a set of axioms for the class of data structures in which, for instance, two data structures are equal iff they are componentwise equal.
p2640
aVA class of program schemas with concurrency is defined as a natural extension of the standard notion of sequential flowchartlike schemas. The question is considered as to whether such a program schema may reach a premature termination (or "hangup") for some interpretation. It is shown that in general this question is undecidable; however, it is shown to be decidable for the class of free program schemas. And an algorithm for testing this property is presented with an upper time bound that grows linearly with the size of the schema.Several structural properties are shown to be equivalent to the hangupfree conditon for free schemas. And we give a method for computing the expected execution time of a program schema if the expected frequencies of choices at the branch points are known.
p2641
aVA new approach to global program data flow analysis which constructs a "node listing" for the control flow graph is discussed and a simple algorithm which uses a node listing to determine the live variables in a program is presented. This algorithm combined with a fast node listing constructor due to Aho and Ullman has produced an 0(n log n) algorithm for live analysis. The utility of the nodelisting method is demonstrated by an examination of the class of graphs for which "short" listings exist. This class is quite similar to the class of graphs for "understandable" programs.
p2642
aVThis paper contains a suggestion for a calculus for constructing 'flowchartable' algorithms. The calculus is a generalization of an Algollike calculus, and hence maintains some discipline over the algorithms constructible with it.The essence of the great 'go to' debate seems to be that the use of the 'go to' device allows the construction of 'spaghettilike' algorithms which are difficult to control intellectually, and hence that only more restrictive, specialpurpose control structures (which are presumably well thought out) should be used. Another way of saying this, perhaps, is that the 'go to' device is the most primitive (and hence most general) possible control structure, and though implementations of any control structure will ultimately have to be done using it, a programmer should no more be content using a 'go to' than if he were forced to use bit strings for data types. He should be demanding higherlevel control structures. To some extent, of course, he has them; but, in discussing the 'go to' debate, the conclusion arrived at by Knuth (1), for instance, is that more powerful control structures than are freely available at present are needed to enable programmers to express their algorithms. He mentions a suggestion by Zahn (2) for strengthening the set of available control structures, but shows that he still needs to resort to the dreaded 'go to' device. (He confesses a sinful urge to jump into the middle of Zahn's loops.) The implication is that even with Zahn's suggestions there is a need for more control structures. We concur with this view, and present here a suggestion for making more such structures available. We invite discussion as to whether the results are profitable for programming language design.The lack of welldeveloped control structures is manifested, incidentally, not only in the dearth of 'public' control structures (i.e. made freely available to all users), but also in the lack of facilities for creating 'private' ones (analogously to the notion of 'subroutines' or userdefined data types). These deficiencies are not all that surprising since control structures are, in a sense, devices at a third level of sophistication: data (being the first level) is acted upon by programs (being the second level) to produce other data; and programs are acted upon by control structures to produce other programs. But the time has perhaps come to pay some more attention to control structures.
p2643
aVIn this paper, we define what exception conditions are, discuss the requirements exception handling language features must satisfy, survey and analyze existing approaches to exception handling, and propose some new language features for dealing with exceptions in an orderly and reliable way. Our objective is not solely to put forward a language proposal. It is also to analyze exception handling issues and principles in detail. The proposed language features serve to highlight exception handling issues by showing how deficiencies in current approaches could be remedied in a coherent and orderly way.
p2644
aVThis paper extends the relational algebra of data bases, presented by Codd [4] and others, in four areas. The first is the use of selector names to remove order dependencies from the columns of a relation. This use of selector names enables us to define a more general class of operations, which include the normal relational operations of union, equijoin etc., as special cases. Thirdly we introduce relations represented algorithmically as well as by a stored set of tuples. Such computed relations cannot always be effectively realised as a finite set of tuples. Finally we consider relational expressions as algorithmic representations of relations and characterize their effectiveness.
p2645
aVA new algorithm for global flow analysis on reducible graphs is presented. The algorithm is shown to treat a very general class of function spaces. For a graph of e edges, the algorithm has a worst case time bound of 0(e log2e) function operations. In programming terms, the number of operations is shown to be proportional to e + the number of exit nodes from program loops. Consequently a restriction to oneentry oneexit control structures guarantees linearity. It is shown that by relaxing these time bounds, a yet wider class of function spaces can be handled.
p2646
aVCS4 is an extensible language currently being developed for the Navy. The language design was influenced strongly by ECL and Algol68; some of the more important aspects of CS4 are as follows: (1) CS4 has strong type checking and a rich data definition facility; mode generators include ARRAY, STRUCTURE, POINTER, STATUS, and SET. (2) Procedures can be defined as GENERICs and can be used as prefix or infix operators. (3) Procedures can be "compiletime", to be invoked during program compilation. (4) CS4 provides a complete set of structured controlflow determiners, similar to those in BLISS. (5) The ACCESS statement allows separately compiled modules to communicate via a strongly checked interface; complete attribute information concerning the ACCESSed modules is thus available to the ACCESSing modules, including any optimization properties ascribed to the ACCESSed functions.Work is currently in progress on the design of a compiler for CS4 (ultimately to be written in CS4). Early in the development it became apparent that a practical optimizer was important; because of the similarity of control structure in CS4 and BLISS, the BLISS\u005c11 compiler was studied with the intention of adapting and improving their global optimization strategies. This paper presents some of the results of this study, the main one of which is a set of new optimization techniques which defer control and data flowprocessing until the information is actually required. Such a deferral scheme is necessary for practicality in compilers such as CS4'S, in which the intermediate representation of a program is not coreresident.
p2647
aVSETL is a settheoretically oriented language of very high level whose repertoire of semantic objects includes finite sets, ordered ntuples, and sets of ordered ntuples useable as mappings. This paper sets forth techniques for the logical analysis and optimization of SETL programs. The techniques described allow relations of inclusion and membership to be established, the domains and ranges of (tabulated) mappings to be estimated from above and below, and the singlevaluedness of (tabulated) mappings to be proved. Once facts of this kind have been established, automatic choice of data structures becomes possible. The methods employed are based upon, and extend, known techniques of dataflow analysis.
p2648
aVA framework for validating surface properties of programming language constructs, composed of proof rules (akin to those of Hoare) and supporting hypotheses, is constructed using the mathematical semantics of Scott and Strachey. The following approach to language design is then considered: the constructs of a language should have surface properties which 1) need few hypotheses other than assumed surface properties; and 2) have proofs consisting, as far as possible, of trivial fixpoint and structural inductions.
p2649
aVThis paper extends the predicate calculus formalization of the partial correctness properties of programs (Ki, Go) to include the preservation of correctness under program transformations. The general notion of "program transformations which preserve properties" is fundamental to the theory of programming and programming languages. In the context of proofs of program correctness, transformations which preserve correctness can be used to improve less efficient, but easier to prove, programs. The basic argument in the use of correctnesspreserving program transformations (hereafter CPTs) is:Assume that G is a program (with attached assertions) which has been proved correct with respect to some inputoutput relation AinAout. Now suppose that S is some part of G, e.g. an expression, assertion, statement, etc., which is to be replaced by some other such part S' to produce the program G'. The goal is to prove that G' is also correct with respect to AinAout and therefore the replacement preserves overall program correctness. Moreover, if the replacement has only a local effect, e.g. the body of a loop, then the proof of correctnesspreservation should be restricted to that part of the program affected by the replacement.Section 2 reviews the current paradigm for proving program correctness. An example in section 3 illustrates CPTs in a sequence of improvements on a correct and simple, but inefficient, initial program. In section 4, the formalization of partial correctness properties of programs is recast as a semantic language definition using Knuth's semantic method (Kn1). This formalization is then used in section 5 to describe the mechanics of performing CPTs. In section 6, several questions about the formalization of sections 4 and 5 are discussed and a generalization is proposed. Finally, section 7 returns to a concrete example and suggests that the most effective use of CPTs is by identification of schematic forms. Related work is mentioned in section 8.
p2650
aVWork on PLANNER73 and actors has led to the development of a basis for semantics of programming languages. Its value in describing programs with sideeffects, parallelism, and synchronization is discussed. Formal definitions are written and explained for sequences, cells, and a simple synchronization primitive. In addition there is discussion of the implications of actor semantics for the controversy over elimination of sideeffects.
p2651
aVWhen proving that a system of processes has a given property it is often convenient to assume that a routine is uninterruptible, i.e. that the routine cannot be interleaved with the rest of the system. Here sufficient conditions are obtained to show that the assumption that a routine is uninterruptible can be relaxed and still preserve basic properties such as halting and determinacy. Thus correctness proofs of a system of processes can often be greatly simplified. This technique  called reduction  is viewed as the replacement of an interruptible routine by an uninterruptible one.
p2652
aVThe concept of grammar forms [4,5] provides evidence that there seems to be no way to base the definitions of many grammar types used in parsing and compiling solely on the concept of productions. Strict interpretations, as introduced in [3,5], of unambiguous or LR(k) grammar forms generate unambiguous or LR(k) languages, respectively. This is not true in the LL(k) case. It is decidable whether a strict interpretation of an unambiguous grammar form is unambiguous. For any two compatible strict interpretations G1 and G2 of an unambiguous grammar form it is decidable whether L(G1)@@@@L(G2), L(G1)@@@@L(G2)&equil;&thgr;, finite, or infinite. For every grammar form F1 there exists a grammar form F2 such that the grammatical family of F1 under unrestricted interpretations is equal to the grammatical family of F2 under strict interpretations.
p2653
aVWe explore the notion of an induction variable in the context of a settheoretic programming langugage. An appropriate definition, we believe, involves both the necessity that changes in the variable around a loop be easily computable and that they be small. We attempt to justify these requirements and show why they are independent assumptions. Next the question of what operators on sets play the role of +, \u2212 and * for arithmetic languages is explored, and several theorems allowing us recursively to detect induction variables in a loop are given. It is shown that most of the usual set operations do fit nicely into the theory and help form induction variables. The reason most variables fail to be induction variables concerns the structure of control flow, more than it does the operators applied.
p2654
aVStructured programming emphasizes programming language constructs such as while loops, until loops, and if then else statements. Properly used, these constructs make occurrences of loops and branching of control obvious. They are preferable to goto statements, which tend to obscure the flow of control [DDH,DIJ]. This paper describes an algorithm which transforms a flowgraph into a program written using repeat (do forever) and if then else statements. The goal of the algorithm is to produce readable programs, rather than to avoid the use of goto statements entirely. goto statements are generated when there is no better way to describe the flow of control. A number of techniques for eliminating goto statements from programs have been previously published [AM, BJ, BS, COO, KF, KOS, PKT]. However, these techniques do not necessarily produce clear flow of control [KN]. Misuse of control constructs may mislead the reader into expecting patterns of control flow which do not exist in the algorithm. For example, these techniques may use a repeat statement when the contained code cannot be executed more than once or add numerous control variables to avoid goto statements. They also may avoid goto statements by copying segments of code or creating subroutines. The former method results in longer programs and bugs may be introduced when all the identical segments must be modified. The latter method may result in subroutines which appear unnatural.
p2655
aVA chain production is a production of the form A\u2192M where A is a nonterminal and M is either a terminal or nonterminal. Pager in [Pag5] has presented an algorithm which removes all chain reductions from LR(1) parsers after they have been constructed. In this paper, we present an algorithm for directly constructing LR(k) parsers with arbitrary subsets of the chain productions, called the useless chain productions, optimized out. If this subset is empty, the algorithm is a standard one [And 1, Kn]. If this subset consists of all chain productions, the result is a parser with all chain reductions optimized away. The algorithm, as in [Pag5], also eliminates from the parsers all nonterminals which occur as the left part of useless chain productions. This latter optimization along with the chain reduction optimization significantly decreases the storage space and execution times of the parsers. This provides an efficient solution of the open problem posed by Aho and Ullman [A&U2] for all LR(k) grammars.
p2656
aVThe syntactical analysis of pictures derived by a contextfree graphgrammar is a rather complicated algorithm. Analogously to the development for Chomskygrammars ten years ago we define the precedencegraphgrammars as a subclass of the contextfree graphgrammars allowing for easier parsing algorithms. By tablelookup we can decide locally if an edge or a node is part of a handle and if it isn't in which direction to proceed to reach one.
p2657
aVWe treat a program as an object of manipulation, determine items of program constancy, and simplify the program based on the constancy. Some motivation for program manipulation is presented, along with two examples of \u201chigher level optimization\u201d written in an Algollike language. A collection of program transformations and a model of the compilation process in terms of sourcetosource transformations are presented. Finally a description of the application of these ideas to an existing programming language is given.
p2658
aVProgram development often proceeds by transforming simple, clear programs into complex, involuted, but more efficient ones. This paper examines ways this process can be rendered more systematic. We show how analysis of program performance, partial evaluation of functions, and abstraction of recursive function definitions from recurring subgoals can be combined to yield many global transformations in a methodical fashion. Examples are drawn from compiler optimization, list processing, very high level languages, and APL execution.
p2659
aVWe describe how to transform certain flowchart programs into equivalent explicit primitive recursive programs. The input/output correctness conditions for the transformed programs are more amenable to proof than the verification conditions for the corresponding flowchart programs. In particular, the transformed correctness conditions can often be verified automatically by the theorem prover developed by Boyer and Moore [1].
p2660
aVA Data Flow program [1,2] is a flowchart like network of operators which compute concurrently, dependent only on the availability of the data which flow along the paths. Each operator has only a local effect, transforming input data to output data. Although operators may exhibit memory and thus not be functional from an input to an output, all operators are functions from input sequences to output sequences. This plus the strong locality of effect allows mathematization of semantics more readily than traditional programming languages which are burdened with omnipresent storage and occasional GOTO's. This paper proves the semantic behavior of some elementary Data Flow programs and proves that certain optimization transformations preserve other behaviors.
p2661
aVSL5 is a programming language developed for experimental work in generalized pattern matching and highlevel data structuring and access mechanisms. This paper describes the procedure mechanism and the conventions for the interpretation of identifiers in SL5. Procedure invocation in SL5 is decomposed into the separate sourcelanguage operations of context creation, argument binding and procedure activation, and allows SL5 procedures to be used as recursive functions or coroutines. This decomposition has led to rules for scoping and for the interpretation of identifiers that are different from those found in other programming languages. Several examples of SL5 procedures are given, including a scanner based on the coroutine model of pattern matching.
p2662
aVSYNVER is an automatic programming system for the synthesis of solutions to problems of synchronization among concurrent processes from specifications written in a high level assertion language (SAL). The correctness of the solutions constructed by SYNVER follows from the soundness of the synthesizer itself and from a verification phase which is applied to the specifications. This verification phase is the main topic of this paper. To provide context for the verification the paper includes a discussion of synchronization problems and a brief overview of both the SYNVER system and the SAL specification language. A formal definition of the correctness of a SAL specification is then presented along with algorithms which may be used to determine if a given specification is correct.
p2663
aVThe close relationship between programming language syntax, contextfree grammars (abbreviated cfgs), parsing, and compiling is wellknown and is extensively discussed in [1]. Unfortunately, many of the problems about programming languages, one might wish to solve, are equivalent to undecidable grammar problems. Two especially important such problems are (1) the emptiness of intersection problem, i.e. determining if the intersection of the languages generated by a pair of grammars is empty, and (2) the grammar class membership problem, i.e. determining for a fixed class of grammars r and a grammar G, if G is an element of T.
p2664
aVEasy as the task may seem, many compilers generate rather inefficient code. Some of the difficulty of generating good code may arise from the lack of realistic models for programming language and machine semantics. In this paper we show that the computational complexity of generating efficient code in realistic situations may also be a major cause of difficulty in the design of good compilers. We consider the problem of generating optimal code for a set of expressions. If the set of expressions has no common subexpressions, then a number of efficient optimal code generation algorithms are known for wide classes of machines [SU, AJ, BL].
p2665
aVThe translation process may be divided into a syntactic phase and a semantic phase. Contextfree grammars can be used to describe the set of syntactically correct source texts in a formal yet intuitively appealing way, and many techniques are now known for automatically constructing parsers from given CF grammars. Knuth's attribute grammars offer the prospect of similarly automating the implementation of the semantic phase. An attribute grammar is an ordinary CF grammar extended to specify the \u201cmeaning\u201d of each string in the language. Each grammar symbol has an associated set of \u201cattributes:\u201d, and each production rule is provided with corresponding semantic rules expressing the relationships between the attributes of symbols in the production. To find the meaning of a string, first we find its parse tree and then we determine the values of all the attributes of symbols in the tree.
p2666
aVAn automated business data processing system designer is described. Attention is centered on the I/O aspects of such systems and the development of optimizing design heuristics. The design of data organization and data accessing procedures for data processing systems operating on large keyed files of data is a common and recurrent activity in modern data processing applications. A considerable amount of understanding and expertise in the this area has been developed and it is time to begin codifying and automating this process. It should be possible to develop a system where the user has merely to specify the characteristics of his (input, output, and intermediate) data objects and their interrelations1 and the system will automatically determine the data organizations and accessing procedures that are optimal for his application. The optimizer for Protosystem I (an automatic programming system prototype at MIT's Project MAC) provides an example of how such automation can be accomplished. This paper describes the theory and algorithms behind the optimizer that is currently operational at Project MAC.
p2667
aVWe are all aware of the development of increasingly sophisticated, elaborate, and expensive computer programs, particularly in the fields of artificial intelligence, data base management, and intelligent systems. The need for techniques to deal with such complexity has renewed interest in programming language research. Recent work on structured programming, intelligent compilers, automatic program generation and verification, and highlevel optimization has resulted. A pattern of approach similar to that of earlier research on programming languages is emerging. The work divides naturally into two parts: the search for good linguistic tools for expressing algorithms and data, and the development of practical methods for translating these to working computer programs. Our emphasis in this paper is in the latter.
p2668
aVThis paper reports on a system, THESYS, that synthesizes LISP recursive programs from examples of what they do. There has been recent interest in this form of program specification[1,4,8]. The theory of such inductive systems has been investigated by Blum and Blum[2], Kugel[5] and Summers[10]. In this paper we describe the practical results of an investigation into the problem of program synthesis from examples. The methodology to be presented is based on a firm theoretical foundation which may provide a basis for generalizing the results to other kinds of program synthesis systems. The various theoretical models for computation and data that comprise the theory are not described in detail in this paper but may be found in [10]. As part of the methodology we describe two processes used in constructing LISP recursive programs from examples. The first is a method for discovering and encoding the relationships or differences between pairs of examples used to specify a program. Unlike systems that synthesize programs from a single example[8], we feel that it is crucial to the process of program synthesis to discover these implicit relationships rather than inferring them from a single example. The multiple example approach permits the construction of a greater variety of programs than does the single example approach. If the algorithm for difference discovery is successful, the relationship between examples is encoded as a kind of recurrence relation. Derivation of such relations effectively determines the program to be constructed as one may prove equivalence between certain recurrence relations and various program schemata.
p2669
aVA new approach to the design of a programming language and its processor is proposed and some of the techniques necessary to realize the design are investigated. The language would have a precisely specified syntax and semantics, with both designed to provide the programmer maximal expressive power and to be as easily understood as possible. The semantics would be based on extremely late binding times, which provide great power to the programmer and are consistent with ease of understanding of the execution process. It would be the responsibility of the processor to implement each program in the most efficient manner consistent with its being correctly executed. Implications of this design philosophy and some of the techniques to be used are discussed in greater detail, focusing particularly on data types and storage allocation.
p2670
aVA different way to execute pure LISP programs is presented. It delays the evaluation of parameters and list structures without ever having to perform more evaluation steps than the usual method. Although the central idea can be found in earlier work this paper is of interest since it treats a rather wellknown language and works out an algorithm which avoids full substitution. A partial correctness proof using ScottStrachey semantics is sketched in a later section.
p2671
aVAn LL(1)based errorcorrector which operates by "insertiononly" is studied. The corrector is able to correct and parse any input string. It is efficient (linear in space and time requirements) and chooses leastcost insertions (as defined by the user) in correcting syntax errors. Moreover, the errorcorrector can be generated automatically from the grammar and a table of terminal symbol insertion costs. The class of LL(1) grammars correctable by this method contains (with minor modifications) grammars used to specify most common programming languages. Preliminary results suggest that this method can be used to advantage in LL(1)driven compilers.
p2672
aVThis paper is concerned with difficult global flow problems which require the symbolic evaluation of programs. We use, as is common in global flow analysis, a model in which the expressions computed are specified, but the flow of control is indicated only by a directed graph whose nodes are blocks of assignment statements. We show that if such a program model is interpreted in the domain of integer arithmetic then many natural global flow problems are unsolvable. We then develop a direct (noniterative) method for finding general symbolic values for program expressions. Our method gives results similar to an iterative method due to Kildall and a direct method due to Fong, Kam, and Ullman. By means of a structure called the global value graph which compactly represents both symbolic values and the flow of these values through the program, we are able to obtain results that are as strong as either of these algorithms at a lower time cost, while retaining applicability to all flow graphs.
p2673
aVA new interprocedural data flow analysis algorithm is presented and analyzed. The algorithm associates with each procedure in a program information about which variables may be modified, which may be used, and which are possibly preserved by a call on the procedure, and all of it subcalls. The algorithm is sufficiently powerful to be used on recursive programs and to deal with the sharing of variables which arises through reference parameters. The algorithm is unique in that it can compute all of this information in a single pass, not requiring a prepass to compute calling relationships or sharing patterns. A lower bound for the computational complexity of gathering interprocedural data flow information is derived and the algorithm is shown to be asymptotically optimal. The algorithm has been implemented and it is practical for use even on quite large programs.
p2674
aVThis paper describes a system which checks correctness of array accesses automatically without any inductive assertions or human interaction. For each array access in the program a condition that the subscript is greater than or equal to the lower bound and a condition that the subscript is smaller than or equal to the upper bound are checked and the results indicating within the bound, out of bound, or undetermined are produced. It can check ordinary programs at about fifty lines per ten seconds, and it shows linear time complexity behavior.It has been long discussed whether program verification will ever become practical. The main argument against program verification is that it is very hard for a programmer to write assertions about programs. Even if he can supply enough assertions, he must have some knowledge about logic in order to prove the lemmas (or verification conditions) obtained from the verifier.However, there are some assertions about programs which must always be true no matter what the programs do; and yet which cannot be checked for all cases. These assertions include: integer values do not overflow, array subscripts are within range, pointers do not fall off NIL, cells are not reclaimed if they are still pointed to, uninitialized variables are not used.Since these conditions cannot be completely checked, many compilers produce dynamic checking code so that if the condition fails, then the program terminates with proper diagnostics. These dynamic checking code sometimes take up much computation time. It is better to have some checking so that unexpected overwriting of data will not occur, but it is still very awkward that the computation stops because of error. Moreover, these errors can be traced back to some other errors in the program. If we can find out whether these conditions will be met or not before actually running the program, we can benefit both by being able to generate efficient code and by being able to produce more reliable programs by careful examination of errors in the programs. Similar techniques can be used to detect semantically equivalent subexpressions or redundant statements to do more elaborate code movement optimization.The system we have constructed runs fast enough to be used as a preprocessor of a compiler. The system first creates logical assertions immediately before array elements such that these assertions must be true whenever the control passes the assertion in order for the access to be valid. These assertions are proved using similar techniques as inductive assertion methods. If an array element lies inside a loop or after a loop a loop invariant is synthesized. A theorem prover was created which has the decision capabilities for a subset of arithmetic formulas. We can use this prover to prove some valid formulas, but we can also use it to generalize nonvalid formulas so that we can hypothesize more general loop invariants.Theoretical considerations on automatic synthesis of loop invariants have been taken into account and a complete formula for loop invariants was obtained. We reduced the problem of loop invariant synthesis to the computation of this formula. This new approach of the synthesis of loop invariant will probably give more firmer basis for the automatic generation of loop invariants in general purpose verifiers.
p2675
aVA programmer spends more time modifying already existing programs than constructing original ones. An attempt is made to formulate techniques of program modification, whereby a program that achieves one result can be transformed into a new program that uses the same principles to achieve a different goal. For example, a program that uses the binary search paradigm to divide two numbers may be modified to calculate the squareroot of a number in a similar manner.Program debugging is considered as a special case of modification if a program computers wrong results, it must be modified to achieve the intended results The application of abstract program schemata to concrete problems is also viewed from the perspective of modification techniques.We, have embedded this approach in a running implementation; our methods are illustrated with several examples that have been performed by it.
p2676
aVWe develop a theory for the correctness of asynchronous parallel programs. A program is considered correct if its behavior is in some sense similar to that of an abstract version of the program. We discuss various criteria for this similarity. We then concentrate on one of them and develop a technique for showing that a parallel program is correct with respect to this criterion.
p2677
aVBrosgol [Br] formalizes the notion that parsing methods can be classified by the positions at which production rules are recognized. In an LL parser, each rule is recognized at the left end, before the rule's yield has been read; in an LR parser, a rule is recognized at its right end, after its yield has been read; and in an LC parser a rule is recognized after the yield of its "left corner" has been read. We generalize on these techniques by allowing the user to specify arbitrarily for each production rule the position at which that rule is to be recognized. The resulting GLC or generalized left corner technique includes the LR, LL, LC, and ELC methods as special cases. It also allows for less conventional parsing strategies, such as grammar splitting [K] with certain component grammars parsed topdown and the others parsed bottomup, as suggested in [AU].This paper is organized as follows. In Section 2 we give some necessary background and notation. Section 3 defines GLC parsing. A canonical GLC(k) parsing technique can be defined analogous to the LR(k) technique. However, the resulting parsers tend to be unacceptably large. We therefore restrict our attention to the SGLC(k), or simple generalized left corner parsing technique, which is analogous to the SLR(k) technique of DeRemer [D]. We give an algorithm for the construction of SGLC(k) parsing tables and prove that the resulting parser is correct.In Section 4 we develop some potentially useful properties of SGLC(1) parsers. We show that there is a welldefined leftmost position at which each production rule can be recognized, and that a minimal left corner parser can be constructed which recognizes each rule as early as possible. We derive a simple expression relating the size (number of states) of an SGLC(1) parser to the size of the corresponding SLR(1) parser. If G is not leftrecursive, then the minimal leftcorner parser is the smallest parser for G in this class, while the SLR(1) parser is the largest.In Section 5 we extend the techniques 5 of [HSU] to give an O(n2) algorithm to test whether a grammar with no left recursion is SGLC(1), and if so to compute its minimal left corners.
p2678
aVOne of the most attractive techniques in optimizing LR parsers\u000ais to eliminate reductions by semantically insignificant\u000aproductions of the form A \u2192 X (single productions), where X is\u000aa nonterminal or a terminal; such a modification can lead to\u000asubstantial savings in both space and time. Therefore, much effort\u000ahas been devoted to the development of methods for eliminating\u000areductions by single productions from LR parsers (Aho and Ullman\u000a[1973a,1973b], Anderson, Eve and Horning[1973], Pager[1973a,1974],\u000aDemers[1974,1975], Backhouse [1976], Joliat[1976], Koskimies[1976],\u000aLaLonde [1976], SoisalonSoininen[1976]).\u000a\u000aAnderson, Eve and Horning[1973] have described a method by which\u000aall reductions by single productions can be eliminated from LR\u000aparsers, but their method may produce a considerable increase in\u000athe number of states in the parser. On the other hand, with the\u000atechniques of Aho and Ullman[1973b] and Demers[1975] no increase in\u000athe number of states can occur, but the elimination of reductions\u000aby single productions is guaranteed only if no two single\u000aproductions have the same left hand side. Pager[1973a,1974] has\u000aextended the method of Aho and Ullman[1973b] so that all reductions\u000aby single productions are eliminated. LaLonde[1976] and\u000aBackhouse[1976] consider versions of the method of Pager, and\u000aJoliat[1976] considers a method which is essentially that given by\u000aAnderson, Eve and Horning[1973] as a suggestion for simplifying\u000atheir general technique.\u000a\u000aIn Anderson, Eve and Horning[1973] the elimination of reductions\u000aby single productions is performed during the construction of the\u000aLR parser, whereas in Aho and Ullman[1973b] and in Pager\u000a[1973a,1974] reductions by single productions are eliminated after\u000aconstruction of the parser. Demers [1975] considers versions of Aho\u000aand Ullman's method such that elimination can be performed both\u000aduring and after construction of the parser, and LaLonde[1976]\u000adescribes a version of Pager's method by which reductions by single\u000aproductions can be eliminated during parser construction.\u000a\u000aExcept the method of Anderson, Eve and Horning [1973] all the\u000aabove techniques rely heavily on the fact that in LR parsing\u000acertain error entries can never be consulted. However, dependence\u000aupon these "don't care" entries leads to some difficulties in the\u000apractical use of the techniques. First, they may not be applicable\u000afor all types of LR parsers: the method of Pager[1973a,1974]\u000aproduces a parser which accepts exactly the strings in the language\u000ain the case of canonical (Knuth[1965]) and SLR (DeRemer[1971])\u000aparsers but may produce a parser which accepts erroneous strings in\u000athe case of LALR parsers (used e.g. by LaLonde[1971],\u000aJohnson[1975], Joliat[1975]) and generalizations of them (Pager\u000a[1973b,1975], SoisalonSoininen[1975]). Furthermore, certain other\u000aparser optimizations may decrease the number of don't care entries,\u000aand these optimizations need special treatment if they are to be\u000aapplied in conjunction with the elimination of reductions by single\u000aproductions. One well known method for optimizing LR parsers, which\u000ais extremely useful especially in list representation of LR parsers\u000aand which may decrease the number of don't care entries is the use\u000aof default reductions (if one or more reduce actions are possible\u000ain some state then one of these reduce actions is chosen for the\u000adefault reduction which is performed instead of reporting error;\u000asee Pager[1973a], Aho and Johnson[1974], Horning[1974],\u000aAho[1976]).\u000a\u000aThe problem in the use of default reductions in conjunction with\u000athe elimination of reductions by single productions is discussed in\u000aPager[1973a] and a method is given there to solve the problem. The\u000abasis of the solution is to apply first the algorithm for\u000aeliminating reductions by single productions and then to check\u000aevery potential default reduction in order to decide whether it can\u000abe used or not. Hence, in the optimized parser all reductions by\u000asingle productions are eliminated, but the use of default\u000areductions can be limited. (Pager [1973a] has found that in the\u000acase of some practical grammars almost all of potential default\u000areductions can be used.)\u000a\u000aIn the present paper we consider another approach to the\u000aproblem. In our method the elimination process itself corresponds\u000ato the technique of Pager[1973a,1974], but the elimination is\u000acarried out only if it does not affect the applicability of default\u000areductions. The main motivation of this approach is the fact that\u000ait leads to a method for eliminating reductions by single\u000aproductions which is applicable for any type of LR parser,\u000aincluding LALR parsers and generalizations of them.\u000a\u000aThe rest of the present paper is organized as follows. Section 2\u000acontains some terminology and a brief review of the theory of LR\u000aparsing. In section 3 we consider the method of Pager[1973a,1974]\u000ain a form similar to that given by LaLonde[1976] and show by an\u000aexample that the method may produce invalid parsers in the case of\u000athe LALR construction. Finally, our method for eliminating\u000areductions by single productions in conjunction with the use of\u000adefault reductions is given in section 4.
p2679
aVWe consider the problem of automating some of the duties of programmers. We take as our point of departure the claim that data management has been automated to the point where the programmer concerned only about the correctness (as opposed to the efficiency) of his program need not involve himself in any aspect of the storage allocation problem. We focus on what we feel is a sensible next step, the problem of automating aspects of control. To accomplish this we propose a definition of control based on a fact/heuristic dichotomy, a variation of Chomsky's competence/performance dichotomy. The dichotomy formalizes an idea originating with McCarthy and developed by Green, Hewitt, McDermott, Sussman, Hayes, Kowalski and others. It allows one to operate arbitrarily on the control component of a program without affecting the program's correctness, which is entirely the responsibility of the fact component. The immediate objectives of our research are to learn how to program keeping fact and control separate, and to identify those aspects of control amenable to automation.
p2680
aVStructuring can be defined independently of what is being structured, and can be applied profitably to more than one domain. Using one mechanism to structure both values and assignments, we obtain equivalents for a variety of data and control structures. Structuring assignments is preferable to structuring control: the former is more conducive to a mathematical style of programming while the latter is more conducive to tracing.
p2681
aVHoarelike deduction systems for establishing partial correctness of programs may fail to be complete because of (a) incompleteness of the assertion language relative to the underlying interpretation or (b) inability of the assertion language to express the invariants of loops. S. Cook has shown that if there is a complete proof system for the assertion language (e.g. all true statements of the assertion language) and if the assertion language satisfies a certain natural expressibility condition, then sound and complete axiom systems for a fairly large subset of Algol may be devised. We exhibit programming language constructs for which it is impossible to obtain sound and complete sets of Hoarelike axioms even in this special sense of Cook's. These constructs include (i) recursive procedures with procedure parameters in a programming language which uses static scope of identifiers and (ii) coroutines in a language which allows parameterless recursive procedures. Modifications of these constructs for which it is possible to obtain sound and complete systems of axioms are also discussed.
p2682
aVProcedure call mechanisms have mainly been studied in the\u000aframework of recursive programs without assignments, for the\u000asimplicity of their operational and denotational semantics (See\u000aScott [16], Nivat [14], Vuillemin [17]).\u000a\u000aAccording to operational semantics, procedure calls act as\u000atextual rewritings ; "computation rules" select at each computation\u000astep the occurrences of unknown functions to be rewritten. A\u000acomputation rule is called correct if the value it computes\u000ais the one given by the denotational semantics. Correctness and\u000aefficiency of computation rules have been studied in Vuillemin\u000a[17,18], MontangeroPaciniTurini [13], DowneySethi [7]. The main\u000aresults are wellknown: innermost evaluation (call by value) is not\u000acorrect, parallel outermost or full substitution are correct.\u000aVuillemin [17,18] gives a sufficient condition for a rule to be\u000acorrect, (safety), later extended by DowneySethi [7] into a\u000anecessary and sufficient one (security). Vuillemin also studies\u000aparticular implementation of callbyname, the delay rule ;\u000ahe shows its optimality with respect to a reasonable\u000aimplementation cost, provided interpretations satisfy a\u000asequentiality condition. This result is in fact twofold :\u000asequentiality allows elimination of useless steps, and\u000aoptimality follows by using sharing mechanisms in term\u000aimplementation.\u000a\u000aThe basis of all these studies is the following theorem\u000a(Vuillemin [17,18]) : provided some restrictive conditions on\u000aprograms are satisfied, the set of terms derivable from a given\u000aterm is a lattice under the derivation ordering.\u000a\u000aOur aim is to extend these results, for the three following\u000areasons : first, though every program can be transformed to match\u000aVuillemin's conditions, the transformations may affect the costs of\u000acomputations : a more direct proof can be investigated. Second, a\u000adirect generalization to the calculus is not\u000astraightforward, since terms definitely do not form\u000alattices. Third, the symbolic (or Herbrand) interpretation [5] is\u000anot sequential in the sense of Vuillemin, and no optimality result\u000ais known for it.\u000a\u000aOur point of view will be purely syntactic : we reconstruct the\u000alattice property in derivations. We study minimal computations\u000a(i.e. finite or infinite derivations)in the symbolic\u000ainterpretation, and transform them into optimal ones. Eventually,\u000awe characterize interpretations to which similar results apply.\u000aExtension towards the calculus is done in [10].\u000a\u000aThe lattice property of terms breaks down in general : two very\u000adifferent derivations may lead to the same term by syntactical\u000aaccidents, which collapse two a priori different terms. In section\u000aI, we take care of this fact by introducing an equivalence and a\u000apreorder on derivations. We give three characterizations of\u000athese relations. For the main one, we extend the classical notion\u000aof residuals [4] by defining residuals of derivations by\u000aderivations. We show that derivation classes form a\u000alattice.\u000a\u000aIn section II, we study the "simple derivations" defined by\u000aVuillemin with use of labels (named here complete\u000aderivations). We give two characterizations of them in the\u000ausual formalism.\u000a\u000aSection III, is devoted to minimality and optimality results.\u000aOrdering infinite derivations as well as finite one, we construct\u000aleast computations of every syntactic approximation of the\u000ainfinite tree determined by the program. The associated complete\u000aderivation are optimal with respect to Vuillemin's cost.\u000aExtension to interpretations is then straightforward, as soon as\u000athey satisfy a syntactic condition. All classes of sequential\u000ainterpretations considered in [2,6,11,17] do satisfy this\u000acondition. The "computation rule" we use for constructing the\u000aoptimal computations is very inefficient in general, but reduces to\u000athe usual delay rules in sequential interpretations.
p2683
aVWe use the concept of evaluation up to a given threshold of information to generalize the semantics of call by value and assignment to nondiscrete domains, and to define a formal semantics for generic procedures. We then prove the correctness of McCarthy's transformation of iterative programs into recursive ones provided the same threshold is used for assignment and parameter passing.
p2684
aVA program denotes computations in some universe of objects. Abstract interpretation of programs consists in using that denotation to describe computations in another universe of abstract objects, so that the results of abstract execution give some information on the actual computations. An intuitive example (which we borrow from Sintzoff [72]) is the rule of signs. The text 1515 * 17 may be understood to denote computations on the abstract universe {(+), (), ()} where the semantics of arithmetic operators is defined by the rule of signs. The abstract execution 1515 * 17 \u2192 (+) * (+) \u2192 () * (+) \u2192 (), proves that 1515 * 17 is a negative number. Abstract interpretation is concerned by a particular underlying structure of the usual universe of computations (the sign, in our example). It gives a summary of some facets of the actual executions of a program. In general this summary is simple to obtain but inaccurate (e.g. 1515 + 17 \u2192 (+) + (+) \u2192 () + (+) \u2192 ()). Despite its fundamentally incomplete results abstract interpretation allows the programmer or the compiler to answer questions which do not need full knowledge of program executions or which tolerate an imprecise answer, (e.g. partial correctness proofs of programs ignoring the termination problems, type checking, program optimizations which are not carried in the absence of certainty about their feasibility, \u2026).
p2685
aVIn his thesis Paterson proved that equivalence is decidable for program schemata such that every instruction falls on at most one loop and only monadic function signs appear. Here we remove the restriction on function signs. The problem reduces to that of showing that the numerical exponents which satisfy certain "word equations" over a semigroup of acyclic directed graphs may be characterized by sentences of Presburger arithmetic; these exponents correspond to loop coefficients of the program schemata. The central construction is a finite automaton whose memory is a window of bounded size on an acyclic directed graph corresponding to a solution to such a "word equation".
p2686
aVPrevious work on optimal code generation has usually assumed that the underlying machine has identical registers and that all operands fit in a single register or memory location. This paper considers the more realistic problem of generating optimal code for expressions involving single and double length operands, using several models of registerpair machines permitting both single and double word instructions. With registerpair machines a new phenomenom arises that is not present in optimal code generation for single register machines: In an optimal evaluation of an expression it may be necessary to oscillate back and forth between evaluating subexpressions of the expression.A lineartime optimal code generation algorithm is derived for a registerpair machine in which all registers are interchangeable. The algorithm is based on showing that for this model there is an optimal evaluation sequence with limited oscillation between the subtrees dominated by the children of a given node. For other machine models including the familiar evenodd registerpair machine, optimal evaluation sequences can always require unlimited oscillation.
p2687
aVThis paper presents a systematic approach to the problem of generating good code with a compiler that is easy to construct. A compiler structure is proposed which relies on interprocedural data flow analysis, global optimization, and an intermediate language schema to simplify the task of writing the code generating portions of a compiler without sacrificing code quality. This structure is contrasted with a more conventional structure to explore the reasons why the new structure solves several problems inherent in the conventional structure. Further advantages which accrue from the new structure are also presented.
p2688
aVControl flow relations in a high level language program can be represented by a hierarchy of small graphs that combines nesting relations among statements in an ALGOLlike syntax with relevant perturbations caused by goto or leave statements. Applications of the new style of representation include denotational semantics, data flow analysis, source level compiler diagnostics, and program proving.
p2689
aVWe propose a new optimization technique applicable to setoriented languages, which we shall call general common subexpression elimination. It involves the computation of the IAVAIL(n) function for all nodes n in a flow graph, where IAVAIL(n) is the set of expressions such that along every path leading to n, there will be found a computation of the expression followed by only incidental assignments (i.e. A = A \u222a {x} or A = A  {x}) to its operands and that the number of such assignments is bounded, independent of the path taken. We shall try to justify our definitions and demonstrate the usefulness of this technique by several examples. We shall show that this optimization problem does not fit into the semilatticetheoretic model for global program optimization [Ki, KU, GW], and that the standard iterative algorithm for such problems does not work in this case. We then give several theorems which allow the problem to be solved for reducible flow graphs. The formulae given in the theorems are in such a form that an efficient algorithm can be found by adapting an algorithm given in [U]. The resulting algorithm takes 0(e log e) steps of an extended type, where bit vector operations are regarded as one step, and e is the number of edges of the flow graph. It takes 0(n log n) extended steps for a program flow graph of n nodes.
p2690
aVThis paper explores the technique of 'strength reduction' or 'formal differentation' in a set theoretic context, as recently introduced by Earley. We give pragmatic rules for the recognition and treatment of reasonably general cases in which the optimization is applicable, and consider some of the problems which arise in actually attempting to install this optimization as part of a compiling system.
p2691
aVA standard approach to the analysis of program structure for the purpose of code optimization is to construct the "control flow graph" which models the possible execution paths through the program. Various graph algorithms can be applied to the control flow graph to produce data flow information, possible optimizations, etc. [A1,A2,AC,AU2,AU3,CS,HU1,HU2.HU3,Ke1,Ke2,Ke3,Ke4,Sc,U]. Studies of the form of typical control flow graphs indicate that such graphs tend to fall into a restricted subclass of general graphs. For example, empirical investigations have shown that the vast majority of program graphs have no multipleentry loops [AC,HU2,HU3,Kn1].The recent work on "structured programming" has suggested that "good" programs fall into an even more restricted subclass. In fact, purists recommend that all programs be synthesized from three basic control structures: sequential statements, ifthenelse statements, and singleentry singleexit loops [Di,Wi].Formal language theory [HoU] has given us a practical way to specify the set of strings which comprise a given language: via a grammar. It is then a natural idea to extend grammars from the strings to graphs in hopes of getting the same power of expression. Several researchers have used this approach [FKZ,J2,Ro].In this paper we study the applicability of a grammatical approach to describing the set of control flow graphs which arise from "good" programs in the sense proposed by many programming practitioners. The resulting flow graph language contains all those programs constructed according to the purists' rules and also admits programs with multipleexit loops if such loops are constructed sensibly. The grammar we use is the "semistructured flow graph" grammar GSSFG which was studied originally in [FKZ]. There are several appealing properties of this grammar; perhaps the most important, from the pointofview of a compilerwriter, is the existence of a lineartime parsing algorithm which leads directly to a lineartime data flow analysis method [FKZ].In the present work we summarize the results from [FKZ] and address several new questions. First, how often do programs written by people with no knowledge of the SSFG rules fall into the language defined by GSSFG? In other words, is the language a natural one for programming? Second, once a program has been parsed according to GSSFG do benefits other than fast data flow analysis accrue?The paper is organized into three main sections. Section II introduces GSSFG and the parsing algorithm from [FKZ]. Section III is devoted to an empirical study conducted by the authors in an attempt to answer the question of naturalness, described above. Section IV discusses several applications of the graph parse in a "graph attribute grammar" framework. The summary at the end of the paper includes suggestions for further investigation.
p2692
aVIn this paper we show that some prevailing ideas on the elimination of left recursion in a contextfree grammar are not valid. An algorithm and a proof are given to show that every proper contextfree grammar is covered by a nonleftrecursive grammar.
p2693
aVMost existing APL implementations are interpretive in nature,\u000athat is, each time an APL statement is encountered it is executed\u000aby a body of code that is perfectly general, i.e. capable of\u000aevaluating any APL expression, and is in no way tailored to the\u000astatement on hand. This costly generality is said to be justified\u000abecause APL variables are typeless and thus can vary arbitrarily in\u000atype, shape, and size during the execution of a program. What this\u000aargument overlooks is that the operational semantics of an APL\u000astatement are not modified by the varying storage requirements of\u000aits variables.\u000a\u000aThe first proposal for a non fully interpretive implementation\u000awas the thesis of P. Abrams [1], in which a high level interpreter\u000acan defer performing certain operations by compiling code which a\u000alow level interpreter must later be called upon to execute. The\u000abenefit thus gained is that intelligence gathered from a wider\u000acontext can be brought to bear on the evaluation of a\u000asubexpression. Thus on evaluating (A+B)[I],\u000aonly the addition A[I]+B[I] will be\u000aperformed. More recently, A. Perlis and several of his students at\u000aYale [9,10] have presented a scheme by which a fullfledged APL\u000acompiler can be written. The compiled code generated can then be\u000avery efficiently executed on a specialized hardware processor. A\u000asimilar scheme is used in the newly released HP/3000 APL [12].\u000a\u000aThis paper builds on and extends the above ideas in several\u000adirections. We start by studying in some depth the two key notions\u000aall this work has in common, namely compilation and\u000adelayed evaluation in the context of APL. By delayed\u000aevaluation we mean the strategy of deferring the computation of\u000aintermediate results until the moment they are needed. Thus large\u000aintermediate expressions are not built in storage; instead their\u000aelements are "streamed" in time. Delayed evaluation for APL was\u000aprobably first proposed by Barton (see [8]).\u000a\u000aMany APL operators do not correspond to any real data\u000aoperations. Instead their effect is to rename the elements of the\u000aarray they act upon. A wide class of such operators, which we will\u000acall the grid selectors, can be handled by essentially\u000apushing them down the expression tree and incorporating their\u000aeffect into the leaf accessors. Semantically this is equivalent to\u000athe dragalong transformations described by Abrams.\u000aPerforming this optimization will be shown to be an integral part\u000aof delayed evaluation.\u000a\u000aIn order to focus our attention on the above issues, we make a\u000anumber of simplifying assumptions. We confine our attention to code\u000acompilation for single APL expressions, such as might occur in an\u000a"APL Calculator", where user defined functions are not allowed. Of\u000acourse we will be critically concerned with the reusability of the\u000acompiled code for future evaluations. We also ignore the\u000adistinctions among the various APL primitive types and assume that\u000aall our arrays are of one uniform numeric type. We have studied the\u000asituation without these simplifying assumptions, but plan to report\u000aon this elsewhere.\u000a\u000aThe following is a list of the main contributions of this\u000apaper.\u000a\u000a" We present an algorithm for incorporating the selector\u000aoperators into the accessors for the leaves of the expression tree.\u000aThe algorithm runs in time proportional to the size of the tree, as\u000aopposed to its path length (which is the case for the algorithms of\u000a[10] and [12]).\u000a\u000aAlthough arbitrary reshapes cannot be handled by the above\u000aalgorithm, an especially important case can: that of a\u000aconforming reshape. The reshape AB is\u000acalled conforming if B is a suffix of A.\u000a\u000a" By using conforming reshapes we can eliminate inner and outer\u000aproducts from the expression tree and replace them with scalar\u000aoperators and reductions along the last dimension. We do this by\u000aintroducing appropriate selectors on the product arguments, then\u000aeventually absorbing these selectors into the leaf accessors. The\u000asame mechanism handles scalar extension, the convention of\u000amaking scalar operands of scalar operators conform to arbitrary\u000aarrays.\u000a\u000a" Once products, scalar extensions, and selectors have been\u000aeliminated, what is left is an expression tree consisting entirely\u000aof scalar operators and reductions along the last dimension. As a\u000aconsequence, during execution, the dimension currently being worked\u000aon obeys a strict stacklike discipline. This implies that we can\u000agenerate extremely efficient code that is independent of the\u000aranks of the arguments.\u000a\u000aSeveral APL operators use the elements of their operands several\u000atimes. A pure delayed evaluation strategy would require multiple\u000areevaluations.\u000a\u000a" We introduce a general buffering mechanism, called\u000aslicing, which allows portions of a subexpression that will\u000abe repeatedly needed to be saved, to avoid future recomputation.\u000aSlicing is well integrated with the evaluation on demand mechanism.\u000aFor example, when operators that break the streaming are\u000aencountered, slicing is used to determine the minimum size buffer\u000arequired between the order in which a subexpression can deliver its\u000aresult, and the order in which the full expression needs it.\u000a\u000a" The compiled code is very efficient. A minimal number of loop\u000avariables is maintained and accessors are shared among as many\u000aexpression atoms as possible. Finally, the code generated is well\u000asuited for execution by an ordinary minicomputer, such as a PDP11,\u000aor a Data General Nova. We have implemented this compiler on the\u000aAlto computer at Xerox PARC.\u000a\u000aThe plan of the paper is this: We start with a general\u000adiscussion of compilation and delayed evaluation. Then we motivate\u000athe structures and algorithms we need to introduce by showing how\u000ato handle a wider and wider class of the primitive APL operators.\u000aWe discuss various ways of tailoring an evaluator for a particular\u000aexpression. Some of this tailoring is possible based only on the\u000aexpression itself, while other optimizations require knowledge of\u000athe (sizes of) the atom bindings in the expression. The reader\u000ashould always be alert to the kind of knowledge being used, for\u000athis affects the validity of the compiled code across reexecutions\u000aof a statement.
p2694
aVA compiler for the C language has recently been constructed which is now compiling C for about half a dozen machines. The compiler was influenced in various ways by recent theoretical developments. This paper gives an overview of the compiler structure and algorithms, emphasizing those areas where theory was helpful, and discussing the approaches taken where theory was lacking.
p2695
aVThe Runcheck Verifier is a working system for proving the absence of common runtime errors. The language accepted is Pascal without variant records, side effects in functions, shared variable parameters to procedures, or functional arguments. The errors checked are: 1) accessing a variable that has not been assigned a value, 2) array subscripting out of range, 3) subrange type error, 4) dereferencing a NIL pointer, 5) arithmetic overflow, and 6) division by zero.
p2696
aVThis paper presents a new version of Hoare's logic including generalized procedure call and assignment rules which correctly handle aliased variables. Formal justifications are given for the new rules.
p2697
aVWe describe a simplifier for use in program manipulation and verification. The simplifier finds a normal form for any expression over the language consisting of individual variables, the usual boolean connectives, the conditional function cond (denoting ifthenelse), the integers (numerals), the arithmetic functions and predicates +,  and \u2264, the LISP constants, functions and predicates nil, car, cdr, cons and atom, the functions store and select for storing into and selecting from arrays, and uninterpreted function symbols. Individual variables range over the union of the rationals, the set of arrays, the LISP sexpressions and the booleans true and false. The constant, function and predicate symbols take their natural interpretations.The simplifier is complete; that is, it simplifies every valid formula to true. Thus it is also a decision procedure for the quantifierfree theory of rationals, arrays and sexpressions under the above functions and predicates.The organization of the simplifier is based on a method for combining decision algorithms for several theories into a single decision algorithm for a larger theory containing the original theories. More precisely, given a set S of functions and predicates over a fixed domain, a satisfiability program for S is a program which determines the satisfiability of conjunctions of literals (signed atomic formulas) whose predicates and function signs are in S. We give a general procedure for combining satisfiability programs for sets S and T into a single satisfiability program for S \u222a T, given certain conditions on S and T. We show how a satisfiability program for a set S can be used to write a complete simplifier for expressions containing functions and predicates of S as well as uninterpreted function symbols.The simplifier described in this paper is currently used in the Stanford Pascal Verifier.
p2698
aVA decision algorithm is given for the quantifierfree theory of recursively defined data structures which, for a conjunction of length n, decides its satisfiability in time linear in n. The firstorder theory of recursively defined data structures, in particular the firstorder theory of LISP list structure (the theory of CONS, CAR, CDR), is shown to be decidable but not elementary recursive.
p2699
aVThe classical common subexpression problem in program optimization is the detection of identical subexpressions. Suppose we have some extra information and are given pairs of expressions ei1=ei2 which must have the same value, and expressions fj1\u2260fj2 which must have different values. We ask if as a result, h1=h2, or h1\u2260h2. This has been called the uniform word problem for finitely presented algebras, and has application in theoremproving and code optimization. We show that such questions can be answered in O(nlogn) time, where n is the number of nodes in a graph representation of all relevant expressions. A linear time algorithm for detecting common subexpressions is derived. Algorithms which process equalities, inequalities and deductions online are discussed.
p2700
aVMost abstract models of a set of parallel processes define a computation of the model to be a sequence. It is either a sequence of actions taken by the system [Lip] or a sequence of states of the system existing between actions [Kel, Lau, Ash]. Parallelsim is represented only by the fact that following a given action or state, the "next" action or state is not necessarily unique. That is, parallesim is represented by nondeterminism. A. W. Holt has called this representation approach "serializable concurrency" as opposed to "true concurrency" [Hol]. He, among others, has questioned the appropriateness of implying a total ordering between events that are only known to be partially ordered.In this paper, a definition of an algebraic model of a set of concurrentlyexecuting sequential processes is presented. The "computations" of this model are directed acyclic graphs. The nodes of each computation graph represent computer operations and the edges represent the partial ordering of operations with respect to time.Examples demonstrate that these directed acyclic graphs aid in focusing a programmer/verifier's attention on the most important features of a computation. The model suggests the following verification paradigm: since systems often execute a certain sequence of actions to achieve a certain goal, verification procedures should identify these sequences, whether or not each is performed within a single process.A key notion in the abstract model is the treatment of synchronization mechanisms as fullfledged processes. A programming language syntax called "path programs" which is suggested by this notion is presented. Path programs are a generalization of the path expressions of Campbell and Habermann [CamHab]. The programming notation is based on the assumption that a processor can be dedicated to each shared data structure in the system. The duty of this processor is to control the synchronization of operations on the data structure.
p2701
aVThe parallelity means "simultaneous performance or execution" and it may concern either computer units of different sorts (e.g. a memory and a processor), or computer units of the same sort (e.g. several processors).The computers Illiac 4 and Burroughs Scientific Processor (BSP) (announced recently) have several arithmetic processors. They are designed for largescale computations with a special sort of numeric data structures, i.e. with large matrices (arrays). Their parallelity concerns naturally and essentially the definitions of matrix operations.According to [Br] the software techniques for exploiting the parallelism of the BSP consists of a "vectorization" of an usual serial program. There are 16 processors in the BSP which are heavily dependent each on the other, because at each instant by each of these processors just one and the same operation may be performed. Therefore a synchronization of all processors is assumed, which is the most important difference from the concept of parallel program scheme of [KM], which allows to speak about a sequence of particular steps, each of which is represented by execution of particular statements in parallel.In this paper a parallel computer with m \u2265 1 processors is assumed, which is also synchronized [Cu 1], but the processors are fully independent each on the other, i.e. on different processors different operations may be performed at each instant. Further, the parallelity concerns arbitrary simple data and arbitrary operations in all generality (and not only matrices). Thus the inherent parallelity of any serial program should be discovered and used for speeding up the duration of computation at most m times, while the memory space requirements remain unchanged.In [Cu 2] parallel flow diagrams were introduced and further the following "parallelization " (i.e. a "computation" of a parallel execution sequence of steps) of a serial program, was discussed in two parts: 1) newly permitted statements are determined by a permitter, and then 2) a subset of m (or less) statements is selected from the set of all permitted (and not yet selected) statements by a selector. The selected statements are executed in parallel on m processors, etc. until all permitted statements are selected (and executed), and the computation terminates. The permitter and the selector should replace the statement counter of serial programs or flow diagrams, by which is determined which statement should be executed as the next one. It is nuclear whether a suitable hardware technology can be designed to perform as a permitter and selector require.The linearity of programs is connected with serial computers (having just one processor, thus m = 1) essentially, and therefore in the following flow diagrams (or program schemes) will be used instead, because they allow a natural and transparent modification to permit diagrams and schemes, which represent a new sort of computing prescription (not necessarily deterministic algorithms) being a generalization of wellknown binary trees, by which usual arithmetic expressions are represented.It is well known that there is no inherent reason for performing an operation from the numerator of a fraction sooner than an operation from the denominator. This arbitrariness of the order conceals an intrinsic parallelism of any expression which contains an nary operation with n \u2265 2 (which corresponds to the fact that the value of such operation does not depend on the order in which the values of its n arguments were achieved). This obvious fact is less clear when algorithms instead of operations are considered.
p2702
aVThis paper describes a programming system based on the metaphor of communicating objects. Experience with a running system shows that this model provides flexibility, modularity and compactness. A compiled representation for the language is presented, along with an interpreter suitable for microcoding. The objectoriented model provides naturally efficient addressing; a corresponding virtual memory is described which offers dense utilization of resident space.
p2703
aVWe study some consequences of the formal language approach to modelling software system behavior for the case of asynchronous, concurrent subsystems. We use the formal language shuffle operation to give an "algebraic" definition of semantics for a simple (structured) concurrent programming language and prove that the use of this operation is necessary. Having established this necessity, we investigate other types of behavioral expressions which use the operation and show that the analysis problem for these expressions is either undecidable or intractable. The results provide some limitations, for example, on the path expression method of system behavior analysis. Our lower bound proofs involve the use of synchronization symbols, which seem to be a formal language analogue of semaphores.
p2704
aVA partial evaluation program for LISP is described, and an application where it has been used. The partial evaluator performs a number of other, related operations such as opening of functions and certain optimizations on programs. The application is based on the fact that we can generate from an interpreter and a partial evaluator the same object code as a corresponding compiler should do. The paper will first formally describe the relationship between an interpreter and a compiler through partial evaluation. The partial evaluator system is then briefly described and finally an experiment is shown where an interpreter for the iterative statement in INTERLISP is partially evaluated.
p2705
aVWe investigate the principles underlying reasoning about nondeterministic programs, and present a logic to support this kind of reasoning. Our logic, an extension of dynamic logic ([22] and [12]), subsumes most existing firstorder logics of nondeterministic programs, including that developed by Dijkstra based on the concept of weakest precondition. A significant feature is the strict separation between the two kinds of nonterminating computations: infinite computations and failures. The logic has a Tarskian truthvalue semantics, an essential prerequisite to establishing completeness of axiomatizations of the logic. We give an axiomatization for flowchart (regular) programs that is complete relative to arithmetic in the sense of Cook. Having a satisfactory tool at hand, we turn to the clarification of the concept of the total correctness of nondeterministic programs, providing in passing, a critical evaluation of the widely used "predicate transformer" approach to the definition of programming constructs, initiated by Dijkstra [5]. Our axiom system supplies a complete axiomatization of wp.
p2706
aVData flow programming languages are especially amenable to mathematization of their semantics in the denotational style of Scott and Strachey. However, many real world programming problems, such as operating systems and data base inquiry systems, require a programming language capable of nondeterminacy because of the nondeterminate behavior of their physical environment. To date, there has been no satisfactory denotational semantics of programming languages with nondeterminacy. This paper presents a straightforward denotational treatment of nondeterminate data flow programs as functions from sets of tagged sequences to sets of tagged sequences. A simple complete partial order on such sets exists, in which the data flow primitives are continuous functions, so that any data flow program computes a well defined function.
p2707
aVAn algorithm is given to translate a relatively lowlevel intermediate representation of a program into assembly code or machine code for a target computer. The algorithm is table driven. A construction algorithm is used to produce the table from a functional description of the target machine. The method produces high quality code for many commercially available computers. By replacing the table, it is possible to retarget a compiler for another kind of computer. In addition techniques are given to prove the correctness of the translator.
p2708
aVA "forward move algorithm", and some of its formal properties, is presented for use in a practical syntactic error recovery scheme for LR parsers. The algorithm finds "valid fragment" (comparable to a valid prefix) just to the right of a point of error detection. For expositional purposes the algorithm is presented as parsing arbitrarily far beyond the point of error detection in a "parallel" mode, as long as all parses agree on the read or reduce action to be taken at each parse step. In practice the forward move is achieved serially by adding "recovery states" to the LR machine. Based on the formal properties of the forward move we propose an error recovery algorithm that uses the accumulated right context. The performance of the recovery algorithm is illustrated in a specific case and discussed in general.
p2709
aVIn this paper we describe how Lucid can be extended to allow userdefined functions and scope conventions, i.e. conventions for limiting the range or scope of the validity of definitions. This is done using new constructs called clauses which are similar in form to the blocks and procedure declarations of Algollike languages, but are nevertheless strictly nonimperative, because a clause is actually a compound assertion, i.e. an assertion formed, as a program is, by combining a collection of assertions.Each type of clause (there are four) has a straightforward mathematical semantics together with its own characteristic "manipulation rules" for general program massage. In addition, the informal operational view of (some) Lucid programs (described in a previous paper) can be extended to give an (incomplete) operational understanding of the effect of the clauses. In this framework a "compute" clause defines a block; a "mapping" clause defines a conventional (pointwise) function; a "produce" clause defines a block with persistent memory (or an anonymous 'process' or 'actor'); and a "function" clause defines a sort of procedure with own variables (or a general kind of coroutine).
p2710
aVThis paper describes a novel approach to the treatment of data types in programming languages, which allows a simple interpretation of "polymorphic" or "generic" procedures, makes a simple set of typechecking rules semantically justifiable and provides a straightforward treatment of encapsulation.
p2711
aVIt has long been known that recursively defined types in a highly typed language such as Algol 68 or Pascal may be tested for structural equivalence by the same algorithm that compares finite automata [5,11]. Several authors (for example, [3,8,9,16]) have proposed that classes of types be simultaneously defined by the use of parameterized type definitions, such asType list(x) = record val:x; next:\u2191list(x) end .This paper shows that unless the use of such parameterized definitions is restricted, new (unparameterized) types may be defined which more closely resemble deterministic contextfree languages. In fact, the equivalence problem for such types becomes as hard as the (currently unsolved) deterministic pushdown automaton equivalence problem. Several restrictions on type definitions are considered which allow known equivalence algorithms to be applied.
p2712
aVIn programming languages which permit both assignment and procedures, distinct identifiers can represent data structures which share storage or procedures with interfering side effects. In addition to being a direct source of programming errors, this phenomenon, which we call interference can impact type structure and parallelism. We show how to eliminate these difficulties by imposing syntactic restrictions, without prohibiting the kind of constructive interference which occurs with higherorder procedures or SIMULA classes. The basic idea is to prohibit interference between identifiers, but to permit interference among components of collections named by single identifiers.
p2713
aVThe earliest data flow analysis research dealt with concrete\u000aproblems (such as detection of available expressions) and with low\u000alevel representations of control flow (with one large graph, each\u000aof whose nodes represents a basic block). Several recent papers\u000ahave introduced an abstract approach, dealing with any problem\u000aexpressible in terms of a semilattice L and a monoid M of isotone\u000amaps from L to L, under various algebraic constraints. Examples\u000ainclude [CC77; GW76; KU76; Ki73; Ta75; Ta76; We75]. Several other\u000arecent papers have introduced a high level representation with many\u000asmall graphs, each of which represents a small portion of the\u000acontrol flow information in a program. The hierarchy of small\u000agraphs is explicit in [Ro77a; Ro77b] and implicit in papers that\u000adeal with syntax directed analysis of programs written within the\u000aconfines of classical structured programming [DDH72, Sec. 1.7].\u000aExamples include [TK76; ZB74]. The abstract papers have retained\u000athe low level representations while the high level papers have\u000aretained the concrete problems of the earliest work. This paper\u000astudies abstract conditions on L and M that lead to rapid data flow\u000aanalysis, with emphasis on high level representations. Unlike some\u000aanalysis methods oriented toward structured programming [TK76;\u000aWu75; ZB74], our method retains the ability to cope with arbitrary\u000aescape and jump statements while it exploits the control flow\u000ainformation implicit in the parse tree.\u000a\u000aThe general algebraic framework for data flow analysis with\u000asemilattices is presented in Section 2, along with some preliminary\u000alemmas. Our "rapid" monoids properly include the "fast" monoids of\u000a[GW76]. Section 3 relates data flow problems to the hierarchies of\u000asmall graphs introduced in [Ro77a; Ro77b]. High level analysis\u000abegins with local information expressed by mapping the arcs of a\u000alarge graph into the monoid M, much as in low level analysis. But\u000aeach arc in our small graphs represents a set (often an infinite\u000aset) of paths in the underlying large graph. Appropriate members of\u000aM are associated with these arcs. This "globalized" local\u000ainformation is used to solve global flow problems in Section 4. The\u000afundamental theorem of Section 4 is applied to programs with the\u000acontrol structures of classical structured programming in Section\u000a5. For a given rapid monoid M, the time required to solve any\u000aglobal data flow problem is linear in the number of statements in\u000athe program. (For varying M, the time is linear in the product of\u000athis number by t@, where t@ is a parameter of\u000aM introduced in the definition of rapidity.) For reasons sketched\u000aat the beginning of Section 6, we feel obliged to cope with source\u000alevel escape and jump statements as well as with classical\u000astructured programming. Section 6 shows how to apply the\u000afundamental theorem of Section 4 to programs with arbitrary escapes\u000aand jumps. The explicit time bound is only derived for programs\u000awithout jumps. A comparison between the results obtained by our\u000amethod and those obtained by [GW76] is in Section 7, which also\u000acontains examples of rapid monoids in the full paper. Finally,\u000aSection 8 lists conclusions and open problems. Proofs of lemmas are\u000aomitted to save space. The full paper will resubmitted to a\u000ajournal.\u000a\u000aWe proceed from the general to the particular, except in some\u000aplaces where bending the rule a little makes a significant\u000aimprovement in the expository flow. Common mathematical notation is\u000aused. To avoid excessive parentheses, the value of a function f at\u000aan argument x is fx rather than f(x). If fx is itself a function\u000athen (fx)y is the result of applying fx to y. The usual\u000a and  symbols are used for arbitrary\u000apartial orders as well as for the usual order among integers. A\u000afunction from a partially ordered set (poset) to a poset is\u000aisotone iff x  y implies fx  fy.\u000a(Isotone maps are sometimes called "monotonic" in the literature.)\u000aA meet semilattice is a poset with a binary operation\u000a such that x  y is the greatest lower\u000abound of the set {x, y}. A meet semilattice wherein every subset\u000ahas a greatest lower bound is complete. In particular, the\u000aempty subset has a greatest lower bound T, so a complete meet\u000asemilattice has a maximum element. A monoid is a set\u000atogether with an associative binary operation &compfn; that has\u000aa unit element 1 : 1 &compfn; m = m &compfn;\u000a1 = m for all m. In all our examples the monoid M will be a\u000amonoid of functions: every member of M is a function (from a\u000aset into itself), the operation &compfn; is the usual\u000acomposition (f &compfn; g)x = f(gx), and the unit 1 is\u000athe identity function with 1X = x for all x. Two\u000aconsiderations governed the notational choices. First, we speak in\u000aways that are common in mathematics and are convenient here.\u000aSecond, we try to facilitate comparisons with [GW76; KU76; Ro77b],\u000ato the extent that the disparities among these works permit. One\u000adisparity is between the meet semilattices of [GW76; KU76; Ki73]\u000aand the join semilattices of [Ro77b; Ta75; We75], where\u000aleast upper bounds are considered instead of greatest lower bounds.\u000aTo speak of meets is more natural in applications that are\u000aintuitively stated in terms of "what must happen on all paths" in\u000asome class of paths in a program, while to speak of joins is more\u000anatural in applications that are intuitively stated in terms of\u000a"what can happen on some paths." By checking whether there are any\u000apaths in the relevant class and by using the rule that 3 is\u000aequivalent to &dlcrop;V&dlcrop;, join oriented applications\u000acan be reduced to meet oriented ones (and vice versa). A general\u000atheory should speak in one way or the other, and we have chosen\u000ameets. For us, strong assertions about a program's data flow are\u000ahigh in the semilattice.
p2714
aVWe present the best known algorithm for the determination of runtime types in a programming language requiring no type declarations. We demonstrate that it is superior to other published algorithms and that it is the best possible algorithm from among all those that use the same set of primitive operators.
p2715
aVA global flow model is assumed; as usual, the flow of control is represented by a digraph called the control flow graph. The objective of our program analysis is the construction of a mapping (a cover) from program text expressions to symbolic expressions for their value holding over all executions of the program. The particular cover constructed by our methods is in general weaker than the covers obtainable by the methods of [Ki, FKU, R1], but our method has the advantage of being very efficient; requiring O(\u2113 + a\u03b1(a)) extended bit vector operations (a logical operation or a shift to the first nonzero bit) on all control flow graphs (whether reducible or not), where a is the number of edges of the control flow graph, \u2113 is the length of the text of the program, and \u03b1 is Tarjan's function (an extremely slowly growing function).
p2716
aVObject code optimizers pay dividends but are usually ad hoc and machinedependent. They would be easier to understand if, instead of performing many ad hoc optimizations, they performed a few general optimizations that give the same effect. They would be easier to implement if they were machineindependent and parametrized by symbolic machine descriptions. This paper describes such a compact, machineindependent peephole optimizer.
p2717
aVA logic for a relational data manipulation language is defined by augmenting a known logic of programs with rules for two new statements: the relational assignment, which assign a relational expression to a relation, and the random tuple selection, which extracts an arbitrary tuple from a relation. The usual operations on relationsretrieve, insert, delete, updateare then defined as special cases of the relational assignment, and the foreach construct scanning a relation tuple by tuple is introduced with the help of the random tuple selection.
p2718
aVWe consider the question of how powerful a relational query language should be and state two principles that we feel any query language should satisfy. We show that although relational algebra and relational calculus satisfy these principles, there are certain queries involving least fixed points that cannot be expressed by these languages, yet that also satisfy the principles. We then consider various extensions of relational algebra to enable it to answer such queries. Finally, we discuss our extensions to relational algebra in terms of a new programming language oriented model for queries.
p2719
aVAttribute grammars are an extension of contextfree grammars devised by Knuth as a formalism for specifying the semantics of a contextfree language along with the syntax of the language. The syntactic phase of the translation process has been extensively studied and many techniques are available for automatically generating efficient parsers for contextfree grammars. Attribute grammars offer the prospect of similarly automating the implementation of the semantic phase. In this paper we present a general method of constructing, for any noncircular attribute grammar, a deterministic translator which will perform the semantic evaluation of each syntax tree of the grammar in time linear with the size of the tree. Each tree is traversed in a manner particularly suited to the shape of the tree, yielding a near optimal evaluation order for that tree. Basically, the translator consists of a finite set of "Local Control Automata", one for each production; these are ordinary finitestate acyclic automata augmented with some special features, which are used to regulate the evaluation process of each syntax tree. With each node in the tree there will be associated the Local Control Automaton of the production applying at the node. At any given time during the translation process all Local Control Automata are inactive, except for the one associated with the currently processed node, which is responsible for directing the next steps taken by the translator until control is finally passed to a neighbour node, reactivating its Local Control Automaton. The Local Control Automata of neighbour nodes communicate with each other.The construction of the translator is custom tailored to each individual attribute grammar. The dependencies among the attributes occurring in the semantic rules are analysed to produce a nearoptimal evaluation strategy for that grammar. This strategy ensures that during the evaluation process, each time the translator enters some subtree of the syntax tree, at least one new attribute evaluation will occur at each node visited. It is this property which distinguishes the method presented here from previously known methods of generating translators for unrestricted attribute grammars, and which causes the translators to be nearoptimal.
p2720
aVA linear recursive procedure is one in which a procedural call can activate at most one other procedural call. When linear recursion cannot be replaced by iteration, it is usually implemented with a stack of size proportional to the depth of recursion. In this paper we analyze implementations of linear recursion which permit large reductions in storage space at the expense of a small increase in computation time. For example, if the depth of recursion is n, storage space can be reduced to \u221an at the cost of a constant factor increase in running time. The problem is treated by abstracting linear recursion into the pebbling of a simple graph and for this abstraction we exhibit the optimal spacetime tradeoffs.
p2721
aVMany wellknown functions are computed by interpretations of the recursion schemaprocedure f(x) ;if p(x)then return a(x)else return b(x,f(c1(x)),\u2026,f(cn(x)))Some of these interpretations define redundant computations because they lead to multiple calls on f with identical argument values. The existence and nature of the redundancy depend on properties of the functions ci. We explore four sets of assumptions about these functions. We analyze directed acyclic graphs formed by merging the nodes of the computation tree for f(x) which are known to be equal for each set of assumptions. In each case there is a transformed program which computes f(x) without redundancy, provided that certain additional assumptions about p, a, and the ci are satisfied. The transformed programs avoid redundancy by saving exactly those intermediate results which will be needed again later in the computation. These programs are all valueless recursive procedures which leave intermediate and final results in specified global locations; in each case recursion can be eliminated without use of a stack. We compare the storage requirements of the transformed programs, discuss the applicability of these transformations to an automatic program improvement system, and present a general criterion for establishing the existence of redundancy.
p2722
aVEquations provide a rich, intuitively understandable notation for describing nonprocedural computing languages such as LISP and Lucid. In this paper, we present techniques for automatically generating interpreters from equations, analagous to wellknown techniques for generating parsers from contextfree grammars. The interpreters so generated are exactly faithful to the simple traditional mathematical meaning of the equationsno latticetheoretic or fixpoint ideas are needed to explain the correspondence. The main technical problem involved is the extension of efficient practical string matching algorithms to trees. We present some new efficient tabledriven matching techniques for a large class of trees, and point out unsolved problems in extending this class. We believe that the techniques of this paper form the beginnings of a useful discipline of interpreting, comparable to the existing discipline of parsing.
p2723
aVHoare and Lauer [1974] have advocated using a variety of styles of programming language definitions to fit the variety of users from implementers to program verifiers. They consider the question of whether different definitions and specifications determine the same language by showing that the definitions are what they call "consistent". However, their treatment skirts the question of whether their definitions can each be taken to specify the language adequately. Although, as we will show, any one of the kinds of semantics they discuss   operational, relational, deductive   can be used to specify meaning uniquely, Hoare and Lauer do not make the case in their paper. In fact, both their relational and deductive definitions are satisfied by several different semantics, only one of which is desired.Thus, the main point of this paper is to clarify the characteristics of a proper specification of language semantics and to formulate alternative specifications each of which is equally good as the language definition. We basically agree with Hoare and Lauer that several specifications can and should be given, but are disturbed by confusions about such specifications, some of which are illustrated in their paper. In particular we refer to confusions between the mathematical object which is designated to be the meaning of a program and methods for specifying that object; the similar confusion between predicate and expression; between consistency and equivalence of two definitions; between completeness of a theory and its having a unique model. While these issues are familiar in mathematical logic, we take this opportunity to survey them in the context of programming language semantics.This paper can be read without prior familiarity with Hoare and Lauer's paper. The authors plan another paper extending this work which will include a more comprehensive bibliography.
p2724
aVThe meaning of "type" in an APL extended to contain nested arrays is discussed. It is shown that "type" is closely related to the variety of empty arrays of the same shape and to the possible fill values needed in the "expand" and "take" functions. Choices for fill functions are systematically presented. They are classified according to the possibility of maintaining important identities involving levelmanipulating functions in the case of empty arguments, to their effect on other design choices still to be made (the restriction to homogeneous arrays and the definition of the nature of basic data), and to their ability to express "type" in a natural way.
p2725
aVSETL is a very high level programming language supporting set theoretical syntax and semantics. It allows algorithms to be programmed rapidly and succinctly without requiring data structure declarations to be supplied, though such declarations can be manually specified later, without recoding the program, to improve the efficiency of program execution. We describe a new technique for automatic selection of appropriate data representations during compiletime for undeclared, or partially declared programs,and present an efficient data structure selection algorithm, whose complexity is comparable with those of the fastest known general dataflow algorithms of Tarjan [TA2] and Reif [RE].
p2726
aVPL/CV is a new formal system which mixes commands and assertions. It includes axioms and rules for a theory of programming over integers and characters. Since arguments in the theory can be checked by the PL/CV Proof Checker, the system offers an approach to mechanical program verification. The Proof Checker is efficient enough for classroom use. Early experience with PL/CV indicates that it nicely supports formal verification of elementary arguments. Continued work should enable the formalization of nonelementary reasoning as well.
p2727
aVOwicki and Gries have developed a proof system for conditional critical regions. In their system logically related variables accessed by more than one process are grouped together as resources, and processes are allowed access to a resource only in a critical region for that resource. Proofs of synchronization properties are constructed by devising predicates called resource invariants which describe relationships among the variables of a resource when no process is in a critical region for the resource. In constructing proofs using the system of Owicki and Gries, the programmer is required to supply the resource invariants.We show that convexity plays a key role in the derivation of strong resource invariants. We also develop methods for automatically synthesizing resource invariants. Specifically, we characterize the resource invariants of a concurrent program as least fixpoints of a functional which can be obtained from the text of the program. By using this fixpoint characterization and a widening operator which exploits our observation on the importance of convexity, good approximations may be obtained for the resource invariants of many concurrent programs.
p2728
aVThere is a wide range of applications for string processing and SNOBOL4 (Griswold, et al. [1971]) has come to be the most widely implemented and accepted language for such applications. No doubt one of the principle reasons for this acceptance is the data structure around which the language is organized, the string pattern. This structure together with the associated pattern matching process provide great flexibility. Nevertheless it has been widely recognized in informal terms that the pattern matching process is often grossly inefficient (Ripley & Griswold [1975], Dewar & McCann [1977]) and that the pattern structure is notoriously difficult to explain and use (Ripley & Griswold [1975], Stewart [1975]). Each of these areas of difficulty relates to such things as two modes of operation (quickfull scan), problems with leftrecursion, heuristics in the scan, etc. Some difficulties are inherent with string patterns but many are not; we feel the developments described here help to clarify this situation.In section 2 we describe the formal model upon which we base this work. This allows the careful analysis of the variety of sets of strings which may be specified by the patterns which we admit and deduction to be made concerning the possibility/impossibility of algorithms of interest. With SNOBOL4 it has been the case that the careful definition of the "meaning" of a pattern is in terms of the actions taken by the pattern matching algorithm. This has led to the incorporation of idiosyncrasies of a particular algorithm into the understanding of the pattern structure. This seems akin to using a compiler as the definition of a programming language and we believe it is important to future progress to have other alternatives.In section 3 we point out that the worstcase execution time of the usual SNOBOL pattern matching algorithm is exponential in the length of the subject string, even on some quite simple patterns. We then present an algorithm whose worstcase time is polynomial and that operates on patterns which include a true set complement operator. As side benefits we find that the algorithm is not multimodal and correctly handles the null string as an alternative and leftrecursion.In order to conserve space we will assume throughout this paper that the reader is familiar with the idea of a string pattern in the sense that it is described in Griswold et al. [1971]. Also it is probably necessary that the reader have some general knowledge of the formal languages area.
p2729
aVPath expressions are a tool for synchronization of concurrent processes. They are an integral part of the data abstraction mechanism in a programming language, and specify synchronization entirely in terms of the allowable sequences of operations on an object of the abstract data type. This paper describes an attempt to push the path expression synchronization construct along three dimensions  specification, verification, and implementation  into a useful theoretical and practical tool. We define Predicate Path Expressions (PPEs), which allow for a more convenient specification of many synchronization problems. The predicate is a powerful extension to path expressions that increases their expressiveness. We formally define the semantics of PPEs by a transformation to a corresponding nondeterministic program, thus allowing the use of known verification techniques for nondeterministic programs to be used for proving properties of the PPE and the data abstraction of which it is a part. We also describe our existing implementation, in Algol 68, of a data abstraction mechanism that incorporates PPEs.
p2730
aVWe present an algorithn for the determination of runtime types which functions in the presence of errors, and show that it provides more information than that obtained using a previously published algorithm.In Section 1 we define the problem and state the requirements for a practically useful type prediction algorithm. In Section 2 we introduce a model programing language and in Section 3 define type inference rules for that language. Section 4 presents a type prediction algorithm and Section 5 describes how to apply the results to solve the problems stated in Section 1. Section 6 presents an example of our procedure and demonstrates how previous work does not satisfy all requirements.
p2731
aVIn [12] the authors introduced the concept of binding time optimization and presented a series of data flow analytic methods for determining some of the binding time characteristics of programs. In this paper we extend that work by providing methods for determining the class of shapes which an unbounded data object may assume during execution of a LISPlike program, and describe a number of uses to which that information may be put to improve storage allocation in compilers and interpreters for advanced programming languages.We are concerned chiefly with finding, for each program point and variable a finite description of a set of graphs which includes all the shapes of values the variable could assume at that point during the execution of a program. If this set is small or regular in structure, this information can be used to optimize the program's execution, mainly by use of more efficient storage allocation schemes.In the first part we show how to construct from a program without selective updating a tree grammar whose nonterminals generate the desired sets of graphs; in this case they will all be trees. The tree grammars are of a more general form than is usually studied [8, 19], so we show that they may be converted to the usual form. The resulting tree grammar could naturally be viewed as a recursive type definition [11] of the values the variables may assume. Further, standard algorithms may be employed to test for infiniteness, emptiness or linearity of the tree structure.In the second part selective updating is allowed, so an alternate semantics is introduced which more closely resembles traditional LISP implementations, and which is equivalent to the tree model for programs without selective updating. In this model data objects are directed graphs. We devise a finite approximation method which provides enough information to detect cell sharing and cyclic structures whenever they can possibly occur. This information can be used to recognize when the use of garbage collection or of reference counts may be avoided.The work reported in the second part of this paper extends that of Schwartz [17] and Cousot and Cousot [7]. They have developed methods for determining whether the values of two or more variables share cells, while we provide information on the detailed structure of what is shared. The ability to detect cycles is also new. It also extends the work of Kaplan [13], who distinguishes only binary relations among the variables of a program, does not handle cycles, and does not distinguish selectors (so that his analysis applies to nodes representing sets rather than ordered tuples).
p2732
aVData flow analysis is a technique essential to the compiletime optimization of computer programs, wherein facts relevant to program optimizations are discovered by the global propagation of facts obvious locally.This paper extends flow analysis techniques developed for sequential programs to the analysis of communicating, concurrent processes.
p2733
aVSemantic analysis of programs is essential in optimizing\u000acompilers and program verification systems. It encompasses data\u000aflow analysis, data type determination, generation of approximate\u000ainvariant assertions, etc.\u000a\u000aSeveral recent papers (among others Cousot & Cousot[77a],\u000aGraham & Wegman[76], Kam & Ullman[76], Kildall[73],\u000aRosen[78], Tarjan[76], Wegbreit[75]) have introduced abstract\u000aapproaches to program analysis which are tantamount to the use of a\u000aprogram analysis framework (A,t,) where A is a\u000alattice of (approximate) assertions, t is an (approximate)\u000apredicate transformer and  is an often implicit function\u000aspecifying the meaning of the elements of A. This paper is devoted\u000ato the systematic and correct design of program analysis frameworks\u000awith respect to a formal semantics.\u000a\u000aPreliminary definitions are given in Section 2 concerning the\u000amerge over all paths and (least) fixpoint programwide analysis\u000amethods. In Section 3 we briefly define the (forward and backward)\u000adeductive semantics of programs which is later used as a formal\u000abasis in order to prove the correctness of the approximate program\u000aanalysis frameworks. Section 4 very shortly recall the main\u000aelements of the lattice theoretic approach to approximate semantic\u000aanalysis of programs.\u000a\u000aThe design of a space of approximate assertions A is studied in\u000aSection 5. We first justify the very reasonable assumption that A\u000amust be chosen such that the exact invariant assertions of any\u000aprogram must have an upper approximation in A and that the\u000aapproximate analysis of any program must be performed using a\u000adeterministic process. These assumptions are shown to imply that A\u000ais a Moore family, that the approximation operator (wich defines\u000athe least upper approximation of any assertion) is an upper closure\u000aoperator and that A is necessarily a complete lattice. We next show\u000athat the connection between a space of approximate assertions and a\u000acomputer representation is naturally made using a pair of isotone\u000aadjoined functions. This type of connection between two complete\u000alattices is related to Galois connections thus making available\u000aclassical mathematical results. Additional results are proved, they\u000ahold when no two approximate assertions have the same meaning.\u000a\u000aIn Section 6 we study and examplify various methods which can be\u000aused in order to define a space of approximate assertions or\u000aequivalently an approximation function. They include the\u000acharacterization of the least Moore family containing an arbitrary\u000aset of assertions, the construction of the least closure operator\u000agreater than or equal to an arbitrary approximation function, the\u000adefinition of closure operators by composition, the definition of a\u000aspace of approximate assertions by means of a complete join\u000acongruence relation or by means of a family of principal\u000aideals.\u000a\u000aSection 7 is dedicated to the design of the approximate\u000apredicate transformer induced by a space of approximate assertions.\u000aFirst we look for a reasonable definition of the correctness of\u000aapproximate predicate transformers and show that a local\u000acorrectness condition can be given which has to be verified for\u000aevery type of elementary statement. This local correctness\u000acondition ensures that the (merge over all paths or fixpoint)\u000aglobal analysis of any program is correct. Since isotony is not\u000arequired for approximate predicate transformers to be correct it is\u000ashown that nonisotone program analysis frameworks are manageable\u000aalthough it is later argued that the isotony hypothesis is natural.\u000aWe next show that among all possible approximate predicate\u000atransformers which can be used with a given space of approximate\u000aassertions there exists a best one which provides the maximum\u000ainformation relative to a programwide analysis method. The best\u000aapproximate predicate transformer induced by a space of approximate\u000aassertions turns out to be isotone. Some interesting consequences\u000aof the existence of a best predicate transformer are examined. One\u000ais that we have in hand a formal specification of the programs\u000awhich have to be written in order to implement a program analysis\u000aframework once a representation of the space of approximate\u000aassertions has been chosen. Examples are given, including ones\u000awhere the semantics of programs is formalized using Hoare[78]'s\u000asets of traces.\u000a\u000aIn Section 8 we show that a hierarchy of approximate analyses\u000acan be defined according to the fineness of the approximations\u000aspecified by a program analysis framework. Some elements of the\u000ahierarchy are shortly exhibited and related to the relevant\u000aliterature.\u000a\u000aIn Section 9 we consider global program analysis methods. The\u000adistinction between "distributive" and "nondistributive" program\u000aanalysis frameworks is studied. It is shown that when the best\u000aapproximate predicate transformer is considered the coincidence or\u000anot of the merge over all paths and least fixpoint global analyses\u000aof programs is a consequence of the choice of the space of\u000aapproximate assertions. It is shown that the space of approximate\u000aassertions can always be refined so that the merge over all paths\u000aanalysis of a program can be defined by means of a least fixpoint\u000aof isotone equations.\u000a\u000aSection 10 is devoted to the combination of program analysis\u000aframeworks. We study and examplify how to perform the "sum",\u000a"product" and "power" of program analysis frameworks. It is shown\u000athat combined analyses lead to more accurate information than the\u000aconjunction of the corresponding separate analyses but this can\u000aonly be achieved by a new design of the approximate predicate\u000atransformer induced by the combined program analysis\u000aframeworks.
p2734
aVIn this paper we study the profitability of applying the "reduction in strength" technique to programs in settheoretic languages, focusing on the high level constructs involving setformers. We define recursively two classes of expressions we shall call inductively computable setformers and inductively computable predicates which can be evaluated with an order of magnitude improvement of the asymptotic running time as compared to the straightforward evaluation. The quantity developed for this comparison can be used to derive further results when additional information is known. For programs written in very high level languages, which often consist mostly of "nested" iterative constructs, this technique amounts to altering the "algorithm" used to compute the program by replacing it with an asymptotically faster algorithm.
p2735
aVConcurrency in Gypsy is based on a unique, formal approach to specifying and proving systems of concurrent processes. The specification and proof methods are designed so that proofs of individual processes are totally independent, even when operating concurrently. These methods can be applied both to terminating and nonterminating processes, and the proof methods are well suited to automated verification aids. The basic principles of these methods and their interaction with the design of Gypsy are described.
p2736
aVHow can one organize the understanding of complex algorithms? People have been thinking about this issue at least since Euclid first tried to explain his innovative greatest common divisor algorithm to his colleagues, but for current research into verifying stateoftheart programs, some precise answers to the question are needed. Over the past decade the various verification methods which have been introduced (inductive assertions, structural induction, leastfixedpoint semantics, etc.) have established many basic principles of program verification (which we define as: establishing that a program text satisfies a given pair of inputoutput specifications). However, it is no coincidence that most published examples of the application of these methods have dealt with "toy programs" of carefully considered simplicity.Experience indicates that these "first generation" principles, with which one can easily verify a threeline greatest common divisor algorithm, do not directly enable one to verify a 10,000 line operating system (or even a 50 line listprocessing algorithm) in complete detail. To verify complex programs, additional techniques of organization, analysis and manipulation are required. (That a similar situation exists in the writing of large, correct programs has long been recognized   structured programming being one solution.)This paper examines the usefulness of correctnesspreserving program transformations (see [6]) in structuring fairly complex correctness proofs. Using our approach one starts with a simple, highlevel (or "abstract") algorithm which can be easily verified, then successively refines it by implementing the abstractions of the initial algorithm to obtain various final, detailed algorithms. In Section 2 we introduce the technique by deriving the DeutschSchorrWaite listmarking algorithm [14]. Our main example is the more complex problem of verifying boundedworkspace listcopying algorithms: Section 3 defines the issues, Section 4 presents the key intermediate algorithm in detail and Section 5 considers three of the most complex (published) implementations of listcopying, one of which is discussed in detail. In Section 6 we make some general remarks on program verification and the relevance of our results to the (larger) field of program correctness; Section 7 mentions some related work.
p2737
aVFirst Order Programming Logic is a simple, yet powerful formal system for reasoning about recursive programs. In its simplest form, it has one major limitation: it cannot establish any property of the least fixed point of a recursive program which is false for some other fixed point. To rectify this weakness, we present two intuitively distinct approaches to strengthening First Order Programming Logic and prove that either extension makes the logic relatively complete. In the process, we prove that the two approaches are formally equivalent. The relative completeness of the extended logic is significant because it suggests it can establish all "ordinary" properties (obviously we cannot escape the Godelian incompleteness inherent in any programming logic) of recursive programs including those which compute partial functions.The second contribution of this paper is to establish that First Order Programming Logic is applicable to iterative programs as well. In particular, we show that the intermittent assertions method an informal proof method for iterative programs which has not been formalized is conveniently formalized simply as sugared First Order Programming Logic applied to the recursive translations of iterative programs.
p2738
aVThe problem of reasoning about recursive programs is considered. Utilizing a simple analogy between iterative and recursive programs viewed as unfinite unions of finite terms, we carry out an investigation analogous to that carried out recently for iterative programs. The main results are the arithmetical completeness of axiom systems for (1) contextfree dynamic logic and (2) its extension for dealing with infinite computations. Having the power of expression of these logics in mind, these results can be seen to supply (as corollaries) complete proof methods for the various kinds of correctness of recursive programs.
p2739
aVWe discuss problems arising in reasoning about ongoing processes, using the modal constructs after, throughout, during, and preserves. Earlier work established decidability of the theory whose language included only the first two of these, along with program connectives | , ; and *. Here we give a complete Gentzentype axiomatizations for useful combinations of the other modalities. We also indicate how such Gentzentype axiomatizations lead to deterministic exponential time upper bounds on the complexity of decision procedures for these languages. It remains an open problem how to completely axiomatize the combination of modalities during and preserves.
p2740
aVPointer manipulation is one of the trickiest operations in programming and is a major source of programming errors. A great deal of research effort has been dedicated to making programs with pointers more reliable. In this paper we will present pointer operations which reduce conceptual difficulties when writing pointer programs, and increase reliability of programs. We will analyze theoretically as well as empirically why these operations are more convenient. Finally, we will define the safety of rotations, and show that it is decidable to check whether the rotation is safe and also show that it is a good measure of the correctness of programs. (That is, if the rotation is safe it is highly probable that the program is correct, and if it is unsafe it is highly probable that the program is incorrect.)Probably the most successful effort to eliminate difficulties associated with pointer manipulation is the invention of abstract data types and the use of implementation modules. Pointers are often used to implement complex data structures, which in turn represent abstract data types. We can textually localize relevant pointer manipulations using abstract data types. Thus, we can easily check correctness of pointer programs. This approach is successful when the structure of abstract data types is simple, or efficiency is not seriously required. However, programs like garbage collection, list marking and copying, and balanced tree manipulations have not been well specified by abstract types.The approach taken in this paper is complementary to abstract data types, and makes programs like list marking and copying more reliable and easy to write. The idea behind this approach is to introduce higherlevel pointer manipulation primitives instead of pointer assignments. Since most pointer assignments do not appear isolated, we can group several assignments together and replace them by one of these new primitives. We also show some theoretical evidence why these operations are nicer.Two primitives are introduced in this paper. One is the nway pointer rotation, which is defined as being equivalent to the following procedure:Rotate: procedure(var x1, x2, \u2026 , xn:T) = begin var w:T; w:=x1; x1:=x2; \u2026 ;xn:=w endwhere all the locations of arguments x1, x2, \u2026 , xn are distinct. The other operation is nway pointer slide, which is as powerful as pointer assignment yet keeps the elegance of rotation.Section 2 of this paper reviews previous work related to this subject. We give definitions of pointer operations analyzed in this paper in section 3. Using these operations, DeutschSchorrWaite algorithm is rewritten in section 4. In section 5 we analyze pointer rotations extensively and obtain several properties which are not attainable by ordinary pointer assignments. In section 6 we define the safety of pointer rotation when applied to linear list and trees. Then, we extend this definition to arbitrary pointer data structures and show a decision procedure to check safety. In section 7 we show two examples, list marking and list copying programs written using rotations and slides. We examine how we can directly code the algorithms using these operations and safety checking algorithms are useful.
p2741
aVA parsing method based on the triconnected decomposition of a biconnected graph is presented. The parsing algorithm runs in linear time and handles a large class of flow graphs. The applications of this algorithm to flow analysis and to the automatic structuring of programs are discussed.
p2742
aVThe goal of automatic program verification is to prove programs correct formally. We argue that the existing notions of formal proof are too syntactic and as such too intimately bound up with details of lowlevel computation. We propose a more semantic notion of formal proof which nevertheless pays due respect to the problem of effectiveness in proof checking. Such a notion supplies a more practical basis for the specification of verifiers than do extant approaches. In particular the problem of constructing verifiers according to our approach is reduced entirely to routine development and implementation of decision methods, while permitting shorter proofs and yet remaining easy to develop proofs with.
p2743
aVOn and off over the period of about a year I have worked on a semantic specification for the C programming language My objective was to construct a readable and precise specification of C, aimed at compiler writers, maintainers, and language pundits. This paper is a report on the project.
p2744
aVA new specification method for data types is presented, which is distinguished by the semantic objects it specifies. In particular, only final data types [GGM,W] are specifiable. A final data type is one in which no two elements are "inputoutput equivalent". It is argued that the mathematical properties of final data types characterize abstractness on the semantic level.Examples are given to show that final data type specifications are easy to construct and use.
p2745
aVIn a strongly typed system supporting user defined data abstractions, the designer of a data abstraction ought to be careful in choosing the operations for the abstraction. If the operation set chosen is not expressive enough, it might be impossible or inconvenient to implement certain useful functions on the values of the data abstraction. In this paper, we characterize the expressive power of the operation set by defining two properties for data abstractions  expressive completeness and expressive richness. The operation set of an expressively complete data abstraction is adequate enough to implement all computable functions on its values. An expressively rich data abstraction is expressively complete with an operation set that is rich enough to conveniently extract from a value, all relevant information required to reconstruct the value from scratch. Practical applications of the properties of expressiveness introduced are also discussed.
p2746
aVThe equational axioms of an algebraic specification of a data type (such as finite sequences) often can be formed into a convergent set of rewrite rules; i.e. such that all sequences of rewrites are finite and uniquely terminating. If one adds a rewrite rule corresponding to a data type property whose proof requires induction (such as associativity of sequence concatenation), convergence may be destroyed, but often can be restored by using the KnuthBendix algorithm to generate additional rules. A convergent set of rules thus obtained can be used as a decision procedure for the equational theory for the axioms plus the property added. This fact, combined with a "full specification" property of axiomatizations, leads to a new method of proof of inductive properties not requiring the explicit invocation of an inductive rule of inference.
p2747
aVThe use of the temporal logic formalism for program reasoning is reviewed. Several aspects of responsiveness and fairness are analyzed, leading to the need for an additional temporal operator: the 'until' operator U. Some general questions involving the 'until' operator are then discussed. It is shown that with the addition of this operator the temporal language becomes expressively complete. Then, two deductive systems DX and DUX are proved to be complete for the languages without and with the new operator respectively.
p2748
aVPnueli [15] has recently introduced the idea of using temporal logic [18] as the logical basis for proving correctness properties of concurrent programs. This has permitted an elegant unifying formulation of previous proof methods. In this paper, we attempt to clarify the logical foundations of the application of temporal logic to concurrent programs. In doing so, we will also clarify the relation between concurrency and nondeterminism, and identify some problems for further research.In this paper, we consider logics containing the temporal operators "henceforth" (or "always") and "eventually" (or "sometime"). We define the semantics of such a temporal logic in terms of an underlying model that abstracts the fundamental concepts common to almost all the models of computation which have been used. We are concerned mainly with the semantics of temporal logic, and will not discuss in any detail the actual rules for deducing theorems.We will describe two different temporal logics for reasoning about a computational model. The same formulas appear in both logics, but they are interpreted differently. The two interpretations correspond to two different ways of viewing time: as a continually branching set of possibilities, or as a single linear sequence of actual events. The temporal concepts of "sometime" and "not never" ("not always not") are equivalent in the theory of linear time, but not in the theory of branching time   hence, our title. We will argue that the logic of linear time is better for reasoning about concurrent programs, and the logic of branching time is better for reasoning about nondeterministic programs.The logic of linear time was used by Pnueli in [15], while the logic of branching time seems to be the one used by most computer scientists for reasoning about temporal concepts. We have found this to cause some confusion among our colleagues, so one of our goals has been to clarify the formal foundations of Pnueli's work.The following section gives an intuitive discussion of temporal logic, and Section 3 formally defines the semantics of the two temporal logics. In Section 4, we prove that the two temporal logics are not equivalent, and discuss their differences. Section 5 discusses the problems of validity and completeness for the temporal logics. In Section 6, we show that there are some important properties of the computational model that cannot be expressed with the temporal operators "henceforth" and "eventually", and define more general operators.
p2749
aVWe indicate below the various results in this paper and the sections where these results are fully described. \u03b41 is introductory).(\u03b42). The partial correctness assertion (PCA) A{\u03b1}B is expressed in PDL in the form A \u2192 [\u03b1]B. We shall consider the question of when a given finite set of PCAs implies another and give optimal complexity bounds. We also give other results of an algebraic nature.(\u03b43). DPDL is less well understood than PDL since the filtration technique of FL that applies to the latter falls for the former. We point out a translation from PDL to DPDL which leads to a lower bound for the latter.(\u03b44). What do models of PDL look like? We show the existence of canonical and universal models and show how an infinite model can be related to its FL factors.
p2750
aVA crucial property of distributed multiprocessing systems is the lack of complete information by any given process about the states of other processes. The contribution of this paper is a fundamental modal logic, MPL, for multiprocessing with incomplete information. (Section 1.5 gives an informal introduction to MPL; the formal definitions are in Section 2.)By way of this logic, we develop a solid (practical and theoretical) correspondence between distributed multiprocessing and multiplayer games of incomplete information.Fischer and Ladner, [1979] have shown a logspace reduction from the outcome problem for two person games of perfect information to satisfiability of formulas in their propositional dynamic logic (PDL). We provide in Section 3 a logspace reduction from the outcome problem for multiplayer games of incomplete information to satisfiability of formulas in our logic MPL. Although in general satisfiability in out logic is undecidable, the satisfiability problem of formulae with hierarchical visibility structure is shown decidable (using the methods for solving hierarchical games developed in Reif [1979] and Peterson and Reif [1979], and also the model theoretic techniques of Fischer and Ladner [1979] and Pratt [1979b]).At a more practical level, we argue that the gamelike semantics of our logic provides a robust paradigm in which to view distributed multiprocessing problems. We apply our logic to describe total correctness properties of multiprocess programs with shared variables as well as communicating processes with "handshake"type message passing.
p2751
aVIn statically typed programming languages, each variable and expression in a program is assigned a unique "type" and the program is checked to ensure that the arguments in each application are "typecompatible" with the corresponding parameters. The rules by which this "typechecking" is performed must be carefully considered for modern languages that allow the programmer to define his own data types and allow parameterized types or types as parameters. (Such languages include Alphard [Wulf78], CLU [Liskov77], Euclid [Lampson77] and Russell [Demers79].) These features increase the expressive power of the languages, but also increase the difficulty of typechecking them.In this paper, we describe a treatment of typechecking that makes it possible to do completely static checking with a general parameterization mechanism allowing parameterized types, types as parameters, and even a disciplined form of selfapplication. Our method defines a calculus of "signatures," where signatures are similar to the "program types" of [Reynolds78]. Each identifier and expression is given a signature, and applications are typecorrect when argument and parameter signatures are equivalent under a simple set of signature transformation rules. Below we present the signature calculus of Russell; we also present a semantic justification of this calculus and specify the language constraints necessary for us to justify our purely static approach to typechecking.
p2752
aVA precise definition is given of how partial correctness or termination assertions serve to specify the semantics of classes of program schemes. Assertions involving only formulas of first order predicate calculus are proved capable of specifying program scheme semantics, and effective axiom systems for deriving such assertions are described. Such axiomatic specifications are possible despite the limited expressive power of predicate calculus.
p2753
aVEfficient algorithms are presented for several grammar problems relevant to compiler construction. These problems include(i) testing, for a reduced contextfree grammar G and an LL(k), uniquely invertible, or BRC(m,n) grammar H, if G is structurally contained by H, and(ii) testing, for a reduced contextfree grammar G and a structurally unambiguous grammar H, if G is Reynolds covered by H or if there is an on to homomorphisem from G to H.Related complexity results are presented for several problems for the regular grammars, program schemes, and monadic program schemes.
p2754
aVIn testing for program correctness, the standard approaches\u000a[11,13,21,22,23,24,34] have centered on finding data D, a finite\u000asubset of all possible inputs to program P, such that\u000a\u000a1) if for all x in D, P(x) = f(x), then P* = f\u000a\u000awhere f is a partial recursive function that specifies the\u000aintended behavior of the program and P* is the function actually\u000acomputed by program P. A major stumbling block in such\u000aformalizations has been that the conclusion of (1) is so strong\u000athat, except for trivial classes of programs, (1) is bound to be\u000aformally undecidable [23].\u000a\u000aThere is an undeniable tendency among practitioners to consider\u000aprogram testing an ad hoc human technique: one creates test data\u000athat intuitively seems to capture some aspect of the program,\u000aobserves the program in execution on it, and then draws conclusions\u000aon the program's correctness based on the observations. To augment\u000athis undisciplined strategy, techniques have been proposed that\u000ayield quantitative information on the degree to which a program has\u000abeen tested. (See Goodenough [14] for a recent survey.) Thus the\u000atester is given an inductive basis for confidence that (1) holds\u000afor the particular application. Paralleling the undecidability of\u000adeductive testing methods, the inductive methods all have had\u000atrivial examples of failure [14,18,22,23].\u000a\u000aThese deductive and inductive approaches have had a common\u000atheme: all have aimed at the strong conclusion of (1). Program\u000amutation [1,7,9,27], on the other hand, is a testing technique that\u000aaims at drawing a weaker, yet quite realistic, conclusion of the\u000afollowing nature:\u000a\u000a(2) if for all x in D, P(x) = f(x), then P* = f OR P is\u000a"pathological."\u000a\u000aTo paraphrase,\u000a\u000a3) if P is not pathological and P(x) = f(x) for all x in D then\u000aP* = f.\u000a\u000aBelow we will make precise what is meant by "P is pathological";\u000afor now it suffices to say that P not pathological means that P was\u000awritten by a competent programmer who had a good understanding of\u000athe task to be performed. Therefore if P does not realize f it is\u000a"close" to doing so. This underlying hypothesis of program mutation\u000ahas become known as the competent programmer hypothesis:\u000aeither P* = f or some program Q "close" to P has the property Q* =\u000af.\u000a\u000aTo be more specific, program mutation is a testing method that\u000aproposes the following version of correctness testing:\u000a\u000aGiven that P was written by a competent programmer, find test\u000adata D for which P(D) = f(D) implies P* = f.\u000a\u000aOur method of developing D, assuming either P or some program\u000aclose to P is correct, is by eliminating the alternatives. Let\u000a&phis; be the set of programs close to P. We restate the method\u000aas follows:\u000a\u000aFind test data D such that:\u000a\u000ai) for all x in D P(x) = f(x) and\u000a\u000aii) for all Q in &phis; either Q* = P* or for some x in D,\u000aQ(x) \u2260 P(x).\u000a\u000aIf test data D can be developed having properties (i) and (ii),\u000athen we say that D differentiates P from &phis;,\u000aalternatively P passes the &phis; mutant test.\u000a\u000aThe goal of this paper is to study, from both theoretical and\u000aexperimental viewpoints, two basic questions:\u000a\u000aQuestion 1: If P is written by a competent programmer and\u000aif P passes the &phis; mutant test with test data D, does P* =\u000af?\u000a\u000aNote that, after formally defining &phis; for P in a fixed\u000aprogramming language L, an affirmative answer to question 1 reduces\u000ato showing that the competent programmer hypothesis holds for this\u000aL and &phis;.\u000a\u000aWe have observed that under many natural definitions of\u000a&phis; there is often a strong coupling between members of\u000a&phis; and a small subset . That is, often one can\u000areduce the problem of finding test data that differentiates P from\u000a&phis; to that of finding test data that differentiates P from\u000a. We will call this subset  the mutants of P\u000aand the second question we will study involves the socalled\u000acoupling effect [9]:\u000a\u000aQuestion 2 (Coupling Effect): If P passes the \u000amutant test with data D, does P pass the &phis; mutant test\u000awith data D?\u000a\u000aIntuitively, one can think of  as representing the\u000aprograms that are "very close" to P.\u000a\u000aIn the next section we will present two types of theoretical\u000aresults concerning the two questions above: general results\u000aexpressed in terms of properties of the language class L, and\u000aspecific results for a class of decision table programs and for a\u000asubset of LISP. Portions of the work on decision tables and LISP\u000ahave appeared elsewhere [5,6], but the presentations given here are\u000aboth simpler and more unified. In the final section we present a\u000asystem for applying program mutation to FORTRAN and we introduce a\u000anew type of software experiment, called a "beat the system"\u000aexperiment, for evaluating how well our system approximates an\u000aaffirmative response to the program mutation questions.
p2755
aVIn his recent Turing Lecture, John Backus delivered a trenchant argument for the proposition that "programming languages are in trouble." Backus claims this to be inevitable: the development of Algollike languages must lead to this sorry state because it begins from faulty assumptions.A less radical interpretation of the difficulties of language design is that we still do not understand the fundamental principles that should guide our design efforts; the machines we use may limit our results, but our ignorance has far greater effect. As evidence to support this position, we can cite the difficulties of building successors to Algol 60 and Pascal. Both languages have been acclaimed as welldesigned, but it has proved extremely difficult to capture just what they "got right." Thus, Hoare has claimed that "Algol 60 was not only a great improvement on its predecessors, but also on nearly all of its successors." [73a] The recent Ada design suggests that the same fate may befall Pascal.In this paper, we expand on an idea of Landin [66] to develop an important principle of programming languages: typecompleteness. We argue that application of this principle is an effective tool in understanding problems of programming language design. In particular, typecompleteness1. allows us to point out many of the flaws and inconsistencies in existing languages (so we can know what mistakes not to repeat) and2. provides a framework for the design of languages (or language families) that have wide variation in their changeable parts.Below, we present the principle of typecompleteness and its ramifications for language design. We then discuss some common examples of incompleteness found in existing Algollike languages. And we end by presenting the type structure of the typecomplete programming language Russell, which has been designed by the authors. As we show, the flexibility provided in Russell by the combination of a small, rich type structure and typecompleteness belie Backus's assertion of inherent weakness of vonNeumann languages.
p2756
aVThis paper proposes the encapsulization and control of contending parallel processes within data structures. The advantage of embedding the contention within data is that the contention, itself, thereby becomes an object which can be handled by the program at a level above the actions of the processes themselves. This means that an indeterminate behavior, never precisely specified by the programmer or by the input, may be shared in the same way that an argument to a function is shared by every use of the corresponding parameter, an ability which is of particular importance to applicativestyle programming.
p2757
aVThe very best documentformatting system is a good secretary. He can be given scrawled handwritten text in no particular format, and without further instruction produce a flawless finished document. Nevertheless, we believe that document formatting should be done by computers, because so much of it is the tedium that computers handle so well. Existing computer document formatting programs have met with some success; indeed, most computer systems offer some sort of text formatting capability. These programs are often difficult to use, and are almost invariably tied to a particular kind of printing device.The documentformatting language Scribe was designed to provide a simple, portable language in which document formatting could be specified; the Scribe compiler was written to process that language into finished documents. In following sections we describe the design goals, the implementation, and report on experience with the completed system.
p2758
aVExperience using and implementing the language Poplar is described. The major conclusions are: Applicative programming can be made more natural through the use of builtin iterative operators and postfix notation. Clever evaluation strategies, such as lazy evaluation, can make applicative programming more computationally efficient. Pattern matching can be performed in an applicative framework. Many problems remain.
p2759
aVThe language SUMMER is intended for the solution of problems in text processing and string manipulation. The language consists of a small kernel which supports successdirected evaluation, control structures, recovery caches and a data abstraction mechanism. It is shown how this kernel can be extended to support simultaneous pattern matching in arbitrary domains.
p2760
aVIt is well known that most questions of interest about the behavior of programs such as equivalence, halting, optimization, and other problems are undecidable. On the other hand, it is possible to make some or all of these questions decidable by introducing appropriate restrictions on the programming language under consideration. And once such restrictions are made, the next step is to ask how hard it is to solve these problems for programming languages for which they are decidable.This analysis of programming languages has been undertaken by others, in particular by Jones and Muchnick [4], who choose to restrict their programs to operate over finite memory. Our approach starts from the language of loopprograms, as defined by Meyer and Ritchie [1], in which we in fact allow more general arithmetical operations. Unlike Jones and Muchnick, we do not place any restriction on memory here; instead, we (primarily) restrict our attention to loopprograms without nested loops.
p2761
aVA class of schemes called synchronous schemes is defined. A synchronous scheme can have several variables, but all the active ones are required to keep a synchronized rate of computation as measured by the height of their respective Herbrand values. A "reset" statement, which causes all the variables to restart a new computation, is admitted. It is shown that equivalence, convergence, and other properties are decidable for schemes in this class. The class of synchronous schemes contains, as special cases, the known decidable classes of Ianov schemes, onevariable schemes with resets, and progressive schemes.
p2762
aVIt is known that not all paths are possible in the run time control flow of many programs. It is also known that data flow analysis cannot restrict attention to exactly those paths that are possible. It is therefore usual for analytic methods to consider all paths. Sharper information can be obtained by considering a recursive set of paths that is large enough to include all possible paths but small enough to exclude many of the impossible ones. This paper presents a simple uniform methodology for sharpening data flow information by considering certain recursive path sets of practical importance. Associated with each control flow arc there is a relation on a finite set Q. The paths that qualify to be considered are (essentially) those for which the composition of the relations encountered is nonempty. For example, Q might be the set of all assignments of values to each of several bit variables used by a program to remember some facts about the past and branch accordingly in the future. Given any data flow problem together with qualifying relations on Q associated with the control flow arcs, we construct a new problem. Considering all paths in the new problem is equivalent to considering only qualifying paths in the old one. Preliminary experiments (with a small set of real programs) indicate that qualified analysis is feasible and substantially more informative than ordinary analysis. The methodology also has a beneficial feedback effect on the delicate task of passing from programs to meaningful data flow analysis problems. Even when all paths qualify, unusually sharp information can be obtained by passing from programs to problems in ways suggested by our theorems.
p2763
aVInterprocedural data flow analysis is complicated by the use of procedure and label variables in programs and by the presence of aliasing among variables. In this paper we present an algorithm for computing possible values for procedure and label variables, thus providing a call graph and a control flow graph. The algorithm also computes the possible aliasing relationships in the program being analyzed.We assume that control flow information is not available to the algorithm; hence, this type of analysis may be termed "flowfree analysis." Given this assumption, we demonstrate the correctness of the algorithm, in the sense that the information it produces is conservative, and show that it is as precise as possible in certain cases. We also show that the problem of determining possible values for procedure variables is Pspace hard. This fact indicates that any algorithm which is precise in all cases must also run very slowly for some programs.
p2764
aVThe idiomatic APL programming style is limited by the constraints of a rectangular, homogeneous array as a data structure. Nonscalar data is difficult to represent and manipulate, and the nonscalar APL functions have no uniform extension to higher rank arrays. The carrier array is an extension to APL which addresses these limitations while preserving the economical APL style. A carrier array is a ragged array with an associated partition which allows functions to be applied to subarrays in parallel. The primitive functions are given base definitions on scalars and vectors, and they are extended to higher rank arrays by uniform application mechanisms. Carrier arrays also allow the last dimensions of an array to be treated as a single datum; the primitive functions are given extended definitions on scalars and vectors of this nonscalar data.This paper defines the carrier array and gives the accompanying changes to the definitions of the APL primitive functions. Examples of programming with carrier arrays are presented, and implementation issues are discussed.
p2765
aVIn this paper we describe how we have combined a number of tools (most of which understand a particular programming language) into a single system to aid in the reading, writing, and running of programs. We discuss the efficacy and the structure of our system. For the last two years the system has been used to build itself; it currently consists of 500 kilobytes of machine code (25,000 lines of LISP/370 code) and approximately one hundred commands with large numbers of options. We will describe some of the experience we have gained in evolving this system. We first indicate the system components which users have found most important; some of the tools described here are new in the literature. Second, we emphasize how these tools form a synergistic union, and we illustrate this point with a number of examples. Third, we illustrate the use of various system commands in the development of a simple program. Fourth, we discuss the implementation of the system components and indicate how some of them have been generalized.
p2766
aVA syntaxdirected editor is a tool for structured program development. Such an editor can enforce syntactic correctness incrementally by restricting editing operations to legitimate modifications of the program's contextfree derivation tree. However, not all language features can be described by the contextfree formalism. To build editors that enforce noncontextfree correctness, a more powerful specification technique is needed. In this paper we discuss the advantages of attribute grammars as a specification technique for a syntaxdirected editing system. We also present an efficient algorithm for incrementally evaluating attributes as a program tree is derived.
p2767
aVIn analysis of programs and many other kinds of computation, algorithms are commonly written to have an input P and an output Q, where both P and Q are large and complicated objects. For example, P might be a routing problem and Q might be a solution to P. Although documented and discussed in this exhaustive style, algorithms are sometimes intended for use in contexts with two departures from the onetime analysis of an entire new input. First, the current value of P is the result of a small change to a previous value of P. We want to update the results of the previous analysis without redoing all of the work. Second, accurate information is only wanted in some designated portion of the large output Q. Possibly inaccurate information may appear elsewhere in Q. We want the analysis to be demanddriven: accurate where accuracy is demanded, but not burdened by the cost of providing much more than is demanded. This paper studies demanddriven algorithms capable of updating without extensive reanalysls. Such algorithms are called incremental, in contrast with the onetime analysis of an entire new input contemplated by exhaustive algorithms. In some cases, it is shown that an exhaustive algorithm can be easily recast in an efficient incremental style. Other algorithms for the same problem may be much less amenable to incremental use. It is shown that traditional exhaustive computational complexity bounds are poor predictors of incremental performance. The main examples are taken from global data flow analysis.
p2768
aVThis paper proposes a practical alternative to program verification   called formal program testing   with similar, but less ambitious goals. Like a program verifier, a formal testing system takes a program annotated with formal specifications as input, generates the corresponding verification conditions, and passes them through a simplifier. After the simplification step, however, a formal testing system simply evaluates the verification conditions on a representative set of test data instead of trying to prove them. Formal testing provides strong evidence that a program is correct, but does not guarantee it. The strength of the evidence depends on the adequacy of the test data.
p2769
aVIt is shown that distributed systems of probabilistic processors are essentially more powerful than distributed systems of deterministic processors, i.e., there are certain useful behaviors that can be realized only by the former. This is demonstrated on the dining philosophers problem. It is shown that, under certain natural hypotheses, there is no way the philosophers can be programmed (in a deterministic fashion) so as to guarantee the absence of deadlock (general starvation). On the other hand, if the philosophers are given some freedom of choice one may program them to guarantee that every hungry philosopher will eat (with probability one) under any circumstances (even an adversary scheduling). The solution proposed here is fully distributed and does not involve any central memory or any process with which every philosopher can communicate.
p2770
aVSufficient conditions are given for partial correctness assertions to determine the inputoutput semantics of quite general classes of programming languages. This determination cannot be unique unless states which are indistinguishable by predicates in the assertions are identified. Even when indistinguishable states are identified, partial correctness assertions may not suffice to determine program semantics.
p2771
aVA theory of partial correctness proofs is formulated in Scott's logic computable junctions. This theory allows mechanical construction of verification condition solely on the basis of a denotational language definition. Extensionally these conditions, the resulting proofs, and the required program augmentation are similar to those of Hoare style proofs; conventional input, output, and invariant assertions in a first order assertion language are required. The theory applies to almost any sequential language defined by a continuation semantics; for example, there are no restrictions on aliasing or sideeffects. Aspects of "static semantics",such as type and declaration constraints, which are expressed in the denotational definition are validated as part of the verification condition generation process.
p2772
aVWhen the "binding mechanisms" of assignment, quantification, and procedure definition are removed from a conventional first order total correctness logic of programs, the remaining logical system is decidable in time approximately one exponential in the length of the input. This system is maximal in the sense that the presence of any one of the three binding mechanisms would make it undecidable. Such a decision procedure can play a central role in the construction of program verifiers based on decision methods.
p2773
aVA temporal language and system are presented which are based on branching time structure. By the introduction of symmetrically dual sets of temporal operators, it is possible to discuss properties which hold either along one path or along all paths. Consequently it is possible to express in this system all the properties that were previously expressible in linear time or branching time systems. We present an exponential decision procedure for satisfiability in the language based on tableaux methods, and a complete deduction system. As associated temporal semantics is illustrated for both structured and graph representation of programs.
p2774
aVThis paper introduces the path, a new programming language construct designed to supplant the use of pointers to access and destructively update recursive data structures. In contrast to the complex semantics and proof rules for pointers, the semantics and proof rules for paths are simple and abstract. In fact, they are easily formalized within a firstorder theory of recursive data objects analogous to firstorder number theory. We present a number of sample programs, including implementations of queues and binary trees, utilizing the new construct and prove that they are correct.
p2775
aVVerification of attribute grammar is discussed. As is widely recognized, attribute grammar of Knuth [8] is a very convenient device to describe semantics of programming languages, especially in automating compiler construction. Many efforts have been made to obtain efficient evaluators for attribute grammar [1,3,4,5,7,10] so that efficient compilers may be produced from the semantic specifications written in the attribute grammar.There is another important problem about the semantic specifications. This is how to verify the correctness of the specification and is essential in ascertaining the correctness of produced compilers. In contrast with the evaluation problem, this has not been studied well and only a few partial results have been reported up to now [2,9].In this paper we propose a verification procedure for proving the correctness of attribute grammar, which is applicable to the wide class of attribute grammars called absolutely noncircular ones [7]. Our method can be extended to accept any noncircular attribute grammar since it is shown that any noncircular attribute grammar is transformed into an equivalent absolutely noncircular one [5].Our verification procedure utilizes dependency relations among attributes and verification is performed by induction based on this order relation, according to which attributes are evaluated. This procedure consists of (1) assigning assertions to relevant pairs of a nonterminal symbol and a parallelly evaluable subset of its attributes, (2) generating a set of verification conditions for each production rule with the aid of dependency graph of the rule and (3) proving the verification conditions. Of course, verification is performed productionrulewise.Our method can accept attribute grammars which contain both synthesized and inherited attributes and handle them in a wellformed way. This contrasts to the previous works where only the synthesized case is considered [2] or there seems no general and formal descriptions about how to verify general attribute grammars [9].In this paper we first give necessary definitions and notations for attribute grammar and then propose a verification procedure to prove its correctness. After giving some examples of verification, we establish consistency and completeness of our procedure.
p2776
aVSmalltalk is an objectoriented language designed and\u000aimplemented by the Learning Research (Group of the Xerox Palo Alto\u000aResearch Center [2, 5, 14]. Some features of this language are:\u000aabstract data classes, information inheritance by a\u000asuperclasssubclass mechanism, message passing semantics, extremely\u000alate binding no type declarations, and automatic storage\u000amanagement. Experience has shown that large complex systems can be\u000awritten in Smalltalk in quite a short period of time; it is also\u000aused to teach programming to children quite effectively.\u000aObjectoriented languages like Smalltalk have begun to be accepted\u000aas friendly languages for novice programmers on personal\u000acomputers.\u000a\u000aHowever, Smalltalk has some drawbacks, too. Smalltalk programs\u000aare inefficient compared with Lisp or Pascal Late binding is a\u000amajor reason of this inefficiency; every time a procedure is\u000acalled, its implementation in the current context has to be\u000afound.\u000a\u000aBecause of late binding, whether there is an implementation of a\u000aprocedure call or not can only be found at runtime. This may be\u000aconvenient in the early stages of system development; one can run a\u000apartially completed system, and when he discovers a runtime error\u000acaused by an unimplemented procedure, he can write the procedure\u000abody and proceed the computation from the point where the error was\u000adiscovered. However, there is no way to guarantee that there will\u000abe no runtime errors. We found many "completed" systems which\u000astill had such runtime errors.\u000a\u000aAnother problem is that it is hard for a novice to read\u000aSmalltalk programs written by other people. The fact that there are\u000ano type declarations and the fact that the bindings are late are\u000amajor causes of unreadability. All the Smalltalk procedures are so\u000acalled generic procedures. Each procedure name is associated with\u000aseveral procedure bodies declared in different classes. Depending\u000aon the classes of the arguments of a procedure call different\u000aprocedure bodies are invoked. Since the classes of the arguments\u000amay differ according to the context, it is impossible to statically\u000apredict the behavior of the procedure calls.\u000a\u000aWe observed that both inefficiency and unreadability are\u000aattributed to late binding; however, early binding can be\u000aeffectively accomplished if we can tell the classes of the\u000aprocedure arguments at compile time. In the long run probably\u000aSmalltalk needs to have "type" declarations probably not rigid\u000adeclarations of Pascal but rather in the form of hints to compilers\u000aand programmers. Even without changing the language it would be\u000anice to have a tool that supplies "type" declarations to current\u000aSmalltalk or partially specified Smalltalk. This will also lead to\u000aefficient compilation.\u000a\u000aWe thus concluded that we need to introduce "types" to\u000aSmalltalk. The introduction of types is more promising in Smalltalk\u000athan in similarly declarationless language Lisp, since Smalltalk\u000ahas a rich userdefined abstract classes. Therefore, the most\u000astraightforward approach to introduce types is to associate types\u000aof variables to classes that variables denote and to associate\u000atypes of procedures to mappings from classes to classes. Since a\u000avariable may denote objects of different classes, we define the\u000atype of a variable to be a union of classes that the variable will\u000aever denote.\u000a\u000aThe aim of this research is not to implement compilers for\u000aSmalltalk with type declarations. We intend to design tools to\u000asupply type declarations to current Smalltalk programs. Complete\u000atype determination is neither possible nor desirable; people do\u000awrite Smalltalk programs that take advantage of late bindings. We\u000aare, therefore, interested in finding a relatively efficient method\u000athat can find types of expressions in a large number of cases.\u000a\u000aThe problem of statically assigning types to\u000atypedeclarationless programs is called typeinference problem. We\u000acan find a number of work on type inference [3, 4, 7, 9, 11, 15];\u000athese techniques are, however, either too restrictive or too\u000ainefficient for our purpose. The only technique implemented, proven\u000ato work for nontrivial cases, and used extensively was developed\u000aby Milner [7] to determine types for ML language of LCF. Even\u000athough ML language is much simpler than Smalltalk, the fact that\u000athere exists an efficient, versatile algorithm encouraged us to\u000ainvestigate whether we can extend the method.\u000a\u000aThe LCF type checker produces a set of equations from procedure\u000adeclarations and solves them by unification [12], to obtain the\u000atypes of the procedures; it can run in linear time due to a fast\u000aunification algorithm invented recently [10]. We extended Milner's\u000amethod so that we can treat unions of types; in our method,we\u000acreate a set of equations and inequalities and solve them by\u000aunification and a transitive closure algorithm. This technique is\u000ageneral and can be applied to other dataflow problems.\u000a\u000aThe advantage of Milner's method and our method is that it\u000areduces the problems to purely mathematical domain so that we can\u000aapply various formula manipulation algorithms, without considering\u000athe execution order or sideeffects. Another advantage is that\u000athese methods can handle functions with polymorphic types.\u000a\u000aIn section 2 we review earlier work on type inference. The brief\u000aintroduction of the syntax and the semantics of Smalltalk is done\u000ain section 3. Then we introduce the "types" into Smalltalk in\u000asection 4. We discuss the first part of our algorithm, how to\u000aextend LCF type checking algorithm for liberal unions of types, in\u000asection 5. Then in section 6 the whole algorithm is presented.\u000aSection 7 is concerned with the implementation and experience.\u000a\u000aSmalltalk has four major different versions of the language and\u000aimplementations. The version we used for our experiments is\u000aSmalltalk76.
p2777
aVOptimization of programs that may contain exception handling facilities requires new techniques. Program optimizations that do not account for exception handling facilities may incorrectly transform a procedure that can raise an exception, producing different results from the unoptimized version. Restricted exception handling mechanisms that allow optimization are discussed. Data flow equations for determining the effects of exception handling are presented.
p2778
aVDependence graphs can be used as a vehicle for formulating and implementing compiler optimizations. This paper defines such graphs and discusses two kinds of transformations. The first are simple rewriting transformations that remove dependence arcs. The second are abstraction transformations that deal more globally with a dependence graph. These transformations have been implemented and applied to several different types of highspeed architectures.
p2779
aVWe investigate the specialization of programs by means of program transformation techniques. There are two goals of this investigation: the construction of program synthesis tools, and a better understanding of the development of algorithms.By extending an ordinary language of recursion equations to include a generalized procedure construct (the expression procedure), our ability to manipulate programs in that language is greatly enhanced. The expression procedure provides a means of expressing information not just about the properties of individual program elements, but also about the way they relate to each other.A set of four operations for transforming programs in this extended language is presented. These operations, unlike the transformation rules of Burstall and Darlington and of Manna and Waldinger, preserve the strong equivalence of programs.This set of operations forms the basis of a generalpurpose specialization technique for recursive programs. The practical value of this technique has been demonstrated in the systematic development of many programs, including several contextfree parsing algorithms. We outline here the development of Earley's algorithm by specialization.This paper is an informal exposition of part of the results of the author's Ph.D. thesis [Scherlis80].
p2780
aVThis paper examines a number of programming primitives in query languages for relational databases. The basic framework is a language based on relational algebra, whose variables take relations as values. The primitives considered are (i) looping, (ii) counters, (iii) generic (or unranked) variables, (iv) equality, (v) bounded looping (which corresponds to counting the number of tuples in a relation), and (vi) isomorphism class (which corresponds to knowing the isomorphism class of the data base). A comparison diagram is given relating all combinations of these six primitives, and several of the resulting classes of queries are characterized by their complexity or algebraic properties. It is shown, for example, that equality cannot be simulated using all the other primitives, that generic variables (with loops) cannot be simulated with only ranked variables and all the other primitives, and that with bounded loops one can determine the isomorphism class of the database when generic variables are allowed, but not otherwise.
p2781
aVThis paper examines "language processing" approach to paging where the of the programming language compiler or interpreter is responsible for generating the necessary control code for the page management of a program. We explore this idea for APL and describe an approach to incorporating in a program the necessary paging functions. The semantics of APL computation are examined to observe how paging operations can be incorporated into the computation. We discuss a model of data access in APL that exhibits storage use for both scalar and page references. A data structure that encodes the logical use of data from an array is introduced. We find that ordering computations efficiently and computing paging needs can be determined by simple transformations on this structure. This analysis leads us to an efficient method for paging an APL computation. Our approach builds on previous studies for efficiently executing APL.
p2782
aVThis paper describes the formal specifications of garbage collection in the programming language Cedar Mesa. They were developed as part of the process of identifying a safe subset of Mesa for which garbage collection was possible. The purpose of the specifications was to provide a precise definition of safety, along with criteria for checking the safety of proposed language features. Thus the specifications had to characterize the "invisibility" of the collector, as well as describe the services it provides. A beneficial effect of the specification effort was that the process of constructing the specifications led to a number of discoveries that improved the quality of the language.
p2783
aVThe asynchronous execution behavior of several concurrent processes, which may use randomization, is studied. Viewing each process as a discrete Markov chain over the set of common execution states, we give necessary and sufficient conditions for the processes to converge almost surely to a given set of goal states, under any fair, but otherwise arbitrary schedule, provided that the state space is finite. (These conditions can be checked mechanically.) An interesting feature of the proof method is that it depends only on the topology of the transitions and not on the actual values of the probabilities. We also show that in our model synchronization protocols that use randomization are in certain cases no more powerful than deterministic protocols. This is demonstrated by (a) Proving lower bounds on the size of a shared variable necessary to ensure mutual exlusion and lockoutfree behavior of the protocol; and (b) Showing that no fully symmetric 'randomized' protocol can ensure mutual exclusion and freedom from lockout.
p2784
aVExperience writing a production compiler based on an attribute grammar is related. The compiler is Intel Corporation's Pascal86 compiler which runs on a microcomputerbased system. An attribute grammar was written describing semantic analysis, storage allocation and translation to intermediate code. Attribute evaluation is done in two alternating passes [J] and the program tree is kept in intermediate files on disk. Various techniques for optimizing the evaluator were tried. Their success is reported and compared with other ideas from the literature.
p2785
aVThe instructionset of a target architecture is represented as a set of attributegrammar productions. A code generator is obtained automatically for any compiler using attributed parsing techniques. A compiler built on this model can automatically perform most popular machinedependent optimizations, including peephole optimizations. The code generator is also easily retargetable to different machine architectures.
p2786
aVPipeline interlocks are used in a pipelined architecture to prevent the execution of a machine instruction before its operands are available. An alternative to this complex piece of hardware is to rearrange the instructions at compiletime to avoid pipeline interlocks. This problem, called code reorganization, is studied. The basic problem of reorganization of machine level instructions at compiletime is shown to be NPcomplete. A heuristic algorithm is proposed and its properties and effectiveness are explored. The impact of code reorganization techniques on the rest of a compiler system are discussed.
p2787
aVCompilers usually eliminate common subexpressions in intermediate code, not object code. This reduces machinedependence but misses the machinedependent common subexpressions introduced by the last phases of code expansion. This paper describes a machineindependent procedure for eliminating machinespecific common subexpressions. It also identifies dead variables, defines windows for a companion peephole optimizer, and forms the basis of a retargetable register allocator. Its techniques for handling machinespecific data should generalize to other optimizations as well.
p2788
aVAn experimental system for declaring and inferring type in Smalltalk is described. (In the current Smalltalk language, the programmer supplies no type declarations.) The system provides the benefits of type declaration in regard to compiletime checking and documentation, while still retaining Smalltalk's flexibility. A type hierarchy, which is integrated with the existing Smalltalk class hierarchy, allows one type to inherit the traits of another type. A type may also have parameters, which are in turn other types.
p2789
aVThe paper describes the design for a software environment which unifies the dialogue management support of a number of existing software tools, such as commandlanguage handlers, operatingsystem shells, transition diagram interpreters, and forms management systems. The unification is accomplished without undue complexity: the resulting design is clean and inspectable.A key notion in the design is the use of three orthogonal abstraction hierarchies, for interaction contexts, for interactive operations (roughly = commands), and for data types. These orthogonal hierarchies are used for obtaining multidimensional inheritance, i.e. mappings of several arguments, which may be stored in the system's data repository, and which inherit their values along the hierarchies represented in the arguments. Mappings with multidimensional inheritance are used in the invocation mechanism, i.e. for selecting the appropriate procedure to be executed in various situations. Currently used programming languages and software tools only use singledimensional inheritance.A working implementation called the CAROUSEL system, is available.
p2790
aVMaple is a statically typed language system, with very general generic facilities. It incorporates its own programming environment and provides parallel processing with a new method of process synchronization. The synchronization is the direct outgrowth of its data abstraction facility.
p2791
aVAttribute grammars permit the specification of static semantics in an applicative and modular fashion, and thus are a good basis for syntaxdirected editors. Such editors represent programs as attributed trees, which are modified by operations such as subtree pruning and grafting. After each modification, a subset of attributes, AFFECTED, requires new values. Membership in AFFECTED is not known a priori; this paper presents an algorithm that identifies attributes in AFFECTED and computes their new values. The algorithm is timeoptimal, its cost is proportional to the size of AFFECTED.
p2792
aVThis paper presents an overview of an integrated programming language and system designed to support the construction and maintenance of distributed programs: programs in which modules reside and execute at communicating, but geographically distinct, nodes. The language is intended to support a class of applications in which the manipulation and preservation of longlived, online, distributed data is important. The language addresses the writing of robust programs that survive hardware failures without loss of distributed information and that provide highly concurrent access to that information while preserving its consistency. Several new linguistic constructs are provided; among them are atomic actions, and modules called guardians that survive node failures.
p2793
aVAn automatic syntax error handling technique applicable to LR parsing is presented and analyzed. The technique includes a "phraselevel" error recovery strategy augmented with certain additional features such as "local correction". Attention has also been paid to diagnostic aspects, i.e. the automatic generation of error message texts. The technique has been implemented in the compiler writing system HLP (Helsinki Language Processor), and some promising experimental results have been obtained by testing the technique with erroneous studentwritten Algol and Pascal programs.
p2794
aVAn essential part of any interactive programming development system is an incremental parser capable of error recovery. This paper presents a general incremental parser for LR(1) grammars allowing several/any form of modifications in the input program. The parser is suplemented with error recovery routines using an error automaton. Errors can be corrected automatically by recovery routines, or by the user at his request.
p2795
aVWe describe a model of netconnected processes that amounts to a reformulation of a model derived by Brock and Ackerman from the KahnMcQueen model of processes as relations on streams of data. The reformulation leads directly to a straightforward definition of process composition. Our notion of processes and their composition constitutes a natural generalization of the notion of functions and their composition. We apply this definition of process composition to the development of an algebra of processes, which we propose as supplying a formal semantics for a language whose domain of discourse includes nets of interconnected processes. This in turn leads us to logics of such nets, about which we raise three open and fundamental problems: existence of a finite basis for the operations of the language, finite axiomatizability of the equational theory, and decidability of this theory. A natural generalization of the model deals with the time complexity of computations.
p2796
aVWe show how to analyze the denotational semantics for a programming language to obtain a compiler and a suitable target machine for the language. We do this by rewriting the equations using suitable combinators. The machine operates by simulating the reduction sequences for the combinator terms. The reduction sequences pass through certain standard forms, which become an architecture for the machine, and the combinators become machine instructions. Despite the abstract nature of its development, the machine greatly resembles a conventional one. The method is illustrated by a simple expression language with procedures and inputoutput.
p2797
aVAn operational semantics of the Prolog programming language is introduced. MetaIV is used to specify the semantics. One purpose of the work is to provide a specification of an implementation of a Prolog interpreter. Another one is an application of this specification to a formal description of program optimization techniques based on the principle of partial evaluation.Transformations which account for pruning, forward data structure propagation and opening (which also provides backward data structure propagation) are formally introduced and proved to preserve meaning of programs. The so defined transformations provide means to inference data structures in an applicative language. The theoretical investigation is then shortly related to research in rulebased systems and logic.An efficient wellintegrated partial evaluation system is available in Qlog   a Lisp programming environment for Prolog.
p2798
aVThis paper presents a simple programming logic LES, which is particularly well suited for reasoning about socalled expression languages, i.e. languages that incorporate imperative features into expressions rather than distinguishing between expressions and statements. An axiomatization of a simple programming language is presented using this formalism. It is shown that this axiomatization is relatively complete, roughly in the sense of [Coo 76].
p2799
aVWe apply an Extended Propositional Temporal Logic (EPTL) to the specification and synthesis of the synchronization part of communicating processes. To specify a process, we give an EPTL formula that describes its sequence of communications. The synthesis is done by constructing a model of the given specifications using a tableaulike satisfiability algorithm for the extended temporal logic. This model can then be interpreted as a program.
p2800
aVThis paper proves the correctness of a translation from HISEL, a relational database query language, to HI, a hierarchical query language.The four components of Morris' [7] program are established. Appropriate semantics for the two languages are defined. A translation function is defined and an Lattributed grammar capturing the translation function is exhibited. The transformation of database trees into database relations is specified.
p2801
aVThe notion of program correctness with respect to an interpretation is defined for a class of programming languages. Under this definition, if a program terminates with an incorrect output then it contains an incorrect procedure. Algorithms for detecting incorrect procedures are developed. These algorithms formalize what experienced programmers may know already.A logic program implementation of these algorithms is described. Its performance suggests that the algorithms can be the backbone of debugging aids that go far beyond what is offered by current programming environments.Applications of algorithmic debugging to automatic program construction are explored.
p2802
aVFor a wide class of programming languages P and expressive interpretations I, we show that there exist sound and relatively complete Hoarelike logics for both partial correctness and termination assertions. In fact, under mild assumptions on P and I, we show that the assertions true for P in I are uniformly decidable in the theory of I (Th(I)) iff the halting problem for P is decidable for finite interpretations. Moreover termination assertions are uniformly r.e. in Th(I) even if the halting problem for P is not decidable for finite interpretations. Since total correctness assertions coincide with termination assertions for deterministic programming languages, this last result unexpectedly suggests that the class of languages with good axiom systems for total correctness may be wider than for partial correctness.
p2803
aVVerifying concurrent systems can be difficult because of the complex interactions possible between system components. In this paper, we propose a technique to simplify the task: modular composition of sequential proofs. We model a parallel program as a set of modules that interact by procedure calls. The properties of each module are proved using a sequentialprogram verification technique. If the modules satisfy a set of constraints presented in this paper, we may compose the modules into a system and the properties of the modules into properties of the system. The constraints ensure that the specifications are robust for each module where they are defined or used, in the sense that they are unaffected by current actions of other modules. A specification can be guaranteed robust for module m by restricting it to local variables of m, or by using monotonic predicates, which once true remain true forever. Our technique can be used to prove safety and liveness properties of parallel programs the liveness properties are specified using temporal logic.
p2804
aVThe termination assertion p<S>q means that whenever the formula p is true, there is an execution of the possibly nondeterministic program S which terminates in a state in which q is true. Termination assertions are more tractable technically than the partial correctness assertions usually treated in the literature. Termination assertions are studied for a programming language which includes local variable declarations, calls to undeclared global procedures, and nondeterministic recursive procedures with callbyaddress and callbyvalue parameters. By allowing formulas p and q to place conditions on global procedures, we provide a method for reasoning about programs with calls to global procedures based on hypotheses about procedure inputoutput behavior. The set of firstorder termination assertions valid over all interpretations is completely axiomatizable without reference to the theory of any interpretation. Although uninterpreted assertions have limited expressive power, the set of valid termination assertions defines the semantics of recursive programs in the sense of Meyer and Halpern [10]. Thus the axiomatization constitutes an axiomatic definition of the semantics of recursive programs.
p2805
aVWith the (necessary) condition that atomic programs in PL be binary, we present an algorithm for the translation of a PL formula X into a PDL program \u03c4 (X) such that a finite path satisfies X iff it belongs to \u03c4 (X). This reduction has two immediate corollaries: 1) validity in this PL can be tested by testing validity of formulas in PDL; 2) all finitepath program properties expressible in this PL are expressible in PDL.The translation, however, seems to be of nonelementary time complexity. The significance of the result to the search for natural and powerful logics of programs is discussed.
p2806
aVThe query languages used in relational database systems are a special class of programming languages. The majority, based on firstorder logic, lend themselves to analysis using formal methods. First, we provide a definition of relational query languages and their expressive power. We prove some general results and show that only a proper subset of firstorder logic formulas may be used as a practical query language. We characterize this subset in both semantic and syntactic terms. We then analyze the expressive power of several real query languages, including languages based on the relational calculus, languages with set operators and aggregate functions, and procedural query languages.Since the partial ordering "is more expressive than" determines a lattice among relational query languages, the results of the paper may be viewed as determining some of the structure of this lattice. We conclude with some applications of the results to the optimization problem for query processing.
p2807
aVNetwork algorithms are usually stated from the viewpoint of the network nodes, but they can often be stated more clearly from the viewpoint of an active message, a process that intentionally moves from node to node. This paper gives some examples of this notion, and then discusses a means of implementing it. This implementation applied in both directions also demonstrates the logical equivalence of the two viewpoints.
p2808
aVWe present two extensions of Communicating Sequential Processes [HO78]: computed communication targets and unspecified communication targets, as well as corresponding extensions to the system of cooperating proofs [AFR80] for verifying distributed programs. These extensions are important for the natural expressibility of many distributed programs. Examples of the use of these extensions are discussed and verified.
p2809
aVThis paper concerns the fundamental problem of synchronizing communication between distributed processes whose speeds (steps per real time unit) vary dynamically. Communication must be established in matching pairs, which are mutually willing to communicate. We show how to implement a distributed local scheduler to find these pairs. The only means of synchronization are boolean "flag" variables, each of which can be written by only one process and read by at most one other process.No global bounds in the speeds of processes are assumed. Processes with speed zero are considered dead. However, when their speed is nonzero then they execute their programs correctly. Dead processes do not harm our algorithms' performance with respect to pairs of other running processes. When the rate of change of the ratio of speeds of neighbour processes (i.e., relative acceleration) is bounded, then any two of these processes will establish communication within a constant number of steps of the slowest process with high likelihood. Thus our implementation has the property of achieving relative real time response. We can use our techniques to solve other problems such as resource allocation and implementation of parallel languages such as CSP and ADA. Note that we do not have any probability assumptions about the system behavior, although our algorithms use the technique of probabilistic choice.
p2810
aVTwo components of a VLSI design environment being built at Princeton are described. The general theme of this effort is to make the design of VLSI circuits as similar to programming as possible. A conscious attempt is being made to apply experience in the design of large software systems to the creation of an appropriate environment for VLSI circuits. The two components described are a procedural language to specify circuit layouts and a switchlevel circuit simulator for layout produced with this language. They have been chosen for presentation because many issues in their design are very similar to the issues that arise in the design of programming languages and software environments.
p2811
aVA new approach to data flow analysis of procedural programs and programs with recursive data structures is described. The method depends on simulation of the interpreter for the subject programming language using a retrieval function to approximate a program's data structures.
p2812
aVWe describe five paradigm shifts in programming language design, some old and some relatively new, namely Effect to Entity, Serial to Parallel, Partition Types to Predicate Types, Computable to Definable, and Syntactic Consistency to Semantic Consistency. We argue for the adoption of each. We exhibit a programming language, Viron, that capitalizes on these shifts.
p2813
aVThe benefits of strong typing to disciplined programming, to\u000acompiletime error detection and to program verification are well\u000aknown. Strong typing is especially natural for functional\u000a(applicative) languages, in which function application is the\u000acentral construct, and type matching is therefore a principal\u000aprogram correctness check. In practice, however, assigning a type\u000ato each and every expression in a functional program can be\u000aprohibitively cumbersome. As expressions are compounded, the task\u000aof assigning a type to each expression and subexpression becomes\u000apractically impossible, even more so because the typeexpressions\u000athemselves grow longer. It becomes imperative therefore to design\u000afriendly programming environments that permit the user typefree\u000aprogramming, but that generate fully typed programs in which the\u000atypes of all expressions are inferred by the system from the\u000aprogram. For interactive functional programming environments of the\u000akind implemented for the Edinburgh functional programming language\u000aML, a typeinference system is an invaluable tool for online\u000aparsetime error detection and debugging.\u000a\u000aThe issue of type inference leads naturally to\u000atypeschemes. If all one has of a functional program is a\u000adefinition statement such as f(x)=3, then all one can\u000asay of the type of f is that it must be of the form\u000a\u2019int, i.e.   an instance of the type scheme\u000at  int, where t is a type variable. As more of\u000athe program appears, say x(true)=z, t may be\u000arestricted, to bools. We are therefore\u000ainterested in typeschemes, using type parameters\u000a(free type variables). All one needs then to obtain a rudimentary\u000aform of generic procedures is a device for type instantiation. This\u000afunction is fulfilled in ML by the construct let. Other methods are\u000adescribed in 4 below.\u000a\u000aDuring the process of inferring a type for a given program, one\u000awould like at each step to find a most general (socalled\u000aprincipal) type scheme for each subexpression, so that types\u000ato be assigned to variables and subexpressions in the sequel may be\u000aassumed to be instances of those assigned earlier. The notion of\u000aprincipal type was introduced by Curry and Feys [CF] for\u000aCombinatory Logic, and Hindley [Hin] showed that the principal type\u000aexists for any Combinatory Logic expression and that it can be\u000afound using J.A. Robinson's Unification algorithm [Rob].The seminal\u000apaper on typeinference for functional programs is Milner's [Mil],\u000awhere an algorithm W is defined for inferring a type for any ML\u000aprogram (without recursively defined types). More recently, Damas\u000aand Milner [DM] defined a simple formal calculus of type inference,\u000awith respect to which W is proved complete.\u000a\u000aIn this paper we study several aspects of type inference for\u000apolymorphic type disciplines. First, we deal with type inference\u000afor parametric type systems, that is   type systems that include\u000asimple types defied over constant types and type variables. We\u000arecast Milner's algorithm W in a general "algebraic" form, which\u000aapplies to a variety of generic type disciplines (2). In\u000a3 we present an alternative algorithm V for type inference,\u000afundamentally different from W, which easily accommodates type\u000acoercion and overloading, allows efficient local updates to the\u000auser program, and is more efficient than W for implementations that\u000aallow concurrency.\u000a\u000aIn 4 we briefly describe four polymorphic disciplines:\u000atype abstraction, type quantification, type conjunction\u000a(intersection) and the ML construct let.\u000a\u000aIn 5 type inference for the abstraction and quantification\u000adisciplines is discussed. We point out that the two disciplines are\u000acombinatorially isomorphic, and we outline a type inference\u000aalgorithm for them.\u000a\u000a6 is devoted to type inference for the conjunctive\u000adiscipline. We mention some limitations to type inference here, in\u000aparticular   the fact that the set of typable expressions is not\u000aeffectively decidable. Moreover, there is no feasible algorithm\u000athat would type expressions even when these are given with a\u000aquantificational typing. This kind of noneffectiveness motivates\u000aour considering a restriction to types where type conjunction is\u000apermitted only at low levels of functionality. Here the level of\u000afunctionality of a function (or procedure) is recursively defined\u000aas one plus the greatest level of functionality of its arguments.\u000aThis gives rise to a hierarchy on polymorphic types, a notion that\u000ais equally natural for the type abstraction and type quantification\u000adisciplines. We show that our algorithm V (of 3) can be\u000anaturally extended to the restriction of the conjunctive discipline\u000ato types of rank 2 in the hierarchy, and we outline an argument\u000ashowing that already for a low rank, seemingly 3, type inference is\u000anot effectively decidable.\u000a\u000aIn 7 we discuss the ML construct let in light of our\u000aprevious results. We point out that the polymorphic discipline of\u000aML is isomorphic to the restriction to rank 2 of each one of the\u000aother disciplines. This gives weight to the particular choice of\u000apolymorphism in ML, and to the completeness of Milner's algorithm W\u000afor that choice. At the same time, the equivalence suggests more\u000aflexible representations of this discipline (namely   either the\u000aconjunctive or the quantificational disciplines restricted to rank\u000a2) in which the anomaly in ML, of legal expressions with\u000awellformed illegal subexpressions, is avoided.
p2814
aVA term rewriting system generator called REVE is described. REVE builds confluent and uniformly terminating term rewriting systems from sets of equations. Particular emphasis is placed on mechanization of termination proof. Indeed, REVE is one of the few such systems which can actually be called automatic because termination is fully integrated into the algorithms. REVE uses an incremental termination method based on recursive decomposition ordering which constructs the termination proof step by step from the presentation of the set of equations and which requires little knowledge of termination methods from the user. All examples from this paper are taken from abstract data type specifications.
p2815
aVThere are two important notions of data types being used in programming languages today. In the concept called abstract data types, types are algebras; the semantics of programs is described in terms of operations in these algebras. Another notion of type found in most of the "typed" programming languages in actual use regards types as sets of objects; the elements of types are the objects manipulated by the operators of a programming language. I shall be careful to use "abstract data type" when I mean the algebraic notion, and "data type" or simply "type" to mean the setbased concept.This paper presents a formal theory of types as an approximation to the exact semantics of a language, following the lead of Milner [Mil78]. The semantics is based upon multisorted algebras [ADJ73, 76, 77, 78] and is applied to a language consisting of equational specifications that define an abstract data type. The principal results of the paper are to characterize a class of typings as uniform approximations to an exact semantics, to extend Milner's theory of types to include types formed by coproduct construction, and to explicate typing as an (approximate) semantics for an equational theory.In developing a formal theory of abstract data types as multisorted algebras, the word "sort" is used to designate names used to distinguish various sets of objects that may constitute the domains of operators. Sorts are thus analogous to data types in programming languages. However, the sort signatures given to operators of abstract data types do not themselves constitute a very richly descriptive language. Sort names can be formed into sequences, corresponding to product construction in an object domain, but the theory (multisorted \u03a3algebras) does not seem to allow alternation sequences of sorts, corresponding to coproduct construction in an object domain. If this were possible, then sort signatures could actually fulfill the role of data types. We shall see that alternation sequences in type signatures arise naturally as a consequence of the semantics given to an equational theory.
p2816
aVWe give an efficient procedure for verifying that a finite state concurrent system meets a specification expressed in a (propositional) branchingtime temporal logic. Our algorithm has complexity linear in both the size of the specification and the size of the global transition graph for the concurrent system. We also show how the logic and our algorithm can be modified to handle fairness. We argue that this technique can provide a practical alternative to manual proof construction or use of a mechanical theorem prover for verifying many finite state concurrent systems.
p2817
aVTemporal logic ([PR57], [PR67]) provides a formalism for\u000adescribing the occurrence of events in time which is suitable for\u000areasoning about concurrent programs (cf. [PN77]). In defining\u000atemporal logic, there are two possible views regarding the\u000aunderlying nature of time. One is that time is linear: at each\u000amoment there is only one possible future. The other is that time\u000ahas a branching, treelike nature: at each moment, time may split\u000ainto alternate courses representing different possible futures.\u000aDepending upon which view is chosen, we classify (cf. [RU71]) a\u000asystem of temporal logic as either a linear time logic in which the\u000asemantics of the time structure is linear, or a system of branching\u000atime logic based on the semantics corresponding to a branching time\u000astructure. The modalities of a temporal logic system usually\u000areflect the semantics regarding the nature of time. Thus,in a logic\u000aof linear time, temporal operators are provided for describing\u000aevents along a single time path (cf. [GPSS80]). In contract, in a\u000alogic of branching time the operators reflect the branching nature\u000aof time by allowing quantification over possible futures cf.\u000a[AB80],[EC80]).\u000a\u000aSome controversy has arisen in the computer science community\u000aregarding the differences between and appropriateness of branching\u000aversus linear time temporal logic. In a landmark paper [LA80]\u000aintended to "clarify the logical foundations of the application of\u000atemporal logic to concurrent programs," Lamport addresses these\u000aissues. He defines a single language based on the temporal\u000aoperators "always" and "sometimes". Two distinct interpretations\u000afor the language are given. In the first interpretation formulae\u000amake assertions about paths, whereas in the second interpretation\u000athey make assertions about states. Lamport associates the former\u000awith linear time and the latter with branching time (although it\u000ashould be noted that in both cases the underlying time structures\u000aare branching). He then compares the expressive power of linear\u000atime and branching time logic. Based on his comparison and other\u000aarguments, he concludes that, while branching time logic is\u000asuitable for reasoning about nondeterministic programs, linear time\u000alogic is preferable for reasoning about concurrent programs.\u000a\u000aIn this paper, we reexamine Lamport's arguments and reach\u000asomewhat different conclusions. We first point out some technical\u000adifficulties with the formalism of [LA80]. For instance, the\u000adefinition of expressive equivalence leads to paradoxical\u000asituations where satisfiable formulae are classified as equivalent\u000ato false. Moreover, the proofs of the results comparing expressive\u000apower do not apply in the case of structures generated by a binary\u000arelation like those used in the logics of [FL79] and [BMP81]. We\u000agive a more refined basis for comparing expressive power that\u000aavoids these technical difficulties. It does turn out that\u000aexpressibility results corresponding to Lamport's still hold.\u000aHowever, it should be emphasized that these results apply only to\u000athe two particular systems that he defines. Sweeping conclusions\u000aregarding branching versus linear time logic in general are not\u000ajustified on this basis.\u000a\u000aWe will argue that there are several different aspects to the\u000aproblem of designing and reasoning about concurrent programs. While\u000athe specific modalities needed in a logic depend on the precise\u000anature of the purpose for which it is intended, we can make some\u000ageneral observations regarding the choice between a system of\u000abranching or linear time. We believe that linear time logics are\u000agenerally adequate for verifying the correctness of preexisting\u000aconcurrent programs. For verification purposes, we are typically\u000ainterested in properties that hold of all computation paths. It is\u000athus satisfactory to pick an arbitrary path and reason about it.\u000aHowever, there are applications where we need the ability to assert\u000athe existence of alternative computation paths as provided by a\u000abranching time logic. This arises from the nondeterminism  beyond\u000athat used to model concurrency  present in many concurrent\u000aprograms. In order to give a complete specification of such a\u000aprogram, we must ensure that there are viable computation path a\u000acorresponding to the nondeterministic choices the program might\u000amake. (An example is given in section 6.) Neither of Lamport's\u000asystems is entirely adequate for such applications.\u000a\u000aIn order to examine these issues more carefully, we define a\u000alanguage, CTL*, in which a universal or existential path quantifier\u000acan prefix an arbitrary linear time assertion. CTL* is an extension\u000aof the Computation Tree Logic, CTL, defined in [CE81] and studied\u000ain [EH82]. This language subsumes both of Lamport's interpretations\u000aand allows us to compare branching with linear time. Moreover, the\u000asyntax of CTL* makes it clear which interpretation is intended.\u000a\u000aThe paper is organized as follows: In section 2 we summarize\u000aLamport's approach and discuss its limitation. In section 3 we\u000apresent the syntax and semantics of CTL*. We also define some\u000anatural sublanguages of CTL* and compare their expressive power in\u000aSection 4. In particular, we show that (cf. Theorem 4.1) a language\u000asubstantially less expressive than CTL* still subsumes both of\u000aLamport's interpretations. Section 5 then shows how CTL* can be\u000aembedded in MPL [AB80] and PL [HKP80]. Finally, section 6 concludes\u000awith a comparison of the utility of branching and linear time\u000alogic.
p2818
aVAn abstract temporal proof system is presented whose programdependent part has a highlevel interface with the programming language actually studied. Given a new language, it is sufficient to deline the interface notions of atomic transitions, justice, and fairness in order to obtain a full temporal proof system for this language. This construction is particularly useful for the analysis of concurrent systems. We illustrate the construction on the sharedvariable model and on CSP. The generic proof system is shown to be relatively complete with respect to pure firstorder temporal logic.
p2819
aVThe semantic modeling of data types has been the subject of\u000aincreased interest over the last few years, enhanced by the\u000adevelopment of applicative languages such as Edinburgh's ML and\u000aHOPE, by the need for flexible highly structured languages that\u000awould nonetheless be amenable to verification, and by ongoing\u000ainquiries on polymorphism in programming languages. In particular,\u000athere has been a growing interest in generic type structures such\u000aas the ReynoldsGirard discipline of full polymorphism, further\u000aextended by McCracken [McC] and MacQueen and Sethi [MS], and in\u000auserdefined and recursivelydefined types.\u000a\u000aOur aim here is to model semantically the notion of type so as\u000ato encompass full polymorphism, but in a very controlled way which\u000aon the one hand allows the modeling to remain simple, and on the\u000aother hand conveys as much of the notion of type as seems feasibly\u000arelevant to programming languages.\u000a\u000aOur leading idea is that a type is a structural condition on\u000adata objects rather than a collection of objects which satisfies\u000acertain closure properties. Roughly, we argue that such structural\u000aconditions are fully conveyed by syntactic expressions, not in\u000aisolation of course, but within a syntaxoriented type\u000adiscipline. In other words, we treat types as discrete objects,\u000awhich merely code the ways data objects are allowed to interact (as\u000ain applying functional object a to object b). Once a\u000atype discipline is set to delineate a set of type expressions, and\u000aonce the meaning of the typeconstructs is imposed on the ways the\u000adata objects relate, the meaning of each type expression is\u000aconveyed by the expression itself (reduced to a canonical form if\u000anecessary). This conception of types permits a strikingly simple\u000amodeling of the use of types as arguments of data objects, a use\u000acentral to Reynolds' polymorphic type system [Rey].\u000a\u000aThe use of types as genuine arguments of procedures is not\u000adevoid of practical interest. E.g., one may wish to treat objects\u000aof type  generically, except for an initial choice depending\u000aon the principal typeconstructor of , that is, according to\u000awhether  is an atomic type, a functional type, a cartesian\u000aproduct, etc.\u000a\u000aA particular situation where such choices may be useful is where\u000aa procedure has a formal parameter intended to range over some data\u000astructure (trees of sequences of objects of type t, say), a\u000adata structure that has a number of accepted representations by\u000aformal data types. It seems desirable to permit the definition of a\u000apolymorphic procedure that would accept as valid a number of such\u000aformal representations, branching on them internally. This kind of\u000afacility would elevate the polymorphism of a procedure from the\u000alevel of uses with in a single program, to the level of\u000atransportability from one set of data structure specifications to\u000aanother.\u000a\u000aThis kind of use of types as arguments is compatible with\u000aReynold's discipline of type abstraction, but not with the\u000aquantificational discipline of [MS] (cf.[Lei83], 4.2). While\u000athe two disciplines, as uninterpreted and unexpanded calculi, are\u000acombinatorially isomorphic the difference between their intended\u000asemantics becomes apparent when primitive functions over types\u000a(such as discriminators over main typeconstructor) are\u000aconsidered.\u000a\u000aThe examples we have given of uses of types as arguments are\u000afairly restricted in nature: all that is used of a type is its\u000asyntactically representable structure. This is not merely an\u000aempirical observation on our limited perception at present time. In\u000aa typed discipline of programming each object carries its type\u000a(explicitly or not), and all types of denotable objects are\u000athemselves denotable. It follows that any question that one may ask\u000aabout the type of a denotable object reduces to a question about\u000atypeexpression(s) denoting that type. The issue of semantically\u000amodeling Reynold's rich discipline of type abstraction is therefore\u000areduced, from a pragmatic point of view, to a modeling in which\u000aquestions about types are all expressible as questions about type\u000aexpressions.\u000a\u000aExisting approaches to the semantic modeling of data types are,\u000ain one form or another, conceptual continuations of the\u000adenotational semantics approach to data objects. McCracken [McC],\u000afollowing [Sco], defines types as retracts of the universal domain.\u000aIn Shamir and Wadge [SW], Milner [Mil] and MacQueen and Sethi [MS]\u000atypes are modeled as some kind of ideal, where an ideal is a\u000asubset of the objectdomain that satisfies certain closure\u000aconditions. In both approaches types are being defined via the\u000aproperties that they must satisfy, as sets of objects, so as to\u000abecome compatible with their use. (This may be compared with the\u000aalgebraic approach to types, where each particular type is\u000adefined via its algebraic properties rather than its intended\u000aconception).\u000a\u000aOur modeling of types too is grafted on top of a denotational\u000asemantics for the object language, However,it has a life of its\u000aown, and can be explained in isolation, or combined with for\u000ainstance, an operational semantics for the object language. The\u000apoint we are trying to make is that the semantics of types has\u000alittle to do with the issues of selfapplication and continuity\u000awhich motivate denotational semantics. Rather than amalgamate\u000aobjects and types, as in [SW] (where types as sets of objects are\u000atreated on an equal footing with objects as sets of\u000aapproximations), we separate the two radically.\u000a\u000aWe make no claim, of course, that our approach should supplant\u000athe modeling of types as retracts or as ideals. Rather, we believe\u000athat the semantics we propose is related to typesasideals in much\u000athe same way as operational semantics is related to denotational\u000asemantics: it is closer to programming practice, permits a smaller\u000aset of objects, and is sensitive to the choice of formal framework\u000a(which for us is the type discipline). We therefore feel that the\u000atwo approaches should shed light on each other and, in combination,\u000aenhance our understanding of complex datatypes and and their\u000aproper use.\u000a\u000aIn particular, we believe that even putting down the definitions\u000aand proving basic existence theorems provides some insight about\u000athe design and use of data types. For one, our modeling of types\u000asuggests a feasible controlled use of types as arguments of data\u000aobjects. For another, it points to directions in setting very rich\u000atype disciplines evolving naturally from a structuralcomputational\u000aview of types. For instance, it may be useful and feasible to\u000aconsider types built as certain recursive sets of other types.\u000a\u000aFollowing preliminaries in sections 1 and 2 we define in section\u000a3 what is a model of the pure polymorphic lambda calculus.A term\u000amodel is constructed in 4,in 5 we show how to construct\u000aa nonextensional model through the solution of a domain equation,\u000aand in 6 we construct an extensional model through such a\u000asolution. The latter construction involves breaking the circularity\u000ain the conditions underlying polymorphic typing, by a method akin\u000ato Girard's in the proof theory of higher order logic [Gir].\u000a\u000aWe plan to extend this work in two directions. One is the\u000atreatment of more general type disciplines, of the kinds defined by\u000aMcCracken [McC] and further studied by MacQueen and Sethi [MS]. The\u000acrucial issue here is that, in contrast to Reynold's discipline,\u000adistinct type expressions may denote the same type. However, each\u000atype expression has a normal form, which may be viewed as its\u000a"value" (this view has been advocated in greater generality by Per\u000aMartinLof [Mar], on somewhat more philosophical grounds). A second\u000adirection, incorporating aspects of the first, is the treatment of\u000arecursively defined data types in a polymorphic context, and\u000apossibly more general notions of types.
p2820
aVIn this paper we present ACINCF and ACINCB, incremental update algorithms for forward and backward data flow problems, which are based on a linear equations model of Allen/Cocke interval analysis [Allen 77, Ryder 82a]. We have studied their performance on a robust structured programming language L. Given a set of localized program changes in a program in L, we can identify a priori the nodes in its flow graph whose corresponding data flow equations will be affected by the changes. We can characterize these affected nodes by their corresponding program structures and their relation to the original change sites.
p2821
aVProgram analysis methods, especially those which support automatic vectorization, are based on the concept of interstatement dependence where a dependence holds between two statements when one of the statements computes values needed by the other. Powerful program transformation systems that convert sequential programs to a form more suitable for vector or parallel machines have been developed using this concept [AllK 82, KKLW 80].The dependence analysis in these systems is based on data dependence. In the presence of complex control flow, data dependence is not sufficient to transform programs because of the introduction of control dependences. A control dependence exists between two statements when the execution of one statement can prevent the execution of the other. Control dependences do not fit conveniently into dependencebased program translators.One solution is to convert all control dependences to data dependences by eliminating goto statements and introducing logical variables to control the execution of statements in the program. In this scheme, action statements are converted to IF statements. The variables in the conditional expression of an IF statement can be viewed as inputs to the statement being controlled. The result is that control dependences between statements become explicit data dependences expressed through the definitions and uses of the controlling logical variables.This paper presents a method for systematically converting control dependences to data dependences in this fashion. The algorithms presented here have been implemented in PFC, an experimental vectorizer written at Rice University.
p2822
aVIn our paper [Wand 82a], we introduced a paradigm for compilation based on combinators. A program from a source language is translated (via a semantic definition) to trees of combinators; the tree is simplified (via associative and distributive laws) to a linear, assemblylanguagelike format: the "compiler writer's virtual machine" operates by simulating a reduction sequence of the simplified tree. The correctness of these transformations follows from general results about the \u03bbcalculus. The code produced by such n generator is always treelike. In this paper, the method is extended to produce target code with explicit loops. This is done by reintroducing variables into the terms of the target language in a restricted way, along with a structured binding operator.
p2823
aVIt is widely known that programming, even at a simple level, is a difficult activity to learn. Why is this so? Are novice difficulties really inherent in programming or are they related, to the nature of the programming tools currently given to novices? To answer this question we have used novice Pascal computer programs collected from their terminal sessions, controlled clinical studies focusing on specific aspects of novice programming techniques, and videotaped interviews of novice programming [Bonar, 1982]. In each we focused on bugs and buggy programs. Bugs and errors illuminate what a novice is actually thinking   providing us a window on the difficulties as they are experienced by the novice.In previous reports we presented evidence that current programming languages do not accurately reflect the cognitive strategies used by novice programmers [Soloway et al, 1981]. Instead, we have found that novice programmers possess knowledge about and experience with stepbystep specifications in natural language. This knowledge and experience gives them powerful intuitions for using a programming language. Programming languages, however, are, not designed to appeal to these intuitions. On a semantic and pragmatic level, there are incompatibilities between the way, natural and programming languages are used. Many novice programming bugs can be directly traced to an inappropriate use of natural language specification style or strategy.As an example of these incompatibility bugs consider the "while demon" bug. Novices with this bug assume that the actions in the body of the while loop are continuously monitored for the exit condition to become true. This interpretation is consistent with the English language usage of the word while: e.g."while the highway is two lanes, continue, north". In an earlier written study [Soloway et al, 1981] found that 34% of the students in an introductory programming course thought the test in a Pascal while loop was performed, as a demon. Furthermore, a later interview study showed that novices could even describe the implementation mechanism for such a demon:"\u2026 everytime I [the variable tested in the while condition] is assigned a new value, the machine needs to check that value \u2026"In this report we describe our use of videotaped interview studies for understanding how novices use a programming system. Interviews provide valuable information not available through statistical analysis of written studies. Written studies allow us to manipulate specific factors and guage the results to performance or style. Furthermore, we can have statistical confidence in those results. An interview study, on the other hand, allows us to examine the source of performance or style differences uncovered. Interviews give us an "execution trace" while written studies give us the final output. We use both kinds of studies, depending on the kind of information needed.We have interviewed seventeen novice programmers. Four were interviewed regularly for the first 810 weeks of their introductory programming course. For these regular subjects we have about 15 hours of interviews tracing their learning and maturation through the course.In this report we show an example natural language specification and discuss strategies used in that specification. We then discuss two examples of novice programming difficulties stemming from an inappropriate use of natural language specification strategies. These examples are illustrated with video tape transcripts. We conclude with a brief discussion of the implications of this work.
p2824
aVWe present algorithms that convert a class of parallel programs, called loop programs, from datadriven mode to synchronous mode. Such algorithms enable programmers to use a highlevel, datadriven programming language without forfeiting the efficiency of a synchronous machine. We characterize loop programs for which conversion is possible in terms of sets of balancing equations and we present two conversion algorithms.
p2825
aVThis paper shows how to rapidly determine the path relationships between k different elements of a graph (of the type primarily resulting from programs) in time proportional to k log k. Given the path relations between elements u,v, and w, it is easy to answer questions like "is there a path from u to w?" and "is there a path from u to w which does not go through v?" (The elements can be either nodes or edges.)This algorithm can be used in a wide variety of contexts. For example, in order to prove that whenever control reaches a point p, the last assignment of a value to a variable, v, has always been of the form v := c, where c is a certain constant, it is only necessary to know the path relations between the point p and all assignments to that variable.Ordinarily one is interested in the possible points, where a variable was assigned its current value (definition points), and at the points at which that value is used. Previously, all flow analysis algorithms would compute the definition points of all variables at all nodes in the graph, despite the fact that any given node may use only one of those variables.This algorithm may also be a generally useful graph algorithm. It can compute transitive closure more rapidly than the standard algorithm using the current best matrix multiplication algorithm on the kinds of graphs resulting from programs. The algorithm proceeds by preprocessing the graph, creating tables that can be used later to rapidly determine flow relationships between any sections of the program. In addition, small changes to the graph need only result in small changes to the tables. The algorithm is therefore suitable for incremental analysis.
p2826
aVA new program representation is presented which permits certain optimizations to be performed at less expense than with other forms. This paper describes code motion, common subexpression elimination and induction variable detection. Scalar propagation and constant folding are sketched here, but detailed elsewhere. The powerful code motion strategy allows entire regions of the program to be moved. The representation described may be used as a compiler intermediate form or simply as a model for program analysis. It has great potential for use in translation for parallel machines.
p2827
aVAssembling a large system from its component elements is not a simple task. An adequate notation for specifying this task must reflect the system structure, accommodate many configurations of the system and many versions as it develops, and be a suitable input to the many tools that support software development. The language described here applies the ideas of \u03bbabstraction, hierarchical naming and typechecking to this problem. Some preliminary experience with its use is also given.
p2828
aVAda is rich in the variety of its abstraction mechanisms. It has both a data abstraction mechanism (packages with private data types) that supports a functional programming style and a program abstraction mechanism (generic program units) that supports an objectoriented program style. Tradeoffs between data and program abstraction are examined and it is pointed out that Ada discourages program abstraction because program units are not firstclass objects. It is shown how program units could be made into firstclass objects by introducing closures as values for functions and records with function components as values for packages. Further unification by allowing types to be firstclass objects conflicts with the requirement of compiletime type invariance. The relaxation of this requirement in a manner that preserves type consistency is examined and leads to a notion of value for types as tuples of operations. It is suggested in the conclusion that our understanding of abstraction for objectoriented languages and of other language design, implementation, and environment issues will have progressed sufficiently by 1985 to warrant the design of a successor to Ada by the late 1980s.
p2829
aVThe programming language B has been designed for personal computing. In B, variables need not be declared, nor formal parameters specified. Nevertheless, B is strongly typed. All type requirements can be checked statically. To signal type violations on the spot during editing, the computations can be organized so that local the source text require a modest amount of recomputation.
p2830
aVThis paper discusses features of a secure systems programming language designed and implemented at IBM's Watson Research Lab. Two features of the language design were instrumental in permitting security to be enforced with minimum runtime cost: (1) Language constructs (e.g. pointer variables) which could result in aliasing were removed from the programmer's direct control and replaced by higher level primitive types; and (2) traditional strong type checking was enhanced with typestate checking, a new mechanism in which the compiler guarantees that for all execution paths, the sequence of operations on each variable obeys a finite state grammar associated with that variable's type. Examples are given to illustrate the application of these mechanisms.
p2831
aVThe BETA programming language is developed as part of the BETA\u000aproject. The purpose of this project is to develop concepts,\u000aconstructs and tools in the field of programming and programming\u000alanguages. BETA has been developed from 1975 on and the various\u000astages of the language are documented in [BETA a].\u000a\u000aThe application area of BETA is programming of embedded as well\u000aas distributed computing systems. For this reason a major goal has\u000abeen to develop constructs that may be efficiently implemented.\u000aFurthermore the BETA language is intended to have a few number of\u000abasic but general constructs. It is then necessary that the\u000aabstraction mechanisms are powerful in order to define more\u000aspecialized constructs.\u000a\u000aBETA is an object oriented language like SIMULA 67\u000a([SIMULA]) and SMALLTALK ([SMALLTALK]). By this is meant that a\u000aconstruct like the SIMULA class/subclass mechanism is fundamental\u000ain BETA. In contrast to SMALLTALK, BETA is a language in the ALGOL\u000a60 ([ALGOL]) family.\u000a\u000aSIMULA 67 is a system description and a programming language.\u000aThe DELTA language ([DELTA]) is a system description language only,\u000aallowing description of full concurrency, continuous change and\u000acomponent interaction, developed from a SIMULA conceptual platform.\u000aBETA started from the system concepts of DELTA, but is a\u000aprogramming language, drawing upon a large number of contributions\u000ato programming research in the 1970s. A basic idea in BETA is to\u000abuild the language upon one, general abstraction mechanism   the\u000apattern ([BETA a 77])   covering both data, procedural and\u000acontrol abstractions, substituting constructs like class,\u000aprocedure, function and type.\u000a\u000aCorrespondingly objects, procedure activation records and\u000avariables are all regarded as special cases of the basic building\u000ablock of program executions: the entity. A pattern thus\u000adescribes a category of entities with identical structure.\u000aAn entity consists of a set of attributes and an\u000aactionpart. An attribute may be a dataitem or a\u000apattern. The actionpart is a sequence of imperatives that may be\u000aexecuted.\u000a\u000aA dataitem may be an entity or a reference to an entity.\u000aA pattern may be used in a procedure like manner in the sense that\u000aan entity (procedure activation record) described by the pattern\u000amay be generated and executed as a part of the action sequence of\u000aanother entity. A pattern may be used to generate entities that\u000aexecute their actionpart in concurrency with other entities. Such\u000aentities may also execute their actions interleaved in a coroutine\u000alike manner.\u000a\u000aEntities may be organized hierarcically by means of a\u000ageneralization of the SIMULA subclass mechanism. This gives\u000apossibilities for grouping common properties of entities of\u000adifferent patterns.\u000a\u000aIn SIMULA 67 a class may have virtual attributes\u000a(procedures, labels, and switches). This is a powerful parameter\u000amechanism that gives the possibility to delay the specification of\u000aan attribute to a subclass specification. However, SIMULA 67 lacks\u000athe possibility to have virtual class attributes. Furthermore it is\u000anecessary to have a runtime check on the parameters of virtual\u000aprocedures, since it is not possible to specify the parameter list\u000aof a virtual procedure. The virtual patterns of BETA is a\u000ageneralization of the virtual concept in SIMULA 67.\u000a\u000aIn this paper the sequential part of BETA will be presented. The\u000amain purpose is to demonstrate the use of the pattern/subpattern\u000amechanism with virtual patterns as a powerful abstraction\u000amechanism. In addition, a further generalization of the virtual\u000aconcept based on syntactic categories will be described.\u000a\u000aWork has been initiated to design and implement an integrated\u000aprogramming system for BETA. The approach to separate\u000acompilation of BETA modules is described in [BETA c].\u000a\u000aThis paper is organised as follows: Section 2 describes\u000aentities, patterns and imperatives. Section 3 describes the\u000asubpattern mechanism. Virtual patterns are described in section 4.\u000aSection 5 describes the generalization of the virtual concept. In\u000asection 6 the remaining elements of BETA not mentioned in the\u000aprevious sections are described. Finally the syntax of BETA is\u000agiven in the appendix. Each section with a brief introduction of\u000athe relevant language elements whereafter a number of examples are\u000agiven. Most of the examples are extended versions of Hoare's\u000aSmallIntSet [Hoare 72].\u000a\u000aThere is a distinction between the base language (called\u000abasic BETA) and standard BETA. Standard BETA is basic\u000aBETA extended with a number of commonly used constructs. These\u000aadditional constructs may all be regarded as patterns in basic\u000aBETA, but will often be given a special syntax. This paper will\u000amainly focus on basic BETA. Occasionally we shall use parts of a\u000astandard BETA, but this will be stated at the appropiate place.
p2832
aVCompiler writers facea bleak future soon everyone will be using Ada on the Nebula Architecture and there will be no further need for our services. Luckily, it appears to be possible to recycle compiler writers by encouraging them to build circuit design tools (Silicon Compilers). Many of the standard techniques used in compilers can be immediately applied in circuit design, especially for LSI circuits. In part, this paper attempts to serve as a tutorial, casting some of the problems of designing in nMOS with the MeadConway design rules into the vocabulary of the compiler writer. The paper also discusses experience with a Silicon compiler that has fabricated over two dozen different designs.
p2833
aVThe best known lineartime list marking algorithms also require a linear amount of workspace. Algorithms working in bounded workspace have been obtained only by allowing quadratic execution time or by restricting the list structures to trees. We improve on this here by deriving a new lineartime, bounded workspace marking algorithm that works for dags. The algorithm is derived using correctnesspreserving program transformations, which prove the correctness of the algorithm. Our derivation of the marking algorithm provides an example where this method has actually been used to derive a new, more efficient algorithm, rather than just to establish the correctness of a previously known algorithm.
p2834
aVA method is presented that permits assertional reasoning about a concurrent program even though the atomicity of the elementary operations is left unspecified. It is based upon a generalization of the dynamic logic operator [\u03b1]. The method is illustrated by verifying the mutual exclusion property for a twoprocess version of the bakery algorithm.
p2835
aVThe paper introduces a reachability predicate for linear lists, develops the elementary axiomatic theory of the predicate, and illustrates its application to program verification with a formal proof of correctness for a short program that traverses and splices linear lists.
p2836
aVDespite the attractiveness of the concept, attempts to date to use proof of correctness techniques on production software have been generally unsuccessful. The obstacles encountered are not fundamental. We have implemented a proof of correctness system to be used for improving the realiability of certain small, realtime programs. It appears that many of the problems of past systems can be avoided.This work is supported by the Long Range Research Program of the Ford Motor Company, Dearborn, Michigan.
p2837
aVOne of the fundamental notions of programming, and thus of programming languages, is the variable. Recently, the variable has come under attack. The proponents of "functional programming" have argued that variables are at the root of all our problems in programming. They claim that we must rid our languages of all manifestations of the "vonNeumann bottleneck" and learn to live in the changeless world of functional combinators.While we may not, believe all the claims of the functional programming advocates, there is evidence that the treatment of variables in most programming languages leaves much to be desired. In this paper we discuss how to make variables "abstract", i.e., how to introduce the notion of variable in to a language so that variables have reasonable mathematical properties. This paper includes:  a discussion of language design principles that allow a more mathematical treatment of variables in programming language, and  a description of an equational logic for the programming language Russell [Boehm80]. Although this logic follows much of the development of abstract data types (cf. [Guttag78]), it is novel in its treatment of a language in which expressions not only produce values but also can have effects. We discuss the over all structure of the logic and present in detail the rules particularly relevant to the semantics of variables. A complete presentation of the logic appears in [Demers82], and the rule numbers in this paper agree with the numbers used there.In the next section, we discuss the underlying language principles needed to make an equational specification of variables possible. In the sections that follow, we present a logic that does so for Russell.
p2838
aVTen years ago Cheatham and Wegbreit [4] proposed a\u000atransformational program development methodology based on notions\u000aof topdown stepwise program refinement first expressed by Dijkstra\u000a[10] and Wirth [45]. A schema describing the process of this\u000amethodology is given in fig. 1. To develop a program by\u000atransformation, we first specify the program in as high a level of\u000aabstraction and as great a degree of clarity as our programming\u000alanguage admits. This high level problem statement program P is\u000aproved correct semimechanically according to some standard approach\u000a(see Flovd and Hoare [15, 21]), Next, using an interactive system\u000aequipped with a library of encoded transformations, each of which\u000amaps a correct program into another equivalent program, we select\u000aand apply transformations one at a time to successive versions of\u000athe program until we obtain a concrete, low level, effecient\u000aimplementation version P'. The goals of transformational\u000aprogramming are to reduce programming labor, improve program\u000areliability, and upgrade program performance. In order for labor to\u000abe reduced, the effort required to obtain P, prove it correct, and\u000aderive P' by transformation should be less than the effort required\u000ato code P from scratch, and also to debug it. Program reliability\u000awill be improved if P can be certified correct, and if each\u000atransformation preserves program meaning. Finally, program\u000aperformance will be upgraded if transformations are directed\u000atowards increased efficiency.\u000a\u000aExperimental transformational systems that emphasize one or more\u000aaspects of the methodology outlined above have been implemented by\u000aCheatham [5], Darlington [3], Loveman [27], Standish [41], Feather\u000a[14] Huet and Lang [11], and others. However, all of these systems\u000afall short of the goals, because of a number of reasons that\u000ainclude,\u000a\u000a1 inability to mechanize the checking of transformation\u000aapplicability conditions\u000a\u000a2 reliance on large, unmanageable collections of low level\u000atransformations, and long arduous derivation sequences\u000a\u000a3 dependency on transformations whose potential for improving\u000aprogram performance is unpredictable\u000a\u000a4 use of source languages insufficiently high level to\u000aaccommodate perspicuous initial program specifications and powerful\u000aalgorithmic transformations\u000a\u000aYet, convincing evidence that this new methodology will succeed\u000ahas come from recent advances in verification, program\u000atransformations, syntax directed editting systems, and high level\u000alanguages. These advances, discussed below, represent partial\u000asolution to the problems stated above, and could eventually be\u000aintegrated into a single system\u000a\u000a1 The transformational approach to verification was pioneered by\u000aGerhart [19] and strengthened by the results of Schwartz [39],\u000aScherlis [36], Broy et al [2], Koenig and Paige [26.31] Blaustein\u000a[1], and others. Due mainly to improved technology for the\u000amechanization of proofs of enabling conditions that justify\u000aapplication of transformations, this approach is now at a point\u000awhere it can be effectively used in a system. Such mechanization\u000adepends strongly on program analysis, and, in particular, on\u000areanalyses after a program is modified. Attribute grammars [24]\u000ahave been shown to be especially useful in facilitating program\u000aanalysis [23]. Moreover, Reps [34] has discovered algorithm that\u000areevaluates attributes in optimal time after a program undergoes\u000asyntax directed editing changes (as are allowed on the Cornell\u000aSynthesizer [43]). He has implemented his algorithm recently, and\u000ahas reported initial success\u000a\u000a2 There are encouraging indications that a transformational\u000asystem can be made to depend mainly on a small but powerful\u000acollection of transformations applied topdown fashion to programs\u000aspecified at various levels of abstraction from logic down to\u000aassembler. We envision such a system as a fairly conventional\u000asemiautomatic compiler which classes of transformations are\u000aselected semimechanically in a predetermined order, and are\u000ajustified by predicates supplied mechanically but proved\u000asemimanually. Of particular importance is nondeterminism removal\u000awhich has formulated by Sharir [40] could lead to a technique for\u000aturning naive, nondeterministic programs into deterministic\u000aprograms with emergent strategies. Such programs could then be\u000atransformed automatically by finite differencing [13, 16, 17, 18,\u000a29, 30, 31] and jamming [28, 31, 20] (which we have implemented)\u000ainto programs whose data access paths are fully determined. The\u000aSETL optimizer could improve these programs further by\u000aautomatically choosing efficient data structure representations and\u000aaggregations\u000a\u000a3 Of fundamental importance to the transformations just\u000amentioned is the fact that they can be associated with speedup\u000apredictions Fong and Ullman [16] were the first to characterize an\u000aimportant class of algorithmic differencing transformations in\u000aterms of accurate asymptotic speedup predictions, eg, they gave\u000aconditions under which repeated calculation of a set former {x in\u000as|k(x)} could be computed on O(#s) + cost(k) steps. By considering\u000astronger conditions and special cases for the boolean valued\u000asubpart k, Paige [31] later gave sharper speedup predictions (eg,\u000aeither O(1) steps for each encounter of the set former or a\u000acumulative cost of O(#s) steps for every encounter) associated with\u000aanother differencing method. Both Morgenstern [28] and Paige [31]\u000aprove constant factor improvements due to their jamming\u000atransformations (implemented by Morgenstern for the improvement of\u000afile processing, and by Paige for the optimization of programs).\u000aConstant factor speedup has also been observed for data structure\u000aselection by the method of basings but a supporting analytic study\u000ahas not been presented [8, 37]\u000a\u000a4 Essential to the whole transformational process is a wide\u000aspectrum programming language (or set of languages) that can\u000aexpress a program at every stage of development from the initial\u000aabstract specification down to its concrete implementation\u000arealization. Since transformations applied to programs written at\u000athe highest levels of abstraction are likely to make the most\u000afundamental algorithmic changes, it is important to stress abstract\u000afeatures in our language. In addition to supporting\u000atransformations, the highest level language dictions should support\u000alucid initial specifications, verification, and even program\u000aanalysts. Of special importance is SETL [38, 9], because its\u000aabstract set theoretic dictions can model data structures and\u000aalgorithms easily, because its philosophy of avoiding hidden a\u000asymptotic costs facilitates program analysis, because its semantics\u000aconforms to finite set theory and can accommodate a set theoretic\u000aprogram logic, and because it is wide spectrum. As is evidenced by\u000athe work of Schwartz, Fong, Paige, and Sharir, SETL is also a rich\u000amedium for transformation.
p2839
aVThis paper proposes an expressional loop notation (XLoop) based on the ideas described in [16,17] which makes it practical to express loops as compositions of functions. The primary benefit of XLoop is that it brings the powerful metaphor of expressions and decomposability to bear on the domain of loops. Wherever this metaphor can be applied, it makes algorithms much easier to construct, understand, and modify. XLoop applies the expressional metaphor to loops by introducing a new data type series. A series is an ordered one dimensional sequence of data objects. Series are used to represent intermediate results during a computation. Algorithms which would typically be rendered as iterative loops are instead represented as compositions of functions operating on series. For example, the program SUM_VECT computes the sum of the elements in a vector of integers by using ENUM_VECTOR to create a series of the integers in the vector and then using SUM to compute their sum.
p2840
aVConcurrent Prolog [28] combines the logic programming computation model with guardedcommand indeterminacy and dataflow synchronization. It will form the basis of the Kernel Language [21] of the Parallel Inference Machine [36], planned by Japan's Fifth Generation Computers Project. This paper explores the feasibility of programming such a machine solely in Concurrent Prolog (in the absence of a lowerlevel programming language), by implementing in it a representative collection of systems programming problems.
p2841
aVUp to this point direct implementations of axiomatic or equational specifications have been limited because the implementation mechanisms used are incapable of capturing the full semantics of the specifications. The programming language Unicorn was designed and implemented with the intention of exploring the full potential of programming with equations. Unicorn introduces a new language mechanism, called constrainingunification. When coupled with semantic unification, constrainingunification closely models the semantics of equational specifications thereby allowing for the implementation of a wider class of specifications. Unlike the language mechanisms of rewriterule and logic programming, constrainingunification is free of order dependencies. The same results are produced regardless of the order in which the axioms are stated. The use of viewpoints contributes to the flexibility of the Unicorn language. Preconditions for partial operations can be specified without added machinery.
p2842
aVThis paper summarizes a project, introduced in [HO79, HO82b], whose goal is the implementation of a useful interpreter for abstract equations that is absolutely faithful to the logical semantics of equations. The Interpreter was first distributed to Berkeley UNIX VAX sites in May, 1983. The main novelties of the interpreter are (1) strict adherence to semantics based on logical consequences; (2) \u201clazy\u201d (outermost)evaluation applied uniformly; (3) an implementation based on tabledriven pattern matching, with no runtime penalty for large sets of equations; (4) strict separation of syntactic and semantic processing, so that different syntaxes may be used for different problems.
p2843
aVTreat is a special purpose language for use in compiler writing. A Treat program translates a graph into ctrees (the intermediate language of the pcc compiler) and uses the back end of the C compiler to generate code. Treat has been developed specifically for use in an AdaTM compiler. A Treat program consists of applicative transformation rules. Treat's distinctive aspects include triggering transformations during pattern matching, a context mechanism for dynamic binding and assignment in a language with lazy evaluation.
p2844
aVInverse currying transformation of an attribute grammar moves a context condition to places in the grammar where the violation of the condition can be detected as soon as the semantic information used in the condition is computed. It thereby takes into account the evaluation order chosen for the attribute grammar. Inverse currying transformations can be used to enhance context sensitive parsing using predicates on attributes, to eliminate sources of backtrack when parsing according to ambiguous grammars, and to facilitate semanticssupported error correction.
p2845
aVA defining characteristic of \u201cfunctional\u201d specifications is the absence of assignments: updates of tables and data structures are expressed by giving the relationship between the new and old values. An obvious implementation allocates separate space for new and old values and may consume a lot of storage. However, even when updates of attributes like symbol tables are expressed functionally, we would like to avoid making copies of the symbol table during attribute evaluation. In other words, if possible, the implementation should have a single global copy of the table that is updated using assignments. Since the value of the global copy changes during computation, the order of evaluation has to be chosen carefully. In this paper, we partition attributes into classes, the problem being to determine if there exists an evaluation order that allows each class to share a global storage area. The solution extends to handle symbol tables for block structured languages. More precisely, consider a directed acyclic graph D in which vertices represent attribute values to be computed. Associated with each vertex is a label indicating the storage area to be shared by the vertex. Storage used during the evaluation of D is studied by playing a pebble game on D with the following steps: (1) pebble (i.e. place a pebble) on a source; (2) pebble a vertex if all its successors have pebbles (a pebble may be moved to the vertex from one of its successors); (3) pick up a pebble. Each vertex must be pebbled exactly once. The use of global storage is formalized by defining single pebblings, in which at most one of the vertices with a given label has a pebble at any time. The results also apply to a form of pebbling, called chain pebbling, that allows symbol tables for block structured languages to be studied.
p2846
aVIn this paper we present a semantics for Milnerstyle polymorphism in which types are sets. The basic picture is that our programs are actually terms in a typed &lgr;calculus, in which the type information can be safely deleted from the concrete syntax. In order to allow for common programming constructs, we allow reflexive or infinite types, and we also allow opaque types, which have private representations. An adaptation of Hindley's Principal Typing Theorem then asserts that the type information can be reconstructed. Thus expressions are polymorphic, since they may have more than one correct typing, but values are not. Expressions that are not welltyped are syntactically illformed, as they are in conventional mathematics, rather than having the meaning \u201cwrong\u201d. The resulting semantics is simpler than that for fully polymorphic models [Leivant 83], and generalizes the standard constructions, such as retracts and ideals.
p2847
aVA simple semantic model of automatic coercion is proposed. This model is used to explain four rules for inferring polymorphic types and providing automatic coercions between types. With the addition of a fifth rule, the rules become semantically complete but the set of types associated with an expression may be undecidable. An efficient type checking algorithm based on the first four rules is presented. The algorithm is guaranteed to find a type whenever a type can be deduced using the four inference rules. The type checking algorithm may be modified so that calls to type conversion functions are inserted at compile time.
p2848
aVAn editing by example system is an automatic program synthesis facility embedded in a text editor that can be used to solve repetitive text editing problems. The user provides the editor with a few examples of a text transformation. The system analyzes the examples and generalizes them into a program that can perform the transformation to the rest of the user's text. This paper presents the design, analysis, and implementation of a practical editing by example system. In particular, we study the problem of synthesizing a text processing program that generalizes the transformation implicitly described by a small number of input/output examples. We define a class of text processing programs called gap programs, characterize their computational power, study the problems associated with synthesizing them from examples, and derive an efficient heuristic that provably synthesizes a gap program from examples of its input/output behavior. We evaluate how well the gap program synthesis heuristic performs on the text encountered in practice. This evaluation inspires the development of several modifications to the gap program synthesis heuristic that act both to improve the quality of the hypotheses proposed by the system and to reduce the number of examples required to converge to a target program. The result is a gap program synthesis heuristic that can usually synthesize a target gap program from two or three input examples and a single output example. The editing by example system derived from this analysis has been embedded in a production text editor. The system is presented as a group of editor commands that use the standard interfaces of the editor to collect examples, show synthesized programs, and run them. By developing an editing by example system that solves a useful class of text processing problems, we demonstrate that program synthesis is feasible in the domain of text editing.
p2849
aVWe have developed a complete formal specification of the translation semantics of the Pascal Pcompiler. The specification is written as a semantic grammar (a variant of extended attribute grammars), and has been extensively tested and debugged with the aid of Lawrence Paulson's experimental semantics processor. The translation semantics models the operational aspects of compilation in detail. It is onepass, embodying static semantic checking, address assignment, code generation, and a certain amount of code improvement. Our paper describes the development history of the project, compares the new compiler with the existing Pcompiler, and discusses our positive experience with semantic grammars and Paulson's system.
p2850
aVA novel lazy evaluation mechanism, patterndriven lazy reduction, is developed that serves as a unifying evaluation mechanism for both functional and logic programs. The reduction of a function call can be viewed as \u201csemantically\u201d unifying the function call with the left hand side of a defining equation, and applying the unifier to the right hand side. Lazy reduction is achieved by the pattern which the function call matches against. Function reductions are actually \u201cdriven\u201d by patterns in this sense. It is shown that this evaluation mechanism works well for both functional programs and logic programs that involve \u201cexecutable\u201d functions. As a result, logic programs can be enhanced with (1) the availability of a functional computing environment where there is no notion of backtracking, thus alleviating the degree of control difficulties typically encountered in logic programs, and (2) the ability to terminate \u201cinfinite computations\u201d without the introduction of complex control issues at the userlevel. On the other hand, functional programs can be equipped with the power of logic programming languages, e.g., Prolog.
p2851
aVAn applicative program denotes a function mapping values from some domain to some range. Abstract interpretation of applicative programs involves using the standard denotation to describe an abstract function from a \u201csimplified\u201d domain to a \u201csimplified\u201d range, such that computation of the abstract function is effective and yields some information, such as type information, about the standard denotation. We develop a general framework for a restricted class of abstract interpretations that deal with nonstrict functions defined on nonflat domains. As a consequence, we can develop inference schemes for a large and useful class of functional programs, including functions defined on streams. We describe several practical problems and solve them using abstract interpretation. These include inferring minor signatures and relevant clauses of functions, which have arisen out of our work on a stronglytyped applicative language.
p2852
aVDenotational semantics for an ALGOLlike language with finitemode procedures, blocks with local storage, and sharing (aliasing) is given by translating programs into an appropriately typed &lgr;calculus. Procedures are entirely explained at a purely functional level  independent of the interpretation of program constructs  by continuous models for &lgr;calculus. However, the usual (cpo) models are not adequate to model local storage allocation for blocks because storage overflow presents an apparent discontinuity. New domains of store models are offered to solve this problem.
p2853
aVIn this paper a generalization of a certain Lipton's theorem (see Lipton [5]) is presented. Namely, we show that for a wide class of programming languages the following holds: the set of all partial correctness assertions true in an expressive interpretation I is uniformly decidable (in I) in the theory of I iff the halting problem is decidable for finite interpretations. In the effect we show that such limitations as effectiveness or Herbrand definability of interpretation (they are relevant in the previous proofs) can be removed in the case of partial correctness.
p2854
aVClarke has shown that it is impossible to obtain a relatively complete axiomatization of a blockstructured programming language if it has features such as static scope, recursive procedure calls with procedure parameters, and global variables, provided that we take firstorder logic as the underlying assertion language [Cl]. We show that if we take a more powerful assertion language, and hence a more powerful notion of expressiveness, such a complete axiomatization is possible. The crucial point is that we need to be able to express weakest preconditions of commands with free procedure parameters. The axioms presented here are natural and reflect the syntax of the programming language. Such an axiom system provides a tool for understanding how to reason about languages with powerful control features.
p2855
aVThe conventional storage allocation scheme for block structured languages requires the allocation of stack space and the building of a display with each procedure call. This paper describes a technique for analyzing the call graph of a program in a block structured language that makes it possible to eliminate these operations from many call sequences, even in the presence of recursion.
p2856
aVIncreasingly computer science research has been done using workstations with highresolution bitmap display systems. Smalltalk80\u2191 is a very attractive programming language for such computation environments, since it has very sophisticated graphical systems and programming environments. Unfortunately there are still very few computer systems on which Smalltalk80 can run with satisfactory speed, and furthermore they are quite expensive. In order to make Smalltalk80 accessible to a large group of people at low cost,. we have developed compiler techniques useful to generate efficient code for standard register machines such as MC68000. We have also extended Smalltalk80 to include type expressions, which allow compilers to generate efficient code
p2857
aVKnowledge of logical inference rules allows a specialized proof editor to provide a user with feedback about errors in a proof under development. Providing such feedback involves checking a collection of constraints on the strings of the proof language. Because attribute grammars allow such constraints to be expressed in a modular, declarative fashion, they are a suitable underlying formalism for a proofchecking editor. This paper discusses how an attribute grammar can be used in an editor for partialcorrectness program proofs in Hoarestyle logic, where verification conditions are proved using the sequent calculus.
p2858
aVWe present a generalization of the known fairness and equifairness notions, called @@@@fairness, in three versions: unconditional, weak and strong. For each such version, we introduce a proof rule for the @@@@fair termination induced by it, using wellfoundedness and countable ordinals. Each such rule is proved to be sound and semantically complete. We suggest directions for further research.
p2859
aVWe examine local area network protocols and verify the correctness of two representative algorithms using temporal logic. We introduce an interval temporal logic that allows us to make assertions of the form \u201cin the next k units, X holds.\u201d This logic encodes intuitive arguments about contention protocols quite directly. We present two proofs of an Ethernetlike contention protocol, one using the interval temporal logic and one using classical temporal logic. We also verify a contentionfree protocol using an invariant that seems to have wide applicability for such protocols.
p2860
aVWe give an algorithm to test the completeness of definitions holding on the rewrite systems that they generate. At the opposite of existing techniques that are very restrictive (lefthand sides of definitions must be linear) or rather inefficient our solution is both powerful and efficient. Also, the algorithm that we give detects ambigous or/and incomplete definitions and can tell you why they are ambigous or/and incomplete. It applies too to definitions in presence of equations.
p2861
aVWe show how a programming language designer may embed the type structure of a programming language in the more robust type structure of the typed lambda calculus. This is done by translating programs of the language into terms of the typed lambda calculus. Our translation, however, does not always yield a welltyped lambda term. Programs whose translations are not welltyped are considered meaningless, that is, illtyped. We give a conditionally typecorrect semantics for a simple language with continuation semantics. We provide a set of static typechecking rules for our source language, and prove that they are sound and complete: that is, a program passes the typing rules if and only if its translation is welltyped. This proves the correctness of our static semantics relative to the wellestablished typing rules of the typed lambdacalculus.
p2862
aVWe consider languages whose operational semantics is given by a set of rewrite rules. For such languages, it is important to be able to determine that there are enough rules to completely reduce all meaningful expressions, but not so many that the system of rules is inconsistent. We develop a formal framework in which to give a precise treatment of these soundness and completeness issues. We believe our approach to be novel in that we make heavy use of denotational semantics in our proof of completeness. The particular language for which we answer these questions is an extended version of the functional programming language FP; however the applicability of these techniques extends beyond the realm of FP rewriting systems.
p2863
aVA model and a sound and complete proof system for networks of processes in which component processes communicate exclusively through messages is given. The model, an extension of the trace model, can describe both synchronous and asynchronous networks. The proof system uses temporallogic assertions on sequences of observations \u2014 a generalization of traces. The use of observations (traces) makes the proof system simple, compositional and modular, since internal details can be hidden. The expressive power of temporal logic makes it possible to prove temporal properties (safety, liveness, precedence, etc.) in the system. The proof system is languageindependent and works for both synchronous and asynchronous networks.
p2864
aVAlthough optimizing compilers have successfully been used to reduce the size and running times of compiled programs, present incremental compilers only support the incremental update of unoptimized code. In this work, we extend the notion of incremental compilation to include optimized code. Techniques to incrementally compile locally optimized code, given intermediate code modifications are developed using a program representation based on flow graphs and dags. A model is designed to represent both unoptimized and optimized code and to maintain an optimizing history. Changes to the optimized code which either destroy optimizations or create conditions for further optimizations are incorporated into the model and the optimized code without recompiling unaffected optimizations.
p2865
aVThe PSEP System represents a novel approach to incremental compilation for block structured languages. PSEP implements a very fine grain, \u201cgreedy\u201d approach as a highly concurrent system of two processes: an editor and a code generator. The design allows the two processes to execute without locking their shared data objects, utilizing semantic information about the concurrent system to guarantee the consistency of the shared objects. This design is compared with the more common \u201cdemand\u201d approach to incremental compilation.
p2866
aVWe consider the problem of generating sequential code for programs written in a language which contains a Multiple GOTO operator, predicates and statements. This problem arises when compiling a parallel intermediate form (such as the PDG [3,4]) to run on a sequential machine; in a sourcetosource FORTRAN translator when vectorization of a loop has failed; and when compiling logic designs written in a parallel design language for simulation on a sequential machine. It is easy to generate sequential code for this sort of parallel program if one allows either duplication of code or the insertion of guard variables at merge points; in fact, it is in general impossible without this addition. However, for a large class of parallel programs (such as those originally arising from sequential programs, even after some optimizations have been applied) it is possible to generate sequential code without duplication or the addition of guard variables. In this paper we present an efficient algorithm which will generate sequential code from a parallel program without duplication or additional guard variables for a large class of parallel programs.
p2867
aVPath expressions were originally proposed by Campbell and Habermann [1] as a mechanism for process synchronization at the monitor level in software. Not unexpectedly, they also provide a useful notation for specifying the behavior of asynchronous circuits. Motivated by this potential application we investigate how to directly translate path expressions into hardware.\u000aOur implementation is complicated in the case of multiple path expressions by the need for synchronization on event names that are common to more than one path. Moreover, since events are inherently asynchronous in our model, all of our circuits must be selftimed.\u000aNevertheless, the circuits produced by our construction have area proportional to N log(N) where N is the total length of the multiple path expression under consideration. This bound holds regardless of the number of individual paths or the degree of synchronization between paths.
p2868
aVA constraint is a relation among program variables that is maintained throughout execution. Type declarations and a very general form of aliasing can be expressed as constraints. A proof system based upon the interpretation of Hoare triples as temporal logic formulas is given for reasoning about programs with constraints. The proof system is shown to be sound and relatively complete, and example program proofs are given.
p2869
aVEZ is a languagebased programming environment that offers the services provided separately by programming languages and operating systems in traditional environments. These services are provided as facilities of a highlevel string processing language with a 'persistent' memory in which values exist indefinitely or until changed. In EZ, strings and associative tables provide traditional file and directory services. This paper concentrates on the use of EZ procedures and their activations, which, like other values, have indefinite lifetimes. In EZ, the lowlevel aspects of procedure execution, such as activation record creation, references to local variables, and access to state information, are accessible via highlevel language constructs. As a result, traditionally distinct services can be provided by a single service in the EZ environment. Furthermore, such services can be written in EZ itself. An editor/debugger that illustrates the details of this approach is described.
p2870
aVConventional Milnerstyle polymorphic type checkers automatically infer types of functions and simple composite objects such as tuples. Types of recursive data structures (e.g. lists) have to be defined by the programmer through an abstract data type definition. In this paper, we show how abstract data types, involving type union and recursion, can be automatically inferred by a type checker. The language for describing such types is that of regular trees, a generalization of regular expressions to denote sets of tree structured terms. Inference of these types is reducible to the problem of solving simultaneous inclusion inequations over regular trees. We present algorithms to solve such inequations. Using these techniques, programs without any type definitions and type annotations for functions can be type checked.
p2871
aVMatchmaker, a language used to specify and automate the generation of interprocess communication interfaces, is presented. The process of and reasons for the evolution of Matchmaker are described. Performance and usage statistics are presented. Comparisons are made between Matchmaker and other related systems. Possible future directions are examined.
p2872
aVWith current compiler technology, changing a single line in a large software system may trigger massive recompilations. If the change occurs in a file with shared definitions, all compilation units depending upon that file must be recompiled to assure consistency. However, many of those recompilations may be redundant, because the change may actually affect only a small fraction of the overall system.\u000aThis paper presents an efficient method for significantly reducing the set of modules that must be recompiled after a change. The method is based on reference sets and the isolation of differences. The cost of determining whether recompilation is necessary is negligible compared to the cost of compilation. The method is easily added to existing compilers, and can be extended to provide guidance to programmers if the change requires software updates.
p2873
aVContinuations, when available as firstclass objects, provide a general control abstraction in programming languages. They liberate the programmer from specific control structures, increasing programming language extensibility. Such continuations may be extended by embedding them in functional objects. This technique is first used to restore a fluid environment when a continuation object is invoked. We then consider techniques for constraining the power of continuations in the interest of security and efficiency. Domain mechanisms, which create dynamic barriers for enclosing control, are implemented using fluids. Domains are then used to implement an unwindprotect facility in the presence of firstclass continuations. Finally, we demonstrate two mechanisms, windunwind and dynamicwind, that generalize unwindprotect.
p2874
aVThis paper considers current solutions to the problem of representing multiple environments, and uses the results to develop a new model. The motivation is partly a consequence of the renewed interest in the more sophisticated forms of access and control [Sussman & Steele 1978], [Smith 1983]. [Friedman et al. 1984], and partly because the problem identified by Moses [1970] has not, as yet, been satisfactorily resolved. The new model is derived from a consideration of the semantics of identifier binding interrogation. The implementation itself rests on the existence of an environment labeling function which solves a variant of a well known graph theory problem called nearest common ancestor. We describe a suitable implementation of such a function. The new scheme has been implemented in two different LISP systems (Cambridge LISP and Portable Standard LISP), and a third (LISP/VM) is under consideration. In addition, pure deep binding and full shallow binding have both been implemented on top the same base system (Cambridge LISP). Thus it is possible to collect comparisons of the relative efficiencies running simple (stack behavior) programs and complex (multiple context) programs. Some timing results for various tests are given in the final section.
p2875
aVLogic programming offers a variety of computational effects which go beyond those customarily found in functional programming languages. Among these effects is the notion of the \u201clogical variable.\u201d i.e. a value determined by the intersection of constraints, rather than by direct binding. We argue that this concept is \u201cseparable\u201d from logic programming, and can sensibly be incorporated into existing functional languages. Moreover, this extension appears to significantly widen the range of problems which can efficiently be addressed in function form, albeit at some loss of conceptual purity. In particular, a form of sideeffects arises under this extension, since a function invocation can exert constraints on variables shared with other function invocations. Nevertheless, we demonstrate that determinacy can be retained, even under parallel execution. The graph reduction language FGL is used for this demonstration, by being extended to a language FGL+LV permitting formal parameter expressions, with variables occurring therein bound by unification. The determinacy argument is based on a novel dataflowlike rendering of unification. In addition the complete partial order employed in this proof is unusual in its explicit representation of demand, a necessity given the \u201cbenign\u201d sideeffects that arise. An implementation technique is suggested, suitable for reduction architectures.
p2876
aVCompilers for languages with callbyreference formal parameters must deal with aliases arising from the renaming effects at call sites. This paper presents a set of techniques for analyzing aliasing patterns. The analysis is divided into detecting the introduction of aliases and tracking their propagation. The algorithm for introduction analysis is simple enough to be performed in a structured editor or parser. A data flow analysis framework is given for the propagation problem, making it possible to solve using standard algorithms from global data flow analysis. Several optimizations are shown which can shrink the size of the problem, and extensions are given to handle ALGOLstyle name scoping. Finally, this technique is compared to an alternative implementation strategy and an approximate technique.
p2877
aVWe discuss the problem of efficiently implementing aggregates (contiguous data structures) such as arrays in functional programming systems. Simple changes to an aggregate conceptually involve making a new copy of the aggregate differing only in the changed component, but such copying can be expensive. We present both static and dynamic techniques for avoiding this copying, and argue that they allow one to program functionally using aggregates, without loss of efficiency over conventional programs.1
p2878
aVWe propose a new machine model in which load operations can be performed in parallel with arithmetic operations by two separate functional units. For this model, the evaluation of expression trees is considered. An efficient algorithm to produce an optimal order of evaluation is described and analyzed. For a tree with n vertices the algorithm runs in time &Ogr;(n log2n). If the arithmetic operations have at most two arguments, the complexity goes down to &Ogr;(n logn).
p2879
aVTo accommodate polymorphic data types and operations, several computer scientists most notably MacQueen, Plotkin, and Sethi have proposed formalizing types as ideals. Although this approach is intuitively appealing, the resulting type system is both complex and restrictive because the type constructor that creates function types is not monotonic, and hence not computable. As a result, types cannot be treated as data values, precluding the formalization of type constructors and polymorphic program modules (where types are values) as higher order computable functions. Moreover, recursive definitions of new types do not necessarily have solutions.\u000aThis paper proposes a new formulation of types called intervals that subsumes the theory of types as ideals, yet avoids the pathologies caused by nonmonotonic type constructors. In particular, the set of interval types contains the set of ideal types as a proper subset and all of the primitive type operations on intervals are extensions of the corresponding operations on ideals. Nevertheless, all of the primitive interval type constructors including the function type constructor and type quantifiers are computable operations. Consequently, types are higher order data values that can be freely manipulated within programs.\u000aThe key idea underlying the formalization of types as intervals is that negative information should be included in the description of a type. Negative information identifies the finite elements that do not belong to a type, just as conventional, positive information identifies the elements that do. Unless the negative information in a type description is the exact complement of the positive information, the description is partial in the sense that it approximates many different types an interval of ideals between the positive information and the complement of the negative information. Although programmers typically deal with total (maximal) types, partial types appear to be an essential feature of a comprehensive polymorphic type system that accommodates types as data, just as partial functions are essential in any universal programming language.
p2880
aVAn attempt is made to apply ideas about algebraic specification in the context of a programming language. Standard ML with modules is extended by allowing axioms in module interface specifications and in place of code. The resulting specification language, called Extended ML, is given a semantics based on the primitive specificationbuilding operations of the kernel algebraic specification language ASL. Extended ML provides a framework for the formal development of programs from specifications by stepwise refinement, which is illustrated by means of a simple example. From its semantic basis Extended ML inherits complete independence from the logical system (institution) used to write specifications. This allows different styles of specification as well as different programming languages to be accommodated.
p2881
aVThe formal correspondence between an implementation and its specification is examined. It is shown that existing specifications that claim to describe priority are either vacuous or else too restrictive to be implemented in some reasonable situations. This is illustrated with a precisely formulated problem of specifying a firstcomefirstserved mutual exclusion algorithm, which it is claimed cannot be solved by existing methods.
p2882
aVWe present an algorithm for checking satisfiability of a linear time temporal logic formula over a finite state concurrent program. The running time of the algorithm is exponential in the size of the formula but linear in the size of the checked program. The algorithm yields also a formal proof in case the formula is valid over the program. The algorithm has four versions that check satisfiability by unrestricted, impartial, just and fair computations of the given program.
p2883
aVA major drawback to the use of attribute grammars in languagebased editors has been that attributes can only depend on neighboring attributes in a program's syntax tree. This paper concerns new attributegrammarbased methods that, for a suitable class of grammars, overcome this fundamental limitation. The techniques presented allow the updating algorithm to skip over arbitrarily large sections of the tree that more straightforward updating methods visit node by node. These techniques are then extended to deal with aggregate values, so that the attribute updating procedure need only follow dependencies due to a changed component of an aggregate value. Although our methods work only for a restricted class of attribute grammars, satisfying the necessary restrictions should not place an undue burden on the writer of the grammar.
p2884
aVAll optimizing compilers must deal with the problem of aliases arising due to the presence of multiple names that reference the same memory areas. Presented in this paper is a staged, highlevel alias analysis methodology that provides detailed alias information to a global optimizer implemented at any level in the compilation process. The framework provides easy portability of optimizing compilers to new architectures, as well as the easy addition, of new compilers to an already existing family of optimizing compilers. The method involves the application of a set of languagespecific alias rules to the source code in order to gather alias information. A languageindependent component then performs a transitive closure of this information and transforms it into a presentation more suitable for use by a global optimizer. Each stage of the methodology is detailed. Results are given for an implemented family of compilers targeted for a reduced instruction set computer.
p2885
aVHighquality local code generation is one of the most difficult tasks the compilerwriter faces. Even if register allocation decisions are postponed and common subexpressions are ignored, instruction selection on machines with complex addressing can be quite difficult. Efficient and general algorithms have been developed to do instruction selection, but these algorithms fail to always find optimal solutions. Instruction selection algorithms based on dynamic programming or complete enumeration always find optimal solutions, but seem to be too costly to be practical. This paper describes a new instruction selection algorithm, and its prototype implementation, based on bottomup tree patternmatching. This algorithm is both time and space efficient, and is capable of doing optimal instruction selection for the DEC VAX11 with its rich set of addressing modes.
p2886
aVA language <b>Crystal</b> and its compiler for parallel programming is presented. The goal of <b>Crystal</b> is to help programmers in seeking efficient parallel implementations of an algorithm, and managing the complexity that might arise in dealing with hundreds of thousands of autonomous parallel processes. In <b>Crystal,</b> a program consists of a system of recursion equations and is interpreted as a parallel system. <b>Crystal</b> views a large complex system as consisting of a hierarchy of parallel subsystems, built upon a set of <b>Crystal</b> programs by composition and abstraction. There is no mention of explicit communications in a <b>Crystal</b> program. The <b>Crystal</b> compiler automatically incorporates pipelining into programs, and generates a parallel program that is optimal with respect to an algorithm. Each optimizing compiler, targeted for a particular machine, determines the appropriate granular size of parallelism and attains a balance between computations and communications. Based on the language, a unified theory for understanding and generating any systolic design has been devised and it constitues a part of the compiler.
p2887
aVExplicit use of knowledge expressions in the design of distributed algorithms is explored. A nontrivial case study is carried through, illustrating the facilities that a design language could have for setting and deleting the knowledge that the processes possess about the global state and about the knowledge of other processes. No implicit capabilities for logical reasoning are assumed. A language basis is used that allows common knowledge not only by an eager protocol but also in the true sense. The observation is made that the distinction between these two kinds of common knowledge can be associated with the level of abstraction: true common knowledge of higher levels of abstraction: true common knowledge of higher levels can be implemented as eager common knowledge on lower levels. A knowledgemotivated abstraction tool is therefore suggested to be useful in supporting stepwise refinement of distributed algorithms.
p2888
aVModules in a distributed program are active, communicating entities. A language for distributed programs must choose a set of communication primitives and a structure for processes. This paper examines one possible choice: synchronous communication primitives (such as rendezvous or remote procedure call) in combination with modules that encompass a fixed number of processes (such as Ada tasks or UNIX processes). An analysis of the concurrency requirements of distributed programs suggests that this combination imposes complex and indirect solutions to common problems and thus is poorly suited for applications such as distributed programs in which concurrency is important. To provide adequate expressive power, a language for distributed programs should abandon either synchronous communication primitives or the static process structure.
p2889
aVThis paper describes our experience implementing CES, a distributed Collaborative Editing System written in Argus, a language that includes facilities for managing longlived distributed data. Argus provides <i>atomic actions,</i> which simplify the handling of concurrency and failures, and mechanisms for implementing <i>atomic data types,</i> which ensure serializability and recoverability of actions that use them. This paper focuses on the support for atomicity in Argus, especially the support for building new atomic types. Overall the mechanisms in Argus made it relatively easy to build CES; however, we encountered interesting problems in several areas. For example, much of the processing of an atomic action in Argus is handled automatically by the runtime system; several examples are presented that illustrate areas where more explicit control in the implementations of atomic types would be useful.
p2890
aVIn this paper we advance the radical notion that a computational model based on the <i>reals</i> provides a more abstract description of concurrent and reactive systems, than the conventional <i>integers</i> based behavioral model of execution <i>sequences.</i> The real model is studied in the setting of temporal logic, and we illustrate its advantages by providing a <i>fully abstract</i> temporal semantics for a simple concurrent language, and an example of verification of a concurrent program within the real temporal logic defined here. It is shown that, by imposing the crucial condition of <i>finite variability,</i> we achieve a balanced formalism that is insensitive to <i>finite</i> stuttering, but can recognize <i>infinite</i> stuttering, a distinction which is essential for obtaining a fully abstract semantics of nonterminating processes. Among other advantages, going into realbased semantics obviates the need for the controversial representation of concurrency by interleaving, and most of the associated fairness constraints.
p2891
aVWe show that the class of properties of programs expressible in propositional temporal logic can be substantially extended if we assume the programs to be <i>dataindependent.</i> Basically, a program is dataindependent if its behavior does not depend on the specific data it operates upon. Our results significantly extend the applicability of program verification and synthesis methods based on propositional temporal logic.
p2892
aVIn this paper the semantics of the programming language POOL is described. It is a language that integrates the objectoriented structure of languages like Smalltalk80 with facilities for concurrency and communication like the ones in Ada. The semantics is described in an operational way; it is based on transition systems. By using a way of representing parallel processes that is different from the traditional one, it is possible to overcome some difficulties pertaining to the latter. The resulting semantics shows a close resemblance with the informal language description and at the same time there are good prospects that it can serve as a secure guide for the implementation of the language. Also a variant is given in which more and maximal parallelism can be expressed in a very natural way.
p2893
aVThe paradigm of equational programming potentially possesses all the features provided by Prologlike languages. In addition, the ability to reason about equations, which is not provided by Prolog, can be accommodated by equational languages. In this paper, we propose an extended equational programming paradigm, and describe an <i>equational logic</i> programming language which is an extension of the equational language defined in [Hoff82]. Semantic foundations for the extension are discussed. The extended language is a powerful logic programming language in the sense of Prolog and thus enjoys the programming features that Prolog possesses. Furthermore, it provides an ability to solve equations, which captures the essential power of equational programming.
p2894
aVAttribute grammars require copy rules to transfer values between attribute instances distant in an attributed parse tree. We introduce copy bypass attribute propagation that dynamically replaces copy rules with nonlocal dependencies, resulting in faster incremental evaluation. A evaluation strategy is used that approximates a topological ordering of attribute instances. The result is an efficient incremental evaluator that allows multiple subtree replacement on any noncircular attribute grammar.
p2895
aVAn elaboration of the Prolog language is described in which the notion of firstorder term is replaced by a more general one. This extended form of terms allows the integration of inheritance an <i>ISA</i> taxonomy directly into the unification process rather than indirectly through the resolutionbased inference mechanism of Prolog. This results in more efficient computations and enhanced language expressiveness. The language thus obtained, called LOGIN, subsumes Prolog, in the sense that conventional Prolog programs are equally well executed by LOGIN.
p2896
aVLanguagespecific editors for typed programming languages must contain a subsystem for semantic analysis in order to guarantee correctness of programs with respect to the context conditions of the language. As programs are usually incomplete during development, the semantic analysis must be able to cope with missing context information, e. g. incomplete variable declarations or calls to procedures imported from still missing modules. In this paper we present an algorithm for incremental semantic analysis, which guarantees immediate detection of semantic errors even in arbitrary incomplete program fragments. The algorithm is generated from the language's context conditions, which are described by inference rules. During editing, these rules are evaluated using a unification algorithm for manysorted algebras with semilattice ordered subsorts and nonempty equational theories. The method has been implemented as part of the PSG system, which generates interactive programming environments from formal language definitions, and has been successfully used to generate an incremental semantic analysis for PASCAL and MODULA2.
p2897
aVA <i>distributed data structure</i> is a data structure that can be manipulated by many parallel processes simultaneously. Distributed data structures are the natural complement to parallel program structures, where a <i>parallel program</i> (for our purposes) is one that is made up of many simultaneously active, communicating processes. Distributed data structures are impossible in most parallel programming languages, but they are supported in the parallel language Linda and they are central to Linda programming style. We outline Linda, then discuss some distributed data structures that have arisen in Linda programming experiments to date. Our intent is neither to discuss the design of the Linda system nor the performance of Linda programs, though we do comment on both topics; we are concerned instead with a few of the simpler and more basic techniques made possible by a language model that, we argue, is subtly but fundamentally different in its implications from most others.This material is based upon work supported by the National Science Foundation under Grant No. MCS8303905. Jerry Leichter is supported by a Digital Equipment Corporation Graduate Engineering Education Program fellowship.
p2898
aVOne of the most important pragmatic advantages of functional languages is that concurrency in a program is <i>implicit</i>   there is no need for special constructs to express parallelism as is required in most conventional languages. Furthermore, it is fairly easy for compilers to automatically determine the concurrency as a step toward decomposing a program for execution on a suitable parallel architecture. Yet it is often the case that one knows precisely the <i>optimal decomposition</i> for execution on a particular machine, and one can never expect a compiler to determine such optimal mappings in all cases. This paper is concerned with ways to allow the programmer to <i>explicitly</i> express this mapping of program to machine, by using annotations that, given a few minor constraints, cannot alter the functional semantics of the program. We show through several detailed examples the expressiveness and conciseness of the resulting "parafunctional" programming methodology, using an experimental language called <i>ParAlfl</i> based on our ideas. We also give a formal denotational description of the mapping semantics using a notion of <i>execution trees.</i>This research was supported in part by NSF Grants DCR8403304 and DCR8451415, and a Faculty Development Award from IBM.
p2899
aVIt has been recognised that languages like Concurrent Prolog and Parlog which use committed choice nondeterminism have departed from the original concept of logic programming, but no new paradigm has been suggested. In this paper we propose that programs in such languages be viewed as rewrite rules that hierarchically decompose a process into a network of distributed processes. Logical variables and unification provide a powerful means of communication, and annotations can provide a synchronization mechanism that complements them.
p2900
aVOne purpose of type checking in programming languages is to guarantee a degree of "representation independence:" programs should not depend on the way stacks are represented, only on the behavior of stacks with respect to push and pop operations. In languages with abstract data type declarations, representation independence should hold for userdefined types as well as builtin types. We study the representation independence properties of a typed functional language (secondorder lambda calculus) with polymorphic functions and abstract data type declarations in which data type implementations (packages) may be passed as function parameters and returned as results. The type checking rules of the language guarantee that two data type implementations P and Q are equivalence whenever there is a correspondence between the behavior of the operations of P and the behavior of the operations of Q.
p2901
aVWriting any large program poses difficult problems of organization. In many modern programming languages these problems are addressed by special linguistic constructs, variously known as modules, packages, or clusters, which provide for partitioning programs into manageable components and for securely combining these components to form complete programs. Some general purpose components are able to take on a life of their own, being separately compiled and stored in libraries of generic, reusable program units. Usually modularity constructs also support some form of information hiding, such as "abstract data types." "Programming in the large" is concerned with using such constructs to impose structure on large programs, in contrast to "programming in the small", which deals with the detailed implementation of algorithms in terms of data structures and control constructs. Our goal here is to examine some of the proposed linguistic notions with respect to how they meet the pragmatic requirements of programming in the large.
p2902
aVA function has a <i>dependent type</i> when the type of its result depends upon the value of its argument. Dependent types originated in the type theory of intuitionistic mathematics and have reappeared independently in programming languages such as CLU, Pebble, and Russell. Some of these languages make the assumption that there exists a <i>typeofalltypes</i> which is its own type as well as the type of all other types. Girard proved that this approach is inconsistent from the perspective of intuitionistic logic. We apply Girard's techniques to establish that the typeofalltypes assumption creates serious pathologies from a programming perspective: a system using this assumption is inherently not normalizing, term equality is undecidable, and the resulting theory fails to be a conservative extension of the theory of the underlying base types. The failure of conservative extension means that classical reasoning about programs in such a system is not sound.
p2903
aVData or program flow analysis is concerned with the static analysis of programs, to obtain as much information as possible about their possible run time behavior without actually having to run the programs. Due to the unsolvability of the halting problem (and nearly any other question concerning program behavior), such analyses are necessarily only approximate whenever the analysis algorithm is guaranteed to terminate. Further, exact analysis may be impossible due to the lack of knowledge of input data values, so the analysis can at best yield information about sets of possible computations.
p2904
aVOur concern is the mechanical certification of transformations of sequential program executions into parallel executions with equivalent semantics. The objective of such transformations is to accelerate the execution of programs. The result reported here is a mechanically certified theorem of optimality. We present a transformation which applies to every program in a particular programming language, the language of sorting networks. This transformation transforms the sequential execution of any sorting network into an execution which is as fast or faster than any other transformation which applies to every sorting network. The theorem is stated formally in a mechanized logic.
p2905
aVGlobal storage allocation for attributes in an attribute grammar evaluator is discussed and an algorithm for determining if a given set of attribute occurrences can share a common global storage is obtained.
p2906
aVIt is a truism that most bugs are detected only at a great distance from their source. Although polymorphic typechecking systems like those in ML help greatly by detecting potential runtime type errors at compiletime, such systems are still not very helpful for locating the source of a type error. Typically, an error is reported only when the typechecker can proceed no further, even though the programmer's actual error may have occurred much earlier in the text. We describe an algorithm which appears to be quite helpful in isolating and explaining the source of type errors. The algorithm works by keeping track of the <i>reasons</i> the checker makes deductions about the types of variables.
p2907
aVA crucial aspect of a program intended for general use is its behavior in the presence of erroneous inputs. For instance, much attention has been devoted to the problems of error detection, reporting, and correction in compilers<sup>1,2</sup>. As programming languages and systems based in one way or another on unification<sup>3</sup> become more common, it becomes increasingly important to develop a theory of error detection and correction for unificationbased systems. We report here on a languagebased editor for a variant of ML<sup>4</sup> that uses a novel approach to the isolation of likely causes of user errors. As the user edits and manipulates his or her program, unification is incrementally applied to determine its type correctness. If a type inconsistency arises, a maximum flow technique is applied to the set of type equations to determine the most likely source of error. In this way a determination can be made as to the relative strengths with which the set of type equations asserts multiple contradictory hypotheses. In a language such as ML, the type of an object is inferred from patterns of usage. Often it is the case that most uses of a given object are mutually consistent, whereas one or a very small number of uses conflict with the general usage pattern. In MOE (MLOriented Editor), if it is possible to discern that such a situation has arisen, likely errors are highlighted at high intensity and all other program components that contributed to the inferred type of the object are highlighted at a lower intensity. In providing error information to the user, two principles are observed:(1) Error indications should be complete but parsimonious; the user should see highlighted on the screen everything that contributed directly to an error, but nothing. more,(2) The user's attention should be drawn to what appear to be the anomalies that are responsible for errors.Languagebased editors permit a new level of quality in the process of helping users when inputs are, for one reason or another, invalid. Unificationbased type inference in languagebased editors appears to have been first considered by Meertens 5. Snelting and Bahlke have more recently also explored this approach.
p2908
aVThe attribute grammar technique used for design of structure editors is suggested as a foundation for building <i>hierarchical incremental design editors</i> for VLSI circuits. The usual definition of attribute grammars is extended: the cycles that occur in VLSI design make us come to terms with circular attributes (under conditions that guarantee their least fixpoint solution, namely that the functions be monotone and yield values over a lattice of bounded height). Many interesting VLSI design problems can be cast in attributes meeting this condition, for example, timing verification, logic simulation, power dissipation, and adherence to clocking disciplines, to name a few. As an illustration of the formalism, attributes are presented which solve the All Bidirectional Edges problem that labels the direction of information flow in a circuit. The incremental evaluation algorithm of [Rep82] is extended to handle fixpoint computations of circular attributes by noting that when the dependency graph is broken into its strongly connected components, a directed acyclic graph results. The worstcase running time of the resulting incremental evaluation algorithm is bounded by <i>O</i> (<i>hk</i> |AFFECTEDSCC|), where <i>h</i> is the height of the largest attribute lattice, <i>k</i> the largest number of attributes in any one strongly connected component, and |AFFECTEDSCC| the number of strongly connected components affected by a single modification to the design tree.
p2909
aVOne trend among programmers is the increased use of abstractions. Through encapsulation techniques, abstractions extend the repertory of data structures and their concomitant operations that are processed directly by a compiler. For example, a compiler might not offer sets or set operations in its base language, but abstractions allow a programmer to define sets in terms of constructs already recognized by the compiler. In particular, abstractions can allow new constructs to be defined in terms of other abstractions. Although significant power is gained through the use of layered abstractions, object code quality suffers as increasingly less of a program's data structures and operations are exposed to the optimization phase of a compiler. Multiple references to abstractions are also inefficient, since the interaction between abstractions is often complex yet hidden from a compiler. Abstractions are most flexible when they are cast in general terms; a specific invocation is then tailored by the abstraction to obtain the appropriate code. A sequence of references to such abstractions can be inefficient due to functional redundancy that cannot be detected at compiletime. By integrating the references, the offending segments of code can be moved to a more advantageous position. Although procedure integration materializes abstracted constructs, the abstractions can still be ineligible for optimization using current techniques; in particular, abstractions often involve loops and conditional branches that can obscure code that would otherwise be eligible for code motion.
p2910
aVComputations can generally be separated into stages, which are distinguished from one another by either frequency of execution or availability of data. <i>Precomputation</i> and <i>frequency reduction</i> involve moving computation among a collection of stages so that work is done as early as possible (so less time is required in later steps) and as infrequently as possible (to reduce overall time).We present, by means of examples, several general transformation techniques for carrying out precomputation transformations. We illustrate the techniques by deriving fragments of simple compilers from interpreters, including an example of Prolog compilation, but the techniques are applicable in a broad range of circumstances. Our aim is to demonstrate how perspicuous accounts of precomputation and frequency reduction can be given for a wide range of applications using a small number of relatively straightforward techniques.Related work in partial evaluation, semantically directed compilation, and compiler optimization is discussed.
p2911
aVA function is said to be <i>strict</i> in one of its formal parameters if, in all calls to the function, either the corresponding actual parameter is evaluated, or the call does not terminate. Detecting which arguments a function will surely evaluate is a problem that arises often in program transformation and compiler optimization. We present a strategy that allows one to infer strictness properties of functions expressed in the lambda calculus. Our analysis improves on previous work in that (1) a settheoretic characterization of strictness is used that permits treatment of <i>free variables,</i> which in turn permits a broader range of interpretations, and (2) the analysis provides an effective treatment of higherorder functions. We also prove a result due to Meyer [15]: the problem of firstorder strictness analysis is complete in deterministic exponential time. However, because the size of most functions is small, the complexity seems to be tractable in practice.This research was supported in part by NSF Grant MCS8302018, and a Faculty Development Award from IBM.
p2912
aV\u2200automata are nondeterministic finitestate automata over infinite sequences. They differ from conventional automata in that a sequence is accepted if all runs of the automaton over the sequence are accepting. These automata are suggested as a formalism for the specification and verification of temporal properties of concurrent programs. It is shown that they are as expressive as extendedtemporallogic (ETL), and in some cases provide a more compact representation of properties than temporal logic. A structured diagram notation is suggested for the graphical representation of these automata. A single sound and complete proof rule is presented for proving that all computations of a program have the property specified by a \u2200automaton.
p2913
aVWe address the problem of designing programming systems to reason with and about constraints. Taking a logic programming approach, we define a class of programming languages, the CLP languages, all of which share the same essential semantic properties. From a conceptual point of view, CLP programs are highly declarative and are soundly based within a unified framework of formal semantics. This framework not only subsumes that of logic programming, but satisfies the core properties of logic programs more naturally. From a user's point of view, CLP programs have great expressive power due to the constraints which they naturally manipulate. Intuition in the reasoning about programs is enhanced as a result of working directly in the intended domain of discourse. This contrasts with working in the Herbrand Universe wherein every semantic object has to be explicitly coded into a Herbrand term; this enforces reasoning at a primitive level. Finally, from an implementor's point of view, CLP systems can be efficient because of the exploitation of constraint solving techniques over specific domains.
p2914
aVA theory of abstract interpretation [CoCo79] is developed for a typed &lgr;calculus. The typed &lgr;calculus is the \u201cstatic\u201d part of a twolevel denotational metalanguage for which abstract interpretation was developed in [Nie86]. The present development relaxes a condition imposed in [Nie86] and this suffices to make the framework applicable to strictness analysis for the &lgr;calculus. This shows the possibility of a general theory (and hence a system) for the analysis of functional programs. Furthermore, it gives more insight into the relative precision of the various analyses. In particular, it is shown that a collecting (static [CoCo79]) semantics exists, thus answering a problem left open in [BHA86].
p2915
aVWe study the strictness problem for the untyped lambdacalculus and for an MLlike typed calculus. We establish that strictness is a necessary and sufficient condition for the \u201ceager\u201d evaluation of function arguments. For the untyped calculus, we show that the strictness problem is elementary and describe a complete algorithm for strictness analysis of convergent terms. We show that typed terms possess a much richer set of strictness properties. A typetheoretic characterization of strictness is developed and it is shown that the strictness properties of a term can be represented as a collection of types related to the standard term type. A set of type inference rules that support reasoning about strictness related types is given. The inference rules are shown to be invariant under type change by substitution. This provides a sound basis for reasoning about strictness properties of terms by considering only their principal type. For our final result, we describe a practical system that carries out type checking and strictness analysis simultaneously in a unified framework. Our main result is thus the discovery that the problem of strictness analysis is a particular case of type inference.
p2916
aVLUSTRE is a synchronous dataflow language for programming systems which interact with their environments in realtime. After an informal presentation of the language, we describe its semantics by means of structural inference rules. Moreover, we show how to use this semantics in order to generate efficient sequential code, namely, a finite state automaton which represents the control of the program. Formal rules for program transformation are also presented.
p2917
aVThe relations among various languages and models for distributed computation and various possible definitions of fairness are considered. Natural semantic criteria are presented which an acceptable notion of fairness should satisfy. These are then used to demonstrate differences among the basic models, the added power of the fairness notion, and the sensitivity of the fairness notion to irrelevant semantic interleavings of independent operations. These results are used to show that from the considerable variety of commonly used possibilities, only strong process fairness is appropriate for CSP if these criteria are adopted. We also show that under these criteria, none of the commonly used notions of fairness are fully acceptable for a model with an nway synchronization mechanism. Finally, the notion of fairness most often mentioned for Ada is shown to be fully acceptable.
p2918
aVUsing concurrent transition systems [Sta86], we establish connections between three models of concurrent process networks, Kahn functions, input/output automata, and labeled processes. For each model, we define three kinds of algebraic operations on processes: the product operation, abstraction operations, and connection operations. We obtain homomorphic mappings, from input/output automata to labeled processes, and from a subalgebra (called \u201cinput/output processes\u201d) of labeled processes to Kahn functions. The proof that the latter mapping preserves connection operations amounts to a new proof of the \u201cKahn Principle.\u201d Our approach yields: (1) extremely simple definitions of the process operations; (2) a simple and natural proof of the Kahn Principle that does not require the use of \u201cstrategies\u201d or \u201cscheduling arguments\u201d; (3) a semantic characterization of a large class of labeled processes for which the Kahn Principle is valid, (4) a convenient operational semantics for nondeterminate process networks.
p2919
aVSpecification and verification techniques for abstract data types that have been successful for sequential programs can be extended in a natural way to provide the same benefits for concurrent programs. We propose an approach to specifying and verifying concurrent objects based on a novel correctness condition, which we call \u201clinearizability.\u201d Linearizability provides the illusion that each operation takes effect instantaneously at some point between its invocation and its response, implying that the meaning of a concurrent object's operations can still be given by pre and postconditions. In this paper, we will define and discuss linearizability, and then give examples of how to reason about concurrent objects and verify their implementations based on their (sequential) axiomatic specifications.
p2920
aVWe present a fully abstract semantics for realtime distributed computing of the Ada and OCCAM kind in a denotational style. This semantics turns termination, communication along channels, and the time communication takes place, into observables. Yet it is the coarsest semantics to do so which is syntaxdirected (this is known as full abstraction). It extends the linear history semantics for CSP of Francez, Lehman and Pnueli. Our execution model is based on maximizing concurrent activity as opposed to interleaving (in which only one action occurs at the time and arbitrary delays are incurred between actions). It is a variant of the maximal parallelism model of Salwicki and Mldner.
p2921
aVIn programming languages of universal power, the computational integers must be distinguished from the classical integers because of the \u201cdivergent\u201d integer. Even the equational theory corresponding to evaluation of integer expressions is distinct from the theory of classical integers, and classical reasoning about computational integers yields inconsistencies. We show that there exist \u201cprogramming languages\u201d, actually extensions of the polymorphic lambda calculus, that have tremendous computing power and yet whose computational integers, or any other algebraically specified abstract data type, coincide with their classical counterpart. In particular, the equational theory of the programming language is a conservative extension of the theory of the underlying base types as given by algebraic data type specifications.
p2922
aVThe notion of relative completeness of logics of programs was delineated almost ten years ago, in particular by Wand, Cook and Clarke. More recently, it has been felt that Cook's notion hinges on a fragile balance between the semantics of a programming language and firstorder expressiveness in structures. This fragility underlies the negative results about relative completeness.\u000aThe main negative result in this area, Clarke's Theorem, states that for a programming language with a sufficiently rich control structure there is no effective formalism for partialcorrectness that is both sound and relatively complete (in the sense of Cook). A conclusion often drawn from this result is that the nonexistence of an adequate Cookcomplete formalization of the partialcorrectness theory of a programming language is an indicator of the complexity of that language. Our most novel result is that this is not always the case. We exhibit a programming language whose control structure is trivial, and yet for which no Cookcomplete Hoare logic exists. The poverty of the language is precisely what permits certain structures to be expressive, structures which would not be expressive had the program constructs been used more freely.\u000aWe also discuss the failure of relative completeness for \u201cfleshy\u201d programming languages, of the kind of Clarke's. We point out the relevance of the Lambda Calculus here, from which we derive the failure of Cookcompleteness for a programming language orthogonal (i.e. incomparable) to Clarke's, whose controlstructure complexity resides to a great extent in a modest use of generic procedures. A variant of the same idea also provides an alternative proof of nonrelativecompleteness for (a variant of) Clarke's language.\u000aFinally, we note in passing a simple proof of Wand's Theorem, which states the failure of local completeness (without Cook's expressiveness condition) for while programs. Our proof uses rudimentary facts from Mathematical Logic. Its interest, compared with previous proofs [Wan78, BT82, Apt81] is that it refers to a structure whose firstorder theory is not decidable. Moreover, our proof shows that the failure of local completeness for while programs is not necessarily due to a tension between firstorder definability and definability by program looping, as one might have been tempted to conclude from previous proofs.
p2923
aVThe model theory of simply typed and polymorphic (secondorder) lambda calculus changes when types are allowed to be empty. For example, the \u201cpolymorphic Boolean\u201d type really has exactly two elements in a polymorphic model only if the \u201cabsurd\u201d type \u2200t.t is empty. The standard &bgr;&egr; axioms and equational inference rules which are complete when all types are nonempty are not complete for models with empty types. Without a little care about variable elimination, the standard rules are not even sound for empty types. We extend the standard system to obtain a complete proof system for models with empty types. The completeness proof is complicated by the fact that equational \u201cterm models\u201d are not so easily obtained: in contrast to the nonempty case, not every theory with empty types is the theory of a single model.
p2924
aVWe consider a machine model in which load operations can be performed in parallel with arithmetic operations by two separate functional units. For this model, the evaluation of a set of expression trees is discussed. A dynamic programming algorithm to produce an approximate solution is described and analyzed. For binary trees its worse case cost is at most 9.1% worse than the optimal cost.
p2925
aVWe provide a scheme for determining which global variables are involved when an expression is evaluated in a language with higher order constructs and imperative features. The heart of our scheme is a mechanism for computing the support of an expression, i.e. the set of global variables involved in its evaluation. This computation requires knowledge of all the aliases of an expression. The inference schemes are presented as abstract semantic interpretations. We prove the soundness of our estimates by establishing a correspondence between the abstract semantics and the standard semantics of the programming language.
p2926
aVWe have developed a new style of semantic definition called highlevel semantics. In constrast to traditional denotational semantics, highlevel semantics is suitable for both defining the functional meaning of programming languages, as well as describing realistic compiler implementations. Moreover, highlevel specifications are considerably more descriptive and intelligible than traditional specifications.\u000aThis paper describes the compiler generator MESS, which embodies the principles of highlevel semantics. MESS has been used to generate compilers for nontrivial languages. The compilers are efficient, and produce object programs that are competitive with those generated by handwritten compilers.
p2927
aVAn extended record facility is described that supports multimodule records by providing:\u000a\u000aincremental and distributed record type definition, allowing field names of a record to be declared in different modules with different subscopes relative to the root record declaration.\u000afieldlevel declaration of access to records by modules and procedures.\u000aspecification of record representation in terms of the underlying computer memory.\u000a\u000aWe also describe the uses of records that motivate these extensions as well as the compiler modifications required to implement this extended record facility.\u000aFrom this work, we conclude that the extended record support can be added as a simple, natural extension to existing programming language designs and its implementation entails only modest additions to the compiler and linker with no significant compilation time cost.
p2928
aVPattern matching and data abstraction are important concepts in designing programs, but they do not fit well together. Pattern matching depends on making public a free data type representation, while data abstraction depends on hiding the representation. This paper proposes the views mechanism as a means of reconciling this conflict. A view allows any type to be viewed as a free data type, thus combining the clarity of pattern matching with the efficiency of data abstraction.
p2929
aVMost tracebased proof systems for networks of processes are known to be incomplete. Extensions to achieve completeness are generally complicated and cumbersome. In this paper, a simple trace logic is defined and two examples are presented to show its inherent incompleteness. Surprisingly, both examples consist of only one process, indicating that network composition is not a cause of incompleteness. Axioms necessary and sufficient for the relative completeness of a trace logic are then presented.
p2930
aVThe parallelization of sequential programs is an adequate approach for the easy use of architectural features provided on parallel computers. We propose to enhance this technique with the notion of semantic parallelization. The principle is to consider the transformations of programs induced by parallelization as nonstandard denotational semantics of the source programming language. We show how to apply this concept to detect, in a toy language ALL, parallelizable complex commands and to deal with some type of programs using indirections. Thanks to results in domain theory and abstract interpretation, we give correctness proofs for these transformations with respect to ALL standard semantic. A byproduct of our approach is that a denotational specification is also an executable   prototype ;hence, we were able to implement a semantic parallelizer in the ML functional language. Running examples are supplied.
p2931
aVIn this paper we present some of the control constructs of the language CP, which is based on a concurrent interpretation of Horn logic programming. We present a formal structural operational semantics and relate the meaning of programs in this language to the underlying (pure) Horn clause axioms.
p2932
aVAn algorithm for transforming sequential programs into equivalent parallel programs is presented. The method concentrates on finding loops whose separate iterations can be run in parallel without synchronization. Although a simple version of the method can be shown to be optimal, the problem of generating optimal code when loop interchange is employed is shown to be intractable. These methods are implemented in an experimental translation system developed at Rice University.
p2933
aVThis paper presents two new developments. First, it describes a \u201cmacrobyexample\u201d specification language for syntactic abstractions in Lisp and related languages. This specification language allows a more declarative specification of macros than conventional macro facilities do by giving a better treatment of iteration and mapping constructs. Second, it gives a formal semantics for the language and a derivation of a compiler from the semantics. This derivation is a practical application of semanticsdirected compiler development methodology.
p2934
aVAn important research goal in software engineering and programming languages is the development of principles underlying the specification of computable problems, the translation of these problems into efficient and correct programs, and the performance analysis of these programs. This paper uncovers some of these principles, which are used to design a problem specification language L1 restricted to problems that can be compiled automatically into programs whose worst case asymptotic time and space complexities are linear in the input/output space. Any problem expressible in L1 can be compiled into a linear cost program. A compiler for L1 has been implemented in the RAPTS transformational programming system.
p2935
aVWe describe a programming language called Symmetric Lisp that treats environments as firstclass objects. Symmetric Lisp allows programmers to write expressions that evaluate to environments, and to create and denote variables and constants of type environment as well. One consequence is that the roles filled in other languages by a variety of limited, special purpose environment forms like records, structures, closures, modules, classes and abstract data types are filled instead by a single versatile and powerful structure. In addition to being its fundamental structuring tool, environments also serve as the basic functional object in the language. Because the elements of an environment are evaluated in parallel, Symmetric Lisp is a parallel programming language; because they may be assembled dynamically as well as statically, Symmetric Lisp accommodates an unusually flexible and simple (parallel) interpreter as well as other historysensitive applications requiring dynamic environments. We show that firstclass environments bring about fundamental changes in a language's structure: conventional distinctions between declarations and expressions, data structures and program structures, passive modules and active processes disappear.
p2936
aVA collecting interpretation of expressions is an interpretation of a program that allows one to answer questions of the sort: \u201cWhat are all possible values to which an expression might evaluate during program execution?\u201d Answering such questions in a denotational framework is akin to traditional data flow analysis, and when used in the context of abstract interpretation allows one to infer properties that approximate the runtime behavior of expression evaluation. In this paper collecting interpretations of expressions are developed for three abstract functional languages: (1) a firstorder language with callbyvalue semantics, (2) a firstorder language with callbyname semantics, and (3) a higherorder language with callbyname semantics (i.e., the full untyped lambda calculus with constants). It is argued that the method is simple (in particular, no powerdomains are needed), natural (it captures the intuitive operational behavior of a cache), yet more expressive than existing methods (it is the first exact collecting interpretation for either lazy or higherorder programs).
p2937
aVAnalyzing time complexity of functional programs in a lazy language is problematic, because the time required to evaluate a function depends on how much of the result is \u201cneeded\u201d in the computation. Recent results in strictness analysis provide a formalisation of this notion of \u201cneed\u201d, and thus can be adapted to analyse time complexity.\u000aThe future of programming may be in this paradigm: to create software, first write a specification that is clear, and then refine it to an implementation that is efficient. In particular, this paradigm is a prime motivation behind the study of functional programming. Much has been written about the process of transforming one functional program into another. However, a key part of the process has been largely ignored, for very little has been written about assessing the efficiency of the resulting programs.\u000aTraditionally, the major indicators of efficiency are time and space complexity. This paper focuses on the former.\u000aFunctional programming can be split into two camps, strict and lazy. In a strict functional language, analysis of time complexity is straightforward, because of the following compositional rule:\u000a\u000aThe time to evaluate (\u0192(g x)) equals the time to evaluate (g x) plus the time to evaluate (\u0192 y), where y is the value of (g x).\u000a\u000aHowever, in a lazy language, this rule only gives an upper bound, possibly a crude one. For example, if \u0192 is the head function, then (g x) need only be evaluated far enough to determine the first element of the list, and this may take much less time than evaluating (g x) completely.\u000aThe key to a better analysis is to describe formally just how much of the result \u201cneeds\u201d to be evaluated; we call such a description a context. Recent results in strictness analysis show how such contexts can be modelled using the domaintheoretic notion of a projection [WH87]. This paper describes how these results can be applied to the analysis of time complexity. The method used was inspired by work of Bror Bjerner on the complexity analysis of programs in MartinLf's type theory [Bje87]. The main contribution of this paper is to simplify Bjerner's notation, and to show how contexts can replace his \u201cdemand notes\u201d.\u000aThe language used in this paper is a firstorder language. This restriction is made because context analysis for higherorder languages is still under development. An approach to higherorder context analysis is outlined in [Hug87b]. Context analysis is based on backwards analysis, rather than the earlier approach of abstract interpretation; both are discussed in [AH87].\u000aSome work on complexity analysis [Weg75,LeM85] has concentrated on automated analysis: algorithms that derive a closed form for the time complexity of a program. The goal here is less ambitious. We are simply concerned with describing a method of converting a functional program into a series of equations that describe its time complexity. This modest beginning is a necessary precursor to any automatic analysis. The time equations can be solved by traditional methods, yielding either an exact solution, an upper bound, or an approximate solution. (Incidentally, although [LeM85] claims to analyse a lazy language, the analysis uses exactly the composition rule above, and so is more suited for a strict language.)\u000aThe analysis given here involves two kinds of equations. First are equations defining projection transformers that specify how much of a value is \u201cneeded\u201d. Second are equations that specify the time complexity; these depend on the projection transformers defined by the first equations.\u000aIn both cases, we will be more concerned with setting up the equations that with finding their solutions. As already noted, traditional methods may be applied to satisfy the time complexity equations. Solving the projection transformer equations is more problematic. In some cases, we can find an appropriate solution by choosing an appropriate finite domain of projections, and then applying the method of [WH87] to find the solution in this domain. In other cases, no finite domain of solutions is appropriate, and we will find a solution by a more adhoc method: guessing one and verifying that it satisfies the required conditions. More work is required to determine what sort of solutions to the projection transformer equations will be most useful for time analysis, and how to find these solutions.\u000aThis paper is organized as follows. Section 1 describes the language to be analysed. Section 2 presents the evaluation model. Section 3 gives a form of time analysis suitable for strict evaluation. Section 4 shows how projections can describe what portion of a value is \u201cneeded\u201d, and introduces projection transformers. Section 5 gives the time analysis for lazy evaluation. Section 6 presents a useful extension to the analysis method. Section 7 concludes.
p2938
aVThe need to integrate several versions of a program into a common one arises frequently, but it is a tedious and time consuming task to integrate programs by hand. The main contribution of this paper is an algorithm, called integrate, that takes as input three programs A, B, and Base, where A and B are two variants of Base. Whenever the changes made to Base to create A and B do not \u201cinterfere\u201d (in a sense defined in the paper), Integrate produces a program M that integrates A and B.
p2939
aVProgram dependence graphs were introduced by Kuck as an intermediate program representation well suited for performing optimizations, vectorization, and parallelization. There are also additional applications for them as an internal program representation in program development environments.\u000aIn this paper we examine the issue of whether a program dependence graph is an adequate structure for representing a program's execution behavior. (This question has apparently never been addressed before in the literature). We answer the question in the affirmative by showing that if the program dependence graphs of two programs are isomorphic then the programs are strongly equivalent.
p2940
aVThe designers of (functional) programming languages are faced with two occasionally conflicting goals: programmer convenience and semantic simplicity. For example, it is convenient to treat I/O operations as primitive \u201cfunctions\u201d with side effects, but doing so destroys referential transparency.\u000aFL is a functional language that is designed to trade some of the semantic simplicity of a pure language for some of the convenience of a procedural language, by treating I/O operations as primitives with \u201cside effects\u201d, but by using a structuring technique that localizes the scope of those effects. In this way, surprisingly little of the semantic simplicity is lost, as can be seen by comparing the underlying algebraic laws of FL with those of its pure counterpart. FP.\u000aThis paper describes that comparison and shows that, in fact, for programs involving I/O, the structures of the algebraic laws of the two languages are identical! It concludes by showing that this technique cannot be extended to allow assignment statements without incurring a massive loss in the expressiveness and simplicity of the underlying algebra.
p2941
aVAn analysis of the &lgr;ugr;Ccalculus and its problematic relationship to operational equivalence leads to a new control facility: the promptapplication. With the introduction of promptapplications, the control calculus becomes a traditional calculus all of whose equations imply operational equivalence. In addition, promptapplications enhance the expressiveness and efficiency of the language. We illustrate the latter claim with examples from such distinct areas as systems programming and tree processing.
p2942
aVThe Store Model of HalpernMeyerTrakhtenbrot is shown\u2014after suitable repair\u2014to be a fully abstract model for a limited fragment of ALGOL in which procedures do not take procedure parameters. A simple counterexample involving a parameter of program type shows that the model is not fully abstract in general. Previous proof systems for reasoning about procedures are typically sound for the HMT store model, so it follows that theorems about the counterexample are independent of such proof systems. Based on a generalization of standard cpo based models to structures called locally complete partial orders (lcpo's), improved models and stronger proof rules are developed to handle such examples.
p2943
aVThree semanticspreserving transformations (static replacement, factoring, and combinator selection) are used to convert a continuation semantics into a formal description of a semantic analyzer and code generator. The result of this derivation is a compilation algorithm which performs type checking before code generation so that typechecking instructions are not generated in the target code. Both the flow analysis and the resulting optimizations are proved correct with respect to the original definition of the source language. The proof consists of showing that all restructuring transformations preserve the semantics of the source language. This transformational approach can be extended to derive correctness proofs of other flow analysis and code optimization techniques.
p2944
aVRewriting techniques have been used to reason about a variety of topics related to programming languages, e.g., abstract data types, Petri Nets, FP programs, and data bases. They have also been used in the implementation and definition of a variety of programming languages.\u000aAt the 1980 POPL Conference, David Musser proposed a new method of proving inductive properties of abstract data types. Since that time, this method, which came to be called inductionless induction, has attracted considerable attention. Numerous applications and improvements have been proposed and several implementations described. However, little or no work has appeared that questions the basic utility of the idea.\u000aThe thesis of this paper is that while induction using equational termrewriting holds great promise, inductionless induction does not. More specifically, we argue that for reasoning about abstract data types traditional inductive methods are usually superior.
p2945
aVBisimulation is the primitive notion of equivalence between concurrent processes in Milner's Calculus of Communicating Systems (CCS); there is a nontrivial gamelike protocol for distinguishing nonbisimular processes. In contrast, process distinguishability in Hoare's theory of Communicating Sequential Processes (CSP) is determined solely on the basis of traces of visible actions. We examine what additional operations are needed to explain bisimulation similarly\u2014specifically in the case of finitely branching processes without silent moves. We formulate a general notion of Structured Operational Semantics for processes with Guarded recursion (GSOS), and demonstrate that bisimulation does not agree with trace congruence with respect to any set of GSOSdefinable contexts. In justifying the generality and significance of GSOS's, we work out some of the basic proof theoretic facts which justify the SOS discipline.
p2946
aVA general definition of the notion of superimposition is presented. We show that previous constructions under the same name can be seen as special cases of our definition. We consider several properties of superimposition definable in our terms, notably the nonfreezing property. We also consider a syntactic representation of our construct in CSP
p2947
aVTwo distinct extensions of temporal logic has been recently advocated in the literature. The first extension is the addition of fixpoint operators that enable the logic to make assertions about arbitrary regular events. The second extension is the addition of past temporal connectives that enables the logic to refer directly to the history of the computation. Both extensions are motivated by the desire to adapt temporal logic to modular, i.e., compositional, verification (as opposed to global verification). We introduce and study here the logic &mgr;TL, which is the extension of temporal logic by fixpoint operators and past temporal connectives. We extend the automatatheoretic paradigm to &mgr;TL. That is, we show how, given an &mgr;TL formula @@@@, we can produce a finitestate Bchi automaton A@@@@, whose size is at most exponentially bigger than the size of @@@@, such that A@@@@ accepts precisely the computations that satisfy @@@@. The result has immediate applications, e.g., an optimal decision procedure and a modelchecking algorithm for &mgr;TL.
p2948
aVWe investigate a framework for efficient flow analyses of logic programs. A major problem in this context is that unification can give rise to aliasing and dependencies between variables whose effects are difficult to predict, and which make sound flow analysis algorithms computationally expensive. We give a simple characterization of the class of flow analysis problems for which aliasing effects can be ignored without loss of soundness, and describe an efficient analysis procedure for this class of problems. The utility of our approach is illustrated by discussing its application to several analysis and optimization problems for logic programs. Our results are useful in the design of efficient flow analysis systems for logic programming languages.
p2949
aVWe present an algorithm for updating data flow information derived from a program, in response to program edits. Our algorithm, applicable to intraprocedural or interprocedural data flow problems, is more general than previous methods because it can update any monotone data flow problem defined on a reducible flow graph and can handle arbitrary program edits.\u000aRather than design yet another specialpurpose data flow update algorithm, we show how to reduce the class of data flow problems to another class of problems which already has a fast update algorithm. More specifically, we reduce a monotone data flow problem formulated for solution using GrahamWegman elimination [12] to the problem of constructing and decorating an attributed tree structurally isomorphic to the dominator tree of the program flow graph.\u000aTo update this dominator tree in response to program changes, we first show how to express domination in terms of two local properties of nodes and edges in the flow graph \u2014 niceness and deepness. Domination is then updated in response to an edge addition or deletion by repeatedly performing two local operations on the dominator tree, each operation restoring the local properties violated by the edge change. Second, we extend Reps' attributed tree update techniques [19,21,20] and his complexity analysis, to update attribute values in response to changes in this structure.
p2950
aVThe choice of binding time disciplines has major consequences for both the runtime efficiency of programs and the convenience of the language expressing algorithms. Late storage binding time, dynamic allocation, provides the flexibility necessary to implement the complex data structures common in today's object oriented style of programming. In this paper we show that compiletime lifetime analysis can be applied to programs written in languages with static type systems and dynamically allocated objects, to provide earlier storage binding time for objects, while maintaining all the advantages of dynamic allocation.
p2951
aVA Rewrite System is a collection of rewrite rules of the form &agr; &bgr; where &agr; and &bgr; are tree patterns. A rewrite system can be extended by associating a cost with each rewrite rule, and by defining the cost of a rewrite sequence as the sum of the costs of all the rewrite rules in the sequence. The REACHABILITY problem for a rewrite system R is, given an input tree T and a fixed goal tree G, to determine if there exists a rewrite sequence in R, rewriting T into G and, if so, to obtain one such sequence. The CREACHABILITY problem is similar except that the obtained sequence must have minimal cost among all those sequences writing T into G.\u000aThis paper introduces a class of rewrite systems called BottomUp Rewrite Systems (BURS), and a tabledriven algorithm to solve REACHABILITY for member of the class. This algorithm is then modified to solve CREACHABILITY and specialized for a subclass of BURS so that all cost manipulation is encoded into the tables and is not performed explicitly at solving time. The subclass extends the simple machine grammars [AGH84], rewrite systems used to describe target machine architectures for code generation, by allowing additional types of rewrite rules such as commutativity transformations.\u000aA tabledriven code generator based on solving CREACHABILITY has been implemented and tested with several machine descriptions. The code generator solves CREACHABILITY faster than a comparable solver based on GrahamGlanville techniques [AGH84] (a nonoptimal technique), yet requires only slightly larger tables. The code generator runs much faster than recent proposals to solve CREACHABILITY that use pattern matching and deal with costs explicitly at solving time [AGT86, HeD87, WeW86]. The BURS theory generalizes and unifies the bottomup approaches of Henry/Damron [HeD87] and Weisgerber/Wilhelm [WeW86].
p2952
aVA programmable systolic array of highperformance cells is an attractive computation engine if it attains the same utilization of dedicated arrays of simple cells. However, typical implementation techniques used in highperformance processors, such as pipelining and parallel functional units, further complicate the already difficult task of systolic algorithm design. This paper shows that highperformance systolic arrays can be used effectively by presenting the machine to the user as an array of conventional processors communicating asynchronously. This abstraction allows the user to focus on the higher level problem of partitioning a computation across cells in the array. Efficient finegrain parallelism can be achieved by code motion of communication operations made possible by the asynchronous communication model. This asynchronous communication model is recommended even for programming algorithms on systolic arrays without dynamic flow control between cells.\u000aThe ideas presented in the paper have been validated in the compiler for the Warp machine [4]. The compiler has been in use in various application areas including robot navigation, lowlevel vision, signal processing and scientific programming. Nearoptimal code has been generated for many published systolic algorithms.
p2953
aVStandard ML is a useful programming language with polymorphic expressions and a flexible module facility. One notable feature of the expression language is an algorithm which allows type information to be omitted. We study the implicitlytyped expression language by giving a \u201csyntactically isomorphic\u201d explicitlytyped, polymorphic function calculus. Unlike the GirardReynolds polymorphic calculus, for example, the types of our ML calculus may be builtup by induction on type levels (universes). For this reason, the pure ML calculus has straightforward settheoretic, recursiontheoretic and domaintheoretic semantics, and operational properties such as the termination of all recursionfree programs may be proved relatively simply. The signatures, structures, and functors of the module language are easily incorporated into the typed ML calculus, providing a unified framework for studying the major features of the language (including the novel \u201csharing constraints\u201d on functor parameters). We show that, in a precise sense, the language becomes inconsistent if restrictions imposed by type levels are relaxed. More specifically, we prove that the important programming features of ML cannot be added to any impredicative language, such as the GirardReynolds calculus, without implicitly assuming a type of all types.
p2954
aVWe present a new approach to programming languages for parallel computers that uses an effect system to discover expression scheduling constraints. This effect system is part of a 'kinded' type system with three base kinds: types, which describe the value that an expression may return; effects, which describe the sideeffects that an expression may have; and regions, which describe the area of the store in which sideeffects may occur. Types, effects and regions are collectively called descriptions.\u000aExpressions can be abstracted over any kind of description variable   this permits type, effect and region polymorphism. Unobservable sideeffects can be masked by the effect system; an effect soundness property guarantees that the effects computed statically by the effect system are a conservative approximation of the actual sideeffects that a given expression may have.\u000aThe effect system we describe performs certain kinds of sideeffect analysis that were not previously feasible. Experimental data from the programming language FX indicate that an effect system can be used effectively to compile programs for parallel computers.
p2955
aVWe extend the functional language ML by allowing the recursive calls to a function F on the righthand side of its definition to be at different types, all generic instances of the (derived) type of F on the lefthand side of its definition. The original definition of ML does not allow this feature. This extension does not produce new types beyond the usual universal polymorphic types of ML and satisfies the properties already enjoyed by ML: the principaltype property and the effective typeassignment property.
p2956
aVA denotational semantic definition of SMALLTALK80 is given. Its most notable characteristic is a surprisingly simple treatment of inheritance. An executable version of the semantics, written in STANDARD ML, is also described.
p2957
aVWe give an algorithm for type inference in a language with functions, records, and variant records. A similar language was studied by Cardelli who gave a type checking algorithm. This language is interesting because it captures aspects of objectoriented programming using subtype polymorphism. We give a type system for deriving types of expressions in the language and prove the type inference algorithm is sound, i.e., it returns a type derivable from the proof system. We also prove that the type the algorithm finds is a \u201cprincipal\u201d type, i.e., one which characterizes all others. The approach taken here is due to Milner for universal polymorphism. The result is a synthesis of subtype polymorphism and universal polymorphism.
p2958
aVFor a typed &lgr;calculus we develop an algorithm that, given some partial information about what must happen at runtime, will work out what actually can be computed at compiletime and what must be deferred to runtime.
p2959
aVPrevious attempts at vectorizing programs written in a sequential high level language focused on converting control dependences to data dependences using a mechanism known as IFconversion. After IFconversion vector optimizations are performed on a data dependence graph. However, IFconversion is an irrevocable process which can introduce high runtime overhead if the input program is not amenable to vectorization.\u000aThis paper uses a program dependence graph as the intermediate representation for a vectorizing compiler. A program dependence graph explicitly represents both control and data dependences, allowing guard values to be generated for vectorized statements. Techniques have been developed to perform code motion on vectorization candidates, to validly eliminate all unnecessary control and data dependence cycles, and to regenerate the newly vectorized program consistent with a topological ordering based on the control and data dependences.
p2960
aVMost of the theoretical work on the semantics of logic programs assumes an interpreter that provides a complete resolution procedure. In contrast, for reasons of efficiency, most logic programming languages are built around incomplete procedures. This difference is rooted in Prolog, which evaluates resolvent trees in a depthfirst rather than a breadthfirst order. The gap is widened by some equational logic languages, which combine the incompleteness of depthfirst evaluation with incomplete approximations to equational unification. Because of this gap, it is unsound to reason about logic programs using their declarative semantics. This in turn makes it difficult to develop abstraction mechanisms that can be used to partition a logic program into independently specifiable modules.\u000aIn this paper we consider the role type systems can play in closing the gap between the operational and declarative semantics of logic programs. We develop the notion of an equational mode system for use in constraining the domains of both predicates and unification procedures. The mode system is used to guide the resolutionbased interpreter, and as a result, we can show that two predicate implementations with the same declarative meaning will be operationally equivalent.
p2961
aVCLP*(D) is a class of constraint logic programming languages which incorporates the notion of abstraction. Predicates in CLP*(D) are (potentially) infinite rational trees which represent abstractions of constraint expressions. This view of predicates as constraint abstractions was motivated by the language Scheme, where closures are viewed as abstractions of functional expressions. A semantics and an efficient implementation of the language are provided, along with several examples of the novel programming techniques provided by this class of languages.
p2962
aVWe propose a framework for discussing fully abstract compositional semantics, which exposes the interrelations between the choices of observables, compositions, and meanings. Every choice of observables and compositions determines a unique fully abstract equivalence. A semantics is fully abstract if it induces this equivalence.\u000aWe study the semantics of logic programs within this framework. We find the classical Herbrandbase semantics of logic programs inadequate, since it identifies programs that should be distinguished and vice versa. We therefore propose alternative semantics, and consider the cases of no compositions, composition by program union, and composition of logic modules (programs with designated exported and imported predicates). Although equivalent programs can be in different vocabularies, we prove that our fully abstract semantics are always in a subvocabulary of that of the program. This subvocabulary, called the essential vocabulary, is common to all equivalent programs. The essential vocabulary is also the smallest subvocabulary in which an equivalent program can be written.
p2963
aVIn this paper we present A Calculus of Higher Order Communicating Systems. This calculus considers sending and receiving processes to be as fundamental as nondeterminism and parallel composition.\u000aThe calculus is an extension of CCS [Mil80] in the sense that all the constructions of CCS are included or may be derived from more fundamental constructs and most of the mathematical framework of CCS carries over almost unchanged.\u000aClearly CCS with processes as first class objects is a powerful metalanguage and we show that it is possible to simulate the untyped &lgr;calculus in CHOCS. The relationship between CHOCS and the untyped &lgr;calculus is further strengthened by a result showing that the recursion operator is unnecessary in the sense that recursion can be simulated by means of process passing and communication.\u000aAs pointed out by R. Milner in [Mil80], CCS has its limitations when one wants to describe unboundedly expanding systems as e.g. an unbounded number of procedure invocations in an imperative concurrent programming language. We show how neatly CHOCS may describe both call by value and call by reference parameter mechanisms for the toy language of chapter 6 in [Mil80].
p2964
aVA dataflow network consists of nodes that communicate over perfect FIFO channels. For dataflow networks containing only deterministic nodes, a simple and elegant semantic model has been presented by Kahn. However, for nondeterministic networks, the straightforward generalization of Kahn's model is not compositional. We present a compositional model for nondeterministic networks, which is fully abstract, i.e., it has added the least amount of extra information to Kahn's model which is necessary for attaining compositionality. The model is based on traces.
p2965
aVThere has been much interest in decision procedures for testing satisfiability (or validity) of formulae in various systems of Temporal Logic. This is due to the potential applications of such decision procedures to mechanical reasoning about correctness of concurrent programs. We show that there exist Temporal Logics that are (i) decidable in polynomial time, and (ii) still useful in applications. One surprising corollary of our results is that the fragment of CTL (Computation Tree Logic) actually used by Emerson & Clarke [EC82] to synthesize concurrent programs from temporal specifications is decidable in polynomial time. Another is that the verification of many correctness properties of concurrent programs (such as in Owicki & Lamport [OL82]) can be efficiently automated. This demonstrates that many useful correctness properties can be expressed with a rather restricted syntax. Finally, our results provide insight into the relation between the structural (i.e., syntactic) complexity of temporal logics and the complexity of their decision problems.
p2966
aVWe consider the synthesis of a reactive module with input x and output y, which is specified by the linear temporal formula @@@@(x, y). We show that there exists a program satisfying @@@@ iff the branching time formula (\u2200x) (\u2203y) A@@@@(x, y) is valid over all tree models. For the restricted case that all variables range over finite domains, the validity problem is decidable, and we present an algorithm for constructing the program whenever it exists. The algorithm is based on a new procedure for checking the emptiness of Rabin automata on infinite trees in time exponential in the number of pairs, but only polynomial in the number of states. This leads to a synthesis algorithm whose complexity is double exponential in the length of the given specification.
p2967
aVMethods for synthesizing concurrent programs from Temporal Logic specifications based on the use of a decision procedure for testing temporal satisfiability have been proposed by Emerson & Clarke [EC82] and Manna & Wolper [MW84]. An important advantage of these synthesis methods is that they obviate the need to manually compose a program and manually construct a proof of its correctness. One only has to formulate a precise problem specification; the synthesis method then mechanically constructs a correct solution. A serious drawback of these methods in practice, however, is that they suffer from the state explosion problem. To synthesize a concurrent system consisting of K sequential processes, each having N states in its local transition diagram, requires construction of the global productmachine having at least NK global states in general. This exponential growth in K makes it infeasible to synthesize systems composed of more than 2 or 3 processes. In this paper, we show how to synthesize concurrent systems consisting of many (i.e., a finite but arbitrarily large number K of) similar sequential processes. Our approach avoids construction of the global productmachine for K processes; instead, it constructs a two process productmachine for a single pair of generic sequential processes. The method is uniform in K, providing a simple template that can be instantiated for each process to yield a solution for any fixed K. The method is also illustrated on synchronization problems from the literature.
p2968
aVThis paper presents an overview of the programming language Modula3, and a more detailed description of its type system.
p2969
aVStaticallytyped programming languages allow earlier error checking, better enforcement of disciplined programming styles, and generation of more efficient object code than languages where all typeconsistency checks are performed at runtime. However, even in staticallytype languages, there is often the need to deal with data whose type cannot be known at compile time. To handle such situations safely, we propose to add a type Dynamic whose values are pairs of a value v and a type tag T where v has the type denoted by T. Instances of Dynamic are built with an explicit tagging construct and inspected with a typesafe typecase construct.\u000aThis paper is an exploration of the syntax, operational semantics, and denotational semantics of a simple language with the type Dynamic. We give examples of how dynamicallytyped values might be used in programming. Then we discuss an operational semantics for our language and obtain a soundness theorem. We present two formulations of the denotational semantics of this language and relate them to the operational semantics. Finally, we consider the implications of polymorphism and some implementation issues.
p2970
aVProgram dependence graphs are an important program representation technique for use in vectorization, parallelization and programming environments. We present a graph rewriting semantics for program dependence graphs and prove the equivalence between the program dependence graphs semantics and the operational semantics for programs. We then propose a framework for studying program transformations using program dependence graphs and a programming language calculus based on this rewriting semantics.
p2971
aVA new general notion of model for the polymorphic lambda calculus based on the simple idea of a universe, is proposed. Although impossible in nonconstructive set theory, the notion is unproblematic for constructive sets, yields completeness and initiality theorems, and can be used to unify and relate many different notions of model that have been proposed in the literature, including those that extend the basic calculus with additional features such as fixpoints or a type of all types. Moreover, the polymorphic lambda calculus and MartinLf type theory are related by a map of logics. A categorical and initial model semantics is given for the basic calculus and for richer calculi that extend the basic one with fixpoints or with a type of all types.
p2972
aVThe definitions of Conjunctive types and their subtype relation, as introduced by CoppoDezani, are extended to consider the conjunction as a partial mapping from pairs of types to types, and the subtype relation as a relation between finite sets of types and types. These extensions basically mean that only conjunctions of compatible types are allowed and that the subtype relation is more like, so to speak, the implication in propositional logic. We show that the basic properties of the typing system with conjunctive types are still true. The terms that have a type are exactly the terms that are convertible to a head normal form and it is possible to characterize the terms with normal form by means of the types that are derivable for them in the system. Furthermore, the type assignment defines the interpretation of terms in a very general class of models of the &lgr;calculus: the models that are based on an information system, as defined by Scott.
p2973
aVThe theory of term rewriting systems has important applications in abstract data type specifications and functional programming languages. We begin here a study of properties of systems that are not necessarily terminating, but allow for infinite derivations that have a limit. In particular, we give conditions for the existence of a limit and for its uniqueness.
p2974
aVWe introduce a programming paradigm in which statements are constraints over partial orders. A partial order programming problem has the form minimize u subject to u1 \u2292 v1, u2 \u2292 v2,  where u is the goal, and u1 \u2292 v1, u2 \u2292 v2,  is a collection of constraints called the program. A solution of the problem is a minimal value for u determined by values for u1, v1, etc. satisfying the constraints. The domain of values here is a partial order, a domain D with ordering relation \u2292.\u000aThe partial order programming paradigm has interesting properties:\u000a\u000aIt generalizes mathematical programming and also computer programming paradigms (logic, functional, and others) cleanly, and offers a foundation both for studying and combining paradigms.\u000aIt takes thorough advantage of known results for continuous functionals on complete partial orders, when the constraints involve expressions using only continuous and monotone operators. The semantics of these programs coincide with recent results on the relaxation solution method for constraint problems.\u000aIt presents a framework that may be effective in modeling, or knowledge representation, of complex systems.
p2975
aVThis paper addresses semantic and expressiveness issues for temporal logic programming and in particular for the TEMPLOG language proposed by Abadi and Manna. Two equivalent formulations of TEMPLOG's declarative semantics are given: in terms of a minimal Herbrand model and in terms of a least fixpoint. By relating these semantics to TEMPLOG's operational semantics, we prove the completeness of the resolution proof system underlying TEMPLOG's execution mechanism. To study TEMPLOG's expressiveness, we consider its propositional version. We show how propositional TEMPLOG programs can be translated into a temporal fixpoint calculus and prove that they can express essentially all regular properties of sequences.
p2976
aVUsing concepts from denotational semantics, we have produced a very simple compiler that can be used to compile standard programming languages and produces object code as efficient as that of production compilers. The compiler is based entirely on sourcetosource transformations performed on programs that have been translated into an intermediate language resembling the lambda calculus. The output of the compiler, while still in the intermediate language, can be trivially translated into machine code for the target machine. The compilation by transformation strategy is simple: the goal is to remove any dependencies on the intermediate language semantics that the target machine cannot implement directly. Frontends have been written for Pascal, BASIC, and Scheme and the compiler produces code for the MC68020 microprocessor.
p2977
aVWe implemented a continuationpassing style (CPS) code generator for ML. Our CPS language is represented as an ML datatype in which all functions are named and most kinds of illformed expressions are impossible. We separate the code generation into phases that rewrite this representation into eversimpler forms. Closures are represented explicitly as records, so that closure strategies can be communicated from one phase to another. No stack is used. Our benchmark data shows that the new method is an improvement over our previous, abstractmachine based code generator.
p2978
aVCopy elimination is an important optimization for compiling functional languages. Copies arise because these languages lack the concepts of state and variable; hence updating an object involves a copy in a naive implementation. Copies are also possible if proper targeting has not been carried out inside functions and across function calls. Targeting is the proper selection of a storage area for evaluating an expression. By abstracting a collection of functions by a target operator, we compute targets of function bodies that can then be used to define an optimized interpreter to eliminate copies due to updates and copies across function calls. The language we consider is typed lambda calculus with higherorder functions and special constructs for array operations. Our approach can eliminate copies in divide and conquer problems like quicksort and bitonic sort that previous approaches could not handle.\u000aWe also present some results of implementing a compiler for a single assignment language called SAL on some small but tough programs. Our results indicate that it is possible to approach a performance comparable to imperative languages like Pascal.
p2979
aVThis paper concerns the algebraic specification of abstract data types. It introduces and motivates the recentlydeveloped framework of unified algebras, and provides a practical notation for their modular specification. It also compares unified algebras with the wellknown framework of ordersorted algebras, which underlies the OBJ specification language.
p2980
aVCircular attribute grammars appear in many data flow analysis problems. As one way of making the notion useful, an automatic translation of circular attribute grammars to equivalent noncircular attribute grammars is presented. It is shown that for circular attribute grammars that arise in many data flow analysis problems, the translation does not increase the asymptotic complexity of the semantic equations. Therefore, the translation may be used in conjunction with any evaluator generator to automate the development of efficient data flow analysis algorithms. As a result, the integration of such algorithms with other parts of a compiler becomes easier.
p2981
aVWe present a new algorithm for computing interprocedural aliases due to passing parameters by reference. This algorithm runs in O(N2+NE) time and, when combined with algorithms for aliasfree, flowinsensitive dataflow problems, yields algorithms for solution of the general flowinsensitive problems that also run in O(N2+NE) time.
p2982
aVThis paper presents type classes, a new approach to adhoc polymorphism. Type classes permit overloading of arithmetic operators such as multiplication, and generalise the \u201ceqtype variables\u201d of Standard ML. Type classes extend the Hindley/Milner polymorphic type system, and provide a new approach to issues that arise in objectoriented programming, bounded type quantification, and abstract data types. This paper provides an informal introduction to type classes, and defines them formally by means of type inference rules.
p2983
aVStrongly typed languages with records may have inclusion rules so that records with more fields can be used instead of records with less fields. But these rules lead to a global treatment of record types as a special case. We solve this problem by giving an ordinary status to records without any ad hoc assertions, replacing inclusion rules by extra information in record types. With this encoding ML naturally extends its polymorphism to records but any other host language will also transmit its power.
p2984
aVWe define in this paper a notion of realizability for the Calculus of Constructions. The extracted programs are terms of the Calculus that do not contain dependent types. We introduce a distinction between informative and noninformative propositions. This distinction allows the removal of the \u201clogical\u201d part in the development of a program. We show also how to use our notion of realizability in order to interpret various axioms like the axiom of choice or the induction on integers. A practical example of development of program is given in the appendix.
p2985
aVWe study the complexity of type inference for a core fragment of ML with lambda abstraction, function application, and the polymorphic let declaration. Our primary technical tool is the unification problem for a class of \u201cpolymorphic\u201d type expressions. This form of unification, which we call polymorphic unification, allows us to separate a combinatorial aspect of type inference from the syntax of ML programs. After observing that ML typing is in DEXPTIME, we show that polymorphic unification is PSPACE hard. From this, we prove that recognizing the typable core ML programs is also PSPACE hard. Our lower bound stands in contrast to the common belief that typing ML programs is \u201cefficient,\u201d and to practical experience which suggests that the algorithms commonly used for this task do not slow compilation substantially.
p2986
aVIn this paper, I introduce a new formal system, ACCL, based on Curien's Categorical Combinators [Cur86a]. I show that ACCL has properties not possessed by Curien's original combinators that make it particularly appropriate as the basis for implementation and analysis of a wide range of reduction schemes using shared environments, closures, or &lgr;terms. As an example of the practical utility of this formalism, I use it to specify a simple lazy interpreter for the &lgr;calculus, whose correctness follows trivially from the properties of ACCL.\u000aI then describe a labeled variant of ACCL, ACCLL, which can be used as a tool to determine the degree of \u201claziness\u201d possessed by various &lgr;reduction schemes. In particular, ACCLL is applied to the problem of optimal reduction in the &lgr;calculus. A reduction scheme for the &lgr;calculus is optimal if the number of redex contractions that must be performed in the course of reducing any &lgr;term to a normal form (if one exists) is guaranteed to be minimal. Results of Lvy [Lv78,Lv80] showed that for a natural class of reduction strategies allowing shared redexes, optimal reductions were, at least in principle, possible. He conjectured that an optimal reduction strategy might be realized in practice using shared closures and environments as well as shared &lgr;terms. I show, however, using ACCLL, a practical optimal reduction scheme for arbitrary &lgr;terms using only shared environments, closures, or terms is unlikely to exist.
p2987
aVIn typed objectoriented languages the subtype relation is typically based on the inheritance hierarchy. This approach, however, leads either to insecure typesystems or to restrictions on inheritance that make it less flexible than untyped Smalltalk inheritance. We present a new typed model of inheritance that allows more of the flexibility of Smalltalk inheritance within a staticallytyped system. Significant features of our analysis are the introduction of polymorphism into the typing of inheritance and the uniform application of inheritance to objects, classes and types. The resulting notion of type inheritance allows us to show that the type of an inherited object is an inherited type but not always a subtype.
p2988
aVThis paper describes a type system for Smalltalk that is typesafe, that allows most Smalltalk programs to be typechecked, and that can be used as the basis of an optimizing compiler.
p2989
aVAbstract interpretation and projection analysis are two techniques for finding out information about lazy functional programs. Two typical uses of these techniques are speeding up sequential implementations, and the introduction of parallelism into parallel implementations.\u000aOur main result is the proof of a relationship between a certain class of projections and a certain class of abstract interpretations.\u000aOne of the claims of projection analysis is that it can find out information about headstrictness, whilst abstract interpretation cannot. We show that there are at least two intuitive notions of headstrictness, and that one of them can be determined using abstract interpretation.
p2990
aVWe present a static analysis method for determining aliasing and lifetime of dynamically allocated data in lexically scoped, higherorder, strict and polymorphic languages with first class continuations. The goal is validate program transformations that introduce imperative constructs such as destructive updatings, stack allocations and explicit deallocations in order to reduce the runtime memory management overhead. Our method is based on an operational model of higher order functional programs from which we construct statically computable abstractions using the abstract interpretation framework. Our method provides a solution to a problem left open [Hudak 86]: determining isolation of data in the case of higher order languages with structured data.
p2991
aVUse of strictness analysis in parallel evaluation and optimization of lazy functional languages is well known. The first formal treatment of strictness analysis appeared in Mycroft's seminal work which however dealt only with flat domains. Unlike flat domains, strictness analysis on nonflat domains involves determining how a function transforms a demand (degree of strictness) on its output into a demand on its arguments. Solutions to this problem in its full generality require large domains and appear both complex and expensive to implement. However, only two kinds of demands arise naturally in lazy normalization of terms, viz., edemand (normal form needed) and ddemand (root stable or head normal form needed). Based on this observation, we identify three useful forms of strictness for nonflat domains  ee, dd and de. Each of these three forms of strictness play an important role in evaluation of functional programs. Specifically, ee strictness is used for transforming callbyneed to callbyvalue and dd strictness is useful in repairing violations of strong sequentiality of equational programs as well as in a critical optimization step used in rewriting implementations of such languages. We present intuitively simple methods to compute them. Our methods are computationally efficient as they are based on small domains (1 point for ee and dd and 2 points for de). They are powerful enough to extract all useful strictness information in practice and are general enough to handle functions defined by rewrite rules. We are able to reason about all user defined data types within a single framework and also handle polymorphism.
p2992
aVOur exhaustive and incremental hybrid data flow analysis algorithms, based on iteration and elimination techniques, are designed for incremental update of a wide variety of monotone data flow problems in response to source program changes. Unlike previous incremental iterative methods, this incremental algorithm efficiently computes precise and correct solutions. We give theoretical results on the imprecision of restarting iteration for incremental update by fixed point iteration which provided motivation for our algorithm design. Described intuitively, the main algorithm idea is to factor the data flow solution on strong connected components of the flow graph into local and external parts, solving for the local parts by iteration and propagating these effects on the condensation of the flow graph to obtain the entire data flow solution. The incremental hybrid algorithm reperforms those algorithm steps affected by the program changes.
p2993
aVThe notion of Cartesian closure on a set of unifiers has been used to define approximations of the least models of logic programs. Such approximations, often called types, are not known to be recursive. In this paper, we use Cartesian closure to define a similar, but more accurate, approximation. The main result proves that our approximation is not only recursive, but that it can be finitely represented in the form of a cyclic term graph. This explicit representation can be used as a starting point for logic program analyzers.
p2994
aVLanguage designers and implementors have avoided specifying and preserving the meaning of programs that produce errors. This is apparently because being forced to preserve error behavior severely limits the scope of program optimization, even for correct programs. However, preserving error behavior is desirable for debugging, and error behavior must be preserved in any language that permits usergenerated exceptions.\u000aThis paper presents a technique for preserving the power of general program transformations in the presence of a rich collection of distinguishable error values. This is accomplished by introducing an annotation, \u201cSafe\u201d, to mark occurrences of functions that cannot produce errors. Succinct and general algebraic laws can be expressed using Safe, giving program transformations in a language with many error values the same power and generality as program transformations in a language with only a single error value.
p2995
aVWe need a programming model that combines the advantages of the synchronous and asynchronous parallel styles. Synchronous programs are determinate (thus easier to reason about) and avoid synchronization overheads. Asynchronous programs are more flexible and handle conditionals more efficiently.\u000aHere we propose a programming model with the benefits of both styles. We allow asynchronous threads of control but restrict sharedmemory accesses and other side effects so as to prevent the behavior of the program from depending on any accidents of execution order that can arise from the indeterminacy of the asynchronous process model.\u000aThese restrictions may be enforced either dynamically (at run time) or statically (at compile time). In this paper we concentrate on dynamic enforcement, and exhibit an implementation of a parallel dialect of Scheme based on these ideas. A single successful execution of a parallel program in this model constitutes a proof that the program is free of race conditions (for that particular set of input data).\u000aWe also speculate on a design for a programming language using static enforcement. The notion of distinctness is important to proofs of noninterference. An appropriately designed programming language must support such concepts as \u201call the elements of this array are distinct,\u201d perhaps through its type system.\u000aThis parallel programming model does not support all styles of parallel programming, but we argue that it can support a large class of interesting algorithms with considerably greater efficiency (in some cases) than a strict SIMD approach and considerably greater safety (in all cases) than a fullblown MIMD approach.
p2996
aVThis paper presents a new and very rich class of (concurrent) programming languages, based on the notion of computing with partial information, and the concomitant notions of consistency and entailment.1 In this framework, computation emerges from the interaction of concurrently executing agents that communicate by placing, checking and instantiating constraints on shared variables. Such a view of computation is interesting in the context of programming languages because of the ability to represent and manipulate partial information about the domain of discourse, in the context of concurrency because of the use of constraints for communication and control, and in the context of AI because of the availability of simple yet powerful mechanisms for controlling inference, and the promise that very rich representational/programming languages, sharing the same set of abstract properties, may be possible.\u000aTo reflect this view of computation, [Sar89] develops the cc family of languages. We present here one member of the family, cc(\u2193, \u2192) (pronounced \u201ccc with Ask and Choose\u201d) which provides the basic operations of blocking Ask and atomic Tell and an algebra of behaviors closed under prefixing, indeterministic choice, interleaving, and hiding, and provides a mutual recursion operator. cc(\u2193, \u2192) is (intentionally!) very similar to Milner's CCS, but for the radically different underlying concept of communication, which, in fact, provides a general\u2014and elegant\u2014alternative approach to \u201cvaluepassing\u201d in CCS. At the same time it adequately captures the framework of committed choice concurrent logic programming languages. We present the rules of behavior for cc agents, motivate a notion of \u201cvisible action\u201d for them, and develop the notion of ctrees and reactive congruence analogous to Milner's synchronization trees and observational congruence. We also present an equational characterization of reactive congruence for Finitary cc(\u2193, \u2192).
p2997
aVWe present an algorithm for lambda expression reduction that avoids any copying that could later cause duplication of work. It is optimal in the sense defined by Lvy. The basis of the algorithm is a graphical representation of the kinds of commonality that can arise from substitutions; the idea can be adapted to represent other kinds of expressions besides lambda expressions. The algorithm is also well suited to parallel implementations, consisting of a fixed set of local graph rewrite rules.
p2998
aVTwo key ideas in garbage collection are generational collection and conservative pointerfinding. Generational collection and conservative pointerfinding are hard to use together, because generational collection is usually expressed in terms of copying objects, while conservative pointerfinding precludes copying. We present a new framework for defining garbage collectors. When applied to generational collection, it generalizes the notion of younger/older to a partial order. It can describe traditional generational and conservative techniques, and lends itself to combining different techniques in novel ways. We study in particular two new garbage collectors inspired by this framework. Both these collectors use conservative pointerfinding. The first one is based on a rewrite of an existing traceandsweep collector to use one level of generation. The second one has a single parameter, which controls how objects are partitioned into generations: the value of this parameter can be changed dynamically with no overhead. We have implemented both collectors and present measurements of their performance in practice.
p2999
aVAn instruction or a set of instructions can be considered time critical if their execution is required to free up a resource. Time critical instructions might be used to make shared resources such as registers more quickly available for reuse; or they might be used for real time computations, portions of which are critical for the operation of some piece of equipment. In this paper we present a polynomial time algorithm for optimally scheduling instructions with or without time critical constraints on RISC machines such as the IBM 801, the Berkeley RISC machine, and the HP Precision Architecture. We also show that in the absence of time critical constraints, the greedy algorithm always produces a schedule for a target machine with multiple identical pipelines that has a length less than twice that of an optimal schedule. The behavior of the greedy algorithm is of interest because, as we show, the instruction scheduling problem becomes NPhard for arbitrary length pipelines, even when the basic block of code being input consists of only several independent streams of straightline code, and there are no timecritical constraints. Finally, we present the first correct proofs that the problem becomes NPhard even for small pipelines, no timecritical constraints, and input of several independent streams of straightline code if there is only a single register or if there is a bus constraint.
p3000
aVIndexing Prolog clauses is an important optimization step that reduces the number of clauses on which unification will be performed and can avoid the pushing of a choice point. It is quite desirable to increase the number of functors used in indexing as this can considerably reduce the size of the filtered set. However this can cause an enormous increase in running time if indexing is done naively. This paper describes a new technique for indexing that utilizes all the functors in a clausehead. More importantly, in spite of using all the functors, this technique is still able to quickly select relevant clauseheads at run time. This is made possible primarily by a finitestate automaton that guides the indexing process. The automaton is constructed at compile time by preprocessing all the clauseheads.
p3001
aVIn this paper, a new fairness notion is proposed for languages with multiparty interactions as the sole interprocess synchronization and communication primitive. The main advantage of this fairness notion is the elimination of starvation occurring solely due to race conditions (i.e., ordering of independent actions). Also, this is the first fairness notion for such languages which is fullyadequate with respect to the criteria presented in [AFK88]. The paper defines the notion, proves its properties, and presents examples of its usefulness.
p3002
aVThe purpose of this paper is to discuss the relationship between the interpretations of nondeterministic programs using the upper (total correctness) powerdomain on the one hand and the lower (partial correctness) powerdomain on the other. It is shown that there is a close semantic relationship between these two interpretations which suggests the formulation of a new operator called the mixed powerdomain. It is shown that the mixed powerdomain has many pleasing domaintheoretic and algebraic properties. The mixed powerdomain is closely related to new powerdomains which have been recently investigated as mathematical models of partial information in databases. Some of the basic intuitions captured by such structures may have uses for the specification of nondeterministic computations. The paper includes a sample nondeterministic programming language and a semantics using the mixed powerdomain.
p3003
aVIn this paper we investigate generalizations of Kahn's principle to nondeterministic dataflow networks. Specifically, we show that for the class of \u201coraclizable\u201d networks a semantic model in which networks are represented by certain sets of continuous functions is fully abstract and has the fixedpoint property. We go on to show that the oraclizable networks are the largest class representable by this model, and are a proper superclass of the networks implementable with the infinity fair merge primitive. Finally, we use this characterization to show that infinity fair merge networks and oraclizable networks are proper subclasses of the networks with EgliMilner monotone inputoutput relations.
p3004
aVWe consider monotone input/output automata, which model a usefully large class of dataflow networks of indeterminate (or nonfunctional) processes. We obtain a characterization of the relations computable by these automata, which states that a relation R : X \u2192 2Y (viewed as a \u201cnondeterministic function\u201d) is the input/output relation of an automaton iff there exists a certain kind of Scott domain D, a continuous function F : X \u2192 [D \u2192 Y] and a continuous function G : X \u2192 P(D), such that R() = F()\u2020(G()) for all inputs  &egr; X. Here P denotes a certain powerdomain operator, and \u2020 denotes the pointwise extension to the powerdomain of a function on the underlying domain. An attractive feature of this result is that it specializes to two subclasses of automata, determinate automata, for which G is singlevalued, and semideterminate automata, for which G is a constant function. A corollary of the latter result is the impossibility of implementing \u201cangelic merge\u201d by a network of determinate processes and \u201cinfinityfair merge\u201d processes.
p3005
aVIn earlier work, we used a typed function calculus, XML, with dependent types to analyze several aspects of the Standard ML type system. In this paper, we introduce a refinement of XML with a clear compiletime/runtime phase distinction, and a direct compiletime type checking algorithm. The calculus uses a finer separation of types into universes than XML and enforces the phase distinction using a nonstandard equational theory for module and signature expressions. While unusual from a typetheoretic point of view, the nonstandard equational theory arises naturally from the wellknown Grothendieck construction on an indexed category.
p3006
aVWe present a functional language featuring a form of dynamic overloading akin to message passing in object oriented languages. We give a dynamic semantics describing a nondeterministic evaluation, as well as a type discipline (static semantics) supporting type inference. The type system ensures that a welltyped program has a correct execution, unique up to a semantic equivalence relation, and allows this execution to proceed deterministically, while resolving overloading at runtime.
p3007
aVThe &lgr;&sgr;calculus is a refinement of the &lgr;calculus where substitutions are manipulated explicitly. The &lgr;&sgr;calculus provides a setting for studying the theory of substitutions, with pleasant mathematical properties. It is also a useful bridge between the classical &lgr;calculus and concrete implementations.
p3008
aVWe present a new approach to dynamic typing in a static framework. Our main innovation is the use of structural subtyping for dynamic types based on the idea that possible dynamic typing as a property should be inherited by objects of all types. Two properties of our system set it apart from existing systems which combine static and dynamic typing: all tagging and checking takes place via implicit coercions, and the semantics of dynamic typing is representation independent. The latter property leads to a significant increase in expressive power\u2014for instance it allows us to define a general callbyvalue fixpoint operator.\u000aThe resulting system\u2014which we call quasistatic typing\u2014is a seamless merger of static and dynamic typing. The system divides programs into three categories: welltyped, illtyped and ambivalent programs. Illtyped programs contain expressions that are guaranteed to go wrong. Runtime checking is limited to doubtful function applications in ambivalent programs. Conceptually, quasistatic typing takes place in an unusual twophase process\u2014a first phase infers types and coercions and a second plausibility checking phase identifies illtyped programs. The typing rules allow minimal typing judgements and plausibility checking can be characterized as simplification via a canonical set of rewrite rules. The two phase process can therefore be implemented with a one pass algorithm.
p3009
aVThe programming language Scheme contains the control construct call/cc that allows access to the current continuation (the current control context). This, in effect, provides Scheme with firstclass labels and jumps. We show that the wellknown formulaeastypes correspondence, which relates a constructive proof of a formula &agr; to a program of type &agr;, can be extended to a typed Idealized Scheme. What is surprising about this correspondence is that it relates classical proofs to typed programs. The existence of computationally interesting \u201cclassical programs\u201d \u2014programs of type &agr;, where &agr; holds classically, but not constructively \u2014 is illustrated by the definition of conjunctively, disjunctive, and existential types using standard classical definitions. We also prove that all evaluations of typed terms in Idealized Scheme are finite.
p3010
aVIn [As87] a correspondence between the subset of Linear Logic [Gi86] involving the conjunctive tensor product only and Place/Transition Petri Nets [Rei85] is established. In this correspondence, formulae are regarded as distributed states and provable sequents are computations in the net. Developing this idea, MartOliet and Meseguer [MaM89] have suggested that all the other computations of Linear Logic, which do not have an immediate correspondence with Petri Nets, should be regarded as \u201cgedanken\u201d or idealized processes, providing a richer language for the specification and the study of properties of distributed computations. In this paper we apply this program to the fundamental connective of linear implication. We prove that the introduction of linear implication allows us to observe the net at a lower, more decentralized level of atomicity, where the preemption of each resource needed for the firing of a transition is represented as a separate move. We give a conservative theorem relating computations at different levels of abstraction. The categorical semantics establishes a tight correspondence among Petri nets, monoidal closed categories and tensor theories, reminiscent of the well known relation among functional languages, Cartesian closed categories and intuitionistic logic [LS86]. The identification of computations in the categorical model naturally suggests the generalisation of the notion of process [DMM89] at the lower level of atomicity.
p3011
aVIn this paper we study concurrent, asynchronous processes and functions on them which can be programmed using the (full) unfair or the fair merge operations. The main result is a normal form theorem for these (relatively) \u201ccomputable process functions\u201d which implies that although they can be very complex when viewed as classical setfunctions, they are all \u201cloosely implementable\u201d in the sense of Park [7]. We also announce a variation and a substantial strengthening of the main \u201ctransfer principle\u201d of [4] which have applications to the semantics and logic of programming languages with interactive (deterministic) or concurrent (nondeterministic) constructs.
p3012
aVWe introduce a new kind of abstract machine based on the chemical metaphor used in the &Ggr; language of Bantre & al. States of a machine are chemical solutions where floating molecules can interact according to reaction rules. Solutions can be stratified by encapsulating subsolutions within membranes that force reactions to occur locally. We illustrate the use of this model by describing the operational semantics of the TCCS and CCS process calculi. We also show how to extract a higherorder concurrent &lgr;calculus out of the basic concepts of the chemical abstract machine.
p3013
aVWe propose a new kind of programming language, with the following features:\u000a\u000aa simple graph rewriting semantics,\u000aa complete symmetry between constructors and destructors,\u000aa type discipline for deterministic and deadlockfree (microscopic) parallelism.\u000a\u000aInteraction nets generalize Girard's proof nets of linear logic and illustrate the advantage of an integrated logic approach, as opposed to the external one. In other words, we did not try to design a logic describing the behaviour of some given computational system, but a programming language for which the type discipline is already (almost) a logic.\u000aIn fact, we shall scarcely refer to logic, because we adopt a nave and pragmatic style. A typical application we have in mind for this language is the design of interactive softwares such as editors or window managers.
p3014
aVThis paper discusses the phenomenon of method specialization in objectoriented programming languages. A typed function calculus of objects and classes is presented, featuring method specialization when methods are added or redefined. The soundness of the typing rules (without subtyping) is suggested by a translation into a more traditional calculus with recursivelydefined record types. However, semantic questions regarding the subtype relation on classes remain open.
p3015
aVThis paper explores the use monads to structure functional\u000aprograms. No prior knowledge of monads or category theory is\u000arequired.\u000a\u000aMonads increase the ease with which programs may be modified.\u000aThey can mimic the effect of impure features such as exceptions,\u000astate, and continuations; and also provide effects not easily\u000aachieved with such features. The types of a program reflect which\u000aeffects occur.\u000a\u000aThe first section is an extended example of the use of monads. A\u000asimple interpreter is modified to support various extra features:\u000aerror messages, state, output, and nondeterministic choice. The\u000asecond section describes the relation between monads and the\u000acontinuationpassing style. The third section sketches how monads\u000aare used in a compiler for Haskell that is written in Haskell.
p3016
aVJade is a language designed to support coarsegrain parallelism on both shared and distributed addressspace machines. Jade is dataoriented: a Jade programmer simply augments a sequential imperative program with declarations specifying how the program accesses data. A Jade implementation dynamically interprets the access specification to execute the program concurrently while enforcing the program's data dependence constraints, thus preserving the sequential semantics.\u000aThis paper describes the Jade constructs and defines both a serial and a parallel formal operational semantics for Jade. The paper proves that the two semantics are equivalent.
p3017
aVWe present a set of concurrency primitives for Standard ML. We define these by giving the transitional semantics of a simple language. We prove that our semantics preserves the expected behaviour of sequential programs. We also show that we can define stores as processes, such that the representation has the same behaviour as a direct definition. These proofs are the first steps towards integrating our semantics with the full definition of Standard ML.
p3018
aVTraditional optimization techniques for sequential programs are not directly applicable to parallel programs where concurrent activities may interfere with each other through shared variables. New compiler techniques must be developed to accommodate features found in parallel languages. In this paper, we use abstract interpretation to obtain useful properties of programs, e.g., side effects, data dependences, object lifetime and concurrent expressions, for a language that supports firstclass functions, pointers, dynamic allocations and explicit parallelism through cobegin. These analyses may facilitate many applications, such as program optimization, parallelization, restructuring, memory management, and detecting access anomalies.\u000aOur semantics is based on a labeled transition system and is instrumented with procedure strings to record the procedural/concurrency movement along the program interpretation. We develop analyses in both concrete domains and abstract domains, and prove the correctness and termination of the abstract interpretation.
p3019
aVWe present a taxonomy of languages for multiparty interaction, which covers all proposals of which we are aware. Based on this taxonomy, we present a comprehensive analysis of the computational complexity of the multiparty interaction implementation problem, the problem of scheduling multiparty interactions in a given execution environment.
p3020
aVPolymorphic record calculi have recently attracted much attention as a typed foundation for objectoriented programming. This is based on the fact that a function that selects a field l of a record can be given a polymorphic type that enables it to be applied to various records containing a field l. Recent studies have established techniques to develop an MLstyle type inference algorithm for such a polymorphic type system. There seems to be, however, no established method to compile an MLstyle polymorphic record calculus into efficient code. The purpose of this paper is to present one such method. We define a polymorphic record calculus as an extension of Damas and Miler's proof system for ML. For this calculus, we define an implementation calculus  where records are represented as arrays of (references to) values and field selection is performed by direct indexing. To represent polymorphic field selection, the implementation calculus contains an abstraction mechanism over indexes. We then develop an algorithm to translate the polymorphic record calculus into the implementation calculus by refining a type inference algorithm; it simultaneously computes a principal type scheme in the polymorphic record calculus and a correct implementation term in the implementation calculus. The type inference is shown to be sound and complete in the sense of DamasMilner's algorithm for ML. Moreover, the polymorphic type system is shown to be sound with respect to an operational semantics of the translated terms in the implementation calculus.
p3021
aVWe show that any functional language with record extension possesses record concatenation for free. We exhibit a translation from the latter into the former. We obtain a type system for a language with record concatenation by composing the translation with typechecking in a language with record extension. We apply this method to a version of ML with a record extension and obtain an extension of ML with either asymmetric or symmetric concatenation. The latter extension is simple, flexible and has a very efficient type inference algorithm in practice. Concatenation together with removal of fields needs one more construct than extension of records. It can be added to the version of ML with record extension. However, many typed languages with record cannot type such a construct. The method still applies to them, producing type systems for record concatenation without removal of fields. Object systems also benefit of the encoding which shows that multiple inheritance does not actually require the concatenation of records but only their extension.
p3022
aVThis paper presents a program transformation that allows languages with polymorphic typing (e.g. ML) to be implemented with unboxed, multiword data representations. The transformation introduces coercions between various representations, based on a typing derivation. A prototype ML compiler utilizing this transformation demonstrates important speedups.
p3023
aVUnder the DamasMilner type discipline for functional languages, every expression has principal type, if it elaborates at all. In the type discipline for ML Modules, a signature expression has a principal signature, if it elaborates at all. However, while functions can be higherorder in ML, parameterised modules in ML are firstorder only. We present a type discipline for a skeletal higherorder module language which has principal signatures. Sharing and multiple views of structures are handled in a manner which is compatible with the semantics of the firstorder ML modules.
p3024
aVThis paper contains a full treatment of isomorphic types for languages equipped with an ML style polymorphic type inference mechanism. Surprisingly enough the results obtained contradict the commonplace feeling that (the core of) ML is a subset of second order &lgr;calculus: we can provide an isomorphism of types that holds in the core ML language, but not in second order &lgr;calculus. This new isomorphism not only allows to provide a complete (and decidable) axiomatisation of all the type isomorphic in ML style languages, a relevant issue for the type as specifications paradigm in library searches, but also suggest a natural extension that in a sense completes the typeinference mechanism in ML. This extension is easy to implement and allows to get a further insight in the nature of the let polymorphic construct.
p3025
aVAttribute grammars have been used for many languageoriented tasks, including the formal description of semantics and the implementation of compilation tasks from simple type checking through code generation. Despite their successful use, attribute grammars have some disadvantages, including the monolithic nature of the grammar and the fixed factoring of all attribute descriptions by a single set of grammar productions. Attribute pattern sets provide a more expressive attribution system by using pattern matching, instead of grammar productions, to perform case analysis. Attribute pattern sets can be implemented in terms of attribute grammars in a way that maintains the dependency structure of the attribute system, making it straightforward to convert many of the practical results from attribute grammar theory to similar results for attribute pattern sets.
p3026
aVLamping discovered an optimal graphreduction implementation of the &lgr;calculus. Simultaneously, Girard invented the geometry of interaction, a mathematical foundation for operational semantics. In this paper, we connect and explain the geometry of interaction and Lamping's graphs. The geometry of interaction provides a suitable semantic basis for explaining and improving Lamping's system. On the other hand, graphs similar to Lamping's provide a concrete representation of the geometry of interaction. Together, they offer a new understanding of computation, as well as ideas for efficient and correct implementations.
p3027
aVThis paper introduces Composable Attribute Grammars (CAGs), a formalism that extends classical attribute grammars to allow for the modular composition of translation specifications and of translators. CAGs bring to complex translator writing systems the same benefits of modularity found in modern programming languages, including comprehensibility, reusability, and incremental metacompilation.\u000aA CAG is built from several smaller component AGs, each of which solves a particular subproblem, such as name analysis or register allocation. A component AG is based upon a simplified phrasestructure that reflects the properties of its subproblem rather than the phrasestructure of the source language. Different component phrasestructures for various subproblems  are combined by mapping them into a phrasestructure for the source language. Both input and output attributes can be associated with the terminal symbols of a component AG. Output attributes enable the results of solving a subproblem to be distributed back to anywhere that originally contributed part of the subproblem, e.g. transparently distributing the results of global name analysis back to every symbolic reference in the source program.\u000aAfter introducing CAGs by way of an example, we provide a formal definition of CAGs and their semantics. We describe a subclass of CAGs and their semantics. We describe a subclass of CAGs, called separable CAGs, that have favorable implementation properties. We discuss the novel aspects of CAGs, compare them to other  proposals for inserting modularity into attribute grammars, and relate our experience using CAGs in the Linguist translatorwriting system.
p3028
aVLR parsing techniques have long been studied as efficient and powerful methods for processing context free languages. A linear time algorithm for recognizing languages representable by LR(k) grammars has long been known. Recognizing substrings of a contextfree language is at least as hard as recognizing full strings of the language, as the latter problem easily reduces to the former. In this paper we present a linear time algorithm for recognizing substrings of LR(k) languages, thus showing that the substring recognition problem for these languages is no harder than the full string recognition problem. An interesting data structure, the Forest Structured Stack, allows the algorithm to track all possible parses of a substring without loosing the efficiency of the original LR parser. We present the algorithm, prove its correctness, analyze its complexity, and mention several applications that have been constructed.
p3029
aVThe notion of dominators is generalized to include multiplevertex dominators in addition to singlevertex dominators. A multiplevertex dominator of a vertex is a group of vertices that collectively dominate the vertex. Existing algorithms compute immediate singlevertex dominators, and an algorithm for computing immediate multiplevertex dominators is presented in this paper. The immediate dominator information is expressed in the form of a directed acyclic graph referred to as the dominator DAG or the DDAG. The generalized dominator set of any vertex dominators of the vertex, can be computed from the DDAG. The singlevertex dominator information restricts the propagation of loop invariant statements and array bound checks out of loops. Generalized dominator information avoids  these restrictions. In addition, it can be used to identify natural loops and improve the existing optimization algorithm for code hoisting. The dual notion of generalized postdominators can be used for computing control dependences and automatic generation of compact test suites for programs.
p3030
aVCompiler generation is often emphasized as being the most important application of partial evaluation. But most of the larger practical applications have, to the best of our knowledge, been outside this field. Expecially, no one has generated compilers for languages other than small languages. This paper describes a large application of partial evaluation where a realistic compiler was generated for a strongly typed lazy functional language. The language, that was called BAWL, was modeled after the language in Bird and Wadler [BW88] and is a combinator language with pattern matching, guarded alternatives, local definitions and list comprehensions. The paper describes the most important techniques used, especially the binding time improvements needed in order to get small and efficient target programs. Finally, the performance of the compiler is compared with two compilers for similar languages: Miranda and LML.
p3031
aVA polymorphic function is parametric if it has uniform behavior for all type parameters. This property is useful when writing, reasoning about, and compiling functional programs.\u000aWe show how to syntactically define and reason about parametricity in a language with intersection types and bounded polymorphism. Within this framework, parametricity is subtyping, and reasoning about parametricity becomes reasoning about the welltypedness of terms. This work also demonstrates the expressiveness of languages that combine intersection types and bounded polymorphism.
p3032
aVWe study the complexity of type inference for programming languages with subtypes. There are three language variations that effect the problem: (i) basic functions may have polymorphic or more limited types, (ii) the subtype hierarchy may be fixed or vary as a result of subtype declarations within a program, and (iii) the subtype hierarchy may be an arbitrary partial order or may have a more restricted form, such as a tree or lattice. The naive algorithm for infering a most general polymorphic type, undervariable subtype hypotheses, requires deterministic exponential time. If we fix the subtype ordering, this upper bound grows to nondeterministic exponential time. We show that it is NPhard to decide whether a lambda term has a type with respect to a fixed subtype hierarchy (involving only atomic type names). This lower bound applies to monomorphic or polymorphic languages. We give PSPACE upper bounds for deciding polymorphic typability if the subtype hierarchy has a lattice structure or the subtype hierarchy varies arbitrarily. We also give a polynomial time algorithm for the limited case where there are of no function constants and the type hierarchy is either variable or any fixed lattice.
p3033
aVF\u2264 is a typed &lgr;calculus with subtyping and bounded secondorder polymorphism. First proposed by Cardelli and Wegner, it has been widely studied as a core calculus for type systems with subtyping.\u000aCurien and Ghelli proved the partial correctness of a recursive procedure for computing minimal types of F\u2264 terms and showed that the termination of this procedure is equivalent to the termination of this procedure is equivalent to the termination of its major component, a procedure for checking the subtype relation between F\u2264 types. This procedure was thought to terminate on all inputs, but the discovery of a subtle bug in a purported proof of this claim recently reopened the question of the decidability of  subtyping, and hence of typechecking.\u000aThis question is settled here in the negative, using a reduction from the halting problem for twocounter Turing machines to show that the subtype relation of F\u2264 is undecidable.
p3034
aVOne of the major challenges in denotational semantics is the construction of fully abstract models for sequential programming languages. For the past fifteen years, research on this problem has focused on developing models for PCF, an idealized functional programming language based on the typed lambda calculus. Unlike most practical languages, PCF has no facilities for observing and exploiting the evaluation order of arguments in procedures. Since we believe that such facilities are crucial for understanding the nature of sequential computation, this paper focuses on a sequential extension of PCF (called SPCF) that includes two classes of control operators: error generators enable us to construct a fully abstract model for SPCF that  interprets higher types as sets of errorsensitive functions instead of continuous functions. The errorsensitve functions form a Scott domain that is isomorphic to a domain of decision trees. We believe that the same construction will yield fully abstract models for functional languages with different control operators for observing the order of evaluation.
p3035
aVWe present a functional interpretation of classical linear logic based on the concept of linear continuations. Unlike their nonlinear counterparts, such continuations lead to a model of control that does not inherently impose any particular evaluation strategy. Instead, such additional structure is expressed by admitting closely controlled copying and discarding of continuations. We also emphasize the importance of classicality in obtaining computationally appealing categorical models of linear logic and propose a simple \u201ccoreflective subcategory\u201d interpretation of the modality \u201c!\u201d.
p3036
aVWe describe a method for using abstraction to reduce the complexity of temporal logic model checking. The basis of this method is a way of constructing an abstract model of a program without ever examining the corresponding unabstracted model. We show how this abstract model can be used to verify properties of the original program. We have implemented a system based on these techniques, and we demonstrate their practicality using a number of examples, including a pipelined ALU circuit with over 101300 states.
p3037
aVDistributed symbolic computations involve the existence of remote references allowing an object, local to a processor, to designate another object located on another processor. To reclaim inaccessible objects is the non trivial task of a distributed Garbage Collector (GC). We present in this paper a new distributed GC algorithm which (i) is faulttolerant, (ii) is largely independent of how a processor garbage collects its own data space, (iii) does not need centralized control nor global stoptheworld synchronization, (iv) allows for multiple concurrent active GCs, (v) does not require to migrate objects from processor to processor and (vi) eventually reclaims all  inaccessible objects including distributed cycles.\u000aThese results are mainly obtained through the concept of a group of processors (or processes). Processors of a same group cooperate together to a GC inside this group; this GC is conservative with respect to the outside of the group. A processor contributes to the global GC of all groups to which it belongs. Garbage collection on small groups reclaims quickly locally distributed garbage clusters, while garbage collection on large groups ultimately reclaims widely distributed garbage clusters, albeit more slowly. Groups can be reorganized dynamically, in particular to tolerate failures of some member processors. These properties make the algorithm usable on very large and evolving networks of processors. Other  than distributed symbolic computations, possible applications include for example distributed file or database systems.
p3038
aVOur research is concerned with compilerindependent, tagfree garbage collection for the C++ programming language. We have previously presented a copying collector based on root registration. This paper presents a markandsweep garbage collector that ameliorates shortcomings of the previous collector. We describe the two collectors and discuss why the new one is an improvement over the old one. We have tested this collector and a conservative collector in a VLSI CAD application, and this paper discusses the differences. Currently this prototype of the collector imposes too much overhead on our application. We intend to solve that problem, and then use the techniques described in this paper to implement a generational MarkandSweep collector for C++.
p3039
aVThis paper presents algorithms for inserting monitoring code to profile and trace programs. These algorithms greatly reduce the cost of measuring programs. Profiling counts the number of times each basic block in a program executes and has a variety of applications. Instruction traces are the basis for tracedriven simulation and analysis, and are also used in tracedriven debugging.\u000aThe profiling algorithm chooses a placement of counters that is optimized\u2014and frequently optimal\u2014with respect to the expected or measured execution frequency of each basic block and branch in the program. The tracing algorithm instruments a program to obtain a subsequence of the basic block trace\u2014whose length is optimized with respect to the program's execution\u2014from  which the entire trace can be efficiently regenerated.\u000aBoth algorithms have been implemented and produce a substantial improvement over previous approaches. The profiling algorithm reduces the number of counters by a factor of two and the number of counter increments by up to a factor of four. The tracing algorithm reduces the file size and overhead of an already highly optimized tracing system by 2040%.
p3040
aVIn the context of abstract interpretation we study the number of times a functional need to be unfolded in order to give the least fixed point. For the cases of the total or monotone functions we obtain an exponential bound and in the case of strict and additive (or distributive) functions we obtain a quadratic bound. These bounds are shown to be tight. Specialising the case of strict and additive functions to functionals of a form that would correspond to iterative programs we show that a linear bound is tight. We demonstrate the relation to several analyses studied in the literature (including strictness analysis).
p3041
aVWe introduce and illustrate a specification method combining rulebased inductive definitions, wellfounded induction principles, fixedpoint theory and abstract interpretation for general use in computer science. Finite as well as infinite objects can be specified, at various levels of details related by abstraction. General proof principles are applicable to prove properties of the specified objects.\u000aThe specification method is illustrated by introducing G  \u221e  SOS, a structured operational semantics generalizing Plotkin's [28] structured operational semantics (SOS) so as to describe the finite, as well as the infinite behaviors of programs in a uniform way and by constructively deriving inductive presentations of the other (relational, denotational, predicate  transformers, \u2026) semantics from G  \u221e  SOS by abstract interpretation.
p3042
aVThe goal of this paper is to construct a semantic basis for the abstract interpretaion of Prolog programs. Prolog is a wellknown logic programming language which applies a depthfirst search strategy in order to provide a practical approximation of Horn clause logic. While pure logic programming has clean fixpoint, modeltheoretic and operational semantics the situation for Prolog is different. Difficulties in capturing the declarative meaning of Prolog programs have led to various semantic definitions which attempt to encode the search strategy in different mathematical frameworks. However semantic based analyses of Prolog are typically achieved by abstracting the more simple but less precise declarative semantics of pure logic Programs.\u000aWe propose instead to model Prolog control in a simple constraint logic language which is presented together with its declarative and operational semantics. This enables us to maintain the usual approach to declarative semantics of logic programs while capturing control aspects such as search strategy and selection rule.
p3043
aVThis paper presents the design and implementation of a \u201cquasi realtime\u201d garbage collector for Concurrent Caml Light, an implementation of ML with threads. This twogeneration system combines a fast, asynchronous copying collector on the young generation with a nondisruptive concurrent marking collector on the old generation. This design crucially relies on the ML compiletime distinction between mutable and immutable objects.
p3044
aVThe continuationpassing style (CPS) transformation is powerful but complex. Our thesis is that this transformation is in fact compound, and we set out to stage it. We factor the CPS transformation into several steps, separating aspects in each step: (1) Intermediate values are named; (2) Continuations are introduced; (3) Sequencing order is decided and administrative reductions are performed.\u000aStep 1 determines the evaluation order (e.g., callbyname or callbyvalue). Step 2 isolates the introduction of continuations and is expressed with local, structurepreserving rewrite rules \u2014 a novel aspect standing in sharp contrast with the usual CPS transformations. Step 3 determines the ordering of continuations (e.g., lefttoright or righttoleft evaluation) and leads to the familiarlooking continuationpassing terms.\u000aStep 2 is completely reversible and Steps 1 and 3 form Galois connections. Together they lead to the direct style (DS) transformation of our earlier work (including firstclass continuations): (1) Intermediate continuations are named and sequencing order is abstracted; (2) Secondclass continuations are eliminated; (3) Administrative reductions are performed.\u000aA subset of these transformations can leverage program manipulation systems: CPSbased compilers can modify sequencing to improve e.g., register allocation; static program analyzers can yield more precise results; and overspecified CPS programs can be rescheduled. Separating aspects of the CPS transformation also enables a new programming style, with applications to nondeterministic programming. As a byproduct, our work also suggests a new continuation semantics for unspecified sequencing orders in programming languages (e.g., Scheme).
p3045
aVMogensen has exhibited a very compact partial evaluator for the pure lambda calculus, using bindingtime analysis followed by specialization. We give a correctness criterion for this partial evaluator and prove its correctness relative to this specification. We show that the conventional properties of partial evaluators, such as the Futamura projections, are consequences of this specification. By considering both a flow analysis and the transformation it justifies together, this proof suggests a framework for the incorporating flow analyses into verified compilers.
p3046
aVWe define an operational semantics for lazy evaluation which provides an accurate model for sharing. The only computational structure we introduce is a set of bindings which corresponds closely to a heap. The semantics is set at a considerably higher level of abstraction than operational semantics for particular abstract machines, so is more suitable for a variety of proofs. Furthermore, because a heap is explicitly modelled, the semantics provides a suitable framework for studies about space behaviour of terms under lazy evaluation.
p3047
aVA polymorphic function is parametric if its behavior does not\u000adepend on the type at which it is instantiated. Starting with Reynolds'\u000awork, the study of parametricity is typically semantic. In this paper,\u000awe develop a syntactic approach to parametricity, and a formal system\u000athat embodies this approach: system \u000a\u000aR\u000a. Girard's system F deals with terms and types;\u000a\u000a\u000aR\u000a is an extension of F that deals also with relations\u000abetween types.\u000a\u000aIn \u000a\u000aR\u000a**, it is possible to derive theorems about functions\u000afrom their types, or \u201ctheorems for free\u201d, as Wadler calls\u000athem. An easy \u201ctheorem for free\u201d \u000a\u000a \u000a\u000aasserts that the type \u000a\u000a\u2200XX\u2192\u000aBool contains only constant\u000afunctions; this is not provable in F. There are many harder and more\u000asubstantial examples. Various metatheorems can also be obtained, such as\u000aa syntactic version of Reynolds' abstraction theorem.
p3048
aVJ. C. Reynolds suggested that Strachey's intuitive concept of \u201cparametric\u201d (i.e., uniform) polymorphism is closely linked to representation independence, and used logical relations to formalize this principle in languages with type variables and userdefined types. Here, we use relational parametricity to address longstanding problems with the semantics of localvariable declarations, by showing that interactions between local and nonlocal entities satisfy certain relational criteria.\u000aThe new model is based on a cartesian closed category of \u201crelationpreserving\u201d functors and natural transformations which is induced by a suitable category of \u201cpossible worlds\u201d with relations assigned to its objects and morphisms. The semantic interpretation supports straightforward validations of all the test equivalences that have been proposed in the literature, and encompasses standard methods of reasoning about data representations; however, it is not known whether it is fully abstract.
p3049
aVWe consider the following problem in proving observational congruences in functional languages: given a callbyname language based on the simplytyped &lgr;calculus with algebraic operations axiomatized by algebraic equations E, is the set of observational congruences between terms exactly those provable from (&bgr;), (&eegr;), and E?  We find conditions for determining whether &bgr;&eegr;Eequational reasoning is complete for proving the observational congruences between such terms. We demonstrate the power and generality of the theorems by presenting a number of easy corollaries for particular algebras.
p3050
aVRecursive data structures are abstractions of simple records and pointers. They impose a shape invariant, which is verified at compiletime and exploited to automatically generate code for building, copying, comparing, and traversing values without loss of efficiency. However, such values are always tree shaped, which is a major obstacle to practical use.\u000aWe propose a notion of graph types, which allow common shapes, such as doublylinked lists or threaded trees, to be expressed concisely and efficiently. We define regular languages of routing expressions to specify relative addresses of extra pointers in a canonical spanning tree. An efficient algorithm for computing such addresses is developed. We employ a secondorder monadic logic to decide wellformedness of graph type specifications. This logic can also be used for automated reasoning about pointer structures.
p3051
aVWe study the typing properties of CPS conversion for an extension of F&ohgr; with control operators. Two classes of evaluation strategies are considered, each with callbyname and callbyvalue variants. Under the \u201cstandard\u201d strategies, constructor abstractions are values, and constructor applications can lead to nontrivial control effects. In contrast, the \u201cMLlike\u201d strategies evaluate beneath constructor abstractions, reflecting the usual interpretation of programs in languages based on implicit polymorphism. Three continuation passing style sublanguages are considered, one on which the standard strategies coincide, one on which the MLlike strategies coincide, and one on which all strategies coincide. Compositional, typepreserving CPS  transformation algorithms are given for the standard strategies, resulting in terms on which all evaluation strategies coincide. This has as a corollary the soundness and termination of welltyped programs under the standard evaluation strategies. A similar result is obtained for the MLlike callbyname strategy. In contrast, such results are obtained for the callbyname strategy. In contrast, such results are obtained for the callby value MLlike strategy only for a restricted sublanguage in which constructor abstractions are limited to values.
p3052
aVDataflow analysis of scalar variables and data dependence analysis on array elements are two important program analyses used in optimizing and parallelizing compilers. Traditional dataflow analysis models accesses of array elements simply as accesses to the entire array, and is inadequate for parallelizing loops in arraybased programs. On the other hand, data dependence analysis differentiates between different array elements but is flowinsensitive.\u000aThis paper studies the combination of these two analyses\u2014dataflow analyses\u2014dataflow analysis of accesses to individual array elements. The problem of finding precise array dataflow information in the domain of loop nests where the loop bounds and array indices are affine functions of loop indices was first  formulated by Feautrier. Feautrier's algorithm, based on parametric integer programming techniques, is general but inefficient. This paper presents an efficient algorithm that can find the same precise information for many of the programs found in practice. In this paper, we argue that dataflow analysis of individual array elements is necessary for effective automatic parallelization. In particular, we demonstrate the use of array dataflow analysis in an important optimization known as array privatization.\u000aBy demonstrating that array dataflow analysis can be computed efficiently and by showing the importance of the optimizations enabled by the analysis, this paper suggests that array dataflow analysis may become just as important in future optimizing and parallelizing  compilers as dataflow and data dependence analysis are in today's compilers.
p3053
aVThis article investigates an MLlike language with byname semantics for polymorphism: polymorphic objects are not evaluated once for all at generalization time, but reevaluated at each specialization. Unlike the standard ML semantics, the byname semantics works well with polymorphic references and polymorphic continuations: the naive typing rules for references and for continuations are sound with respect to this semantics. Polymorphism by name leads to a better integration of these imperative features into the ML type discipline. Practical experience shows that it retains most of the efficiency and predictability of polymorphism by value.
p3054
aVWe present practical approximation methods for computing interprocedural aliases and side effects for a program written in a language that includes pointers, reference parameters and recursion. We present the following results: 1) An algorithm for flowsensitive interprocedural alias analysis which is more precise and efficient than the best interprocedural method known. 2) An extension of traditional flowinsensitive alias analysis which accommodates pointers and provides a framework for a family of algorithms which trade off precision for efficiency. 3) An algorithm which correctly computes side effects in the presence of pointers. Pointers cannot be correctly handled by conventional methods for side effect analysis. 4) An alias naming technique which handles dynamically allocated objects and guarantees the correctness of dataflow analysis. 5) A compact representation based on transitive reduction which does not result in a loss of precision and improves precision in some case. 6) A method for intraprocedural alias analysis which is based on a sparse representation.
p3055
aVWe have designed and implemented an interprocedural program analyzer generator, called system Z. Our goal is to automate the generation and management of semanticsbased interprocedural program analysis for a wide range of target languages.\u000aSystem Z is based on the abstract interpretation framework. The input to system Z is a highlevel specification of an abstract interpreter. The output is a C code for the specified interprocedural program analyzer. The system provides a highlevel command set (called projection expressions) in which the user can tune the analysis in accuracy and cost. The user writes projection expressions for selected domains; system Z takes care of the remaining things so that the generated analyzer conducts an   analysis over the projected domains, which will vary in cost and accuracy according to the projections.\u000aWe demonstrate the system's capabilities by experiments with a set of generated analyzers which can analyze C, FORTRAN, and SCHEME programs.
p3056
aVWe describe and prove algorithms to convert programs which use the Parallel Computing Forum Parallel Sections construct into Static Single Assignment (SSA) form. This proces allows compilers to apply classical scalar optimization algorithms to explicitly parallel programs. To do so, we must define what the concept of dominator and dominance frontier mean in parallel programs. We also describe how we extend SSA form to handle parallel updates and still preserve the SSA properties.
p3057
aVIn this paper we introduce a staticallytyped, functional, objectoriented programming language, TOOPL, which supports classes, objects, methods, instance variable, subtypes, and inheritance.\u000aIt has proved to be surprisingly difficult to design staticallytyped objectoriented languages which are nearly as expressive as Smalltalk and yet have no holes in their typing systems. A particular problem with statically type checking objectoriented languages is determining whether a method provided in a superclass will continue to type check when inherited in a subclass. This program is solved in our language by providing type checking rules which guarantee that a method which type checks as part of a class will type check correctly in all legal subclasses in which it is inherited. This feature enables library providers to provide only the interfaces of classes with executables and still allow users to safely create subclasses.\u000aThe design of TOOPL has been guided by an analysis of the semantics of the language, which is given in terms of a sufficiently rich model of the Fbounded secondorder lambda calculus. This semantics supported the language design by providing a means of proving that the typechecking rules for the language are sound, ensuring that welltyped terms produce objects of the appropriate type. In particular, in a welltyped program it is impossible to send a message to an object which lacks a corresponding method.
p3058
aVIt is widely agreed that recursive types are inherent in the static typing of the essential mechanisms of objectoriented programming: encapsulation, message passing, subtyping, and inheritance. We demonstrate here that modeling object encapsulation in terms of existential types yields a substantially more straightforward explanation of these features in a simpler calculus without recursive types.
p3059
aVWe show how a higher order logic, the calculus of constructions, can be used to give a simple, first principles treatment of record calculi, polymorphism, and subtyping. The development follows the constructive idiom of extracting implementations of equationally specified programs from proofs of their termination, with a logic for reasoning about programs, and a semantics that comes as a gratuity. In this framework, records are finitely specified functions where equality is decidable over the domain, with types that are a particular kind of logical assertion. By proving that a record specification satisfies its type, we can extract its implementation. While program extraction serves as a sort of compiler, proof normalization serves as an interpreter; the latter serves to ensure in some sense the coherence of the translation embedded in the former.\u000aThis simple minded approach lets us show, for example, that many inference rules found in record and object calculi can be derived\u2014they are just provable lemmas in higher order logic. We see explicitly how from subtyping proofs we can extract, using conventional methods, coercion functions between underlying representations of data types. By further exploiting the computational metaphor of higher order logic, we can realize an interpreter for recursively defined objects, as well as subtype and inheritance relations between them. Recursive types for objects are explained by primitive recursion in higher types. The approach also gives a computational understanding of Fbounded polymorphism.
p3060
aVCentral to constraint logic programming (CLP) languages is the notion of a global constraint solver which is queried to direct execution and to which constraints are monotonically added. We present a methodology for use in the compilation of CLP languages which is designed to reduce the overhead of the global constraint solver. This methododology is based on three optimizations. The first, refinement, involves adding new constraints, which in effect make information available earlier in the computation, guiding subsequent execution away from unprofitable choices. The second, removal, involves eliminating constraints from the solver when they are redundant. The last, reordering, involves moving constraint addition later and constraint removal earlier in the computation. Determining the applicability of each optimization requires sophisticated global analysis. These analyses are based on abstract interpretation and provide information about potential and definite interaction between constraints.
p3061
aVDataparallel languages like Fortran 90 express parallelism in the form of operations on data aggregates such as arrays. Misalignment of the operands of an array operation can reduce program performance on a distributedmemory parallel machine by requiring nonlocal data accesses. Determining array alignments that reduce communication is therefore a key issue in compiling such languages.\u000aWe present a framework for the automatic determination of array alignments in dataparallel languages such as Fortran 90. Our language model handles array sectioning, reductions, spreads, transpositions, and masked operations. We decompose alignment functions into three constituents: axis, stride, and offset. For each of these subproblems, we show how to solve the alignment problem for a   basic block of code, possibly containing common subexpressions. Alignments are generated for all array objects in the code, both named program variables and intermediate results. The alignments obtained by our algorithms are more general than those provided by the \u201cownercomputes\u201d rule. Finally, we present some ideas for dealing with control flow, replication, and dynamic alignments that depend on loop induction variables.
p3062
aVWe present in this paper a structure\u2013sharing framework originally developed for a Dynamic Programming interpreter of Logic programs called DyALog. This mechanism should be of interest for alternative execution models of PROLOG which maintain multiple computation branches and reuse subcomputations in various contexts (computation sharing). This category includes, besides our Dynamic Programming model, the tabular models (OLDT, SLDAL, XWAM), the \u201cmagicset\u201d models, and the independent AND and OR parallelism with solution sharing models. These models raise the problem of storing vast amount of data, motivating us to discard copying mechanisms in favor of structuresharing mechanisms. Unfortunately, computation sharing requires joining computation branches and possibly renaming some variables, which generally leads to complex structuresharing mechanisms. The proposed \u201clayersharing\u201d framework succeeds however in remaining understandable and easy to implement.
p3063
aVIn this paper we define a compositional semantics for a generalized composition operator on logic programs. Static and dynamic inheritance as well as composition by union of clauses can all be obtained by specializing the general operator. The semantics is based on the notion of differential programs, logic programs annotated with declarations that establish the programs' external interfaces.
p3064
aVA symbolic debugger allows a user to display the values of program variables at a breakpoint. However, problems arise if the program is translated by an optimizing compiler. This paper addresses the effects of global register allocation and assignment: a register assigned to a variable V may not be holding V's value at a breakpoint since the register can also be assigned to other variables. We define the problem of determining whether a variable is in its assigned register as the residence problem. Prior work on debugging of optimized code has focused on the currency problem; detecting whether a variable's runtime value is the expected value. Determining residence is a more serious problem than currency detection. We present a data flow algorithm that accurately computes a variable's residency, by determining when a variable becomes evicted from its register. We measure the effectiveness of different approaches to determine variable residence for three C programs from the SPEC suite.
p3065
aVProgram dependence graphs have been proposed for use in optimizing, vectorizing, and parallelizing compilers, and for program integration. This paper proposes their use as the basis for incremental program testing when using test data adequacy criteria. Test data adequacy is commonly used to provide some confidence that a particular test suite does a reasonable job of testing a program. Incremental program testing using test data adequacy criteria addresses the problem of testing a modified program given an adequate test suite for the original program. Ideally, one would like to create an adequate test suite for the modified program that reuses as many files from the old test suite as possible. Furthermore, one would like to know, for every file that is in both the old and the new test suites, whether the program components exercised by that file have been affected by the program modification; if no components have been affected, then it is not necessary to rerun the program using that file.\u000aIn this paper we define adequacy criteria based on the program dependence graph, and propose techniques based on program slicing to identify components of the modified program that can be tested using files from the old test suite, and components that have been affected by the modification. This information can be used to reduce the time required to create new test files, and to avoid unproductive retesting of unaffected components. Although exact identification of the components listed above is, in general, undecidable, we demonstrate that our techniques provide safe approximations.
p3066
aVThe concept of an information flow path arising from the generalized theory of data flow analysis [21] is used to analyze the complexity of data flow analysis. The width (w) of a program flow graph with respect to a class of data flow problems is introduced as a measure of the complexity of roundrobin iterative analysis. This provides the first known complexity result for round robin iterative analysis of bidirectional data flows commonly used in algorithms based on the suppression of partial redundancies [6, 7, 8, 9, 17, 18, 25]. We also show that width provides a better bound on the complexity of unidirectional data flows than the classical notion of depth.\u000aThe paper presents ways to reduce the width, and thereby the  complexity of flow analysis, for several interesting problems. Complexity analysis using the notion of width is also shown to motivate efficient solution methods for various bidirectional problems, viz. The alternating iterations method, and an interval analysis based elimination method for the partial redundancy elimination problems. The paper also presents a condition for the decomposability of a bidirectional problem into a sequence of unidirectional problems.
p3067
aVWe study the type inference problem for a system with type classes as in the functional programming language Haskell. Type classes are an extension of MLstyle polymorphism with overloading. We generalize Milner's work on polymorphism by introducing a separate context constraining the type variables in a typing judgement. This lead to simple type inference systems and algorithms which closely resemble those for ML. In particular we present a new unification algorithm which is an extension of syntactic unification with constraint solving. The existence of principal types follows from an analysis of this unification algorithm.
p3068
aVSubtyping in the presence of recursive types for the &lgr;calculus was studied by Amadio and Cardelli in 1991 [1]. In that paper they showed that the problem of deciding whether one recursive type is a subtype of another is decidable in exponential time.\u000aIn this paper we give an O(n2) algorithm. Our algorithm is based on a simplification of the definition of the subtype relation, which allows us to reduce the problem to the emptiness problem for a certain finite automaton with quadratically many states.\u000aIt is known that equality of recursive types and the covariant Bohm order can be decided efficiently by means of finite automata. Our results extend the automatatheoretic approach to handle orderings based on contravariance.
p3069
aVIn Milner's polyadic &pgr;calculus there is a notion of sorts which is analogous to the notion of types in functional programming. As a welltyped program applies functions to arguments in a consistent way, a wellsorted process uses communication channels in a consistent way. An open problem is whether there is an algorithm to infer sorts in the &pgr;calculus in the same way that types can be inferred in functional programming. Here we solve the problem by presenting an algorithm which infers the most general sorting for a process in the firstorder calculus, and proving its correctness. The algorithm is similar in style to those used for HindleyMilner type inference in functional languages.
p3070
aVTo separately compile a program module in traditional staticallytyped languages, one has to manually write down an import interface which explicitly specifies all the external symbols referenced in the module. Whenever the definitions of these external symbols are changed, the module has to be recompiled. In this paper, we present an algorithm which can automatically infer the \u201cminimum\u201d import interface for any module in languages based on the DamasMilner type discipline (e.g., ML). By \u201cminimum\u201d, we mean that the interface specifies a set of assumptions (for external symbols) that are just enough to make the module typecheck and compile. By compiling each module using its \u201cminimum\u201d import interface, we get a separate compilation method that can  achieve the following optimal property: A compilation unit never needs to be recompiled unless its own implementation changes.
p3071
aVThis paper describes a semantic basis for a compositional approach to the analysis of logic programs. A logic program is viewed as consisting of a set of modules, each module defining a subset of the program's predicates. Analyses are constructed by considering abstract interpretations of a compositional semantics. The abstract meaning of a module corresponds to its analysis and composition of abstract meanings corresponds to composition of analyses. Such an approach is essential for large program development so that altering one module does not require reanalysis of the entire program. We claim that for a substantial class of programs, compositional analyses which are based on a notion of abstract unfolding provide the same precision as noncompositional analysis. A compositional analysis for ground dependencies is included to illustrate the approach. To the best of our knowledge this is the first account of a compositional framework for the analysis of logic programs.
p3072
aVAlthough software pipelining has been proposed as one of the most important loop scheduling methods, simultaneous scheduling and register allocation is less understood and remains an open problem [28]. The objective of this paper is to develop a unified algorithmic framework for concurrent scheduling and register allocation to support timeoptimal software pipelining. A key intuition leading to this surprisingly simple formulation and its efficient solution is the association of maximum computation rate of a program graph with its critical cycles due to Reiter's pioneering work on KarpMiller computation graphs [29]. In particular, our method generalizes the work by Callahan, Carr and Kennedy on scalar expansion[6], the work by Lam on modular variable expansion for software pipelined loops [20], and the work by Rau et al. on register allocation for modulo scheduled loops[28].
p3073
aVWe extend term unification techniques used to type extensible records in order to solve the two main typing problems for modules in Standard ML: matching and sharing. We obtain a type system for modules based only on well known unification problems, modulo some equational theories we define. Our formalization is simple and has the elegance of polymorphic type disciplines based on unification. It can be seen as a synthesis of previous work on module and record typing.
p3074
aVStatic scoping embodies a strong encapsulation mechanism for hiding the details of program units. Yet, it does not allow the sharing of variable bindings (locations) across independent program units. Facilities such as module and object systems that require cross references of variables therefore must be added as special features. In this paper we present an alternative: quasistatic scoping. Quasistatic scoping is more flexible than static scoping, but has the same encapsulation mechanism. The user can control when and in what scope to resolve a quasistatic variable, i.e., to associate it with a variable binding. To demonstrate its versatility, we add quasistatic scoping to Scheme and show how to build the aforementioned facilities at the userlevel. We also show that quasistatic scoping can be implemented efficiently.
p3075
aVThe last years have witnessed a flurry of new results in the area of partial evaluation. These tutorial notes survey the field and present a critical assessment of the state of the art.
p3076
aVWe define an extension of the callbyname lambda calculus with additional constructs and reduction rules that represent mutable variables and assignments. The extended calculus has neither a concept of an explicit store nor a concept of evaluation order; nevertheless, we show that programs in the calculus can be implemented using a singlethreaded store. We also show that the new calculus has the ChurchRosser property and that it is a conservative extension of classical lambda calculus with respect to operational equivalence; that is, all algebraic laws of the functional subset are preserved.
p3077
aVAccording to folklore, Algol is an \u201corthogonal\u201d extension of a simple imperative programming language with a callbyname functional language. The former contains assignments, branching constructs, and compound statements; the latter is based on the typed &lgr;calculus. In an attempt to formalize the claim of \u201corthogonality\u201d, we define a simple version of Algol and an extended &lgr;calculus. The calculus includes the full &bgr;rule and rules for the reduction of assignment statements and commands. It has the usual properties, e.g., it satisfies a ChurchRosser and Strong Normalization Theorem. In support of the claim that the imperative and functional components are orthogonal to each other, we show that the proofs of these theorems are  combinations of separate ChurchRosser and Strong Normalization theorems for each sublanguage.\u000aAn acclaimed consequence of Algol's orthogonal design is the idea that the evaluation of a program has two distinct phases. The first phase corresponds to an unrolling of the program according to the usual &bgr; and fixpoint reductions, which provide the formal counterpart to Algol's famous copy rule. The result of this phase is essentially an imperative program. The second phase executes the output of the first phase in the imperative fashion of a stack machine. Given our calculus, we can prove a Postponement Theorem and can thus formalize this phase separation.
p3078
aVWe present a new model, based on monads, for performing input/output in a nonstrict, purely functional language. It is composable, extensible, efficient, requires no extensions to the type system, and extends smoothly to incorporate mixedlanguage working and inplace array updates.
p3079
aVWe present a new programming paradigm called Communicating Reactive Processes or CRP that unifies the capabilities of asynchronous and synchronous concurrent programming languages. Asynchronous languages such as CSP, OCCAM, or ADA are wellsuited for distributed algorithms; their processes are loosely coupled and communication takes time. The ESTEREL synchronous language is dedicated to reactive systems; its processes are tightly coupled and deterministic, communication being realized by instantaneous broadcasting. Complex applications such as process of robot control require to couple both forms of concurrency, which is the object of CRP. A CRP program consists of independent locally reactive ESTEREL nodes that communicate with each other by CSP rendezvous. CRP faithfully extends both ESTEREL and CSP and adds new possibilities such as precise local watchdogs on rendezvous. We present the design of CRP, its semantics, a translation into classical process calculi for program verification, and application example, and implementation issues.
p3080
aVWe propose a method to extend an MLstyle polymorphic language with transparent communication primitives, and give their precise operational semantics. These primitives allow any polymorphic programs definable in ML to be used remotely in a manner completely transparent to the programmer. Furthermore, communicating programs may be based on different architecture and use different data representations.\u000aWe define a polymorphic functional calculus with transparent communication primitives, which we call dML, as an extension of Damas and Milner's proof system for ML. We then develop an algorithm to translate dML to a \u201ccore\u201d language containing only lowlevel communication primitives that are readily implementable in most of distributed environments. To establish the type safety of communicating programs, we define an operational semantics of the core language and prove that the polymorphic type system of dML is sound with respect to the operational semantics of the translated terms of the core language.
p3081
aVHeap allocation with copying garbage collection is believed to have poor memory subsystem performance. We conducted a study of the memory subsystem performance of heap allocation for memory subsystems found on many machines. We found that many machines support heap allocation poorly. However, with the appropriate memory subsystem organization, heap allocation can have good memory subsystem performance.
p3082
aVThis paper presents a variant of the SML module system that introduces a strict distinction between abstract types and manifest types (types whose definitions are part of the module specification), while retaining most of the expressive power of the SML module system. The resulting module system provides much better support for separate compilation.
p3083
aVThe design of a module system for constructing and maintaining large programs is a difficult task that raises a number of theoretical and practical issues. A fundamental issue is the management of the flow of information between program units at compile time via the notion of an interface. Experience has shown that fully opaque interfaces are awkward to use in practice since too much information is hidden, and that fully transparent interfaces lead to excessive interdependencies, creating problems for maintenance and separate compilation. The \u201csharing\u201d specifications of Standard ML address this issue by allowing the programmer to specify equational relationships between types in separated modules, but are not expressive enough to allow the programmer complete control over    the propagation of type information between modules.\u000aThese problems are addressed from a typetheoretic viewpoint by considering a calculus based on Girard's system F&ohgr;. The calculus differs form those considered in previous studies by relying exclusively on a new form of weak sum type to propagate information at compiletime, in contrast to approaches based on strong sums which rely on substitution. The new form of sum type allows for the specification of equational, as well as type and kind, information in interfaces. This provides complete control over the propagation of compiletime information between program units and is sufficient to encode in a straightforward way most users of type sharing specifications in Standard ML. \u000aModules are treated  as   \u201cfirstclass\u201d citizens, and therefore the system supports higherorder modules and some objectoriented programming idioms; the language may be easily restricted to \u201csecondclass\u201d modules found in MLlike languages.
p3084
aVRAPIDE is a programming language framework designed for the development of large, concurrent, realtime systems by prototyping. The framework consists of a type language and default executable, specification and architecture languages, along with associated programming tools. We describe the main features of the type language, its intended use in a prototyping environment, and rationale for selected design decisions.
p3085
aVThe standard formulation of bounded quantification, system F\u2264, is difficult to work with and lacks important syntactic properties, such as decidability. More tractable variants have been studied, but those studied so far either exclude significant classes of useful programs or lack a compelling semantics.\u000aWe propose here a simple variant of F\u2264 that ameliorates these difficulties. It has a natural semantic interpretation, enjoys a number of important properties that fail in F\u2264, and includes all of the programming examples for which F\u2264 has been used in practice.
p3086
aVWe present a simple and powerful type inference method for dynamically typed languages where no type information is supplied by the user. Type inference is reduced to the problem of solvability of a system of type inclusion constraints over a type language that includes function types, constructor types, union, intersection, and recursive types, and conditional types. Conditional types enable us to analyze control flow using type inference, thus facilitating computation of accurate types. We demonstrate the power and practicality of the method with examples and performance results from an implementation.
p3087
aVThe need to fit together reusable components and system designs in spite of differences in protocol and representation choices occurs often in objectoriented software construction. It is therefore necessary to use adapters to achieve an exact fit between the available \u201csocket\u201d for a reusable part and the actual part. In this paper we discuss an approach to the construction of tools that largely automate the synthesis of adapter code. Such tools are important in reducing the effort involved in reuse since adapter synthesis can be challenging and errorprone in the complex type environment of an objectoriented language. Our approach is applicable to statically typed languages like C++ and Eiffel, and is based on a formal notion of adaptability which is related to but distinct from both subtyping and inheritance.
p3088
aVWe present a translation scheme for the polymorphically typed callbyvalue &lgr;calculus. All runtime values, including function closures, are put into regions. The store consists of a stack of regions. Region inference and effect inference are used to infer where regions can be allocated and deallocated. Recursive functions are handled using a limited form of polymorphic recursion. The translation is proved correct with respect to a store semantics, which models as a regionbased runtime system. Experimental results suggest that regions tend to be small, that region allocation is frequent and that overall memory demands are usually modest, even without garbage collection.
p3089
aVThe role of nonstandard type inference in static program analysis has been much studied recently. Early work emphasised the efficiency of type inference algorithms and paid little attention to the correctness of the inference system. Recently more powerful inference systems have been investigated but the connection with efficient inference algorithms has been obscured. The contribution of this paper is twofold: first we show how to transform a program logic into an algorithm and, second, we introduce the notion of lazy types and show how to derive an efficient algorithm or strictness analysis.
p3090
aVAn important implementation decision in polymorphically typed functional programming language is whether to represent data in boxed or unboxed form and when to transform them from one representation to the other. Using a language with explicit representation types and boxing/unboxing operations we axiomatize equationally the set of all explicitly boxed versions, called completions, of a given source program. In a twostage process we give some of the equations a rewriting interpretation that captures eliminating boxing/unboxing operations without relying on a specific implementation or even semantics of the underlying language. The resulting reduction systems operate on congruence classes of completions defined by the remaining equations E, which can  be understood as moving boxing/unboxing operations along data flow paths in the source program. We call a completion eopt formally optimal if every other completion for the same program (and at the same representation type) reduces to eopt under this twostage reduction.\u000aWe show that every source program has formally optimal completions, which are unique modulo E. This is accomplished by first \u201cpolarizing\u201d the equations in E and orienting them to obtain two canonical (confluent and strongly normalizing) rewriting systems. The completions produced by Leroy's and Poulsen's algorithms are generally not formally optimal in our sense.\u000aThe rewriting systems have  been implemented and applied to some simple Standard ML programs. Our results show that the amount of boxing and unboxing operations is also in practice substantially reduced in comparison to Leroy's completions. This analysis is intended to be integrated into Tofte's regionbased implementation of Standard ML currently underway at DIKU.
p3091
aVAbstract interpretation [7] is a systematic methodology to design\u000astatic program analysis which has been studied extensively in the logic\u000aprogramming community, because of the potential for optimizations in\u000alogic programming compilers and the sophistication of the analyses which\u000arequire conceptual support. With the emergence of efficient generic\u000aabstract interpretation algorithms for logic programming, the main\u000aburden in building an analysis is the abstract domain which gives a safe\u000aapproximation of the concrete domain of computation. However, accurate\u000aabstract domains for logic programming are often complex because of the\u000avariety of analyses to perform their interdependence, and the need to\u000amaintain structural information. The purpose of this paper is to propose\u000aconceptual and  software support for the design of abstract domains. It\u000acontains two main contributions: the notion of open product and a\u000ageneric pattern domain. The open\u000aproduct is a new way of combining abstract domains\u000aallowing each combined domain to benefit from information from the other\u000acomponents through the notions of queries and open operations. The open\u000aproduct is generalpurpose and can be used for other programming\u000aparadigms as well. The generic pattern\u000adomain Pat (R )automatically upgrades a domain D with structural\u000ainformation yielding a more accurate domain Pat (D) without additional\u000adesign or implementation cost. The two contributions are orthogonal and\u000acan be combined  in various ways to obtain sophisticated domains while\u000aimposing minimal requirements on the designer. Both contributions are\u000acharacterized theoretically and experimentally and were used to design\u000avery complex abstract domains such as PAT(OProp\u000a\u000a\u2297\u000aOMode\u000a\u000a\u2297\u000aOPS) which would be very difficult to\u000adesign otherwise. On this last domain, designers need only contribute\u000aabout 20% (about 3,400 lines) of the complete system (about 17,700\u000alines).
p3092
aVIt is common for debuggers to implement breakpoints by a combination of planting traps and single stepping. When the target program contains multiple threads of execution, a debugger that is not carefully implemented may miss breakpoints. This paper gives a formal model of a breakpoint in a twothreaded program. The model describes correct and incorrect breakpoint implementations. Automatic search of the model's state space shows that the correct implementation does miss a breakpoint. The results apply even to debuggers like dbx and gdb, which are apparently for singlethreaded programs; when the user evaluates an expression containing function calls, the debugger executes the call in the target address space, in effect creating a new thread.
p3093
aVTraditional logic programming languages, such as Prolog, use a fixed lefttoright atom scheduling rule. Recent logic programming languages, however, usually provide more flexible scheduling in which computation generally proceed lefttoright but in which some calls are dynamically \u201cdelayed\u201d until their arguments are sufficiently instantiated to allow the call to run efficiently. Such dynamic scheduling has a significant cost. We give a framework for the global analysis of logic programming languages with dynamic scheduling and show that program analysis based on this framework supports optimizations which remove much of the overhead of dynamic scheduling.
p3094
aVHigherorder equational logic programming is a paradigm which combines firstorder equational and higherorder logic programming, where higherorder logic programming is based on a subclass of simply typed &lgr;terms, called higherorder patterns. Central to the notion of higherorder equational logic programming is the socalled higherorder equational unification. This paper extends several important classes of firstorder equational unification algorithms to the higherorder setting: only problems of the extensions are discussed and firstorder equational unifications are viewed as black boxes whenever possible.\u000aWe first extend narrowing and show that the completeness of many higherorder narrowing strategies reduces to that of their underlying firstorder counterparts. Then we propose an algorithm or higherorder equational unification of free higherorder patterns in an arbitrary equational theory. Finally a general approach to extend firstorder unification combination algorithms is sketched informally. The termination property of the above higherorder extensions is considered in a uniform way.
p3095
aVNarrowing is the operational principle of languages that integrate functional and logic programming. We propose a notion of a needed narrowing step that, for inductively sequential rewrite systems, extends the Huet and Levy notion of a needed reduction step. We define a strategy, based on this notion, that computes only needed narrowing steps. Our strategy is sound and complete for a large class of rewrite systems, is optimal w.r.t. the cost measure that counts the number of distinct steps of a derivation, computes only independent unifiers, and is efficiently implemented by pattern matching.
p3096
aVWe present a new incremental algorithm for the problem of maintaining the dominator tree of a reducible flowgraph as the flowgraph undergoes changes such as the insertion and deletion of edges. Such an algorithm has applications in incremental dataflow analysis and incremental compilation.
p3097
aVThe value dependence graph (VDG) is a sparse dataflowlike representation that simplifies program analysis and transformation. It is a functional representation that represents control flow as data flow and makes explicit all machine quantities, such as stores and I/O channels. We are developing a compiler that builds a VDG representing a program, analyzes and transforms the VDG, then produces a control flow graph (CFG) [ASU86] from the optimized VDG. This framework simplifies transformations and improves upon several published results. For example, it enables more powerful code motion than [CLZ86, FOW87], eliminates as many redundancies as [AWZ88, RWZ88] (except for redundant loops), and provides important information to the code scheduler [BR91]. We exhibit a fast, onepass method for elimination of partial redundancies that never performs redundant code motion [KFS92, DS93] and is simpler than the classical [MR79, Dha91] or SSA [RWZ88] methods. These results accrue from eliminating the CFG from the analysis/transformation phases and using demand dependences in preference to control dependences.
p3098
aVAutomatic parallelization of real FORTRAN programs does not live up to users expectations yet, and dependence analysis algorithms which either produce too many false dependences or are too slow to contribute significantly to this. In this paper we introduce dataflow dependence analysis algorithm which exactly computes valuebased dependence relations for program fragments in which all subscripts, loop bound and IF conditions are affine. Our algorithm also computes good affine approximations of dependence relations for nonaffine program fragments. Actually, we do not know about any other algorithm which can compute better approximations.\u000aAnd our algorithm is efficient too, because it is lazy. When searching for write statements that supply values used by a given read statement, it starts with statements which are lexicographically close to the read statement in iteration space. Then if some of the read statement instances are not \u201csatisfied\u201d with these close writes, the algorithm broadens its search scope by looking into more distant writes. The search scope keeps broadening until all read instances are satisfied or no write candidates are left.\u000aWe timed our algorithm on several benchmark programs and the timing results suggest that our algorithm is fast enough to be used in commercial compilers\u2014it usually takes 5 to 15 percent of f77 02 compilation time to analyze a program. Most programs in the 100line range take less than 1 second to analyze on a SUN SparcStation IPX.
p3099
aVThis paper develops a semantic framework for concurrent languages with value passing. An operation analogous to substitution in the &lgr;calculus is given, and a semantics is given for a valuepassing version of Milner's Calculus of Communicating Systems (CCS). An operational equivalence is then defined and shown to coincide with Milner's (early) bisimulation equivalence. We also show how semantics maybe given for languages with asynchronous communication primitives. In contrast with existing approaches to value passing, this semantics does not reduce data exchange to pure synchronization over (potentially infinite) families of ports indexed by data, and it avoids variable renamings that are not local to processes engaged in communication.
p3100
aVWe propose a general definition of higherorder process calculi, generalizing CHOCS [Tho89] and related calculi, and investigate its basic properties. We give sufficient conditions under which a calculus is finitelybranching and effective. We show that a suitable notion of higherorder bisimulation is a congruence for a subclass of higherorder calculi. We illustrate our definitions with a simple calculus strictly stronger than CHOCS.
p3101
aVA theory of combinators in the setting of concurrent processes is formulated. The new combinators are derived from an analysis of the operation called asynchronous name passing, just as an analysis of logical substitution gave rise to the sequential combinators. A system with seven atoms and fixed interaction rules, but with no notion of prefixing, is introduced, and is shown to be capable of representing input and output prefixes over arbitrary terms in a behaviourally correct way, just as SKcombinators are closed under functional abstraction without having it as a proper syntactic construct. The basic equational correspondence between concurrent combinators and a system of asynchronous mobile processes, as well as the embedding of the finite part of &pgr;calculus in concurrent combinators, is proved. These results will hopefully serve as a cornerstone for further investigation of the theoretical as well as pragmatic possibilities of the presented construction.
p3102
aVIn this paper we present techniques to find subsets of nodes of a flowgraph that satisfy the following property: A test set that exercises all nodes in a subset exercises all nodes in the flowgraph. Analogous techniques to find subsets of edges are also proposed. These techniques may be used to significantly reduce the cost of coverage testing of programs. A notion of a super block consisting of one or more basic blocks in that super block must be exercised by the same input. Dominator relationships among super blocks are used to identify a subset of the super blocks whose coverage implies that of all super blocks and, in turn, that of all basic blocks. Experiments with eight systems in the range of 175K lines of code show that, on the average, test cases targeted to cover just 29% of the basic blocks and 32% of the branches ensure 100% block and branch coverage, respectively.
p3103
aVAn operational semantics for functional logic programs is presented. In such programs functional terms provide for reduction of expressions, provided that they ground. The semantics is based on multipass evaluation techniques originally developed for attribute grammars. Program execution is divided into two phases: (1) construction of an incomplete proof tree, and (2) its decoration into a complete proof tree. The construction phase applies a modified SLDresolution scheme, and the decoration phase a partial (multipass) traversal over the tree. The phase partition is generated by static analysis where data dependencies are extracted for the functional elements of the program. The method guarantees that all the functional terms of a program can be evaluated, and no dynamic groundness checks are needed.
p3104
aVUsing the simple tree attributions described in this paper, attribute values can themselves be trees, enabling attribution to be used for tree transformations. Unlike higherorder attribute grammars, simple tree attributions have the property of descriptional composition, which allows a complex transformation to be built up from simpler ones, yet be executed efficiently. In contrast to other formalisms that admit descriptional composition, notably composable attribute grammars, simple tree attributions have the expressive power to handle remote references and recursive syntactic (treegenerating) functions, providing significantly more general forms of attribution and transformation.
p3105
aVWe develop a calculus in which the computation steps required to execute a computer program can be separated into discrete stages. The calculus, denoted &lgr;2, is embedded within the pure untyped &lgr;calculus. The main result of the paper is a characterization of sufficient conditions for confluence for terms in the calculus. The condition can be taken as a correctness criterion for translators that perform reductions in one stage leaving residual redexes over for subsequent computation stages. As an application of the theory, we verify the correctness of a macro expansion algorithm. The expansion algorithm is of some interest in its own right since it solves the problem of desired variable capture using only the familiar capture avoiding substitutions.
p3106
aVModern computer architectures increasingly depend on mechanisms that estimate future control flow decisions to increase performance. Mechanisms such as speculative execution and prefetching are becoming standard architectural mechanisms that rely on control flow prediction to prefetch and speculatively execute future instructions. At the same time, computer programmers are increasingly turning to objectoriented languages to increase their productivity. These languages commonly use run time dispatching to implement object polymorphism. Dispatching is usually implemented using an indirect function call, which presents challenges to existing control flow prediction techniques.\u000aWe have measured the occurrence  of indirect function calls in a collection of C++ programs. We show that, although it is more important to predict branches accurately, indirect call prediction is also an important factor in some programs and will grow in importance with the growth of objectoriented programming. We examine the improvement offered by compiletime optimizations and static and dynamic prediction techniques, and demonstrate how compilers can use existing branch prediction mechanisms to improve performance in C++ programs. Using these methods with the programs we examined, the number of instructions between mispredicted breaks in control can be doubled on existing computers.
p3107
aVThis paper discusses call forwarding, a simple interprocedural optimization technique for dynamically typed languages. The basic idea behind the optimization is straightforward: find an ordering for the \u201centry actions\u201d of a procedure, and generate multiple entry points for the procedure, so as to maximize the savings realized from different call sites bypassing different sets of entry actions. We show that the problem of computing optimal solutions to arbitrary call forwarding problems is NPcomplete, and describe an efficient greedy algorithm for the problem. Experimental results indicate that (i) this algorithm is effective, in that the solutions produced are generally close to optimal; and (ii) the resulting optimization leads to significant performance improvements for a number of benchmarks tested.
p3108
aVThe notion that a definition of a variable is dead is used by optimizing compilers to delete code whose execution is useless. We extend the notion of deadness to that of partial deadness, and define a transformation, the revival transformation, which eliminates useless executions of a (partially dead) definition by tightening its execution conditions without changing the set of uses which it reaches or the conditions under which it reaches each of them.
p3109
aVWe consider the problem of selective and lightweight closure conversion, in which multiple procedurecalling protocols may coexist in the same code. Flow analysis is used to match the protocol expected by each procedure and the protocol used at each of its possible call sites. We formulate the flow analysis as the solution of a set of constraints, and show that any solution to the constraints justifies the resulting transformation. Some of the techniques used are suggested by those of abstract interpretation, but others arise out of alternative approaches.
p3110
aVWe show that any monad whose unit and extension operations are expressible as purely functional terms can be embedded in a callbyvalue language with \u201ccomposable continuations\u201d. As part of the development, we extend Meyer and Wand's characterization of the relationship between continuationpassing and direct style to one for continuationpassing vs. general \u201cmonadic\u201d style. We further show that the composablecontinuations construct can itself be represented using ordinary, noncomposable firstclass continuations and a single piece of state. Thus, in the presence of two specific computational effects  storage and escapes  any expressible monadic structure (e.g., nondeterminism as represented by the list monad) can be added as a purely definitional extension, without requiring a reinterpretation of the whole language. The paper includes an implementation of the construction (in Standard ML with some New Jersey extensions) and several examples.
p3111
aVWe unify previous work on the continuationpassing style (CPS) transformations in a generic framework based on Moggi's computational metalanguage. This framework is used to obtain CPS transformations for a variety of evaluation strategies and to characterize the corresponding administrative reductions and inverse transformations. We establish generic formal connections between operational semantics and equational theories. Formal properties of transformations for specific evaluation orders follow as corollaries.\u000aEssentially, we factor transformations through Moggi's computational m\u000aetalanguage. Mapping &lgr;terms into the metalanguage captures computation properties (e.g., partiality, strictness) and evaluation order explicitly in both the term and the type structure of the metalanguage. The CPS transformation is then obtained by applying a generic transformation from terms and types in the metalanguage to CPS terms and types, based on a typed term representation of the continuation monad. We prove an adequacy property for the generic transformation and establish an equational correspondence between the metalanguage and CPS terms.\u000aThese generic results generalize Plotkin's seminal theorems, subsume more recent results, and enable new uses of CPS transformations and their inverses. We discuss how to aply these results to compilation.
p3112
aVFormal calculi of record structures have recently been a focus of active research. However, scarcely anyone has studied formally the dual notion\u2014i.e., argumentpassing to functions by keywords, and its harmonization with currying. We have. Recently, we introduced the labelselective &lgr;calculus, a conservative extension of &lgr;calculus that uses a labeling of abstractions and applications to perform unordered currying. In other words, it enables some form of commutation between arguments. This improves program legibility, thanks to the presence of labels, and efficiency, thanks to argument commuting. In this paper, we propose a simply typed version of the calculus, then extend it to one with MLlike polymorphic types. For the latter calculus, we establish the existence of principal types and we give an algorithm to compute them. Thanks to the fact that labelselective &lgr;calculus is a conservative extension of &lgr;calculus by adding numeric labels to stand for argument positions, its polymorphic typing provides us with a keyword argumentpassing extension of ML obviating the need of records. In this context, conventional ML syntax can be seen as a restriction of the more general keywordoriented syntax limited to using only implicit positions instead of keywords.
p3113
aV&lgr;v is an extension of the &lgr;calculus with a binding construct for local names. The extension has properties analogous to classical &lgr;calculus and preserves all observational equivalences of &lgr;. It is useful as a basis for modeling widespectrum languages that build on a functional core.
p3114
aVThis paper gives a systematic description of several calculi of explicit substitutions. These systems are orthogonal and have easy proofs of termination of their substitution calculus. The last system, called &lgr;v, entails a very simple environment machine for strong normalization of &lgr;terms.
p3115
aVWe describe and prove the correctness of a new concurrent markandsweep garbage collection algorithm. This algorithm derives from the classical onthefly algorithm from Dijkstra et al. [9]. A distinguishing feature of our algorithm is that it supports multiprocessor environments where the registers of running processes are not readily accessible, without imposing any overhead on the elementary operations of loading a register or reading or initializing a field. Furthermore our collector never blocks running mutator processes except possibly on requests for free memory; in particular, updating a field or creating or marking or sweeping a heap object does not involve systemdependent synchronization primitives such as locks. We also provide support for process creation and deletion, and for managing an extensible heap of variablesized objects.
p3116
aVConcurrent ML (CML) is an extension of the functional language Standard ML(SML) with primitives for the dynamic creation of processes and channels and for the communication of values over channels. Because of the powerful abstraction mechanisms the communication topology of a given program may be very complex and therefore an efficient implementation may be facilitated by knowledge of the topology.\u000aThis paper presents an analysis for determining when a bounded number \u000aof processes and channels will be generated. The analysis proceeds in two stages. First we extend a polymorphic type system for SML to deduce not only the type of CML programs but also their communication behaviour expressed as terms in a new process algebra. Next we develop an analysis that given the communication behaviour predicts the number of processes and channels required during the execution of the CML program. The correctness of the analysis is proved using a subject reduction property for the type system.
p3117
aVWe develop a compositional proofsystem for the partial correctness of concurrent constraint programs. Soundness and (relative) completeness of the system are proved with respect to a denotational semantics based on the notion of strongest postcondition. The strongest postcondition semantics provides a justification of the declarative nature of concurrent constraint programs, since it allows to view programs as theories in the specification logic.
p3118
aVIt is well known that adding side effects to functional languages changes the operational equivalences of the language. We develop a new language construct, encap, that forces imperative pieces of code to behave purely functionally, i.e.,without any visible side effects. The coercion operator encap provides a means of extending the simple reasoning principles for equivalences of code in a functional language to a language with side effects. In earlier work, similar coercion operators were developed, but their correctness required the underlying functional language to include parallel operations. The coercion operators developed here are simpler and are proven correct for purely sequential languages. The sequential setting  requires the construction of fully abstract models for sequential callbyvalue languages and the formulation of a weak form of \u201cmonad\u201d suitable for expressing the semantics of callbyvalue languages with side effects.
p3119
aVStandard specification languages have very limited abilities to define new operations on processes. We introduce the concept of a Protean specification language, with general definitional facilities supported by the appropriate theory. Protean languages allow elegant, readable, and useful specifications at all levels of abstraction. A good Protean specification language will admit methods of verifying that one specification is a refinement of another. We sketch a family of Protean specification languages (with references to the full details) which allow a vast amount of expressive power in defining operations, but nonetheless have all the essential theoretical and specification power of CCS and ACP.We illustrate these techniques by presenting several specifications of the job of protecting an arbitrary server by a checkpoint/backup scheme. The highlevel specification of the protected server simply says, \u201cIt does everything it did before, and it doesn't crash.\u201d The middlelevel specification describes checkpointing cleanly and abstractly, without prescribing any particular implementation. The lowlevel specification is fairly close to an implementation. We show the high and mediumlevel specifications equivalent by bisimulation relation techniques, and the medium and lowlevel specifications equivalent by equational reasoning using automaticallygenerated equations. We also show that the operations expressing checkpointing behavior are not definable in standard process algebras.
p3120
aVWe present the extensional polymorphism, a framework to type check ad hoc polymorphic functions. This formalism is compatible with parametric polymorphism, and supports a large class of functions defined by structural pattern matching on types.
p3121
aVTraditional techniques for implementing polymorphism use a universal representation for objects of unknown type. Often, this forces a compiler to use universal representations even if the types of objects are known. We examine an alternative approach for compiling polymorphism where types are passed as arguments to polymorphic routines in order to determine the representation of an object. This approach allows monomorphic code to use natural, efficient representations, supports separate compilation of polymorphic definitions and, unlike coercionbased implementations of polymorphism, natural representations can be used for mutable objects such as refs and arrays.We are particularly interested in the typing properties of an intermediate language that allows runtime type  analysis to be coded within the language. This allows us to compile many representation transformations and many language features without adding new primitive operations to the language. In this paper, we provide a core target language where typeanalysis operators can be coded within the language and the types of such operators can be accurately tracked. The target language is powerful enough to code a variety of useful features, yet type checking remains decidable. We show how to translate an MLlike language into the target language so that primitive operators can analyze types to produce efficient representations. We demonstrate the power of the \u201cuserlevel\u201d operators by coding flattened tuples, marshalling, type classes, and a form of type dynamic within the  language.
p3122
aVwe present a variety of the Standard ML module system where parameterized abstract types (i.e. functors returning generative types) map provably equal arguments to compatible abstract types, instead of generating distinct types at each applications as in Standard ML. This extension solves the full transparency problem (how to give syntactic signatures for higherorder functors that express exactly their propagation of type equations), and also provides better support for nonclosed code fragments.
p3123
aVThe programming language Standard ML provides firstorder functors, i.e. modules parameterized by modules. Firstorder functors in the language have a simple and elegant static semantics. The type structure of higherorder modules, i.e. modules parameterized by functors, is well understood. But it is only in the recent past that we have seen an implementation of higherorder functors with a formally defined static semantics in a dialect of Standard ML, SML/NJ. A study of this static semantics shows it to be much more complicated than the static semantics of firstorder functors. This paper investigates whether we can trade some semantic features in the module language to obtain a simpler static semantics, closer in spirit to that of firstorder functors. This work helps in a conceptual understanding of the semantics of higherorder modules.
p3124
aVWe investigate type inference for programming languages with subtypes. As described in previous work, there are several type inference problems for any given expression language, depending on the form of the subtype partial order and the ability to define new subtypes in programs. Our first main result is that for any specific subtype partial order, the problem of determining whether a lambda term is typable is algorithmically (polynomialtime) equivalent to a form of satisfiability problem over the same partial order. This gives the first exact characterization of the problem that is independent of the syntax of expressions. In addition, since this form of satisfiability problem is PSPACEhard over certain partial orders, this equivalence strengthens the previous lower bound of NPhard to PSPACEhard. Our second main result is a lower bound on the length of most general types when the subtype hierarchy may change as a result of additional type declarations within the program. More specifically, given any input expression, a type inference algorithm tries to find a most general (or principal) typing. The property of a most general typing is that it has all other possible typings as instances. However, there are several sound notions of instance in the presence of subtyping. Our lower bound is that no sound definition of instance would allow the set of additional subtyping hypotheses about a term to grow less than linearly in the size of the term.
p3125
aVThe statement S\u2264T in a &lgr;calculus with subtyping is traditionally interpreted by a semantic coercion function of type [[S]]\u2192[[T]] that extracts the \u201cT part\u201d of an element of S. If the subtyping relation is restricted to covariant positions, this interpretation may be enriched to include both the implicit coercion and an overwriting function put[S,T]  \u2208   [[S]]\u2192[[T]]\u2192[[S]] that updates the T part of an element of S. We give a realizability model and a sound equational theory  for a secondorder calculus of positive subtyping.Though weaker than familiar calculi of bounded quantification, positive subtyping retains sufficient power to model objects, encapsulation, and message passing. Moreover, inheritance may be implemented very straightforwardly in this setting, using the put functions arising from ordinary subtyping of records in place of the sophisticated systems of record extension and update often used for this purpose. The equational laws relating the behavior of coercions and put functions can be used to prove simple properties of the resulting classes in such a way that proofs for superclasses are \u201cinherited\u201d by subclasses.
p3126
aVWe investigate implementation techniques arising directly from Girard's Geometry of Interaction semantics for Linear Logic, specifically for a simple functional programming language (PCF). This gives rise to a very simple, compact, compilation schema and runtime system. We analyse various properties of this kind of computation that suggest substantial optimisations that could make this paradigm of implementation not only practical, but potentially more efficient than extant paradigms.
p3127
aVThe future annotations of MultiLisp provide a simple method for taming the implicit parallelism of functional programs. Past research concerning futures has focused on implementation issues. In this paper, we present a series of operational semantics for an idealized functional language with futures with varying degrees of intensionality. We develop a setbased analysis algorithm from the most intensional semantics, and use that algorithm to perform touch optimization on programs. Experiments with the Gambit compiler indicates that this optimization substantially reduces program execution times.
p3128
aVWe call language L1 intensionally more expressive than L2 if there are functions which can be computed faster in L1 than in L2. We study the intensional expressiveness of several languages: the BerryCurien programming language of sequential algorithms, CDS0, a deterministic parallel extension to it, named CDSP, and various parallel extensions to the functional programming language PCF. The paper consists of two parts.In the first part, we show that CDS0 can compute the minimum of two numbers n and p in unary representation in time O(min(n,p)). However, it  cannot compute a \u201cnatural\u201d version of this function. CDSP allows us to compute this function, as well as functions like parallelor. This work can be seen as an extension of the work of Colson with primitive recursive algorithms to the setting of sequential algorithms.In the second part, we show that deterministic parallelism adds intensional expressiveness, settling a \u201cfolk\u201d conjecture from the literature in the negative. We show that CDSP is more expressive intensionally than CDS0. We also study three parallel extensions to PCF: parallelor (por) and parallel conditionals on booleans (pif&ogr;) and integers (pif&igr;). The situation is more complicated there:  pif&igr; is still more expressive than both pif&ogr; and por. However, pif&igr; still is not as expressive as the deterministic query construct of CDSP. Thus, we identify a hierarchy of intensional expressiveness for deterministic parallelism.
p3129
aVThe goal of program transformation is to improve efficiency while preserving meaning. One of the best known transformation techniques is Burstall and Darlington's unfoldfold method. Unfortunately the unfoldfold method itself guarantees neither improvement in efficiency nor totalcorrectness. The correctness problem for unfoldfold is an instance of a strictly more general problem: transformation by locally equivalencepreserving steps does not necessarily preserve (global) equivalence.This paper presents a condition for the total correctness of transformations on recursive programs, which, for the first time, deals with higherorder functional languages (both strict and nonstrict) including lazy data structures. The main technical result is an improvement  theorem which says that if the local transformation steps are guided by certain optimisation concerns (a fairly natural condition for a transformation, then correctness of the transformation follows.The improvement theorem makes essential use of a formalised improvementtheory; as a rather pleasing corollary it also guarantees that the transformed program is a formal improvement over the original. The theorem has immediate practical consequences:\u2022 It is a powerful tool for proving the correctness of existing transformation methods for higherorder functional programs, without having to ignore crucial factors such as memoization or folding. We have applied the theorem to obtain a particularly simple proof of correctness  for a higherorder variant of deforestation.\u2022 It yields a simple syntactic method for guiding and constraining the unfold/fold method in the general case so that total correctness (and improvement) is always guaranteed.
p3130
aVThe mismatch between the operational semantics of the lambda calculus and the actual behavior of implementations is a major obstacle for compiler writers. They cannot explain the behavior of their evaluator in terms of source level syntax, and they cannot easily compare distinct implementations of different lazy strategies. In this paper we derive an equational characterization of callbyneed and prove it correct with respect to the original lambda calculus. The theory is a strictly smaller theory than the lambda calculus. Immediate applications of the theory concern the correctness proofs of a number of implementation strategies, e.g., the callbyneed continuation passing transformation and the realization of sharing via assignments.
p3131
aVThe efficiency of resolutionbased logic programming languages, such as Prolog, depends critically on selecting and executing sets of applicable clause heads to resolve against subgoals. Traditional approaches to this problem have focused on using indexing to determine the smallest possible applicable set. Despite their usefulness, these approaches ignore the nondeterminism inherent in many programming languages to the extent that they do not attempt to optimize execution after the applicable set theory has been determined.Unification factoring seeks to rectify this omission by regarding the indexing and unification phases of clause resolution as a single process. This paper formalizes that process through the construction of factoring automata. A polynomialtime algorithm is given for constructing optimal factoring automata which preserve the clause selection strategy of Prolog. More generally, when the clause selection strategy is not fixed, constructing such an optimal automaton is shown to be NPcomplete, solving an open trie minimization problem.Unification factoring is implemented through a source code transformation that preserves the full semantics of Prolog. This transformation is specified in the paper, and using it, several wellknown programs show performance improvements of up to 100% across three different systems. A prototype of unification factoring is available by anonymous ftp.
p3132
aVIn this paper we present substantially improved thread partitioning algorithms for modern implicitly parallel languages. We present a new block partitioning algorithm, separation constraint partitioning, which is both more powerful and more flexible than previous algorithms. Our algorithm is guaranteed to derive maximal threads. We present a theoretical framework for proving the correctness of our partitioning approach, and we show how separation constraint partitioning makes interprocedural partitioning viable.We have implemented the partitioning algorithms in an Id90 compiler for workstations and parallel machines. Using this experimental platform, we quantify the effectiveness of different partitioning schemes on whole applications.
p3133
aVWe extend the model of [SJG94b] to express strong timeouts (and preemption): if an event A does not happen through time t, cause event B to happen at time t. Such constructs arise naturally in practice (e.g. in modeling transistors) and are supported in languages such as ESTEREL (through instantaneous watchdogs) and LUSTRE (through the \u201ccurrent\u201d operator).The fundamental conceptual difficulty posed by these operators is that they are nonmonotonic. We provide a simple compositional semantics to the nonmonotonic version of concurrent constraint programming (CCP) obtained by changing the underlying logic from intuitionist logic to Reiter's default logic. This allows us to use the same  construction (uniform extension through time) to develop Default Timed CCP (Default tcc) as we had used to develop Timed CCP (tcc) from CCP. Indeed the smooth embedding of CCP processes into Default cc processes lifts to a smooth embedding of tcc processes into Default tcc processes. Interesting tcc properties such as determinacy, multiform time, a uniform preemption construct (\u201cclock\u201d), fullabstraction, and compositional compilation into automata are preserved.Default tcc thus provides a simple and natural (denotational) model capable of representing the full range of preemption constructs supported in ESTEREL, LUSTRE and other synchronous programming languages.
p3134
aVObliq is a lexicallyscoped, untyped, interpreted language that supports distributed objectoriented computation. Obliq objects have state and are local to a site. Obliq computations can roam over the network, while maintaining network connections. Distributed lexical scoping is the key mechanism for managing distributed computation.
p3135
aVProcedure calling conventions are used to provide uniform procedurecall interfaces. Applications, such as compilers and debuggers, which generate, or process procedures at the machinelanguage abstraction level require knowledge of the calling convention. In this paper, we develop a formal model for procedure calling conventions called PFSA's. Using this model, we are able to ensure several completeness and consistency properties of calling conventions. Currently, applications that manipulate procedures implement conventions in an adhoc manner. The resulting code is complicated with details, difficult to maintain, and often riddled with errors. To alleviate the situation, we introduce a calling convention specification language, called CCL. The combination of CCL and PFSA's facilitates the accurate specification of conventions that can be shown to be both consistent and complete.
p3136
aVConcurrent objectoriented programming (COOP) languages focus the abstraction and encapsulation power of abstract data types on the problem of concurrency control. In particular, pure finegrained concurrent objectoriented languages (as opposed to hybrid or data parallel) provides the programmer with a simple, uniform, and flexible model while exposing maximum concurrency. While such languages promise to greatly reduce the complexity of largescale concurrent programming, the popularity of these languages has been hampered by efficiency which is often many orders of magnitude less than that of comparable sequential code. We present a sufficiency set of techniques which enables the efficiency of finegrained concurrent objectoriented languages to equal that of traditional sequential languages (like C) when the required data is available. These techniques are empirically validated by the application to a COOP implementation of the Livermore Loops.
p3137
aVThis paper introduces superoperators, an optimization technique for bytecoded interpreters. Superoperators are virtual machine operations automatically synthesized from smaller operations to avoid costly peroperation overheads. Superoperators decrease executable size and can double or triple the speed of interpreted programs. The paper describes a simple and effective heuristic for inferring powerful superoperators from the usage patterns of simple operators.The paper describes the design and implementation of a hybrid translator/interpreter that employs superoperators. From a specification of the superoperators (either automatically inferred or manually chosen), the system builds an efficient implementation of the virtual machine in assembly language. The system is easily retargetable and currently runs on the MIPS R3000 and the SPARC.
p3138
aVWe show how a set of building blocks can be used to construct programming language interpreters, and present implementations of such building blocks capable of supporting many commonly known features, including simple expressions, three different function call mechanisms (callbyname, callbyvalue and lazy evaluation), references and assignment, nondeterminism, firstclass continuations, and program tracing.The underlying mechanism of our system is monad transformers, a simple form of abstraction for introducing a wide range of computational behaviors, such as state, I/O, continuations, and exceptions.Our work is significant in the following respects. First, we have succeeded in designing a fully modular interpreter based on monad transformers that incudes features missing from Steele's, Espinosa's, and Wadler's earlier efforts. Second, we have found new ways to lift monad operations through monad transformers, in particular difficult cases not achieved in Moggi's original work. Third, we have demonstrated that interactions between features are reflected in liftings and that semantics can be changed by reordering monad transformers. Finally, we have implemented our interpreter in Gofer, whose constructor classes provide just the added power over Haskell's type classes to allow precise and convenient expression of our ideas. This implementation includes a method for constructing extensible unions and a form of subtyping that is interesting in its own right.
p3139
aVIn the early 80's Oles and Reynolds devised a semantic model of Algollike languages using a category of functors from a category of store shapes to the category of predomains. Here we will show how a variant of this idea can be used to define the translation of an Algollike language to intermediate code in a uniform way that avoids unnecessary temporary variables, provides controlflow translation of boolean expressions, permits online expansion of procedures, and minimizes the storage overhead of calls of closed procedures. The basic idea is to replace continuations by instruction sequences and store shapes by descriptions of the structure of the runtime stack.
p3140
aVDepthfirst search is the key to a wide variety of graph algorithms. In this paper we express depthfirst search in a lazy functional language, obtaining a lineartime implementation. Unlike traditional imperative presentations, we use the structuring methods of functional languages to construct algorithms from individual reusable components. This style of algorithm construction turns out to be quite amenable to formal proof, which we exemplify through a calculationalstyle proof of a far from obvious stronglyconnected components algorithm.
p3141
aVWe present the first profiler for a compiled, nonstrict, higherorder, purely functional language capable of measuring time as well as space usage. Our profiler is implemented in a productionquality optimising compiler for Haskell, has low overheads, and can successfully profile large applications.A unique feature of our approach is that we give a formal specification of the attribution of execution costs to cost centres. This specification enables us to discuss our design decisions in a precise framework. Since it is not obvious how to map this specification onto a particular implementation, we also present an implementationoriented operational semantics, and prove it equivalent to the specification.
p3142
aVFlowbased safety analysis of higherorder languages has been studied by Shivers, and Palsberg and Schwartzbach. Open until now is the problem of finding a type system that accepts exactly the same programs as safety analysis.In this paper we prove that Amadio and Cardelli's type system with subtyping and recursive types accepts the same programs as a certain safety analysis. The proof involves mappings from types to flow information and back. As a result, we obtain an inference algorithm for the type system, thereby solving an open problem.
p3143
aVProgram slicing is a technique for isolating computational threads in programs. In this paper, we show how to mechanically extract a family of practical algorithms for computing slices directly from semantic specifications. These algorithms are based on combining the notion of dynamic dependence tracking in term rewriting systems with a program representation whose behavior is defined via an equational logic. Our approach is distinguished by the fact that changes to the behavior of the slicing algorithm can be accomplished through simple changes in rewriting rules that define the semantics of the program representation. Thus, e.g., different notions of dependence may be specified, properties of languagespecific datatypes can be exploited, and various time, space, and precision tradeoffs may be made. This flexibility enables us to generalize the traditional notions of static and dynamic slices to that of a constrained slice, where any subset of the inputs of a program may be supplied.
p3144
aVWe describe a framework for flow analysis in higherorder languages. It is both a synthesis and extension of earlier work in this area, most notably [20, 22]The framework makes explicit use of flow graphs for modeling control and data flow properties of untyped higherorder programs. The framework is parameterized, and can express a hierarchy of analyses with different cost/accuracy tradeoffs. The framework is also amenable to a direct, efficient implementation.We develop several instantiations of the framework, and prove their runningtime complexity. In addition, we use the simplest instantiation to demonstrate the equivalence of a 0CFA style analysis and the setbased analysis of [8].
p3145
aVThis paper presents a general framework for deriving demanddriven algorithms for interprocedural data flow analysis of imperative programs. The goal of demanddriven analysis is to reduce the time and/or space overhead of conventional exhaustive analysis by avoiding the collection of information that is not needed. In our framework, a demand for data flow information is modeled as a set of date flow queries. The derived demanddriven algorithms find responses to these queries through a partial reversal of the respective data flow analysis. Depending on whether minimizing time or space is of primary concern, result caching may be incorporated in the derived algorithm. Our framework is applicable to interprocedural data flow problems with a finite domain set. If the problem's flow functions are distributive, the derived demand algorithms provide as precise information as the corresponding exhaustive analysis. For problems with monotone but nondistributive flow functions the provided data flow solutions are only approximate. We demonstrate our approach using the example of interprocedural copy constant propagation.
p3146
aVThe paper shows how a large class of interprocedural dataflowanalysis problems can be solved precisely in polynomial time by transforming them into a special kind of graphreachability problem. The only restrictions are that the set of dataflow facts must be a finite set, and that the dataflow functions must distribute over the confluence operator (either union or intersection). This class of probable problems includes\u2014but is not limited to\u2014the classical separable problems (also known as \u201cgen/kill\u201d or \u201cbitvector\u201d problems)\u2014e.g., reaching definitions, available expressions, and live variables. In addition, the class of problems that our techniques handle includes many nonseparable problems, including trulylive variables, copy constant propagation, and possiblyuninitialized variables.Results are reported from a preliminary experimental study of C programs (for the problem of finding possiblyuninitialized variables).
p3147
aVDataflow analysis framework based on Static Single Assignment (SSA) form and Sparse Evaluation Graphs (SEGs) demand fast computation of program points where data flow information must be merged, the socalled &fgr;nodes. In this paper, we present a surprisingly simple algorithm for computing &fgr;nodes for arbitrary flowgraphs (reducible or irreducible) that runs in linear time. We employ a novel program representation\u2014the DJ graph\u2014by augmenting the dominator tree of a flowgraph with edges which may lead to a potential \u201cmerge\u201d of dataflow information. In searching for &fgr;nodes we never visit an edge in the DJgraph more than once by guiding the search of nodes by their levels in the dominator  tree.The algorithm has been implemented and the results are compared with the well known algorithm due to Cytron et al. A consistent and significant speedup has been observed over a range of 46 Fortran procedures taken from a number of benchmark programs. We also ran experiments on increasingly taller ladder graphs and confirmed the linear time complexity of our algorithm.
p3148
aVThe paper presents methods that we have implemented to improve the quality of the defuses reported for dynamically allocated locations. The methods presented are based on the Ruggieri/Murtagh naming scheme for dynamically created locations. We expand upon this scheme to name dynamically allocated locations for some user written allocation routines. Using this expanded naming scheme, we introduce an inexpensive, noniterative, and localized calculation of extended must alias analysis to handle dynamically allocated locations, and show how this information can be used to improve defuse information. This is the first attempt to specify must alias information for names which represent a set of dynamically allocated locations. Empirical results are presented to illustrate the usefulness of our method. We consider this work a step towards developing practical reengineering tools for C.
p3149
aVThe ring is a useful means of structuring concurrent processes. Processes communicate by passing a token in a fixed direction; the process that possesses the token is allowed to make certain moves. Usually, correctness properties are expected to hold irrespective of the size of the ring. We show that the problem of checking many useful correctness properties for rings of all sizes can be reduced to checking them on a ring of small size. The results do not depend on the processes being finite state. We illustrate our results on examples.
p3150
aVWe investigate the verification problem of infinitestate process w.r.t. logicbased specifications that express properties which may be nonregular. We consider the process algebra PA which integrates and strictly subsumes the algebras BPA (basic process algebra) and BPP (basic parallel processes), by allowing both sequential and parallel compositions as well as nondeterministic choice and recursion. Many relevant properties of PA processes are nonregular, and thus can be expressed neither by classical temporal logics nor by finite state &ohgr;automata. Properties of particular interest are those involving constraints on numbers of occurrences of events. In order to express such properties, which are nonregular in general, we use the temporal logic PCTL which combines the  branchingtime temporal logic CTL with Presburger arithmetics. Then we tackle the verification problem of guarded PA processes w.r.t. PCTL formulas. We mainly prove that, while this problem is undecidable for the full PCTL, it is actually decidable for the class of guarded PA processes (and thus for the class of guarded BPA's and guarded BPP's), and a large fragment of PCTL called PCTL+.
p3151
aVThis paper is a contribution to the already existing series of work on the algorithmic principles of interprocedural analysis. We consider the generalization to the case of parallel programs. We give algorithms that compute the sets of backward resp. forward reachable configurations for parallel flow graph systems in linear time in the size of the graph viz. the program. These operations are important in dataflow analysis and in model checking. In our method, we first model configurations as terms (viz. trees) in the process algebra PA that can express call stack operations and parallelism. We then give a 'declarative' Hornclause specification of the sets of predecessors resp. successors. The 'operational' computation of these sets is carried out using the DowlingGallier procedure for HornSat.
p3152
aVThis paper describes a new approach to generic functional programming, which allows us to define functions generically for all datatypes expressible in Haskell. A generic function is one that is defined by induction on the structure of types. Typical examples include pretty printers, parsers, and comparison functions. The advanced type system of Haskell presents a real challenge: datatypes may be parameterized not only by types but also by type constructors, type definitions may involve mutual recursion, and recursive calls of type constructors can be arbitrarily nested. We show that\u2014despite this complexity\u2014a generic function is uniquely defined by giving cases for primitive types and type constructors (such as disjoint unions and cartesian products). Given this information a generic function can be specialized to arbitrary Haskell datatypes. The key idea of the approach is to model types by terms of the simply typed &lgr;calculus augmented by a family of recursion operators. While conceptually simple, our approach places high demands on the type system: it requires polymorphic recursion, rankn types, and a strong form of type constructor polymorphism. Finally, we point out connections to Haskell's class system and show that our approach generalizes type classes in some respects.
p3153
aVIn modern Scheme, a macro captures the lexical environment where it is defined. This creates an opportunity for extending Scheme so that macros are firstclass values. The key to achieving this goal, while preserving the ability to compile programs into reasonable code, is the addition of a type system. Many interesting things can be done with firstclass macros, including the construction of a useful module system in which modules are also firstclass. Clams got legs! \u2014 B.C.
p3154
aVThe ambient calculus is a calculus of computation that allows active processes to move between sites. We present an analysis inspired by stateoftheart pointer analyses that safety and accurately predicts which processes may turn up at what sites during the execution of a composite system. The analysis models sets of processes by sets of regular tree grammars enhanced with contextdependent counts, and it obtains its precision by combining a powerful redex materialisation with a strong redex reduction (in the manner of the strong updates performed in pointer analyses). The underlying ideas are flexible and scale up to general tree structures admitting powerful restructuring operations.
p3155
aVProcedure extraction is an important program transformation that can be used to make programs easier to understand and maintain, to facilitate code reuse, and to convert \u201cmonolithic\u201d code to modular or objectoriented code. Procedure extraction involves the following steps:\u000a\u000aThe statements to be extracted are identified (by the programmer or by a programming tool).\u000aIf the statements are not contiguous, they are moved together so that they form a sequence that can be extracted into a procedure, and so that the semantics of the original code is preserved.\u000aThe statements are extracted into a new procedure, and are replaced with an appropriate call.\u000a\u000aThis paper addresses step 2: in particular, the conditions under which it is possible to move a set of selected statements together so that they become \u201cextractable\u201d, while preserving semantics. Since semantic equivalence is, in general, undecidable, we identify sufficient conditions based on control and data dependences, and define an algorithm that moves the selected statements together when the conditions hold. We also include an outline of a proof that our algorithm is semanticspreserving.\u000aWhile there has been considerable previous work on procedure extraction, we believe that this is the first paper to provide an algorithm for semanticspreserving procedures extraction given an arbitrary set of selected statements in an arbitrary controlflow graph.
p3156
aVIn this article, we add a third dimension to partial redundancy elimination by considering code size as a further optimization goal in addition to the more classical consideration of computation costs and register pressure. This results in a family of sparse code motion algorithms coming as modular extensions of the algorithms for busy and lazy code motion. Each of them optimally captures a predefined choice of priority between these three optimization goals, e.g. code size can be minimized while (1) guaranteeing at least the performance of the argument program, or (2) even computational optimality. Each of them can further be refined to simultaneously reduce the lifetimes of temporaries to a minimum. These algorithms are wellsuited for sizecritical application areas like smart cards and embedded systems, as they provide a handle to control the code replication problem of classical code motion techniques. In fact, we believe that our systematic, prioritybased treatment of tradeoffs between optimization goals may substantially decrease development costs of sizecritical applications: users may \u201cplay\u201d with the priorities until the algorithm automatically delivers a satisfactory solution.
p3157
aVVarious code certification systems allow the certification and static verification of important safety properties such as memory and controlflow safety. These systems are valuable tools for verifying that untrusted and potentially malicious code is safe before execution. However, one important safety property that is not usually included is that programs adhere to specific bounds on resource consumption, such as running time.\u000aWe present a decidable type system capable of specifying and certifying bounds on resource consumption. Our system makes two advances over previous resource bound certification systems, both of which are necessary for a practical system: We allow the execution time of programs and their subroutines to vary, depending on their arguments, and we provide a fully automatic compiler generating certified executables from sourcelevel programs. The principal device in our approach is a strategy for simulating dependent types using sum and inductive kinds.
p3158
aVDistributedmemory programs are often written using a global address space: any process can name any memory location on any processor. Some languages completely hide the distinction between local and remote memory, simplifying the programming model at some performance cost. Other languages give the programmer more explicit control, offering better potential performance but sacrificing both soundness and ease of use.\u000aThrough a series of progressively richer type systems, we formalize the complex issues surrounding sound computation with explicitly distributed data structures. We then illustrate how type inference can subsume much of this complexity, letting programmers work at whatever level of detail is needed. Experiments conducted with the Titanium programming language show that this can result in easier development and significant performance improvements over manual optimization of local and global memory.
p3159
aVWork on the TILT compiler for Standard ML led us to study a language with singleton kinds: S(A) is the kind of all types provably equivalent to the type A. Singletons are interesting because they provide a very general form of definitions for type variables, allow finegrained control of type computations, and allow many equational constraints to be expressed within the type system.\u000aInternally, TILT represents programs using a predicative variant of Girard's F&ohgr; enriched with singleton kinds, dependent product and function kinds (&Sgr; and &Pgr;), and a subkinding relation. An important benefit of using a typed language as the representation of programs is that typechecking can detect many common compiler implementation errors. However, the decidability of typechecking for our particular representation is not obvious. In order to typecheck a term, we must be able to determine whether two type constructors are provably equivalent. But in the presence of singleton kinds, the equivalence of type constructors depends both on the typing context in which they are compared and on the kind at which they are compared.\u000a In this paper we concentrate on the key issue for decidability of typechecking: determining the equivalence of wellformed type constructors. We define the &lgr;&Pgr;&Sgr;S< calculus, a model of the constructors and kinds of TILT's intermediate language. Inspired by Coquand's result for type theory, we prove decidability of constructor equivalence for &lgr;&Pgr;&Sgr;S\u2264 by exhibiting a novel \u2014 though slightly inefficient \u2014 typedirected comparison algorithm. The correctness of this algorithm is proved using an interesting variant of Kripkestyle logical relations: unary relations are indexed by a single possible world (in our case, a typing context), but binary relations are indexed by two worlds. Using this result we can then show the correctness of a natural, practical algorithm used by the TILT compiler.
p3160
aVJava source code is strongly typed, but the translation from Java source to bytecode omits much of the type information originally contained within methods. Type elaboration is a technique for reconstructing strongly typed programs from incompletely typed bytecode by inferring types for local variables.\u000aThere are situations where, technically, there are not enough types in the original type hierarchy to type a bytecode program. Subtype completion is a technique for adding necessary types to an arbitrary type hierarchy to make type elaboration possible for all verifiable Java bytecode.\u000aType elaboration with subtype completion has been implemented as part of the Marmot Java compiler.
p3161
aVProofcarrying code is a framework for proving the safety of machinelanguage programs with a machinecheckable proof. Previous PCC frameworks have defined typechecking rules as part of the logic. We show a universal type framework for proofcarrying code that will allow a code producer to choose a programming language, prove the type rules for that language as lemmas in higherorder logic, then use those lemmas to prove the safety of a particular program. We show how to handle traversal, allocation, and initialization of values in a wide variety of types, including functions, records, unions, existentials, and covariant recursive types.
p3162
aVWe study the abstract interpretation of temporal calculi and logics in a general syntax, semantics and abstraction independent setting. This is applied to the @@@@calculus, a generalization of the &mgr;calculus with new reversal and abstraction modalities as well as a new timesymmetric tracebased semantics. The more classical setbased semantics is shown to be an abstract interpretation of the tracebased semantics which leads to the understanding of modelchecking and its application to dataflow analysis as incomplete temporal abstract interpretations. Soundness and incompleteness of the abstractions are discussed. The sources of incompleteness, even for finite systems, are pointed out, which leads to the identification of relatively complete sublogics,  la CTL.
p3163
aVCertified code is a general mechanism for enforcing security properties. In this paradigm, untrusted mobile code carries annotations that allow a host to verify its trustworthiness. Before running the agent, the host checks the annotations and proves that they imply the host's security policy. Despite the flexibility of this scheme, so far, compilers that generate certified code have focused on simple type safety properties rather than more general security properties.\u000aSecurity automata can specify an expressive collection of security policies including access control and resource bounds. In this paper, we describe how to instrument welltyped programs with security checks and typing annotations. The resulting programs obey the policies specified by security automata and can be mechanically checked for safety. This work provides a foundation for the process of automatically generating certified code for expressive security policies.
p3164
aVSystems that authenticate a user based on a shared secret (such as a password or PIN) normally allow anyone to query whether the secret is a given value. For example, an ATM machine allows one to ask whether a string is the secret PIN of a (lost or stolen) ATM card. Yet such queries are prohibited in any model whose programs satisfy an informationflow property like Noninterference. But there is complexitybased justification for allowing these queries. A type system is given that provides the access control needed to prove that no welltyped program can leak secrets in polynomial time, or even leak them with nonnegligible probability if secrets are of sufficient length and randomly chosen. However, there are welltyped deterministic programs in a synchronous concurrent model capable of leaking secrets in linear time.
p3165
aVDesign patterns have earned a place in the developer's arsenal of tools and techniques for software development. They have proved so useful, in fact, that some have called for their promotion to programming language features. In turn this has rekindled the ageold debate over the mechanism that belong in programming languages versus those that are better served by tools. The debate comes full circle when one contemplates code generation and methodological tool support for patterns. The authors compare and contrast programming languages, tools, and patterns to assess their relative merits and to clarify their roles in the development process.
p3166
aVMany interactive Web services use the CGI interface for communication with clients. They will dynamically create HTML documents that are presented to the client who then resumes the interaction by submitting data through incorporated form fields. This protocol is difficult to statically typecheck if the dynamic documents are created by arbitrary script code using printflike statements. Previous proposals have suggested using static document templates which trades flexibility for safety. We propose a notion of typed, higherorder templates that simultaneously achieve flexibility and safety. Our type system is based on a flow analysis of which we prove soundness. We present an efficient runtime implementation that respects the semantics of only welltyped programs. This work is fully implemented as part of the <bigwig> system for defining interactive Web services.
p3167
aVAdopting a programminglanguage perspective, we study the problem of implementing authentication in a distributed system. We define a process calculus with constructs for authentication and show how this calculus can be translated to a lowerlevel language using marshaling, multiplexing, and cryptographic protocols. Authentication serves for identitybased security in the source language and enables simplifications in the translation. We reason about correctness relying on the concepts of observational equivalence and full abstraction.
p3168
aVWe introduce a language for creating and manipulating certificates, that is, digitally signed data based on public key cryptography, and a system for revoking certificates. Our approach provides a uniform mechanism for secure distribution of public key bindings, authorizations, and revocation information. An external language for the description of these and other forms of data is compiled into an intermediate language with a welldefined denotational and operational semantics. The internal language is used to carry out consistency checks for security, and optimizations for efficiency. Our primary contribution is a technique for treating revocation data dually to other sorts of information using a polarity discipline in the intermediate language.
p3169
aVSetbased analysis of logic programs provides an accurate method for descriptive typechecking of logic programs. The key idea of this method is to upper approximate the least model of the program by a regular set of trees. In 1991, Frhwirth, Shapiro, Vardi and Yardeni raised the question whether it can be more efficient to use the domain of sets of paths instead, i.e., to approximate the least model by a regular set of words. We answer the question negatively by showing that typechecking for pathbased analysis is as hard as the setbased one, that is DEXPTIMEcomplete. This result has consequences also in the areas of set constraints, automata theory and model checking.
p3170
aVIn this paper we describe the syntax, semantics, and implementation of the constraint logic programming language CLP(F) and we prove that the implementation is sound. A CLP(F) constraint is a conjunction of equations and inequations in a first order theory of analytic univariate functions over the reals. The theory allows vectorvalued functions over closed intervals to be constrained in several ways, including specifying functional equations (possibly involving the differentiation operator) that must hold at each point in the domain, arithmetic constraints on the value of a function at a particular point in its domain, and bounds on the range of a function over its domain. After describing the syntax and semantics of the constraint language for CLP(F) and giving several examples, we show how to convert these analytic constraints into a subclass of simpler functional constraints which involve neither differentiation nor evaluation of functions. We then present an algorithm for solving these latter constraints and prove that it is sound. This implies the soundness of the CLP(F) interpreter. We also provide some timing results from an implementation of CLP(F) based on GNU Prolog. The current implementation is able to solve a wide variety of analytic constraints, but on particular classes of constraints (such as initial value problems for autonomous ODEs), it is not competitive with other nonconstraint based, interval solvers such as Lohner's AWA system. CLP(F) should be viewed as a first step toward the long term goal of developing a practical, declarative, logicbased approach to numerical analysis.
p3171
aVTwo forms of interferences are individuated in Cardelli and Gordon's Mobile Ambients (MA): plain interferences, which are similar to the interferences one finds in CCS and &phgr;calculus; and grave interferences, which are more dangerous and may be regarded as programming errors. To control interferences, the MA movement primitives are modified. On the new calculus, the Mobile Safe Ambients (SA), a type system is defined that: controls the mobility of ambients; removes all grave interferences. Other advantages of SA are: a useful algebraic theory; programs sometimes more robust (they require milder conditions for correctness) and/or simpler. These points are illustrated on several examples.
p3172
aVThe Ambient Calculus is a process calculus where processes may reside within a hierarchy of locations and modify it. The purpose of the calculus is to study mobility, which is seen as the change of spatial configurations over time. In order to describe properties of mobile computations we devise a modal logic that can talk about space as well as time, and that has the Ambient Calculus as a model.
p3173
aVWe present a general framework for combining program verification and program analysis. This framework enhances program analysis because it takes advantage of user assertions, and it enhances program verification because assertions can be refined using automatic program analysis. Both enhancements in general produce a better way of reasoning about programs than using verification techniques alone or analysis techniques alone. More importantly, the combination is better than simply running the verification and analysis in isolation and then combining the results at the last step. In other words, our framework explores synergistic interaction between verification and analysis.\u000aIn this paper, we start with a representation of a program, user assertions, and a given analyzer for the program. The framework we describe induces an algorithm which exploits the assertions and the analyzer to produce a generally more accurate analysis. Further, it has some important features:\u000a\u000ait is flexible: any number of assertions can be used anywhere;\u000ait is open: it can employ an arbitrary analyzer;\u000ait is modular: we reason with conditional correctness of assertions;\u000ait is incremental: it can be tuned for the accuracy/efficiency tradeoff.
p3174
aVMark and sweep garbage collectors are known for using time proportional to the heap size when sweeping memory, since all objects in the heap, regardless of whether they are live or not, must be visited in order to reclaim the memory occupied by dead objects. This paper introduces a sweeping method which traverses only the live objects, so that sweeping can be done in time dependent only on the number of live objects in the heap.\u000aThis allows each collection to use time independent of the size of the heap, which can result in a large reduction of overall garbage collection time in empty heaps. Unfortunately, the algorithm used may slow down overall garbage collection if the heap is not so empty. So a way to select the sweeping algorithm depending on the heap occupancy is introduced, which can avoid any significant slowdown.
p3175
aVOne aspect of security in mobile code is privacy: private (or secret) data should not be leaked to unauthorised agents. Most of the work on secure information flow has until recently only been concerned with detecting direct and indirect flows. Secret information can however be leaked to the attacker also through covert channels. It is very reasonable to assume that the attacker, even as an external observer, can monitor the timing (including termination) behaviour of the program. Thus to claim a program secure, the security analysis must take also these into account.\u000aIn this work we present a surprisingly simple solution to the problem of detecting timing leakages to external observers. Our system consists of a type system in which welltyped programs do not leak secret information directly, indirectly or through timing, and a transformation for removing timing leakages. For any program that is well typed according to Volpano and Smith [VS97a], our transformation generates a program that is also free of timing leaks.
p3176
aVWe propose an automatic method to enforce trace properties on programs. The programmer specifies the property separately from the program; a program transformer takes the program and the property and automatically produces another \u201cequivalent\u201d pogram satisfying the property. This separation of concerns makes the program easier to develop and maintain. Our approach is both static and dynamic. It integrates static analyses in order to avoid useless transformations. On the other hand, it never rejects programs but adds dynamic checks when necessary. An important challenge is to make this dynamic enforcement as inexpensive as possible. The most obvious application domain is the enforcement of security policies. In particular, a potential use of the method is the securization of mobile code upon receipt.
p3177
aVThis paper attempts to address the question of why certain dataflow analysis problems can be solved efficiently, but not others. We focus on flowsensitive analyses, and give a simple and general result that shows that analyses that require the use of relational attributes for precision must be PSPACEhard in general. We then show that if the language constructs are slightly strengthened to allow a computation to maintain a very limited summary of what happens along an execution path, interprocedural analyses become EXPTIMEhard. We discuss applications of our results to a variety of analyses discussed in the literature. Our work elucidates the reasons behind the complexity results given by a number of authors, improves on a number of such complexity results, and exposes conceptual commonalities underlying such results that are not readily apparent otherwise.
p3178
aVInclusionbased program analyses are implemented by adding new edges to directed graphs. In most analyses, there are many different ways to add a transitive edge between two nodes, namely through each different path connecting the nodes. This path redundancy limits the scalability of these analyses. We present projection merging, a technique to reduce path redundancy. Combined with cycle elimination [7], projection merging achieves orders of magnitude speedup of analysis time on programs over that of using cycle elimination alone.
p3179
aVThis paper introduces a language feature, called implicit parameters, that provides dynamically scoped variables within a staticallytyped HindleyMilner framework. Implicit parameters are lexically distinct from regular identifiers, and are bound by a special with construct whose scope is dynamic, rather than static as with let. Implicit parameters are treated by the type system as parameters that are not explicitly declared, but are inferred from their use.\u000aWe present implicit parameters within a small callbyname &lgr;calculus. We give a type system, a type inference algorithm, and several semantics. We also explore implicit parameters in the wider settings of callbyneed languages with overloading, and callbyvalue languages with effects. As a witness to the former, we have implemented implicit parameters as an extension of Haskell within the Hugs interpreter, which we use to present several motivating examples.
p3180
aVThis invited talk will give a personal view of the field of computer security and summarize some ways that methods from the study of programming language principles can be applied to problems in computer security. Some background information is provided here in this short document.
p3181
aVWe study the interaction of the "new" construct with a rich but common form of (firstorder) communication. This interaction is crucial in security protocols, which are the main motivating examples for our work; it also appears in other programminglanguage contexts. Specifically, we introduce a simple, general extension of the pi calculus with value passing, primitive functions, and equations among terms. We develop semantics and proof techniques for this extended language and apply them in reasoning about some security protocols.
p3182
aVThis paper addresses the design and verification of infrastructure for mobile computation. In particular, we study language primitives for communication between mobile agents. They can be classi ed into two groups. At a low level there are location dependent primitives that require a programmer to know the current site of a mobile agent in order to communicate with it. At a high level there are location independent primitives that allow communication with a mobile agent irrespective of any migrations. Implementation of the high level requires delicate distributed infrastructure algorithms. In earlier work with Wojciechowski and Pierce we made the two levels precise as process calculi, allowing such algorithms to be expressed as encodings of the high level into the low level; we built NOMADIC PICT, a distributed programming language for experimenting with such encodings. In this paper we turn to semantics, giving a de nition of the core language and proving correctness of an example infrastructure. This requires novel techniques: we develop equivalences that take migration into account, and reasoning principles for agents that are temporarily immobile (eg. waiting on a lock elsewhere in the system).
p3183
aVWe propose a general, powerful framework of type systems for the &pi;calculus, and show that we can obtain as its instances a variety of type systems guaranteeing nontrivial properties like deadlockfreedom and racefreedom. A key idea is to express types and type environments as abstract processes: We can check various properties of a process by checking the corresponding properties of its type environment. The framework clarifies the essence of recent complex type systems, and it also enables sharing of a large amount of work such as a proof of type preservation, making it easy to develop new type systems.
p3184
aVWe present a variant of ProofCarrying Code (PCC) in which the trusted inference rules are represented as a higherorder logic program, the proof checker is replaced by a nondeterministic higherorder logic interpreter and the proof by an oracle implemented as a stream of bits that resolve the nondeterministic interpretation choices. In this setting, ProofCarrying Code allows the receiver of the code the luxury of using nondeterminism in constructing a simple yet powerful checking procedure.This oraclebased variant of PCC is able to adapt quite naturally to situations when the property being checked is simple or there is a fairly directed search procedure for it. As an example, we demonstrate that if PCC is used to verify type safety of assembly language programs compiled from Java source programs, the oracles that are needed are on the average just 12% of the size of the code, which represents an improvement of a factor of 30 over previous syntactic representations of PCC proofs.
p3185
aVThe region analysis of Tofte and Talpin is an attempt to determine statically the life span of dynamically allocated objects. But the calculus is at once intuitively simple, yet deceptively subtle, and previous theoretical analyses have been frustratingly complex: no analysis has revealed and explained in simple terms the connection between the subleties of the calculus and the imperative features it builds on. We present a novel approach for proving safety and correctness of a simplified version of the region calculus. We give a stratified operational semantics, composed of a highlevel semantics dealing with the conceptual difficulties of effect annotations, and a lowlevel one with explicit operations on a regionindexed store. The main results of the paper are a proof simpler than previous ones, and a modular approach to type safety and correctness. The flexibility of this approach is demonstrated by the simplicity of the extension to the full calculus with type and region polymorphism.
p3186
aVBy combining existing type systems with standard typebased compilation techniques, we describe how to write strongly typed programs that include a function that acts as a t racing garbage collector for the program. Since the garbage collector is an explicit function, we do not need to provide a t rusted garbage collector as a runtime service to manage memory.Since our language is strongly typed, the standard type soundness guarantee "Well typed programs do not go wrong" is extended to include the collector. Our type safety guarantee is nontrivial since not only does it guarantee the type safety of the garbage collector, but it guarantees that the collector preservers the type safety of the program being garbage collected. We describe the technique in detail and report performance measurements for a few microbenchmarks as well as sketch the proofs of type soundness for our system.
p3187
aVExploiting spatial and temporal locality is essential for obtaining high performance on modern computers. Writing programs that exhibit high locality of reference is difficult and errorprone. Compiler researchers have developed loop transformations that allow the conversion of programs to exploit locality. Recently, transformations that change the memory layouts of multidimensional arrays called data transformations have been proposed. Unfortunately, both data and loop transformations have some important drawbacks. In this work, we present an integrated framework that uses loop and data transformations in concert to exploit the benefits of both approaches while minimizing the impact of their disadvantages. Our approach works interprocedurally on acyclic call graphs, uses profile data to eliminate layout conflicts, and is unique in its capability of resolving conflicting layout requirements of different references to the same array in the same nest and in different nests for regular arraybased applications.The optimization technique presented in this paper has been implemented in a sourcetosource translator. We evaluate its performance using standard benchmark suites and several math libraries (complete programs) with large input sizes. Experimental results show that our approach reduces the overall execution times of original codes by 17.5% on the average. This reduction comes from three important characteristics of the technique, namely, resolving layout conflicts between references to the same array in a loop nest, determining a suitable order to propagate layout modifications across loop nests, and propagating layouts between different procedures in the program   all in a unified framework.
p3188
aVCurrent verification condition (VC) generation algorithms, such as weakest preconditions, yield a VC whose size may be exponential in the size of the code fragment being checked. This paper describes a twostage VC generation algorithm that generates compact VCs whose size is worstcase quadratic in the size of the source fragment, and is close to linear in practice.This twostage VC generation algorithm has been implemented as part of the Extended Static Checker for Java. It has allowed us to check large and complex methods that would otherwise be impossible to check due to time and space constraints.
p3189
aVWe consider the problem of monitoring an interactive device, such as an implementation of a network protocol, in order to check whether its execution is consistent with its specification. At rst glance, it appears that a monitor could simply follow the inputoutput trace of the device and check it against the specification. However, if the monitor is able to observe inputs and outputs only from a vantage point external to the device as is typically the case the problem becomes surprisingly difficult. This is because events may be bu ered, and even lost, between the monitor and the device, in which case, even for a correctly running device, the trace observed at the monitor could be inconsistent with the specification.In this paper, we formulate the problem of external monitoring as a language recognition problem. Given a specification that accepts a certain language of inputoutput sequences, we de ne another language that corresponds to inputoutput sequences observable externally. We also give an algorithm to check membership of a string in the derived language. It turns out that without any assumptions on the specification, this algorithm may take unbounded time and space. To address this problem, we de ne a series of properties of device specifications or protocols that can be exploited to construct e cient language recognizers at the monitor. We characterize these properties and provide complexity bounds for monitoring in each case.To illustrate our methodology, we describe properties of the Internet Transmission Control Protocol (TCP), and identify features of the protocol that make it challenging to monitor e ciently.
p3190
aVThe ambient logic has been proposed for expressing properties of process mobility in the calculus of Mobile Ambients (MA), and as a basis for query languages on semistructured data. To understand the extensionality and the intensionality of the logic, the equivalence on MA processes induced by the logic (=L) iscompared with the standard MA behavioural equivalence and with structural congruence (an intensional equivalence, used as an auxiliary relation in thedefinition of satisfaction of the logic). The main contributions include a coinductive characterisation of L as a form of labelled bisimilarity, and axiomatisations of L on the synchronous and asynchronous (finite) calculus. The study shows that, surprisingly, the logic allows us to observe the internal structure of the processes at a very finegrained detail, much in the same way as structural congruence does. A spinoff of the study is a better understanding of behavioural equivalence in Ambientlike calculi. For instance, behavioural equivalence is shown to be insensitive to stuttering phenomena originated by processes that may repeatedly enter and exit an ambient.
p3191
aVSecure Safe Ambients (SSA) are a typed variant of Safe Ambients [9], whose type system allows behavioral invariants of ambients to be expressed and verified. The most significant aspect of the type system is its ability to capture both explicit and implicit process and ambient behavior: process types account not only for immediate behavior, but also for the behavior resulting from capabilities a process acquires during its evolution in a given context. Based on that, the type system provides for static detection of security attacks such as Trojan Horses and other combinations of malicious agents.We study the type system of SSA, define algorithms for type checking and type reconstruction, define powerful languages for expressing security properties, and study a distributed version of SSA and its type system. For the latter, we show that distributed type checking ensures security even in illtyped contexts, and discuss how it relates to the security architecture of the Java Virtual Machine.
p3192
aVIn a widearea distributed system it is often impractical to synchronise software updates, so one must deal with many coexisting versions. We study static typing support for modular widearea programming, modelling separate compilation/linking and execution of programs that interact along typed channels. Interaction may involve communication of values of abstract types; we provide the developer with finegrain versioning control of these types to support interoperation of old and new code. The system makes use of a secondclass module system with singleton kinds; we give a novel operational semantics for separate compilation/linking and execution and prove soundness.
p3193
aVThe Microsoft .NET Framework is a new computing architecture designed to support a variety of distributed applications and webbased services. .NET software components are typically distributed in an objectoriented intermediate language, Microsoft IL, executed by the Microsoft Common Language Runtime. To allow convenient multilanguage working, IL supports a wide variety of highlevel language constructs, including classbased objects, inheritance, garbage collection, and a security mechanism based on type safe execution.This paper precisely describes the type system for a substantial fragment of IL that includes several novel features: certain objects may be allocated either on the heap or on the stack; those on the stack may be boxed onto the heap, and those on the heap may be unboxed onto the stack; methods may receive arguments and return results via typed pointers, which can reference both the stack and the heap, including the interiors of objects on the heap. We present a formal semantics for the fragment. Our typing rules determine welltyped IL instruction sequences that can be assembled and executed. Of particular interest are rules to ensure no pointer into the stack outlives its target. Our main theorem asserts type safety, that welltyped programs in our IL fragment do not lead to untrapped execution errors.Our main theorem does not directly apply to the product. Still, the formal system of this paper is an abstraction of informal and executable specifications we wrote for the full product during its development. Our informal specification became the basis of the product team's working specification of typechecking. The process of writing this specification, deploying the executable specification as a test oracle, and applying theorem proving techniques, helped us identify several security critical bugs during development.
p3194
aVRecord calculi use labels to distinguish between the elements of products and sums. This paper presents a novel variation, typeindexed rows, in which labels are discarded and elements are indexed by their type alone. The calculus, &lambda;TIR, can express tuples, recursive datatypes, monomorphic records, polymorphic extensible records, and closedworld style typebased overloading. Our motivating application of &lambda;TIR, however, is to encode the "choice" types of XML, and the "unordered tuple" types of SGML. Indeed, &lambda;TIR is the kernel of the language XM&lambda;, a lazy functional language with direct support for XML types ("DTDs") and terms ("documents").The system is built from rows, equality constraints, insertion constraints and constrained, or qualified, parametric polymorphism. The test for constraint satisfaction is complete, and for constraint entailment is only mildly incomplete. We present a type checking algorithm, and show how &lambda;TIR may be implemented by a typedirected translation which replaces typeindexing by conventional naturalnumber indexing. Though not presented in this paper, we have also developed a constraint simplification algorithm and type inference system.
p3195
aVWe consider the type system formed by a finite set of primitive types such as integer, character, real, etc., and three type construction operators: (i) Cartesian product, (ii) disjoint sum, and (iii) recursive type definitions. Type equivalence is defined to obey the arithmetical rules: commutativity and associativity of product and sum and distributivity of product over sum. We offer a compact representation of the types in this system as multivariate algebraic functions. This type system admits two natural notions of subtyping: "multiplicative", which roughly corresponds to the notion of objectoriented subtyping, and "additive", which seems to be more appropriate in our context. Both kinds of subtyping can be efficiently computed if no recursive definitions are allowed. Our main result is that additive subtyping is undecidable in the general case. Perhaps surprisingly, this undecidability result is by reduction from Hilbert's Tenth Problem (HIO): the solution of Diophantine equations.
p3196
aVReynolds has developed a logic for reasoning about mutable data structures in which the pre and postconditions are written in an intuitionistic logic enriched with a spatial form of conjunction. We investigate the approach from the point of view of the logic BI of bunched implications of O'Hearnand Pym. We begin by giving a model in which the law of the excluded middleholds, thus showing that the approach is compatible with classical logic. The relationship between the intuitionistic and classical versions of the system is established by a translation, analogous to a translation from intuitionistic logic into the modal logic S4. We also consider the question of completeness of the axioms. BI's spatial implication is used to express weakest preconditions for objectcomponent assignments, and an axiom for allocating a cons cell is shown to be complete under an interpretation of triplesthat allows a command to be applied to states with dangling pointers. We make this latter a feature, by incorporating an operation, and axiom, for disposing of memory. Finally, we describe a local character enjoyed by specifications in the logic, and show how this enables a class of frame axioms, which say what parts of the heap don't change, to be inferred automatically.
p3197
aVWe provide a parametric framework for verifying safety properties of concurrent Java programs. The framework combines threadscheduling information with information about the shape of the heap. This leads to errordetection algorithms that are more precise than existing techniques. The framework also provides the most precise shapeanalysis algorithm for concurrent programs. In contrast to existing verification techniques, we do not put a bound on the number of allocated objects. The framework even produces interesting results when analyzing Java programs with an unbounded number of threads. The framework is applied to successfully verify the following properties of a concurrent program: &bull;Concurrent manipulation of linkedlist based ADT preserves the ADT datatype invariant [19]. &bull;The program does not perform inconsistent updates due to interference. &bull;The program does not reach a deadlock. &bull;The program does not produce runtime errors due to illegal thread interactions. We also find bugs in erroneous versions of such implementations. A prototype of our framework has been implemented.
p3198
aVWe present a type system for a language based on F&le;, which allows certain type annotations to be elided in actual programs. Local type inference determines types by a combination of type propagation and local constraint solving, rather than by global constraint solving. We re ne the previously existing local type inference system of Pierce and Turner[PT98] by allowing partial type information to be propagated. This is expressed by coloring types to indicate propagation directions. Propagating partial type information allows us to omit type annotations for the visitor pattern, the analogue of pattern matching in languages without sum types.
p3199
aVWe present a novel approach to scalable implementation of typebased flow analysis with polymorphic subtyping. Using a new presentation of polymorphic subytping with instantiation constraints, we are able to apply contextfree language (CFL) reachability techniques to typebased flow analysis. We develop a CFLbased algorithm for computing flowinformation in time O(n&sup3;), where n is the size of the typed program. The algorithm substantially improves upon the best previously known algorithm for flow analysis based on polymorphic subtyping with complexity O(n8). Our technique also yields the first demanddriven algorithm for polymorphic subtypebased flowcomputation. It works directly on higherorder programs with structured data of finite type (unbounded data structures are incorporated via finite approximations), supports contextsensitive, global flow summariztion and includes polymorphic recursion.
p3200
aVWe propose regular expression pattern matching as a core feature for programming languages for manipulating XML (and similar treestructured data formats). We extend conventional patternmatching facilities with regular expression operators such as repetition (*), alternation (I), etc., that can match arbitrarily long sequences of subtrees, allowing a compact pattern to extract data from the middle of a complex sequence. We show how to check standard notions of exhaustiveness and redundancy for these patterns.Regular expression patterns are intended to be used in languages whose type systems are also based on the regular expression types. To avoid excessive type annotations, we develop a type inference scheme that propagates type constraints to pattern variables from the surrounding context. The type inference algorithm translates types and patterns into regular tree automata and then works in terms of standard closure operations (union, intersection, and difference) on tree automata. The main technical challenge is dealing with the interaction of repetition and alternation patterns with the firstmatch policy, which gives rise to subtleties concerning both the termination and the precision of the analysis. We address these issues by introducing a data structure representing closure operations lazily.
p3201
aVThe "sizechange termination" principle for a firstorder functional language with wellfounded data is: a program terminates on all inputs if every infinite call sequence (following program control flow) would cause an infinite descent in some data values.Sizechange analysis is based only on local approximations to parameter size changes derivable from program syntax. The set of infinite call sequences that follow program flow and can be recognized as causing infinite descent is an &omega;regular set, representable by a B&uuml;chi automaton. Algorithms for such automata can be used to decide sizechange termination. We also give a direct algorithm operating on "sizechange graphs" (without the passage to automata).Compared to other results in the literature, termination analysis based on the sizechange principle is surprisingly simple and general: lexical orders (also called lexicographic orders), indirect function calls and permuted arguments (descent that is not insitu) are all handled automatically and without special treatment, with no need for manually supplied argument orders, or theoremproving methods not certain to terminate at analysis time.We establish the problem's intrinsic complexity. This turns out to be surprisingly high, complete for PSPACE, in spite of the simplicity of the principle. PSPACE hardness is proved by a reduction from Boolean program termination. An ineresting consequence: the same hardness result applies to many other analyses found in the termination and quasitermination literature.
p3202
aVWe introduce a new method, combination of random testing and abstract interpretation, for the analysis of programs featuring both probabilistic and nonprobabilistic nondeterminism. After introducing "ordinary" testing, we show how to combine testing and abstract interpretation and give formulas linking the precision of the results to the number of iterations. We then discuss complexity and optimization issues and end with some experimental results.
p3203
aVThe goal of the SLAM project is to check whether or not a program obeys "API usage rules" that specify what it means to be a good client of an API. The SLAM toolkit statically analyzes a C program to determine whether or not it violates given usage rules. The toolkit has two unique aspects: it does not require the programmer to annotate the source program (invariants are inferred); it minimizes noise (false error messages) through a process known as "counterexampledriven refinement". SLAM exploits and extends results from program analysis, model checking and automated deduction. We have successfully applied the SLAM toolkit to Windows XP device drivers, to both validate behavior and find defects in their usage of kernel APIs.
p3204
aVThe growing gap between the speed of memory access and cache access has made cache misses an influential factor in program efficiency. Much effort has been spent recently on reducing the number of cache misses during program run. This effort includes wise rearranging of program code, cacheconscious data placement, and algorithmic modifications that improve the program cache behavior. In this work we investigate the complexity of finding the optimal placement of objects (or code) in the memory, in the sense that this placement reduces the cache misses to the minimum. We show that this problem is one of the toughest amongst the interesting algorithmic problems in computer science. In particular, suppose one is given a sequence of memory accesses and one has to place the data in the memory so as to minimize the number of cache misses for this sequence. We show that if P \u2260 NP, then one cannot efficiently approximate the optimal solution even up to a very liberal approximation ratio. Thus, this problem joins the small family of extremely inapproximable optimization problems. The other two famous members in this family are minimum coloring and maximum clique.
p3205
aVSome compilation systems, such as offline partial evaluators and selective dynamic compilation systems, support staged optimizations. A staged optimization is one where a logically single optimization is broken up into stages, with the early stage(s) performing preplanning setup work, given any available partial knowledge about the program to be compiled, and the final stage completing the optimization. The final stage can be much faster than the original optimization by having much of its work performed by the early stages. A key limitation of current staged optimizers is that they are written by hand, sometimes in an ad hoc manner. We have developed a framework called the Staged Compilation Framework (SCF) for systematically and automatically converting singlestage optimizations into staged versions. The framework is based on a combination of aggressive partial evaluation and deadassignment elimination. We have implemented SCF in Standard ML. A preliminary evaluation shows that SCF can speed up classical optimization of some commonly used C functions by up to 12 (and typically between 4.5 and 5.5).
p3206
aVThis invited talk presents an overview of the TeachScheme! and DrScheme projects. The former aims at bringing the principles of programming and programming languages into the introductory programming curriculum. The latter is a program development environment that supports the pedagogy and design principles of the TeachScheme! project. Carrying out these projects enriched our research life with many interesting programming language and system design problems.
p3207
aVIn this paper we propose a scheme that combines type inference and runtime checking to make existing C programs type safe. We describe the CCured type system, which extends that of C by separating pointer types according to their usage. This type system allows both pointers whose usage can be verified statically to be type safe, and pointers whose safety must be checked at run time. We prove a type soundness result and then we present a surprisingly simple type inference algorithm that is able to infer the appropriate pointer kinds for existing C programs.Our experience with the CCured system shows that the inference is very effective for many C programs, as it is able to infer that most or all of the pointers are statically verifiable to be type safe. The remaining pointers are instrumented with efficient runtime checks to ensure that they are used safely. The resulting performance loss due to runtime checks is 0150%, which is several times better than comparable approaches that use only dynamic checking. Using CCured we have discovered programming bugs in established C programs such as several SPECINT95 benchmarks.
p3208
aVDatalayout optimizations rearrange fields within objects, objects within objects, and objects within the heap, with the goal of increasing spatial locality. While the importance of datalayout optimizations has been growing, their deployment has been limited, partly because they lack a unifying framework. We propose a parameterizable framework for datalayout optimization of generalpurpose applications. Acknowledging that finding an optimal layout is not only NPhard, but also poorly approximable, our framework finds a good layout by searching the space of possible layouts, with the help of profile feedback. The search process iteratively prototypes candidate data layouts, evaluating them by "simulating" the program on a representative trace of memory accesses. To make the search process practical, we develop spacereduction heuristics and optimize the expensive simulation via memoization. Equipped with this iterative approach, we can synthesize layouts that outperform existing noniterative heuristics, tune applicationspecific memory allocators, as well as compose multiple datalayout optimizations.
p3209
aVProbability distributions are useful for expressing the meanings of probabilistic languages, which support formal modeling of and reasoning about uncertainty. Probability distributions form a monad, and the monadic definition leads to a simple, natural semantics for a stochastic lambda calculus, as well as simple, clean implementations of common queries. But the monadic implementation of the expectation query can be much less efficient than current best practices in probabilistic modeling. We therefore present a language of measure terms, which can not only denote discrete probability distributions but can also support the best known modeling techniques. We give a translation of stochastic lambda calculus into measure terms. Whether one translates into the probability monad or into measure terms, the results of the translations denote the same probability distribution.
p3210
aVDenotational semantics is given for a Javalike language with pointers, subclassing and dynamic dispatch, class oriented visibility control, recursive types and methods, and privilegebased access control. Representation independence (relational parametricity) is proved, using a semantic notion of confinement similar to ones for which static disciplines have been recently proposed.
p3211
aVWe introduce a general uniform languageindependent framework for designing online and offline sourcetosource program transformations by abstract interpretation of program semantics. Iterative sourcetosource program transformations are designed constructively by composition of sourcetosemantics, semanticstotransformed semantics and semanticstosource abstractions applied to fixpoint trace semantics. The correctness of the transformations is expressed through observational and performance abstractions. The framework is illustrated on three examples: constant propagation, program specialization by online and offline partial evaluation and static program monitoring.
p3212
aVSoftware verification is an important and difficult problem. Many static checking techniques for software require annotations from the programmer in the form of method specifications and loop invariants. This annotation overhead, particularly of loop invariants, is a significant hurdle in the acceptance of static checking. We reduce the annotation burden by inferring loop invariants automatically.Our method is based on predicate abstraction, an abstract interpretation technique in which the abstract domain is constructed from a given set of predicates over program variables. A novel feature of our approach is that it infers universallyquantified loop invariants, which are crucial for verifying programs that manipulate unbounded data such as arrays. We present heuristics for generating appropriate predicates for each loop automatically; the programmer can specify additional predicates as well. We also present an efficient algorithm for computing the abstraction of a set of states in terms of a collection of predicates.Experiments on a 44KLOC program show that our approach can automatically infer the necessary predicates and invariants for all but 31 of the 396 routines that contain loops.
p3213
aVWe investigate the firstorder of subtyping constraints. We show that the firstorder theory of nonstructural subtyping is undecidable, and we show that in the case where all constructors are either unary or nullary, the firstorder theory is decidable for both structural and nonstructural subtyping. The decidability results are shown by reduction to a decision problem on tree automata. This work is a step towards resolving longstanding open problems of the decidability of entailment for nonstructural subtyping.
p3214
aVProgram verification is a promising approach to improving program quality, because it can search all possible program executions for specific errors. However, the need to formally describe correct behavior or errors is a major barrier to the widespread adoption of program verification, since programmers historically have been reluctant to write formal specifications. Automating the process of formulating specifications would remove a barrier to program verification and enhance its practicality.This paper describes specification mining, a machine learning approach to discovering formal specifications of the protocols that code must obey when interacting with an application program interface or abstract data type. Starting from the assumption that a working program is well enough debugged to reveal strong hints of correct protocols, our tool infers a specification by observing program execution and concisely summarizing the frequent interaction patterns as state machines that capture both temporal and data dependences. These state machines can be examined by a programmer, to refine the specification and identify errors, and can be utilized by automatic verification tools, to find bugs.Our preliminary experience with the mining tool has been promising. We were able to learn specifications that not only captured the correct protocol, but also discovered serious bugs.
p3215
aVA certified binary is a value together with a proof that the value satisfies a given specification. Existing compilers that generate certified code have focused on simple memory and controlflow safety rather than more advanced properties. In this paper, we present a general framework for explicitly representing complex propositions and proofs in typed intermediate and assembly languages. The new framework allows us to reason about certified programs that involve effects while still maintaining decidable typechecking. We show how to integrate an entire proof system (the calculus of inductive constructions) into a compiler intermediate language and how the intermediate language can undergo complex transformations (CPS and closure conversion) while preserving proofs represented in the type system. Our work provides a foundation for the process of automatically generating certified binaries in a typetheoretic framework.
p3216
aVFunctional logic overloading is a novel approach to userdefined overloading that extends Haskell's concept of type classes in significant ways. Whereas type classes are conceptually predicates on types in standard Haskell, they are type functions in our approach. Thus, we can base type inference on the evaluation of functional logic programs. Functional logic programming provides a solid theoretical foundation for type functions and, at the same time, allows for programmable overloading resolution strategies by choosing different evaluation strategies for functional logic programs. Type inference with type functions is an instance of type inference with constrained types, where the underlying constraint system is defined by a functional logic program. We have designed a variant of Haskell which supports our approach to overloading, and implemented a prototype frontend for the language.
p3217
aVThis lecture will provide an overview of the field of asynchronous VLSI, and show how formal methods have played a critical role in the design of complex asynchronous systems. In particular, I will talk about program transformations and their application to asynchronous VLSI, as well as describe a simple language that I developed to describe these circuits and aid in their validation.
p3218
aVAn adaptive computation maintains the relationship between its input and output as the input changes. Although various techniques for adaptive computing have been proposed, they remain limited in their scope of applicability. We propose a general mechanism for adaptive computing that enables one to make any purelyfunctional program adaptive.We show that the mechanism is practical by giving an efficient implementation as a small ML library. The library consists of three operations for making a program adaptive, plus two operations for making changes to the input and adapting the output to these changes. We give a general bound on the time it takes to adapt the output, and based on this, show that an adaptive Quicksort adapts its output in logarithmic time when its input is extended by one key.To show the safety and correctness of the mechanism we give a formal definition of AFL, a callbyvalue functional language extended with adaptivity primitives. The modal type system of AFL enforces correct usage of the adaptivity mechanism, which can only be checked at run time in the ML library. Based on the AFL dynamic semantics, we formalize the changepropagation algorithm and prove its correctness.
p3219
aVWe study the expressive power of nonsize increasing recursive definitions over lists. This notion of computation is such that the size of all intermediate results will automatically be bounded by the size of the input so that the interpretation in a finite model is sound with respect to the standard semantics. Many wellknown algorithms with this property such as the usual sorting algorithms are definable in the system in the natural way. The main result is that a characteristic function is definable if and only if it is computable in time O(2p(n)) for some polynomial p.The method used to establish the lower bound on the expressive power also shows that the complexity becomes polynomial time if we allow primitive recursion only. This settles an open question posed in [1, 7].The key tool for establishing upper bounds on the complexity of derivable functions is an interpretation in a finite relational model whose correctness with respect to the standard interpretation is shown using a semantic technique.
p3220
aVDataflow analyses can have mutually beneficial interactions. Previous efforts to exploit these interactions have either (1) iteratively performed each individual analysis until no further improvements are discovered or (2) developed "superanalyses" that manually combine conceptually separate analyses. We have devised a new approach that allows analyses to be defined independently while still enabling them to be combined automatically and profitably. Our approach avoids the loss of precision associated with iterating individual analyses and the implementation difficulties of manually writing a superanalysis. The key to our approach is a novel method of implicit communication between the individual components of a superanalysis based on graph transformations. In this paper, we precisely define our approach; we demonstrate that it is sound and it terminates; finally we give experimental results showing that in practice (1) our framework produces results at least as precise as iterating the individual analyses while compiling at least 5 times faster, and (2) our framework achieves the same precision as a manually written superanalysis while incurring a compiletime overhead of less than 20%.
p3221
aVMany classical compiler optimizations can be elegantly expressed using rewrite rules of form: I \u21d2 I\u2032 if &phis;, where I, I\u2032 are intermediate language instructions and &phis; is a property expressed in a temporal logic suitable for describing program data flow. Its reading: If the current program \u03c0 contains an instruction of form I at some control point p, and if flow condition &phis; is satisfied at p, then replace I by I\u2032.The purpose of this paper is to show how such transformations may be proven correct. Our methodology is illustrated by three familiar optimizations, dead code elimination, constant folding and code motion. The meaning of correctness is that for any program \u03c0, if Rewrite(\u03c0, \u03c0\u2032, p,I \u21d2 I\u2032 if &phis;) then [[\u03c0]] = [[\u03c0\u2032]], i.e. \u03c0 and \u03c0\u2032 have exactly the same semantics.
p3222
aVIn this paper, we introduce the notion of prolific and nonprolific types, based on the number of instantiated objects of those types. We demonstrate that distinguishing between these types enables a new class of techniques for memory management and data locality, and facilitates the deployment of known techniques. Specifically, we first present a new typebased approach to garbage collection that has similar attributes but lower cost than generational collection. Then we describe the short type pointer technique for reducing memory requirements of objects (data) used by the program. We also discuss techniques to facilitate the recycling of prolific objects and to simplify object coallocation decisions.We evaluate the first two techniques on a standard set of Java benchmarks (SPECjvm98 and SPECjbb2000). An implementation of the typebased collector in the Jalapeo VM shows improved pause times, elimination of unnecessary write barriers, and reduction in garbage collection time (compared to the analogous generational collector) by up to 15%. A study to evaluate the benefits of the shorttype pointer technique shows a potential reduction in the heap space requirements of programs by up to 16%.
p3223
aVStack inspection is a security mechanism implemented in runtimes such as the JVM and the CLR to accommodate components with diverse levels of trust. Although stack inspection enables the finegrained expression of access control policies, it has rather a complex and subtle semantics. We present a formal semantics and an equational theory to explain how stack inspection affects program behaviour and code optimisations. We discuss the security properties enforced by stack inspection, and also consider variants with stronger, simpler properties.
p3224
aVThis paper presents a typebased information flow analysis for a callbyvalue \u03bbcalculus equipped with references, exceptions and letpolymorphism, which we refer to as Core ML. The type system is constraintbased and has decidable type inference. Its noninterference proof is reasonably lightweight, thanks to the use of a number of orthogonal techniques. First, a syntactic segregation between values and expressions allows a lighter formulation of the type system. Second, noninterference is reduced to subject reduction for a nonstandard language extension. Lastly, a semisyntactic approach to type soundness allows dealing with constraintbased polymorphism separately.
p3225
aVWe present a new role system in which the type (or role) of each object depends on its referencing relationships with other objects, with the role changing as these relationships change. Roles capture important object and data structure properties and provide useful information about how the actions of the program interact with these properties. Our role system enables the programmer to specify the legal aliasing relationships that define the set of roles that objects may play, the roles of procedure parameters and object fields, and the role changes that procedures perform while manipulating objects. We present an interprocedural, compositional, and contextsensitive role analysis algorithm that verifies that a program maintains role constraints.
p3226
aVWe study and further develop two languagebased techniques for analyzing security protocols. One is based on a typed process calculus; the other, on untyped logic programs. Both focus on secrecy properties. We contribute to these two techniques, in particular by extending the former with a flexible, generic treatment of many cryptographic operations. We also establish an equivalence between the two techniques.
p3227
aVAbstraction and composition are the fundamental issues in making model checking viable for software. This paper proposes new techniques for automating abstraction and decomposition using source level type information provided by the programmer. Our system includes two novel components to achieve this end: (1) a behavioral typeandeffect system for the \u03c0calculus, which extracts sound models as types, and (2) an assumeguarantee proof rule for carrying out compositional model checking on the types. Open simulation between CCS processes is used as both the subtyping relation in the type system and the abstraction relation for compositional model checking.We have implemented these ideas in a tool PIPER. PIPER exploits type signatures provided by the programmer to partition the model checking problem, and emit model checking obligations that are discharged using the SPIN model checker. We present the details on applying PIPER on two examples: (1) the SIS standard for managing trouble tickets across multiple organizations and (2) a file reader from the pipelined implementation of a web server.
p3228
aVOne approach to model checking software is based on the abstractcheckrefine paradigm: build an abstract model, then check the desired property, and if the check fails, refine the model and start over. We introduce the concept of lazy abstraction to integrate and optimize the three phases of the abstractcheckrefine loop. Lazy abstraction continuously builds and refines a single abstract model on demand, driven by the model checker, so that different parts of the model may exhibit different degrees of precision, namely just enough to verify the desired property. We present an algorithm for model checking safety properties using lazy abstraction and describe an implementation of the algorithm applied to C programs. We also provide sufficient conditions for the termination of the method.
p3229
aVWe study a variant of Levi and Sangiorgi's Safe Ambients (SA) enriched with passwords (SAP). In SAP by managing passwords, for example generating new ones and distributing them selectively, an ambient may now program who may migrate into its computation space, and when. Moreover in SAP an ambient may provide different services depending on the passwords exhibited by its incoming clients.We give an lts based operational semantics for SAP and a labelled bisimulation based equivalence which is proved to coincide with barbed congruence.Our notion of bisimulation is used to prove a set of algebraic laws which are subsequently exploited to prove more significant examples.
p3230
aVThe \u03c0calculus is a formalism of computing in which we can compositionally represent dynamics of major programming constructs by decomposing them into a single communication primitive, the name passing. This work reports our experience in using a linear/affine typed \u03c0calculus for the analysis and development of type systems of programming languages, focussing on secure information flow analysis. After presenting a basic typed calculus for secrecy, we demonstrate its usage by a sound embedding of the dependency core calculus (DCC) and by the development of a novel type discipline for imperative programs which extends both a secure multithreaded imperative language by Smith and Volpano and (a callbyvalue version of) DCC. In each case, the embedding gives a simple proof of noninterference.
p3231
aVConservative garbage collectors can automatically reclaim unused memory in the absence of precise pointer location information. If a location can possibly contain a pointer, it is treated by the collector as though it contained a pointer. Although it is commonly assumed that this can lead to unbounded space use due to misidentified pointers, such extreme space use is rarely observed in practice, and then generally only if the number of misidentified pointers is itself unbounded.We show that if the program manipulates only data structures satisfying a simple GCrobustness criterion, then a bounded number of misidentified pointers can at most result in increasing space usage by a constant factor. We argue that nearly all common data structures are already GCrobust, and it is typically easy to identify and replace those that are not. Thus it becomes feasible to prove space bounds on programs collected by mildly conservative garbage collectors, such as the one in [2]. The worstcase space overhead introduced by such mild conservatism is comparable to the worstcase fragmentation overhead for inherent in any nonmoving storage allocator.The same GCrobustness criterion also ensures the absence of temporary space leaks of the kind discussed in [13] for generational garbage collectors.
p3232
aVThe WorldWide Web Consortium (W3C) promotes XML and related standards, including XML Schema, XQuery, and XPath. This paper describes a formalization of XML Schema. A formal semantics based on these ideas is part of the official XQuery and XPath specification, one of the first uses of formal methods by a standards body. XML Schema features both named and structural types, with structure based on tree grammars. While structural types and matching have been studied in other work (notably XDuce, Relax NG, and a previous formalization of XML Schema), this is the first work to study the relation between named types and structural types, and the relation between matching and validation.
p3233
aVMany program analysis techniques used by compilers are applicable only to programs whose control flow graphs are reducible. Nodesplitting is a technique that can be used to convert any control flow graph to a reducible one. However, as has been observed for various nodesplitting algorithms, there can be an exponential blowup in the size of the graph.We prove that exponential blowup is unavoidable. In particular, we show that any reducible graph that is equivalent to the complete graph on n nodes (or to related boundeddegree control flow graphs) must have at least 2n1 nodes. While this result is not a surprise, it may be relevant to the quest for finding methods of obfuscation for software protection.
p3234
aVGiven a program and two variables p and q, the goal of pointsto analysis is to check if p can point to q in some execution of the program. This wellstudied problem plays a crucial role in compiler optimization. The problem is known to be undecidable when dynamic memory is allowed. But the result is known only when variables are allowed to be structures. We extend the result to show that, the problem remains undecidable, even when only scalar variables are allowed. Our second result deals with a version of pointsto analysis called flowinsensitive analysis, where one ignores the control flow of the program and assumes that the statements can be executed in any order. The problem is known to be NPHard, even when dynamic memory is not allowed and variables are scalar. We show that when the variables are further restricted to have welldefined data types, the problem is in P. The corresponding flowsensitive version, even with further restrictions, is known to be PSPACEComplete. Thus, our result gives some theoretical evidence that flowinsensitive analysis is easier than flowsensitive analysis. Moreover, while most variations of the pointsto analysis are known to be computationally hard, our result gives a rare instance of a nontrivial pointsto problem solvable in polynomial time.
p3235
aVA fundamental problem in the implementation of objectoriented languages is that of a frugal dispatching data structure, i.e., support for quick response to dispatching queries combined with compact representation of the type hierarchy and the method families. Previous theoretical algorithms tend to be impractical due to their complexity and large hidden constant. In contrast, successful practical heuristics, including Vitek and Horspool's compact dispatch tables (CT) [16] designed for dynamically typed languages, lack theoretical support. In subjecting CT to theoretical analysis, we are not only able to improve and generalize it, but also provide the first nontrivial bounds on the performance of such a heuristic.Let n,ml denote the total number of types, messages, and different method implementations, respectively. Then, the dispatching matrix, whose size isnm, can be compressed by a factor of at most \u03b9 \u2261 (nm)/l. Our main variant to CT achieves a compression factor of  \u221a\u03b9. More generally, we describe a sequence of algorithms CT1, CT2, CT3,..., where CTd achieves compression by a factor of (at least) 1overd\u03b91\u20141/d, while using d memory dereferencing operations during dispatch. This tradeoff represents the first bounds on the compression ratio of constanttime dispatching algorithms.A generalization of these algorithms to a multipleinheritance setting, increases the space by a factor of \u03ba11/d, where \u03ba is a metric of the complexity of the topology of the inheritance hierarchy, which (as indicated by our measurements) is typically small. The most important generalization is an incremental variant of the CTd scheme for a singleinheritance setting. This variant uses at most twice the space of CTd, and its time of inserting a new type into the hierarchy is optimal. We therefore obtain algorithms for efficient management of dispatching in dynamictyping, dynamicloading languages, such as Smalltalk and even the Java invokeinterface instruction.
p3236
aVFirstclass continuations are a powerful computational effect, allowing the programmer to express any form of jumping. Types and effect systems can be used to reason about continuations, both in the source language and in the target language of the continuationpassing transform. In this paper, we establish the connection between an effect system for firstclass continuations and typed versions of continuationpassing style. A region in the effect system determines a local answer type for continuations, such that the continuation transforms of pure expressions are parametrically polymorphic in their answer types. We use this polymorphism to derive transforms that make use of effect information, in particular, a mixed linear/nonlinear continuationpassing transform, in which expressions without control effects are passed their continuations linearly.
p3237
aVWe present a coercive subtyping system for the calculus of constructions. The proposed system \u03bbCCOover\u2264 is obtained essentially by adding coercions and \u03b7conversion to \u03bbC\u2264[10], which is a subtyping extension to the calculus of constructions without coercions. Following [17, 18], the coercive subtyping c : A \u03b7 B is understood as a special case of typing in arrow type c : A \u2192 B such that the term c behaves like an identity function. We prove that, with respect to this semantic interpretation, the proposed coercive subtyping system is sound and complete, and that this completeness leads to transitivity elimination (transitivity rule is admissible). In addition, we establish the equivalence between \u03bbCCOover\u2264 and CC\u03b7, this fact implies that \u03bbCCOover\u2264 has confluence, subject reduction and strong normalization. We propose a formalization of coercion inference problem and present a sound and complete coercion inference algorithm.
p3238
aVThe first order isomorphism problem is to decide whether two nonrecursive types using product and functiontype constructors, are isomorphic under the axioms of commutative and associative products, and currying and distributivity of functions over products. We show that this problem can be solved in O(n log2 n) time and O(n) space, where is n the input size. This result improves upon the O(n log2 n) time and O(n2) space bounds of the best previous algorithm. We also describe an O(n) time algorithm for the linear isomorphism problem, which does not include the distributive axiom, whereby improving upon the O(n log n) time of the best previous algorithm for this problem.
p3239
aVOrdered type theory is an extension of linear type theory in which variables in the context may be neither dropped nor reordered. This restriction gives rise to a natural notion of adjacency. We show that a language based on ordered types can use this property to give an exact account of the layout of data in memory. The fuse constructor from ordered logic describes adjacency of values in memory, and the mobility modal describes pointers into the heap. We choose a particular allocation model based on a common implementation scheme for copying garbage collection and show how this permits us to separate out the allocation and initialization of memory locations in such a way as to account for optimizations such as the coalescing of multiple calls to the allocator.
p3240
aVWe show how to efficiently obtain linear a priori bounds on the heap space consumption of firstorder functional programs.The analysis takes space reuse by explicit deallocation into account and also furnishes an upper bound on the heap usage in the presence of garbage collection. It covers a wide variety of examples including, for instance, the familiar sorting algorithms for lists, including quicksort.The analysis relies on a type system with resource annotations. Linear programming (LP) is used to automatically infer derivations in this enriched type system.We also show that integral solutions to the linear programs derived correspond to programs that can be evaluated without any operating system support for memory management. The particular integer linear programs arising in this way are shown to be feasibly solvable under mild assumptions.
p3241
aVWe present the design of a typed assembly language called TALT that supports heterogeneous tuples, disjoint sums, and a general account of addressing modes. TALT also implements the von Neumann model in which programs are stored in memory, and supports relative addressing. Type safety for execution and for garbage collection are shown by machinecheckable proofs. TALT is the first formalized typed assembly language to provide any of these features.
p3242
aVOwnership types provide a statically enforceable way of specifying object encapsulation and enable local reasoning about program correctness in objectoriented languages. However, a type system that enforces strict object encapsulation is too constraining: it does not allow efficient implementation of important constructs like iterators. This paper argues that the right way to solve the problem is to allow objects of classes defined in the same module to have privileged access to each other's representations; we show how to do this for inner classes. This approach allows programmers to express constructs like iterators and yet supports local reasoning about the correctness of the classes, because a class and its inner classes together can be reasoned about as a module. The paper also sketches how we use our variant of ownership types to enable efficient software upgrades in persistent object stores.
p3243
aVWe present a framework for applying memoization selectively. The framework provides programmer control over equality, space usage, and identification of precise dependences so that memoization can be applied according to the needs of an application. Two key properties of the framework are that it is efficient and yields programs whose performance can be analyzed using standard techniques.We describe the framework in the context of a functional language and an implementation as an SML library. The language is based on a modal type system and allows the programmer to express programs that reveal their true data dependences when executed. The SML implementation cannot support this modal type system statically, but instead employs runtime checks to ensure correct usage of primitives.
p3244
aVWe introduce a notion of guarded recursive (g.r.) datatype constructors, generalizing the notion of recursive datatypes in functional programming languages such as ML and Haskell. We address both theoretical and practical issues resulted from this generalization. On one hand, we design a type system to formalize the notion of g.r. datatype constructors and then prove the soundness of the type system. On the other hand, we present some significant applications (e.g., implementing objects, implementing staged computation, etc.) of g.r. datatype constructors, arguing that g.r. datatype constructors can have farreaching consequences in programming. The main contribution of the paper lies in the recognition and then the formalization of a programming notion that is of both theoretical interest and practical use.
p3245
aVWe present a type theory for higherorder modules that accounts for many central issues in module system design, including translucency, applicativity, generativity, and modules as firstclass values. Our type system harmonizes design elements from previous work, resulting in a simple, economical account of modular programming. The main unifying principle is the treatment of abstraction mechanisms as computational effects. Our language is the first to provide a complete and practical formalization of all of these critical issues in module system design.
p3246
aVWe introduce a new framework of algebraic pure type systems in which we consider rewrite rules as lambda terms with patterns and rewrite rule application as abstraction application with builtin matching facilities. This framework, that we call "Pure Pattern Type Systems", is particularly wellsuited for the foundations of programming (meta)languages and proof assistants since it provides in a fully unified setting higherorder capabilities and pattern matching ability together with powerful type systems. We prove some standard properties like confluence and subject reduction for the case of a syntactic theory and under a syntactical restriction over the shape of patterns. We also conjecture the strong normalization of typable terms. This work should be seen as a contribution to a formal connection between logics and rewriting, and a step towards new proof engines based on the CurryHoward isomorphism.
p3247
aVWe compare two different facilities for running cleanup actions for objects that are about to reach the end of their life.Destructors, such as we find in C++, are invoked synchronously when an object goes out of scope. They make it easier to implement cleanup actions for objects of wellknown lifetime, especially in the presence of exceptions.Languages like Java[8], Modula3[12], and C\u005c#[6] provide a different kind of "finalization" facility: Cleanup methods may be run when the garbage collector discovers a heap object to be otherwise inaccessible. Unlike C++ destructors, such methods run in a separate thread at some much less welldefined time.Languages like Java[8], Modula3[12], and C\u005c#[6] provide a different kind of "finalization" facility: Cleanup methods may be run when the garbage collector discovers a heap object to be otherwise inaccessible. Unlike C++ destructors, such methods run in a separate thread at some much less welldefined time.We argue that these are fundamentally different, and potentially complementary, language facilities. We also try to resolve some common misunderstandings about finalization in the process. In particular: The asynchronous nature of finalizers is not just an accident of implementation or a shortcoming of tracing collectors; it is necessary for correctness of client code, fundamentally affects how finalizers must be written, and how finalization facilities should be presented to the user. An object may legitimately be finalized while one of its methods are still running. This should and can be addressed by the language specification amd client code.
p3248
aVWe present an interprocedural and compositional algorithm for finding pairs of compatible allocation sites, which have the property that no object allocated at one site is live at the same time as any object allocated at the other site. If an allocation site is compatible with itself, it is said to be unitary: at most one object allocated at that site is live at any given point in the, execution of the program. We use the results of the analysis to statically preallocate memory space for the objects allocated at unitary sites, thus simplifying the computation of an upper bound on the amount of memory required to execute the program. We also use the analysis to enable objects allocated at several compatible allocation sites to share the same preallocated memory. Our experimental results show that, for our set of Java benchmark programs, 60% of the allocation sites are unitary and can be statically preallocated. Moreover, allowing compatible unitary allocation sites to share the same preallocated memory leads to a 95% reduction in the amount of memory preallocated for these sites.
p3249
aVThis paper proposes and develops the basic theory for a new approach to typing multistage languages based a notion of environment classifiers. This approach involves explicit but lightweight tracking   at typechecking time   of the origination environment for futurestage computations. Classification is less restrictive than the previously proposed notions of closedness, and allows for both a more expressive typing of the "run" construct and for a unifying account of typed multistage programmin.The proposed approach to typing requires making crossstage persistence (CSP) explicit in the language. At the same time, it offers concrete new insights into the notion of levels and in turn into CSP itself. Type safety is established in the simplytyped setting. As a first step toward introducing classifiers to the HindleyMilner setting, we propose an approach to integrating the two, and prove type preservation in this setting.
p3250
aVA bigraphical reactive system (BRS) involves bigraphs, in which the nesting of nodes represents locality, independently of the edges connecting them. BRSs represent a wide variety of calculi for mobility, including \u03bbcalculus and ambient calculus. A labelled transition system (LTS) for each BRS is here derived uniformly, adapting previous work of Leifer and Milner, so that under certain conditions the resulting bisimilarity is automatically a congruence. For an asynchronous \u03bbcalculus, this LTS and its bisimilarity agree closely with the standard.
p3251
aVThis paper presents a new distributed process calculus, called the Mcalculus, that can be understood as a higherorder version of the Distributed Join calculus with programmable localities. The calculus retains the implementable character of the Distributed Join calculus while overcoming several important limitations: insufficient control over communication and mobility, absence of dynamic binding, and limited locality semantics. The calculus is equipped with a polymorphic type system that guarantees the unicity of locality names, even in presence of higherorder communications   a crucial property for the determinacy of message routing in the calculus.
p3252
aVWe present a generic aproach to the static analysis of concurrent programs with procedures. We model programs as communicating pushdown systems. It is known that typical dataflow problems for this model are undecidable, because the emptiness problem for the intersection of contextfree languages, which is undecidable, can be reduced to them. In this paper we propose an algebraic framework for defining abstractions (upper approximations) of contextfree languages. We consider two classes of abstractions: finitechain abstractions, which are abstractions whose domains do not contain any infinite chains, and commutative abstractions corresponding to classes of languages that contain a word if and only if they contain all its permutations. We show how to compute such approximations by combining automata theoretic techniques with algorithms for solving systems of polynomial inequations in Kleene algebras.
p3253
aVWe present a new polynomialtime randomized algorithm for discovering affine equalities involving variables in a program. The key idea of the algorithm is to execute a code fragment on a few random inputs, but in such a way that all paths are covered on each run. This makes it possible to rule out invalid relationships even with very few runs.The algorithm is based on two main techniques. First, both branches of a conditional are executed on each run and at joint points we perform an affine combination of the joining states. Secondly, in the branches of an equality conditional we adjust the data values on the fly to reflect the truth value of the guarding boolean expression. This increases the number of affine equalities that the analysis discovers.The algorithm is simpler to implement than alternative deterministic versions, has better computational complexity, and has an extremely small probability of error for even a small number of runs. This algorithm is an example of how randomization can provide a tradeoff between the cost and complexity of program analysis, and a small probability of unsoundness.
p3254
aVMultimedia and network processing applications make extensive use of subword data. Since registers are capable of holding a full data word, when a subword variable is assigned a register, only part of the register is used. New embedded processors have started supporting instruction sets that allow direct referencing of bit sections within registers and therefore multiple subword variables can be made to simultaneously reside in the same register without hindering accesses to these variables. However, a new register allocation algorithm is needed that is aware of the bitwidths of program variables and is capable of packing multiple subword variables into a single register. This paper presents one such algorithm.The algorithm we propose has two key steps. First, a combination of forward and backward data flow analyses are developed to determine the bitwidths of program variables throughout the program. This analysis is required because the declared bitwidths of variables are often larger than their true bitwidths and moreover the minimal bitwidths of a program variable can vary from one program point to another. Second, a novel interference graph representation is designed to enable support for a fast and highly accurate algorithm for packing of subword variables into a single register. Packing is carried out by a node coalescing phase that precedes the conventional graph coloring phase of register allocation. In contrast to traditional node coalescing, packing coalesces a set of interfering nodes. Our experiments show that our bitwidth aware register allocation algorithm reduces the register requirements by 10\u005c% to 50% over a traditional register allocation algorithm that assigns separate registers to simultaneously live subword variables.
p3255
aVThere is significant room for improving users' experiences with model checking tools. An error trace produced by a model checker can be lengthy and is indicative of a symptom of an error. As a result, users can spend considerable time examining an error trace in order to understand the cause of the error. Moreover, even stateoftheart model checkers provide an experience akin to that provided by parsers before syntactic error recovery was invented: they report a single error trace per run. The user has to fix the error and run the model checker again to find more error traces.We present an algorithm that exploits the existence of correct traces in order to localize the error cause in an error trace, report a single error trace per error cause, and generate multiple error traces having independent causes. We have implemented this algorithm in the context of slam, a software model checker that automatically verifies temporal safety properties of C programs, and report on our experience using it to find and localize errors in device drivers. The algorithm typically narrows the location of a cause down to a few lines, even in traces consisting of hundreds of statements.
p3256
aVWe present a framework for the certification of compilation and of compiled programs. Our approach uses a symbolic transfer functionsbased representation of programs, so as to check that source and compiled programs present similar behaviors. This checking can be done either for a concrete semantic interpretation (Translation Validation) or for an abstract semantic interpretation (Invariant Translation) of the symbolic transfer functions. We propose to design a checking procedure at the concrete level in order to validate both the transformation and the translation of abstract invariants. The use of symbolic transfer functions makes possible a better treatment of compiler optimizations and is adapted to the checking of precise invariants at the assembly level. The approach proved successful in the implementation point of view, since it rendered the translation of very precise invariants on very large assembly programs feasible.
p3257
aVFor decades we have been using Chomsky's generative system of grammars, particularly contextfree grammars (CFGs) and regular expressions (REs), to express the syntax of programming languages and protocols. The power of generative grammars to express ambiguity is crucial to their original purpose of modelling natural languages, but this very power makes it unnecessarily difficult both to express and to parse machineoriented languages using CFGs. Parsing Expression Grammars (PEGs) provide an alternative, recognitionbased formal foundation for describing machineoriented syntax, which solves the ambiguity problem by not introducing ambiguity in the first place. Where CFGs express nondeterministic choice between alternatives, PEGs instead use prioritized choice. PEGs address frequently felt expressiveness limitations of CFGs and REs, simplifying syntax definitions and making it unnecessary to separate their lexical and hierarchical components. A lineartime parser can be built for any PEG, avoiding both the complexity and fickleness of LR parsers and the inefficiency of generalized CFG parsing. While PEGs provide a rich set of operators for constructing grammars, they are reducible to two minimal recognition schemas developed around 1970, TS/TDPL and gTS/GTDPL, which are here proven equivalent in effective recognition power.
p3258
aVThis paper aims at providing confluence and determinism properties in concurrent processes, more specifically within the paradigm of objectoriented systems. Such results should allow one to program parallel and distributed applications that behave in a deterministic manner, even if they are distributed over local or wide area networks. For that purpose, an object calculus is proposed. Its key characteristics are asynchronous communications with futures, and sequential execution within each process.While most of previous works exhibit confluence properties only on specific programs   or patterns of programs, a general condition for confluence is presented here. It is further put in practice to show the deterministic behavior of a typical example.
p3259
aVWe prove the decidability of the quantifierfree, static fragment of ambient logic, with composition adjunct and iteration, which corresponds to a kind of regular expression language for semistructured data. The essence of this result is a surprising connection between formulas of the ambient logic and counting constraints on (nested) vectors of integers.Our proof method is based on a new class of tree automata for unranked, unordered trees, which may result in practical algorithms for deciding the satisfiability of a formula. A benefit of our approach is to naturally lead to an extension of the logic with recursive definitions, which is also decidable. Finally, we identify a simple syntactic restriction on formulas that improves the effectiveness of our algorithms on large examples.
p3260
aVThis paper introduces a new expressive theory of types for the higherorder \u03c0calculus and demonstrates its applicability via two security analyses for higherorder code mobility. The new theory significantly improves our previous one presented in [55] by the use of channel dependent/existential types. New dependent types control dynamic change of process accessibility via channel passing, while existential types guarantee safe scopeextrusion in higherorder process passing. This solves an open issue in [55], leading to significant enlargement of original typability. The resulting typing system is coherently integrated with the linear/affine typing disciplines as well as state, concurrency and distribution [53, 5, 56, 22], allowing precise analysis of software behaviour with higherorder mobility. As illustration of the usage of the typed calculus, two basic security concerns for mobile computation, secrecy for data confidentiality and rlebased access control for authorised resources, are analysed in a uniform typebased framework, leading to the noninterference theorem and authorityerror freedom in the presence of higherorder code mobility.
p3261
aVWe define \u03bbseal, an untyped callbyvalue \u03bbcalculus with primitives for protecting abstract data by sealing, and develop a bisimulation proof method that is sound and complete with respect to contextual equivalence. This provides a formal basis for reasoning about data abstraction in open, dynamic settings where static techniques such as type abstraction and logical relations are not applicable.
p3262
aVSoftware watermarking consists in the intentional embedding of indelible stegosignatures or watermarks into the subject software and extraction of the stegosignatures embedded in the stegoprograms for purposes such as intellectual property protection. We introduce the novel concept of abstract software watermarking. The basic idea is that the watermark is hidden in the program code in such a way that it can only be extracted by an abstract interpretation of the (maybe nonstandard) concrete semantics of this code. This static analysisbased approach allows the watermark to be recovered even if only a small part of the program code is present and does not even need that code to be executed. We illustrate the technique by a simple abstract watermarking protocol for methods of Java\u2122 classes. The concept applies equally well to any other kind of software (including hardware originally specified by software).
p3263
aVIn this paper we generalize the notion of noninterference making it parametric relatively to what an attacker can analyze about the input/output information flow. The idea is to consider attackers as dataflow analyzers, whose task is to reveal properties of confidential resources by analyzing public ones. This means that no unauthorized flow of information is possible from confidential to public data, relatively to the degree of precision of an attacker. We prove that this notion can be fully specified in standard abstract interpretation framework, making the degree of security of a program a property of its semantics. This provides a comprehensive account of noninterference features for languagebased security. We introduce systematic methods for extracting attackers from programs, providing domaintheoretic characterizations of the most precise attackers which cannot violate the security of a given program. These methods allow us both to compare attackers and program secrecy by comparing the corresponding abstractions in the lattice of abstract interpretations, and to design automatic program certification tools for languagebased security by abstract interpretation.
p3264
aVWe consider the problem of specifying and verifying cryptographic security protocols for XML web services. The security specification WSSecurity describes a range of XML security tokens, such as username tokens, publickey certificates, and digital signature blocks, amounting to a flexible vocabulary for expressing protocols. To describe the syntax of these tokens, we extend the usual XML data model with symbolic representations of cryptographic values. We use predicates on this data model to describe the semantics of security tokens and of sample protocols distributed with the Microsoft WSE implementation of WSSecurity. By embedding our data model within Abadi and Fournet's applied pi calculus, we formulate and prove security properties with respect to the standard DolevYao threat model. Moreover, we informally discuss issues not addressed by the formal model. To the best of our knowledge, this is the first approach to the specification and verification of security protocols based on a faithful account of the XML wire format.
p3265
aVReference counting memory management is often advocated as a technique for reducing or avoiding the pauses associated with tracing garbage collection. We present some measurements to remind the reader that classic reference count implementations may in fact exhibit longer pauses than tracing collectors.We then analyze reference counting with lazy deletion, the standard technique for avoiding long pauses by deferring deletions and associated reference count decrements, usually to allocation time. Our principal result is that if each reference count operation is constrained to take constant time, then the overall space requirements can be increased by a factor of \u03a9(R) in the worst case, where R is the ratio between the size of the largest and smallest allocated object. This bound is achievable, but probably large enough to render this design point useless for most realtime applications.We show that this space cost can largely be avoided if allocating an $n$ byte object is allowed to additionally perform O(n) reference counting work.
p3266
aVWe present a programming language, model, and logic appropriate for implementing and reasoning about a memory management system. We then state what is meant by correctness of a copying garbage collector, and employ a variant of the novel separation logics [18, 23] to formally specify partial correctness of Cheney's copying garbage collector [8]. Finally, we prove that our implementation of Cheney's algorithm meets its specification, using the logic we have given, and auxiliary variables [19].
p3267
aVWe show how some classical static analyses for imperative programs, and the optimizing transformations which they enable, may be expressed and proved correct using elementary logical and denotationaltechniques. The key ingredients are an interpretation of program properties as relations, rather than predicates, and a realization that although many program analyses are traditionally formulated in very intensional terms, the associated transformations are actually enabled by more liberal extensional properties.We illustrate our approach with formal systems for analysing and transforming whileprograms. The first is a simple type system which tracks constancy and dependency information and can be used to perform deadcode elimination, constant propagation and program slicing as well as capturing a form of secure information flow. The second is a relational version of Hoare logic, which significantly generalizes our first type system and can also justify optimizations including hoisting loop invariants. Finally we show how a simple available expression analysis and redundancy elimination transformation may be justified by translation into relational Hoare logic.
p3268
aVThe success of model checking for large programs depends crucially on the ability to efficiently construct parsimonious abstractions. A predicate abstraction is parsimonious if at each control location, it specifies only relationships between current values of variables, and only those which are required for proving correctness. Previous methods for automatically refining predicate abstractions until sufficient precision is obtained do not systematically construct parsimonious abstractions: predicates usually contain symbolic variables, and are added heuristically and often uniformly to many or all control locations at once. We use Craig interpolation to efficiently construct, from a given abstract error trace which cannot be concretized, a parsominous abstraction that removes the trace. At each location of the trace, we infer the relevant predicates as an interpolant between the two formulas that define the past and the future segment of the trace. Each interpolant is a relationship between current values of program variables, and is relevant only at that particular program location. It can be found by a linear scan of the proof of infeasibility of the trace.We develop our method for programs with arithmetic and pointer expressions, and callbyvalue function calls. For function calls, Craig interpolation offers a systematic way of generating relevant predicates that contain only the local variables of the function and the values of the formal parameters when the function was called. We have extended our model checker Blast with predicate discovery by Craig interpolation, and applied it successfully to C programs with more than 130,000 lines of code, which was not possible with approaches that build less parsimonious abstractions.
p3269
aVThe ability to summarize procedures is fundamental to building scalable interprocedural analyses. For sequential programs, procedure summarization is wellunderstood and used routinely in a variety of compiler optimizations and software defectdetection tools. However, the benefit of summarization is not available to multithreaded programs, for which a clear notion of summaries has so far remained unarticulated in the research literature.In this paper, we present an intuitive and novel notion of procedure summaries for multithreaded programs. We also present a model checking algorithm for these programs that uses procedure summarization as an essential component. Our algorithm can also be viewed as a precise interprocedural dataflow analysis for multithreaded programs. Our method for procedure summarization is based on the insight that in wellsynchronized programs, any computation of a thread can be viewed as a sequence of transactions, each of which appears to execute atomically to other threads. We summarize within each transaction; the summary of a procedure comprises the summaries of all transactions within the procedure. We leverage the theory of reduction [17] to infer boundaries of these transactions.The procedure summaries computed by our algorithm allow reuse of analysis results across different call sites in a multithreaded program, a benefit that has hitherto been available only to sequential programs. Although our algorithm is not guaranteed to terminate on multithreaded programs that use recursion (reachability analysis for multithreaded programs with recursive procedures is undecidable [18]), there is a large class of programs for which our algorithm does terminate. We give a formal characterization of this class, which includes programs that use shared variables, synchronization, and recursion.
p3270
aVEnsuring the correctness of multithreaded programs is difficult, due to the potential for unexpected interactions between concurrent threads. Much previous work has focused on detecting race conditions, but the absence of race conditions does not by itself prevent undesired thread interactions. We focus on the more fundamental noninterference property of atomicity; a method is atomic if its execution is not affected by and does not interfere with concurrentlyexecuting threads. Atomic methods can be understood according to their sequential semantics, which significantly simplifies (formal and informal) correctness arguments.This paper presents a dynamic analysis for detecting atomicity violations. This analysis combines ideas from both Lipton's theory of reduction and earlier dynamic race detectors. Experience with a prototype checker for multithreaded Java code demonstrates that this approach is effective for detecting errors due to unintended interactions between threads. In particular, our atomicity checker detects errors that would be missed by standard race detectors, and it produces fewer false alarms on benign races that do not cause atomicity violations. Our experimental results also indicate that the majority of methods in our benchmarks are atomic, supporting our hypothesis that atomicity is a standard methodology in multithreaded programming.
p3271
aVWe investigate proof rules for information hiding, using the recent formalism of separation logic. In essence, we use the separating conjunction to partition the internal resources of a module from those accessed by the module's clients. The use of a logical connective gives rise to a form of dynamic partitioning, where we track the transfer of ownership of portions of heap storage between program components. It also enables us to enforce separation in the presence of mutable data structures with embedded addresses that may be aliased.
p3272
aVIn prior work we introduced a pure type assignment system that encompasses a rich set of property types, including intersections, unions, and universally and existentially quantified dependent types. This system was shown sound with respect to a callbyvalue operational semantics with effects, yet is inherently undecidable.In this paper we provide a decidable formulation for this system based on bidirectional checking, combining type synthesis and analysis following logical principles. The presence of unions and existential quantification requires the additional ability to visit subterms in evaluation position before the context in which they occur, leading to a tridirectional type system. While soundness with respect to the type assignment system is immediate, completeness requires the novel concept of contextual type annotations, introducing a notion from the study of principal typings into the source program.
p3273
aVIn the interest of designing a recursive module extension to ML that is as simple and general as possible, we propose a novel type system for general recursion over effectful expressions. The presence of effects seems to necessitate a backpatching semantics for recursion similar to that of Scheme. Our type system ensures statically that recursion is wellfounded that the body of a recursive expression will evaluate without attempting to access the undefined recursive variable which avoids some unnecessary runtime costs associated with backpatching. To ensure wellfounded recursion in the presence of multiple recursive variables and separate compilation, we track the usage of individual recursive variables, represented statically by "names". So that our type system may eventually be integrated smoothly into ML's, reasoning involving names is only required inside code that uses our recursive construct and need not infect existing ML code, although instrumentation of some existing code can help to improve the precision of our type system.
p3274
aVThe contribution of the paper is twofold. First, we define a general notion of type system equipped with an entailment relation between type environments; this generalisation serves as a pattern for instantiating type systems able to support separate compilation and interchecking of Javalike languages, and allows a formal definition of soundess and completeness of interchecking w.r.t. global compilation. These properties are important in practice since they allow selective recompilation. In particular, we show that they are guaranteed when the type system has principal typings and provides sound and complete entailment relation between type environments.The second contribution is more specific, and is an instantiation of the notion of type system previously defined for Featherweight Java with method overloading and field hiding. The aim is to show that it is possible to define type systems for Javalike languages, which, in contrast to those used by standard compilers, have principal typings, hence can be used as a basis for selective recompilation.
p3275
aVWe present a new technique for the generation of nonlinear (algebraic) invariants of a program. Our technique uses the theory of ideals over polynomial rings to reduce the nonlinear invariant generation problem to a numerical constraint solving problem. So far, the literature on invariant generation has been focussed on the construction of linear invariants for linear programs. Consequently, there has been little progress toward nonlinear invariant generation. In this paper, we demonstrate a technique that encodes the conditions for a given template assertion being an invariant into a set of constraints, such that all the solutions to these constraints correspond to nonlinear (algebraic) loop invariants of the program. We discuss some tradeoffs between the completeness of the technique and the tractability of the constraintsolving problem generated. The application of the technique is demonstrated on a few examples.
p3276
aVWe apply linear algebra techniques to precise interprocedural dataflow analysis. Specifically, we describe analyses that determine for each program point identities that are valid among the program variables whenever control reaches that program point. Our analyses fully interpret assignment statements with affine expressions on the right hand side while considering other assignments as nondeterministic and ignoring conditions at branches. Under this abstraction, the analysis computes the set of all affine relations and, more generally, all polynomial relations of bounded degree precisely. The running time of our algorithms is linear in the program size and polynomial in the number of occurring variables. We also show how to deal with affine preconditions and local variables and indicate how to handle parameters and return values of procedures.
p3277
aVWe aim to specify program transformations in a declarative style, and then to generate executable program transformers from such specifications. Many transformations require nontrivial program analysis to check their applicability, and it is prohibitively expensive to rerun such analyses after each transformation. It is desirable, therefore, that the analysis information is incrementally updated.We achieve this by drawing on two pieces of previous work: first, Bernhard Steffen's proposal to use model checking for certain analysis problems, and second, John Conway's theory of language factors. The first allows the neat specification of transformations, while the second opens the way for an incremental implementation. The two ideas are linked by using regular patterns instead of Steffen's modal logic: these patterns can be viewed as queries on the set of program paths.
p3278
aVWe present a formalization of the implementation of generics in the .NET Common Language Runtime (CLR), focusing on two novel aspectsof the implementation: mixed specialization and sharing, and efficient support for runtime types. Some crucial constructs used in the implementation are dictionaries and runtime type representations. We formalize these aspects typetheoretically in a way that corresponds in spirit to the implementation techniques used in practice. Both the techniques and the formalization also help us understand the range of possible implementation techniques for other languages, e.g., ML, especially when additional source language constructs such as runtime types are supported. A useful byproduct of this study is a type system for a subset of the polymorphic IL proposed for the .NET CLR.
p3279
aVWe present a generalization of the ideal model for recursive polymorphic types. Types are defined as sets of terms instead of sets of elements of a semantic domain. Our proof of the existence of types (computed by fixpoint of a typing operator) does not rely on metric properties, but on the fact that the identity is the limit of a sequence of projection terms. This establishes a connection with the work of Pitts on relational properties of domains. This also suggests that ideals are better understood as closed sets of terms defined by orthogonality with respect to a set of contexts.
p3280
aVWe present a notion of \u03b7long \u03b2normal term for the typed lambda calculus with sums and prove, using Grothendieck logical relations, that every term is equivalent to one in normal form. Based on this development we give the first typedirected partial evaluator that constructs %able to construct normal forms of terms in this calculus.
p3281
aVThis paper gives the first decidability results on type isomorphism for recursive types, establishing the explicit decidability of type isomorphism for the type theory of sums and products over an inhabited generic recursive polynomial type. The technical development provides connections between themes in programminglanguage theory (type isomorphism) and computational algebra (Grbner bases).
p3282
aVDefunctionalization is a program transformation that aims to turn a higherorder functional program into a firstorder one, that is, to eliminate the use of functions as firstclass values. Its purpose is thus identical to that of closure conversion. It differs from closure conversion, however, by storing a tag, instead of a code pointer, within every closure. Defunctionalization has been used both as a reasoning tool and as a compilation technique.Defunctionalization is commonly defined and studied in the setting of a simplytyped \u03bbcalculus, where it is shown that semantics and welltypedness are preserved. It has been observed that, in the setting of a polymorphic type system, such as ML or System F, defunctionalization is not typepreserving. In this paper, we show that extending System F with guarded algebraic data types allows recovering type preservation. This result allows adding defunctionalization to the toolbox of typepreserving compiler writers.
p3283
aVParametric polymorphism constrains the behavior of pure functional programs in a way that allows the derivation of interesting theorems about them solely from their types, i.e., virtually for free. Unfortunately, the standard parametricity theorem fails for nonstrict languages supporting a polymorphic strict evaluation primitive like Haskell's seq. Contrary to the folklore surrounding seq and parametricity, we show that not even quantifying only over strict and bottomreflecting relations in the $\u005cforall$clause of the underlying logical relation   and thus restricting the choice of functions with which such relations are instantiated to obtain free theorems to strict and total ones   is sufficient to recover from this failure. By addressing the subtle issues that arise when propagating up the type hierarchy restrictions imposed on a logical relation in order to accommodate the strictness primitive, we provide a parametricity theorem for the subset of Haskell corresponding to a GirardReynoldsstyle calculus with fixpoints, algebraic datatypes, and seq. A crucial ingredient of our approach is the use of an asymmetric logical relation, which leads to "inequational" versions of free theorems enriched by preconditions guaranteeing their validity in the described setting. Besides the potential to obtain corresponding preconditions for standard equational free theorems by combining some new inequational ones, the latter also have value in their own right, as is exemplified with a careful analysis of seq's impact on familiar program transformations.
p3284
aVHaskell's type classes allow adhoc overloading, or typeindexing, of functions. A natural generalisation is to allow typeindexing of data types as well. It turns out that this idea directly supports a powerful form of abstraction called associated types, which are available in C++ using traits classes. Associated types are useful in many applications, especially for selfoptimising libraries that adapt their data representations and algorithms in a typedirected manner.In this paper, we introduce and motivate associated types as a rather natural generalisation of Haskell's existing type classes. Formally, we present a type system that includes a typedirected translation into an explicitly typed target language akin to System F; the existence of this translation ensures that the addition of associated data types to an existing Haskell compiler only requires changes to the front end.
p3285
aVWe present a new approach to partialorder reduction for model checking software. This approach is based on initially exploring an arbitrary interleaving of the various concurrent processes/threads, and dynamically tracking interactions between these to identify backtracking points where alternative paths in the state space need to be explored. We present examples of multithreaded programs where our new dynamic partialorder reduction technique significantly reduces the search space, even though traditional partialorder algorithms are helpless.
p3286
aVThis paper presents a procedure for the verification of multiprocess systems based on considering a series of underapproximated models. The procedure checks models with an increasing set of allowed interleavings of the given set of processes, starting from a single interleaving. The procedure relies on SAT solvers' ability to produce proofs of unsatisfiability: from these proofs it derives information that guides the process of adding interleavings on the one hand, and determines termination on the other. The presented approach is integrated in a SATbased Bounded Model Checking (BMC) framework. Thus, a BMC formulation of a multiprocess system is introduced, which allows controlling which interleavings are considered. Preliminary experimental results demonstrate the practical impact of the presented method.
p3287
aVPredicate abstraction is the basis of many program verification tools. Until now, the only known way to overcome the inherent limitation of predicate abstraction to safety properties was to manually annotate the finitestate abstraction of a program. We extend predicate abstraction to transition predicate abstraction. Transition predicate abstraction goes beyond the idea of finite abstractstate programs (and checking the absence of loops). Instead, our abstraction algorithm transforms a program into a finite abstracttransition program. Then, a second algorithm checks fair termination. The two algorithms together yield an automated method for the verification of liveness properties under full fairness assumptions (justice and compassion). In summary, we exhibit principles that extend the applicability of predicate abstractionbased program verification to the full set of temporal properties.
p3288
aVWe define a language CQP (Communicating Quantum Processes) for modelling systems which combine quantum and classical communication and computation. CQP combines the communication primitives of the picalculus with primitives for measurement and transformation of quantum state; in particular, quantum bits (qubits) can be transmitted from process to process along communication channels. CQP has a static type system which classifies channels, distinguishes between quantum and classical data, and controls the use of quantum state. We formally define the syntax, operational semantics and type system of CQP, prove that the semantics preserves typing, and prove that typing guarantees that each qubit is owned by a unique process within a system. We illustrate CQP by defining models of several quantum communication systems, and outline our plans for using CQP as the foundation for formal analysis and verification of combined quantum and classical systems.
p3289
aVIn traditional informationflow type systems, the security policy is often formalized as noninterference properties. However, noninterference alone is too strong to express security properties useful in practice. If we allow downgrading in such systems, it is challenging to formalize the security policy as an extensional property of the system.This paper presents a generalized framework of downgrading policies. Such policies can be specified in a simple and tractable language and can be statically enforced by mechanisms such as type systems. The security guarantee is then formalized as a concise extensional property using program equivalences. This relaxed noninterference generalizes traditional pure noninterference and precisely characterizes the information released due to downgrading.
p3290
aVAs probabilistic computations play an increasing role in solving various problems, researchers have designed probabilistic languages that treat probability distributions as primitive datatypes. Most probabilistic languages, however, focus only on discrete distributions and have limited expressive power. In this paper, we present a probabilistic language, called \u03bb\u03bf, which uniformly supports all kinds of probability distributions   discrete distributions, continuous distributions, and even those belonging to neither group. Its mathematical basis is sampling functions, i.e., mappings from the unit interval (0.0,1.0] to probability domains.We also briefly describe the implementation of \u03bb\u03bf as an extension of Objective CAML and demonstrate its practicality with three applications in robotics: robot localization, people tracking, and robotic mapping. All experiments have been carried out with real robots.
p3291
aVDynamic software updates can be used to fix bugs or add features to a running program without downtime. Essential for some applications and convenient for others, lowlevel dynamic updating has been used for many years. Perhaps surprisingly, there is little highlevel understanding or language support to help programmers write dynamic updates effectively.To bridge this gap, we present Proteus, a core calculus for dynamic software updating in Clike languages that is flexible, safe, and predictable. Proteus supports dynamic updates to functions (even active ones), to named types and to data, allowing online evolution to match sourcecode evolution as we have observed it in practice. We ensure updates are typesafe by checking for a property we call "confreeness" for updated types t at the point of update. This means that nonupdated code will not use t concretely beyond that point (concrete usages are via explicit coercions) and thus t's representation can safely change. We show how confreeness can be enforced dynamically for a particular program state. We additionally define a novel and efficient static updateability analysis to establish confreeness statically, and can thus automatically infer program points at which all future (wellformed) updates will be typesafe. We have implemented our analysis for C and tested it on several wellknown programs.
p3292
aVWe introduce transactors, a faulttolerant programming model for composing looselycoupled distributed components running in an unreliable environment such as the internet into systems that reliably maintain globally consistent distributed state. The transactor model incorporates certain elements of traditional transaction processing, but allows these elements to be composed in different ways without the need for central coordination, thus facilitating the study of distributed faulttolerance from a semantic point of view. We formalize our approach via the \u03c4calculus, an extended lambdacalculus based on the actor model, and illustrate its usage through a number of examples. The \u03c4calculus incorporates constructs which distributed processes can use to create globallyconsistent checkpoints. We provide an operational semantics for the \u03c4calculus, and formalize the following safety and liveness properties: first, we show that globallyconsistent checkpoints have equivalent execution traces without any node failures or applicationlevel failures, and second, we show that it is possible to reach globallyconsistent checkpoints provided that there is some bounded failurefree interval during which checkpointing can occur.
p3293
aVA key aspect when aggregating business processes and web services is to assure transactional properties of process executions. Since transactions in this context may require long periods of time to complete, traditional mechanisms for guaranteeing atomicity are not always appropriate. Generally the concept of long running transactions relies on a weaker notion of atomicity based on compensations. For this reason, programming languages for service composition cannot leave out two key aspects: compensations, i.e. ad hoc activities that can undo the effects of a process that fails to complete, and transactional boundaries to delimit the scope of a transactional flow. This paper presents a hierarchy of transactional calculi with increasing expressiveness. We start from a very small language in which activities can only be composed sequentially. Then, we progressively introduce parallel composition, nesting, programmable compensations and exception handling. A running example illustrates the main features of each calculus in the hierarchy.
p3294
aVModern applications are designed in multiple tiers to separate concerns. Since each tier may run at a separate location, middleware is required to mediate access between tiers. However, introducing this middleware is tiresome and errorprone.We propose a multitier calculus and a splitting transformation to address this problem. The multitier calculus serves as a sequential core programming language for constructing a multitier application. The application can be developed in the sequential setting. Splitting extracts one process per tier from the sequential program such that their concurrent execution behaves like the original program.The splitting transformation starts from an assignment of primitive operations to tiers. A program analysis determines communication requirements and inserts remote procedure calls. The next transformation step performs resource pooling: it optimizes the communication behavior by transforming sequences of remote procedure calls to a streambased protocol. The final transformation step splits the resulting program into separate communicating processes.The multitier calculus is also applicable to the construction of interactive Web applications. It facilitates their development by providing a uniform programming framework for clientside and serverside programming.
p3295
aVIn 1996, Gil and Lorenz proposed programming language constructs for specifying environmental acquisition in addition to inheritance acquisition for objects. They noticed that in many programs, objects are arranged in containment hierarchies and need to obtain information from their container objects. Therefore, if languages allowed programmers to specify such relationships directly, type systems and runtime environments could enforce the invariants that make these programming patterns work.In this paper, we present a formal version of environmental acquisition for classbased languages. Specifically, we introduce an extension of the ClassicJava model with constructs for environmental acquisition of fields and methods, a type system for the model, a reduction semantics, and a type soundness proof. We also discuss how to scale the model to a fullscale Javalike programming language.
p3296
aVWe propose a novel approach to the wellknown view update problem for the case of treestructured data: a domainspecific programming language in which all expressions denote bidirectional transformations on trees. In one direction, these transformations dubbed lenses map a "concrete" tree into a simplified "abstract view"; in the other, they map a modified abstract view, together with the original concrete tree, to a correspondingly modified concrete tree. Our design emphasizes both robustness and ease of use, guaranteeing strong wellbehavedness and totality properties for welltyped lenses.We identify a natural space of wellbehaved bidirectional transformations over arbitrary structures, study definedness and continuity in this setting, and state a precise connection with the classical theory of "update translation under a constant complement" from databases. We then instantiate this semantic framework in the form of a collection of lens combinators that can be assembled to describe transformations on trees. These combinators include familiar constructs from functional programming (composition, mapping, projection, conditionals, recursion) together with some novel primitives for manipulating trees (splitting, pruning, copying, merging, etc.). We illustrate the expressiveness of these combinators by developing a number of bidirectional listprocessing transformations as derived forms.
p3297
aVIn this paper we address the problem of writing specifications for programs that use various forms of modularity, including procedures and Javalike classes. We build on the formalism of separation logic and introduce the new notion of an abstract predicate and, more generally, abstract predicate families. This provides a flexible mechanism for reasoning about the different forms of abstraction found in modern programming languages, such as abstract datatypes and objects. As well as demonstrating the soundness of our proof system, we illustrate its utility with a series of examples.
p3298
aVA lightweight logical approach to racefree sharing of heap storage between concurrent threads is described, based on the notion of permission to access. Transfer of permission between threads, subdivision and combination of permission is discussed. The roots of the approach are in Boyland's [3] demonstration of the utility of fractional permissions in specifying noninterference between concurrent threads. We add the notion of counting permission, which mirrors the programming technique called permission counting. Both fractional and counting permissions permit passivity, the specification that a program can be permitted to access a heap cell yet prevented from altering it. Models of both mechanisms are described. The use of two different mechanisms is defended. Some interesting problems are acknowledged and some intriguing possibilities for future development, including the notion of resourcing as a step beyond typing, are paraded.
p3299
aVSpatial logics have been used to describe properties of treelike structures (Ambient Logic) and in a Hoare style to reason about dynamic updates of heaplike structures (Separation Logic). We integrat this work by analyzing dynamic updates to treelike structures with pointers (such as XML with identifiers and idrefs). Nave adaptations of the Ambient Logic are not expressive enough to capture such local updates. Instead we must explicitly reason about arbitrary tree contexts in order to capture updates throughout the tree. We introduce Context Logic, study its proof theory and models, and show how it generalizes Separation Logic and its general theory BI. We use it to reason locally about a small imperative programming language for updating trees, using a Hoare logic in the style of O'Hearn, Reynolds and Yang, and show that weakest preconditions are derivable. We demonstrate the robustness of our approach by using Context Logic to capture the locality of term rewrite systems.
p3300
aV"Adoption" is when on piece of stat is logically embedded in another piece of state. Adoption provides information hiding (the adopter can be used as a proxy for the adoptee) and with linear existentials, provides a way to store unique pointers in shared state. In this paper, we give an operational semantics of adoption in a simple procedural language with pointers to records. We define a "permission" typesystem that uses adoption to model both effects and uniqueness. We prove type soundness (welltyped programs don't go wrong) and state separation (separatelytyped statements cannot access the same state). Then we show how highlevel effects and uniqueness annotations can be expressed in the typesystem. The distinction between read and write effects is ignored in the body of this paper.
p3301
aVThe goal of this work is to develop compiletime algorithms for automatically verifying properties of imperative programs that manipulate dynamically allocated storage. The paper presents an analysis method that uses a characterization of a procedure's behavior in which parts of the heap not relevant to the procedure are ignored. The paper has two main parts: The first part introduces a nonstandard concrete semantics, LSL, in which called procedures are only passed parts of the heap. In this semantics, objects are treated specially when they separate the "local heap" that can be mutated by a procedure from the rest of the heap, which from the viewpoint of that procedure is nonaccessible and immutable. The second part concerns abstract interpretation of LSL and develops a new staticanalysis algorithm using canonical abstraction.
p3302
aVThis paper proposes a novel approach to shape analysis: using local reasoning about individual heap locations instead of global reasoning about entire heap abstractions. We present an interprocedural shape analysis algorithm for languages with destructive updates. The key feature is a novel memory abstraction that differs from traditional abstractions in two ways. First, we build the shape abstraction and analysis on top of a pointer analysis. Second, we decompose the shape abstraction into a set of independent configurations, each of which characterizes one single heap location. Our approach: 1) leads to simpler algorithm specifications, because of local reasoning about the single location; 2) leads to efficient algorithms, because of the smaller granularity of the abstraction; and 3) makes it easier to develop contextsensitive, demanddriven, and incremental shape analyses.We also show that the analysis can be used to enable the static detection of memory errors in programs with explicit deallocation. We have built a prototype tool that detects memory leaks and accesses through dangling pointers in C programs. The experiments indicate that the analysis is sufficiently precise to detect errors with low false positive rates; and is sufficiently lightweight to scale to larger programs. For a set of three popular C programs, the tool has analyzed about 70K lines of code in less than 2 minutes and has produced 97 warnings, 38 of which were actual errors.
p3303
aVWe describe a unified framework for random interpretation that generalizes previous randomized intraprocedural analyses, and also extends naturally to efficient interprocedural analyses. There is no such natural extension known for deterministic algorithms. We present a general technique for extending any intraprocedural random interpreter to perform a contextsensitive interprocedural analysis with only polynomial increase in running time. This technique involves computing random summaries of procedures, which are complete and probabilistically sound.As an instantiation of this general technique, we obtain the first polynomialtime randomized algorithm that discovers all linear relationships interprocedurally in a linear program. We also obtain the first polynomialtime randomized algorithm for precise interprocedural value numbering over a program with unary uninterpreted functions.We present experimental evidence that quantifies the precision and relative speed of the analysis for discovering linear relationships along two dimensions: intraprocedural vs. interprocedural, and deterministic vs. randomized. We also present results that show the variation of the error probability in the randomized analysis with changes in algorithm parameters. These results suggest that the error probability is much lower than the existing conservative theoretical bounds.
p3304
aVAutomatic discovery of relationships among values of array elements is a challenging problem due to the unbounded nature of arrays. We present a framework for analyzing array operations that is capable of capturing numeric properties of array elements.In particular, the analysis is able to establish that all array elements are initialized by an arrayinitialization loop, as well as to discover numeric constraints on the values of initialized elements.The analysis is based on the combination of canonical abstraction and summarizing numeric domains. We describe a prototype implementation of the analysis and discuss our experience with applying the prototype to several examples, including the verification of correctness of an insertionsort procedure.
p3305
aVWe describe a software errordetection tool that exploits recent advances in boolean satisfiability (SAT) solvers. Our analysis is path sensitive, precise down to the bit level, and models pointers and heap data. Our approach is also highly scalable, which we achieve using two techniques. First, for each program function, several optimizations compress the size of the boolean formulas that model the control and dataflow and the heap locations accessed by a function. Second, summaries in the spirit of type signatures are computed for each function, allowing interprocedural analysis without a dramatic increase in the size of the boolean constraints to be solved.We demonstrate the effectiveness of our approach by constructing a lock interface inference and checking tool. In an interprocedural analysis of more than 23,000 lock related functions in the latest Linux kernel, the checker generated 300 warnings, of which 179 were unique locking errors, a false positive rate of only 40%.
p3306
aVWe define compositional compilation as the ability to typecheck source code fragments in isolation, generate corresponding binaries,and link together fragments whose mutual assumptions are satisfied, without reinspecting the code. Even though compositional compilation is a highly desirable feature, in Javalike languages it can hardly be achieved. This is due to the fact that the bytecode generated for a fragment (say, a class) is not uniquely determined by its source code, but also depends on the compilation context.We propose a way to obtain compositional compilation for Java, by introducing a polymorphic form of bytecode containing type variables (ranging over class names) and equipped with a set of constraints involving type variables. Thus, polymorphic bytecode provides a representation for all the (standard) bytecode that can be obtained by replacing type variables with classes satisfying the associated constraints.We illustrate our proposal by developing a typing and a linking algorithm. The typing algorithm compiles a class in isolation generating the corresponding polymorphic bytecode fragment and constraints on the classes it depends on. The linking algorithm takes a collection of polymorphic bytecode fragments, checks their mutual consistency, and possibly simplifies and specializes them. In particular, linking a selfcontained collection of fragments either fails, or produces standard bytecode (the same as would have been produced by standard compilation of all fragments).
p3307
aVWe present Rhodium, a new language for writing compiler optimizations that can be automatically proved sound. Unlike our previous work on Cobalt, Rhodium expresses optimizations using explicit dataflow facts manipulated by local propagation and transformation rules. This new style allows Rhodium optimizations to be mutually recursively defined, to be automatically composed, to be interpreted in both flowsensitive and insensitive ways, and to be applied interprocedurally given a separate contextsensitivity strategy, all while retaining soundness. Rhodium also supports infinite analysis domains while guaranteeing termination of analysis. We have implemented a soundness checker for Rhodium and have specified and automatically proven the soundness of all of Cobalt's optimizations plus a variety of optimizations not expressible in Cobalt, including Andersen's pointsto analysis, arithmeticinvariant detection, loopinductionvariable strength reduction, and redundant array load elimination.
p3308
aVTraditional class and object encodings are difficult to use in practical typepreserving compilers because of the complexity of the encodings. We propose a simple typed intermediate language for compiling objectoriented languages and prove its soundness. The key ideas are to preserve lightweight notions of classes and objects instead of compiling them away and to separate namebased subclassing from structurebased subtyping. The language can express standard implementation techniques for both dynamic dispatch and runtime type tests. It has decidable type checking even with subtyping between quantified types with different bounds. Because of its simplicity, the language is a more suitable starting point for a practical typepreserving compiler than traditional encoding techniques.
p3309
aVDespite the extensiveness of recent investigations on static typing for XML, parametric polymorphism has rarely been treated. This wellestablished typing discipline can also be useful in XML processing in particular for programs involving "parametric schemas," i.e., schemas parameterized over other schemas (e.g., SOAP). The difficulty in treating polymorphism for XML lies in how to extend the "semantic" approach used in the mainstream (monomorphic) XML type systems. A naive extension would be "semantic" quantification over all substitutions for type variables. However, this approach reduces to an NEXPTIMEcomplete problem for which no practical algorithm is known. In this paper, we propose a different method that smoothly extends the semantic approach yet is algorithmically easier. In this, we devise a novel and simple marking technique, where we interpret a polymorphic type as a set of values with annotations of which subparts are parameterized. We exploit this interpretation in every ingredient of our polymorphic type system such as subtyping, inference of type arguments, and so on. As a result, we achieve a sensible system that directly represents a usual expected behavior of polymorphic type systems "values of variable types are never reconstructed" in a reminiscence of Reynold's parametricity theory. Also, we obtain a set of practical algorithms for typechecking by local modifications to existing ones for a monomorphic system.
p3310
aVWe present a sound, complete, and elementary proof method, based on bisimulation, for contextual equivalence in a \u03bbcalculus with full universal, existential, and recursive types. Unlike logical relations (either semantic or syntactic), our development is elementary, using only sets and relations and avoiding advanced machinery such as domain theory, admissibility, and \u03a4\u03a4closure. Unlike other bisimulations, ours is complete even for existential types. The key idea is to consider sets of relations instead of just relations as bisimulations.
p3311
aVThis paper outlines an elementary approach for showing the decidability of type checking for type theories with \u03b2\u03b7equality, relevant to foundations for modules systems and type theorybased proof systems. The key to the approach is a syntactic translation mapping terms in the \u03b2\u03b7 presentation into their full \u03b7expansions in the \u03b2 presentation. Decidability of type checking is lifted from the target \u03b2 presentation to the \u03b2\u03b7 presentation. The approach extends to other inductive kinds with a single constructor, and is demonstrated for singletons and dependent pairs.
p3312
aVWe present a gamesbased denotational semantics for a quantitative analysis of programming languages. We define a HylandOngstyle games framework called slot games, which consists of HO games augmented with a new action called token. We develop a slotgame model for the language Idealised Concurrent Algol by instrumenting the strategies in its HO game model with token actions. We show that the slotgame model is a denotational semantics induced by a notion of observation formalised in the operational theory of improvement of Sands, and we give a full abstraction result. A quantitative analysis of programs has many potential applications, from compiler optimisations to resourceconstrained execution and static performance profiling. We illustrate several such applications with putative examples that would be nevertheless difficult, if not impossible, to handle using known operational techniques.
p3313
aVWhile a typical software component has a clearly specified (static) interface in terms of the methods and the input/output types they support, information about the correct sequencing of method calls the client must invoke is usually undocumented. In this paper, we propose a novel solution for automatically extracting such temporal specifications for Java classes. Given a Java class, and a safety property such as "the exception E should not be raised", the corresponding (dynamic) interface is the most general way of invoking the methods in the class so that the safety property is not violated. Our synthesis method first constructs a symbolic representation of the finite statetransition system obtained from the class using predicate abstraction. Constructing the interface then corresponds to solving a partialinformation twoplayer game on this symbolic graph. We present a sound approach to solve this computationallyhard problem approximately using algorithms for learning finite automata and symbolic model checking for branchingtime logics. We describe an implementation of the proposed techniques in the tool JIST  Java Interface Synthesis Tool and demonstrate that the tool can construct interfaces accurately and efficiently for sample Java2SDK library classes.
p3314
aVLanguage designers have in recent years proposed a wealth of richer type systems for programming which seek to extend the range of statically enforced guarantees on data and code. Most such proposals have been evolutionary extensions of ML or Haskell, offering programmers a balanced compromise between expressive strength and existing wellunderstood technology. Typically they revolve around type or kindindexed types such as GADTs, supported by limited equality reasoning at the typechecking level, thus separating the dynamic behaviour of programs from the (simpler) static behaviour of indexing information occurring in their types.I want to argue in this talk for a more radical departure from such practice by examining full spectrum type dependency, lifting such restrictions on the data upon which types may depend. Conor McBride and I designed the language EPIGRAM for experiments in programming with inductive families of data (of which GADTs are a special case). Using it for illustration, I will explore some of the possibilities and challenges afforded by full spectrum type dependency at the static and dynamic level: types directly support modelling complex invariants in terms of other data (rather than their types), with a CurryHoward flavour of dataasevidence; such complexity is on a 'payasyougo' basis, while keeping type annotations and other syntactic overheads to a minimum; data decomposition steps, e.g. case analysis, furnish more informative interactions between types and values during typechecking; such steps may moreover be abstractly specified by their types, and thus user definable; this supports a style of programming embracing 'learning by testing', views, and Burstall's 'hand simulation plus a little induction'; the absence of a rigid phase distinction need not lead to typepassing or excessive runtime overhead; effectful computation, in particular partiality, can be incorporated via variations on existing ideas such as monads.This talk is based on joint work with Conor McBride, Edwin Brady and Thorsten Altenkirch.
p3315
aVWe propose a type system based on regular tree grammars, where algebraic datatypes are interpreted in a structural way. Thus, the same constructors can be reused for different types and a flexible subtyping relation can be defined between types, corresponding to the inclusion of their semantics. For instance, one can define a type for lists and a subtype of this type corresponding to lists of even length. Patterns are simply types annotated with binders. This provides a generalization of algebraic patterns with the ability of matching arbitrarily deep in a value. Our main contribution, compared to languages such as XDuce and CDuce, is that we are able to deal with both polymorphism and function types.
p3316
aVWe describe a novel method for verifying programs that manipulate linked lists, based on two new predicates that characterize reachability of heap cells. These predicates allow reasoning about both acyclic and cyclic lists uniformly with equal ease. The crucial insight behind our approach is that a circular list invariably contains a distinguished head cell that provides a handle on the list. This observation suggests a programming methodology that requires the heap of the program at each step to be wellfounded, i.e., for any field f in the program, every sequence u.f, u.f.f,... contains at least one head cell. We believe that our methodology captures the most common idiom of programming with linked data structures. We enforce our methodology by automatically instrumenting the program with updates to two auxiliary variables representing these predicates and adding assertions in terms of these auxiliary variables.To prove program properties and the instrumented assertions, we provide a firstorder axiomatization of our two predicates. We also introduce a novel induction principle made possible by the wellfoundedness of the heap. We use our induction principle to derive from two basic axioms a small set of additional firstorder axioms that are useful for proving the correctness of several programs.We have implemented our method in a tool and used it to verify the correctness of a variety of nontrivial programs manipulating both acyclic and cyclic singlylinked lists and doublylinked lists. We also demonstrate the use of indexed predicate abstraction to automatically synthesize loop invariants for these examples.
p3317
aVWe describe a new programanalysis framework, based on CPS and procedurestring abstractions, that can handle critical analyses which the kCFA framework cannot. We present the main theorems concerning correctness, show an application analysis, and describe a running implementation.
p3318
aVWe introduce a new notion of bisimulation for showing contextual equivalence of expressions in an untyped lambdacalculus with an explicit store, and in which all expressed values, including higherorder values, are storable. Our notion of bisimulation leads to smaller and more tractable relations than does the method of Sumii and Pierce [31]. In particular, our method allows one to write down a bisimulation relation directly in cases where [31] requires an inductive specification, and where the principle of local invariants [22] is inapplicable. Our method can also express examples with higherorder functions, in contrast with the most widely known previous methods [4, 22, 32] which are limited in their ability to deal with such examples. The bisimulation conditions are derived by manually extracting proof obligations from a hypothetical direct proof of contextual equivalence.
p3319
aVWe define a new fixpoint modal logic, the visibly pushdown \u03bccalculus (VP\u03bc), as an extension of the modal \u03bccalculus. The models of this logic are execution trees of structured programs where the procedure calls and returns are made visible. This new logic can express pushdown specifications on the model that its classical counterpart cannot, and is motivated by recent work on visibly pushdown languages [4]. We show that our logic naturally captures several interesting program specifications in program verification and dataflow analysis. This includes a variety of program specifications such as computing combinations of local and global program flows, pre/post conditions of procedures, security properties involving the context stack, and interprocedural dataflow analysis properties. The logic can capture flowsensitive and interprocedural analysis, and it has constructs that allow skipping procedure calls so that local flows in a procedure can also be tracked. The logic generalizes the semantics of the modal \u03bccalculus by considering summaries instead of nodes as firstclass objects, with appropriate constructs for concatenating summaries, and naturally captures the way in which pushdown models are modelchecked. The main result of the paper is that the modelchecking problem for VP\u03bc is effectively solvable against pushdown models with no more effort than that required for weaker logics such as CTL. We also investigate the expressive power of the logic VP\u03bc: we show that it encompasses all properties expressed by a corresponding pushdown temporal logic on linear structures (caret [2]) as well as by the classical \u03bccalculus. This makes VP\u03bc the most expressive known program logic for which algorithmic software model checking is feasible. In fact, the decidability of most known program logics (\u03bccalculus, temporal logics LTL and CTL, caret, etc.) can be understood by their interpretation in the monadic secondorder logic over trees. This is not true for the logic VP\u03bc, making it a new powerful tractable program logic.
p3320
aVThis paper investigates what is essentially a callbyvalue version of PCF under a complexitytheoretically motivated type system. The programming formalism, ATR1, has its firstorder programs characterize the polytime computable functions, and its secondorder programs characterize the type2 basic feasible functionals of Mehlhorn and of Cook and Urquhart. (The ATR1types are confined to levels 0, 1, and 2.) The type system comes in two parts, one that primarily restricts the sizes of values of expressions and a second that primarily restricts the time required to evaluate expressions. The sizerestricted part is motivated by Bellantoni and Cook's and Leivant's implicit characterizations of polytime. The timerestricting part is an affine version of Barber and Plotkin's DILL. Two semantics are constructed for ATR1. The first is a pruning of the nave denotational semantics for ATR1. This pruning removes certain functions that cause otherwise feasible forms of recursion to go wrong. The second semantics is a model for ATR1's time complexity relative to a certain abstract machine. This model provides a setting for complexity recurrences arising from ATR1 recursions, the solutions of which yield secondorder polynomial time bounds. The timecomplexity semantics is also shown to be sound relative to the costs of interpretation on the abstract machine.
p3321
aVThe design of highperformance streamprocessing systems is a fast growing domain, driven by markets such like highend TV, gaming, 3D animation and medical imaging. It is also a surprisingly demanding task, with respect to the algorithmic and conceptual simplicity of streaming applications. It needs the close cooperation between numerical analysts, parallel programming experts, realtime control experts and computer architects, and incurs a very high level of quality insurance and optimization.In search for improved productivity, we propose a programming model and language dedicated to highperformance stream processing. This language builds on the synchronous programming model and on domain knowledge   the periodic evolution of streams   to allow correctbyconstruction properties to be proven by the compiler. These properties include resource requirements and delays between input and output streams. Automating this task avoids tedious and errorprone engineering, due to the combinatorics of the composition of filters with multiple data rates and formats. Correctness of the implementation is also difficult to assess with traditional (asynchronous, simulationbased) approaches. This language is thus provided with a relaxed notion of synchronous composition, called nsynchrony: two processes are nsynchronous if they can communicate in the ordinary (0)synchronous model with a FIFO buffer of size n.Technically, we extend a core synchronous dataflow language with a notion of periodic clocks, and design a relaxed clock calculus (a type system for clocks) to allow non strictly synchronous processes to be composed or correlated. This relaxation is associated with two subtyping rules in the clock calculus. Delay, buffer insertion and control code for these buffers are automatically inferred from the clock types through a systematic transformation into a standard synchronous program. We formally define the semantics of the language and prove the soundness and completeness of its clock calculus and synchronization transformation. Finally, the language is compared with existing formalisms.
p3322
aVIncreasing complexity in the communication patterns of embedded applications parallelized over multiple processing units makes it difficult to continue using the traditional busbased onchip communication techniques. The main contribution of this paper is to demonstrate the importance of compiler technology in reducing power consumption of applications designed for emerging multi processor, NoC (NetworkonChip) based embedded systems. Specifically, we propose and evaluate a compilerdirected approach to NoC power management in the context of arrayintensive applications, used frequently in embedded image/video processing. The unique characteristic of the compilerbased approach proposed in this paper is that it increases the idle periods of communication channels by reusing the same set of channels for as many communication messages as possible. The unused channels in this case take better advantage of the underlying power saving mechanism employed by the network architecture. However, this channel reuse optimization should be applied with care as it can hurt performance if two or more simultaneous communications are mapped onto the same set of channels. Therefore, the problem addressed in this paper is one of reducing the number of channels used to implement a set of communications without increasing the communication latency significantly. To test the effectiveness of our approach, we implemented it within an optimizing compiler and performed experiments using twelve application codes and a network simulation environment. Our experiments show that the proposed compilerbased approach is very successful in practice and works well under both hardware based and software based channel turnoff schemes.
p3323
aVFunctional programmers often reason about programs as if they were written in a total language, expecting the results to carry over to nontotal (partial) languages. We justify such reasoning.Two languages are defined, one total and one partial, with identical syntax. The semantics of the partial language includes partial and infinite values, and all types are lifted, including the function spaces. A partial equivalence relation (PER) is then defined, the domain of which is the total subset of the partial language. For types not containing function spaces the PER relates equal values, and functions are related if they map related values to related values.It is proved that if two closed terms have the same semantics in the total language, then they have related semantics in the partial language. It is also shown that the PER gives rise to a bicartesian closed category which can be used to reason about values in the domain of the relation.
p3324
aVIn the spirit of Landin, we present a calculus of dependent types to serve as the semantic foundation for a family of languages called data description languages. Such languages, which include pads, datascript, and packettypes, are designed to facilitate programming with ad hoc data, ie, data not in wellbehaved relational or xml formats. In the calculus, each type describes the physical layout and semantic properties of a data source. In the semantics, we interpret types simultaneously as the inmemory representation of the data described and as parsers for the data source. The parsing functions are robust, automatically detecting and recording errors in the data stream without halting parsing. We show the parsers are typecorrect, returning data whose type matches the simpletype interpretation of the specification. We also prove the parsers are "errorcorrect," accurately reporting the number of physical and semantic errors that occur in the returned data. We use the calculus to describe the features of various data description languages, and we discuss how we have used the calculus to improve PADS.
p3325
aVIn PLT Scheme, programs consist of modules with contracts. The latter describe the inputs and outputs of functions and objects via predicates. A runtime system enforces these predicates; if a predicate fails, the enforcer raises an exception that blames a specific module with an explanation of the fault.In this paper, we show how to use such module contracts to turn setbased analysis into a fully modular parameterized analysis. Using this analysis, a static debugger can indicate for any given contract check whether the corresponding predicate is always satisfied, partially satisfied, or (potentially) completely violated. The static debugger can also predict the source of potential errors, i.e., it is sound with respect to the blame assignment of the contract system.
p3326
aVStratified type inference for generalized algebraic data types.
p3327
aVTraditional static type systems are very effective for verifying basic interface specifications, but are somewhat limited in the kinds specifications they support. Dynamicallychecked contracts can enforce more precise specifications, but these are not checked until run time, resulting in incomplete detection of defects.Hybrid type checking is a synthesis of these two approaches that enforces precise interface specifications, via static analysis where possible, but also via dynamic checks where necessary. This paper explores the key ideas and implications of hybrid type checking, in the context of the simplytyped \u03bbcalculus with arbitrary refinements of base types.
p3328
aVThis article presents a polymorphic modal type system and its principal type inference algorithm that conservatively extend ML by all of Lisp's staging constructs (the quasiquotation system). The combination is meaningful because ML is a practical higherorder, impure, and typed language, while Lisp's quasiquotation system has long evolved complying with the demands from multistaged programming practices. Our type system supports open code, unrestricted operations on references, intentional variablecapturing substitution as well as captureavoiding substitution, and lifting values into code, whose combination escaped all the previous systems.
p3329
aVGame developers have long been early adopters of new technologies. This is so because we are largely unburdened by legacy code: With each new hardware generation, we are free to rethink our software assumptions and develop new products using new tools and even new programming languages. As a result, games are fertile ground for applying academic advances in these areas.And never has our industry been in need of such advances as it is now! The scale and scope of game development has increased more than tenfold over the past ten years, yet the underlying limitations of the mainstream C/C++/Java/C# language family remain largely unaddressed.The talk begins with a highlevel presentation of the game developer's world: the kinds of algorithms we employ on modern CPUs and GPUs, the difficulties of componentization and concurrency, and the challenges of writing very complex software with realtime performance requirements.The talk then outlines the ways that future programming languages could help us write better code, providing examples derived from experience writing games and software frameworks that support games. The major areas covered are abstraction facilities   how we can use them to develop more extensible frameworks and components; practical opportunities for employing stronger typing to reduce runtime failures; and the need for pervasive concurrency support, both implicit and explicit, to effectively exploit the several forms of parallelism present in games and graphics.
p3330
aVVirtual classes are classvalued attributes of objects. Like virtual methods, virtual classes are defined in an object's class and may be redefined within subclasses. They resemble inner classes, which are also defined within a class, but virtual classes are accessed through object instances, not as static components of a class. When used as types, virtual classes depend upon object identity   each object instance introduces a new family of virtual class types. Virtual classes support largescale program composition techniques, including higherorder hierarchies and family polymorphism. The original definition of virtual classes in BETA left open the question of static type safety, since some type errors were not caught until runtime. Later the languages Caesar and gbeta have used a more strict static analysis in order to ensure static type safety. However, the existence of a sound, statically typed model for virtual classes has been a longstanding open question. This paper presents a virtual class calculus, VC, that captures the essence of virtual classes in these fullfledged programming languages. The key contributions of the paper are a formalization of the dynamic and static semantics of VC and a proof of the soundness of VC.
p3331
aVThis paper introduces interruptible iterators, a language feature that makes expressive iteration abstractions much easier to implement. Iteration abstractions are valuable for software design, as shown by their frequent use in welldesigned data structure libraries such as the Java Collections Framework. While Java iterators support iteration abstraction well from the standpoint of client code, they are awkward to implement correctly and efficiently, especially if the iterator needs to support imperative update of the underlying collection, such as removing the current element. Some languages, such as CLU and C# 2.0, support iteration through a limited coroutine mechanism, but these mechanisms do not support imperative updates. Interruptible iterators are more powerful coroutines in which the loop body is able to interrupt the iterator with requests to perform updates. Interrupts are similar to exceptions, but propagate differently and have resumption semantics. Interruptible iterators have been implemented as part of the JMatch programming language, an extended version of Java. A JMatch reimplementation of the Java Collections Framework shows that implementations can be made substantially shorter and simpler; performance results show that this language mechanism can also be implemented efficiently.
p3332
aVC++ templates are key to the design of current successful mainstream libraries and systems. They are the basis of programming techniques in diverse areas ranging from conventional generalpurpose programming to software for safetycritical embedded systems. Current work on improving templates focuses on the notion of concepts (a type system for templates), which promises significantly improved error diagnostics and increased expressive power such as conceptbased overloading and function template partial specialization. This paper presents C++ templates with an emphasis on problems related to separate compilation. We consider the problem of how to express concepts in a precise way that is simple enough to be usable by ordinary programmers. In doing so, we expose a few weakness of the current specification of the C++ standard library and suggest a far more precise and complete specification. We also present a systematic way of translating our proposed concept definitions, based on usepatterns rather than function signatures, into constraint sets that can serve as convenient basis for concept checking in a compiler.
p3333
aVWe define a type system, which may also be considered as a simple Hoare logic, for a fragment of an assembly language that deals with code pointers and jumps. The typing is aimed at local reasoning in the sense that only the type of a code pointer is needed, and there is no need to know the whole code itself. The main features of the type system are separation logic connectives for describing the heap, and polymorphic answer types of continuations for keeping track of jumps. Specifically, we address an interaction between separation and answer types: frame rules for local reasoning in the presence of jumps are recovered by instantiating the answer type. However, the instantiation of answer types is not sound for all types. To guarantee soundness, we restrict instantiation to closed types, where the notion of closedness arises from biorthogonality (in a sense inspired by Krivine and Pitts). A machine state is orthogonal to a disjoint heap if their combination does not lead to a fault. Closed types are sets of machine states that are orthogonal to a set of heaps. We use closed types as wellbehaved answer types.
p3334
aVEmbedded code pointers (ECPs) are stored handles of functions and continuations commonly seen in lowlevel binaries as well as functional or higherorder programs. ECPs are known to be very hard to support well in Hoarelogic style verification systems. As a result, existing proofcarrying code (PCC) systems have to either sacrifice the expressiveness or the modularity of program specifications, or resort to construction of complex semantic models. In Reynolds's LICS'02 paper, supporting ECPs is listed as one of the main open problems for separation logic.In this paper we present a simple and general technique for solving the ECP problem for Hoarelogicbased PCC systems. By adding a small amount of syntax to the assertion language, we show how to combine semantic consequence relation with syntactic proof techniques. The result is a new powerful framework that can perform modular reasoning on ECPs while still retaining the expressiveness of Hoare logic. We show how to use our techniques to support polymorphism, closures, and other language extensions and how to solve the ECP problem for separation logic. Our system is fully mechanized. We give its complete soundness proof and a full verification of Reynolds's CPSstyle "listappend" example in the Coq proof assistant.
p3335
aVIn POPL 2002, Petrank and Rawitz showed a universal result finding optimal data placement is not only NPhard but also impossible to approximate within a constant factor if P \u2260 NP. Here we study a recently published concept called reference affinity, which characterizes a group of data that are always accessed together in computation. On the theoretical side, we give the complexity for finding reference affinity in program traces, using a novel reduction that converts the notion of distance into satisfiability. We also prove that reference affinity automatically captures the hierarchical locality in divideandconquer computations including matrix solvers and Nbody simulation. The proof establishes formal links between computation patterns in time and locality relations in space.On the practical side, we show that efficient heuristics exist. In particular, we present a sampling method and show that it is more effective than the previously published technique, especially for data that are often but not always accessed together. We show the effect on generated and real traces. These theoretical and empirical results demonstrate that effective data placement is still attainable in generalpurpose programs because common (albeit not all) locality patterns can be precisely modeled and efficiently analyzed.
p3336
aVConcurrencyrelated bugs may happen when multiple threads access shared data and interleave in ways that do not correspond to any sequential execution. Their absence is not guaranteed by the traditional notion of "data race" freedom. We present a new definition of data races in terms of 11 problematic interleaving scenarios, and prove that it is complete by showing that any execution not exhibiting these scenarios is serializable for a chosen set of locations. Our definition subsumes the traditional definition of a data race as well as highlevel data races such as stalevalue errors and inconsistent views. We also propose a language feature called atomic sets of locations, which lets programmers specify the existence of consistency properties between fields in objects, without specifying the properties themselves. We use static analysis to automatically infer those points in the code where synchronization is needed to avoid data races under our new definition. An important benefit of this approach is that, in general, far fewer annotations are required than is the case with existing approaches such as synchronized blocks or atomic sections. Our implementation successfully inferred the appropriate synchronization for a significant subset of Java's Standard Collections framework.
p3337
aVThe movement to multicore processors increases the need for simpler, more robust parallel programming models. Atomic sections have been widely recognized for their ease of use. They are simpler and safer to use than manual locking and they increase modularity. But existing proposals have several practical problems, including high overhead and poor interaction with I/O. We present pessimistic atomic sections, a fresh approach that retains many of the advantages of optimistic atomic sections as seen in "transactional memory" without sacrificing performance or compatibility. Pessimistic atomic sections employ the locking mechanisms familiar to programmers while relieving them of most burdens of lockbased programming, including deadlocks. Significantly, pessimistic atomic sections separate correctness from performance: they allow programmers to extract more parallelism via finergrained locking without fear of introducing bugs. We believe this property is crucial for exploiting multicore processor designs.We describe a tool, Autolocker, that automatically converts pessimistic atomic sections into standard lockbased code. Autolocker relies extensively on program analysis to determine a correct locking policy free of deadlocks and race conditions. We evaluate the expressiveness of Autolocker by modifying a 50,000 line highperformance web server to use atomic sections while retaining the original locking policy. We analyze Autolocker's performance using microbenchmarks, where Autolocker outperforms software transactional memory by more than a factor of 3.
p3338
aVRepresentation exposure is a wellknown problem in the objectoriented realm. Object encapsulation mechanisms have established a tradition for solving this problem based on a principle of reference containment. This paper proposes a novel type system which is based on a different principle, we call effect encapsulation, which confines side effects, rather than object references, according to an ownership structure. Compared to object encapsulation, effect encapsulation liberates us from the restriction on object referenceability and offers more flexibility. In this paper, we show that effect encapsulation can be statically type checked.
p3339
aVWeb applications typically interact with a backend database to retrieve persistent data and then present the data to the user as dynamically generated output, such as HTML web pages. However, this interaction is commonly done through a lowlevel API by dynamically constructing query strings within a generalpurpose programming language, such as Java. This lowlevel interaction is ad hoc because it does not take into account the structure of the output language. Accordingly, user inputs are treated as isolated lexical entities which, if not properly sanitized, can cause the web application to generate unintended output. This is called a command injection attack, which poses a serious threat to web application security. This paper presents the first formal definition of command injection attacks in the context of web applications, and gives a sound and complete algorithm for preventing them based on contextfree grammars and compiler parsing techniques. Our key observation is that, for an attack to succeed, the input that gets propagated into the database query or the output document must change the intended syntactic structure of the query or document. Our definition and algorithm are general and apply to many forms of command injection attacks. We validate our approach with SqlCheckS, an implementation for the setting of SQL command injection attacks. We evaluated SqlCheckS on realworld web applications with systematically compiled realworld attack data as input. SqlCheckS produced no false positives or false negatives, incurred low runtime overhead, and applied straightforwardly to web applications written in different languages.
p3340
aVThis paper defines an objectoriented language with harmless aspectoriented advice. A piece of harmless advice is a computation that, like ordinary aspectoriented advice, executes when control reaches a designated controlflow point. However, unlike ordinary advice, harmless advice is designed to obey a weak noninterference property. Harmless advice may change the termination behavior of computations and use I/O, but it does not otherwise influence the final result of the mainline code. The benefit of harmless advice is that it facilitates local reasoning about program behavior. More specifically, programmers may ignore harmless advice when reasoning about the partial correctness properties of their programs. In addition, programmers may add new pieces of harmless advice to preexisting programs in typical "afterthefact" aspectoriented style without fear they will break important data invariants used by the mainline code.In order to detect and enforce harmlessness, the paper defines a novel type and effect system related to informationflow type systems. The central technical result is that welltyped harmless advice does not interfere with the mainline computation. The paper also presents an implementation of the language and a case study using harmless advice to implement security policies.
p3341
aVWe present a verifiable lowlevel program representation to embed, propagate, and preserve safety information in high performance compilers for safe languages such as Java and C#. Our representation precisely encodes safety information via static singleassignment (SSA) [11, 3] proof variables that are firstclass constructs in the program.We argue that our representation allows a compiler to both (1) express aggressively optimized machineindependent code and (2) leverage existing compiler infrastructure to preserve safety information during optimization. We demonstrate that this approach supports standard compiler optimizations, requires minimal changes to the implementation of those optimizations, and does not artificially impede those optimizations to preserve safety. We also describe a simple type system that formalizes type safety in an SSAstyle controlflow graph program representation. Through the types of proof variables, our system enables compositional verification of memory safety in optimized code. Finally, we discuss experiences integrating this representation into the machineindependent global optimizer of STARJIT, a highperformance justintime compiler that performs aggressive controlflow, dataflow, and algebraic optimizations and is competitive with top production systems.
p3342
aVWe present optimization techniques for high level equational programs that are generalizations of affine control loops (ACLs). Significant parts of the SpecFP and PerfectClub benchmarks are ACLs. They often contain reductions: associative and commutative operators applied to a collection of values. They also often exhibit reuse: intermediate values computed or used at different index points being identical. We develop various techniques to automatically exploit reuse to simplify the computational complexity of evaluating reductions. Finally, we present an algorithm for the optimal application of such simplifications resulting in an equivalent specification with minimum complexity.
p3343
aVThis paper reports on the development and formal certification (proof of semantic preservation) of a compiler from Cminor (a Clike imperative language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its correctness. Such a certified compiler is useful in the context of formal methods applied to the certification of critical software: the certification of the compiler guarantees that the safety properties proved on the source code hold for the executable compiled code as well.
p3344
aVThe TCP/IP protocols and Sockets API underlie much of modern computation, but their semantics have historically been very complex and illdefined. The real standard is the de facto one of the common implementations, including, for example, the 15,000 20,000 lines of C in the BSD implementation. Dealing rigorously with the behaviour of such bodies of code is challenging.We have recently developed a posthoc specification of TCP, UDP, and Sockets that is rigorous, detailed, readable, has broad coverage, and is remarkably accurate. In this paper we describe the novel techniques that were required.Working within a generalpurpose proof assistant (HOL), we developed language idioms (within higherorder logic) in which to write the specification: operational semantics with nondeterminism, time, system calls, monadic relational programming, etc. We followed an experimental semantics approach, validating the specification against several thousand traces captured from three implementations (FreeBSD, Linux, and WinXP). Many differences between these were identified, and a number of bugs. Validation was done using a specialpurpose symbolic model checker programmed above HOL.We suggest that similar logic engineering techniques could be applied to future critical software infrastructure at design time, leading to cleaner designs and (via specificationbased testing using a similar checker) more predictable implementations.
p3345
aVNoninterference is the basic semantical condition used to account for confidentiality and integrityrelated properties in programming languages. There appears to be an at least implicit belief in the programming languages community that partial approaches based on type systems or other static analysis techniques are necessary for noninterference analyses to be tractable. In this paper we show that this belief is not necessarily true. We focus on the notion of strong low bisimulation proposed by Sabelfeld and Sands. We show that, relative to a decidable expression theory, strong low bisimulation is decidable for a simple parallel whilelanguage, and we give a sound and relatively complete proof system for deriving noninterference assertions. The completeness proof provides an effective proof search strategy. Moreover, we show that common alternative noninterference relations based on traces or inputoutput relations are undecidable. The first part of the paper is cast in terms of multilevel security. In the second part of the paper we generalize the setting to accommodate a form of intransitive interference. We discuss the model and show how the decidability and proof system results generalize to this richer setting.
p3346
aVThis article investigates formal properties of a family of semantically sound flowsensitive type systems for tracking information flow in simple While programs. The family is indexed by the choice of flow lattice.By choosing the flow lattice to be the powerset of program variables, we obtain a system which, in a very strong sense, subsumes all other systems in the family (in particular, for each program, it provides a principal typing from which all others may be inferred). This distinguished system is shown to be equivalent to, though more simply described than, Amtoft and Banerjee's Hoarestyle independence logic (SAS'04).In general, some lattices are more expressive than others. Despite this, we show that no type system in the family can give better results for a given choice of lattice than the type system for that lattice itself.Finally, for any program typeable in one of these systems, we show how to construct an equivalent program which is typeable in a simple flowinsensitive system. We argue that this general approach could be useful in a proofcarryingcode setting.
p3347
aVThis paper specifies, via a Hoarelike logic, an interprocedural and flow sensitive (but termination insensitive) information flow analysis for objectoriented programs. Pointer aliasing is ubiquitous in such programs, and can potentially leak confidential information. Thus the logic employs independence assertions to describe the noninterference property that formalizes confidentiality, and employs region assertions to describe possible aliasing. Programmer assertions, in the style of JML, are also allowed, thereby permitting a more finegrained specification of information flow policy.The logic supports local reasoning about state in the style of separation logic. Small specifications are used; they mention only the variables and addresses relevant to a command. Specifications are combined using a frame rule. An algorithm for the computation of postconditions is described: under certain assumptions, there exists a strongest postcondition which the algorithm computes.
p3348
aVPerl is a generalpurpose language, known for its vast number of freely available libraries. The Perl 6 project was started to improve the language's support for multiparadigmatic programming, while retaining compatibility with the existing code base. This talk discusses how Perl 6 attempts to reconcile various competing paradigms in the field of programming language design, such as static vs. dynamic typechecking, nominal vs. structural subtyping, prototype vs. classbased objects, and lazy vs. eager evaluation. Moreover, this talk also covers the design and development of Pugs, a selfhosting Perl 6 implementation bootstrapped from Haskell, targeting multiple runtime environments, including Perl 5, JavaScript and Parrot.
p3349
aVWe motivate and discuss a novel functional programming construct that allows convenient modular runtime nonstandard interpretation via reflection on closure environments. This mapclosure construct encompasses both the ability to examine the contents of a closure environment and to construct a new closure with a modified environment. From the user's perspective, mapclosure is a powerful and useful construct that supports such tasks as tracing, security logging, sandboxing, error checking, profiling, code instrumentation and metering, runtime code patching, and resource monitoring. From the implementor's perspective, mapclosure is analogous to call/cc. Just as call/cc is a nonreferentiallytransparent mechanism that reifies the continuations that are only implicit in programs written in direct style, mapclosure is a nonreferentiallytransparent mechanism that reifies the closure environments that are only implicit in higherorder programs. Just as CPS conversion is a nonlocal but purely syntactic transformation that can eliminate references to call/cc, closure conversion is a nonlocal but purely syntactic transformation that can eliminate references to mapclosure. We show how the combination of mapclosure and call/cc can be used to implement set! as a procedure definition and a local macro transformation.
p3350
aVMassive amounts of useful data are stored and processed in ad hoc formats for which common tools like parsers, printers, query engines and format converters are not readily available. In this paper, we explain the design and implementation of PADS/ML , a new language and system that facilitates the generation of data processing tools for ad hoc formats. The PADS/ML design includes features such as dependent, polymorphic and recursive datatypes, which allow programmers to describe the syntax and semantics of ad hoc data in a concise, easytoread notation. The PADS/ML implementation compiles these descriptions into ml structures and functors that include types for parsed data, functions for parsing and printing, and auxiliary support for userspecified, formatdependent and formatindependent tool generation.
p3351
aVThis paper is concerned with a programming language construct for typed name binding that enforces \u03b1equivalence. It proves a new result about what operations on names can coexist with this construct. The particular form of typed name binding studied is that used by the FreshML family of languages. Its characteristic feature is that a name binding is represented by an abstract (name,value)pair that may only be deconstructed via the generation of fresh bound names. In FreshML the only observation one can make of names is to test whether or not they are equal. This restricted amount of observation was thought necessary to ensure that there is no observable difference between \u03b1equivalent name binders. Yet from an algorithmic point of view it would be desirable to allow other operations and relations on names, such as a total ordering. This paper shows that, contrary to expectations, one may add not just ordering, but almost any relation or numerical function on names without disturbing the fundamental correctness result about this form of typed name binding (that objectlevel \u03b1equivalence precisely corresponds to contextual equivalence at the programming metalevel), so long as one takes the state of dynamically created names into account.
p3352
aVWe present a formal model of memory that both captures the lowlevel features of C's pointers and memory, and that forms the basis for an expressive implementation of separation logic. At the low level, we do not commit common oversimplifications, but correctly deal with C's model of programming language values and the heap. At the level of separation logic, we are still able to reason abstractly and efficiently. We implement this framework in the theorem prover Isabelle/HOL and demonstrate it on two case studies. We show that the divide between detailed and abstract does not impose undue verification overhead, and that simple programs remain easy to verify. We also show that the framework is applicable to real, security and safetycritical code by formally verifying the memory allocator of the L4 microkernel.
p3353
aVWe present a model of recursive and impredicatively quantified types with mutable references. We interpret in this model all of the type constructors needed for typed intermediate languages and typed assembly languages used for objectoriented and functional languages. We establish in this purely semantic fashion a soundness proof of the typing systems underlying these TILs and TALs ensuring that every welltyped program is safe. The technique is generic, and applies to any smallstep semantics including \u03bbcalculus, labeled transition systems, and von Neumann machines. It is also simple, and reduces mainly to defining a Kripke semantics of the GdelLb logic of provability. We have mechanically verified in Coq the soundness of our type system as applied to a von Neumann machine.
p3354
aVSeparation Logic, Ambient Logic and Context Logic are based on a similar style of reasoning about structured data. They each consist of a structural (separating) composition for reasoning about disjoint subdata, and corresponding structural adjoint(s) for reasoning hypothetically about data. We show how to interpret these structural connectives as modalities in Modal Logic and prove completeness results. The structural connectives are essential for describing properties of the underlying data, such as weakest preconditions for Hoare reasoning for Separation and Context Logic, and security properties for Ambient Logic. In fact, we introduced Context Logic to reason about tree update, precisely because the structural connectives of the Ambient Logic did not have enough expressive power. Despite these connectives being essential, first Lozes then Dawar, Gardner and Ghelli proved elimination results for Separation Logic and Ambient Logic (without quantifiers). In this paper, we solve this apparent contradiction. We study parametric inexpressivity results, which demonstrate that the structural connectives are indeed fundamental for this style of reasoning.
p3355
aVFuture software development will rely on product synthesis, i.e., the synthesis of code and noncode artifacts for a target component or application. Prior work on featurebased product synthesis can be understood and generalized using elementary ideas from category theory. Doing so reveals (a) practical and previously unrecognized properties that product synthesis tools must satisfy, and (b) nonobvious generalizations of current techniques that will guide future research efforts in automated product development.
p3356
aVXML programming involves idioms for expressing 'structure shyness' such as the descendant axis of XPath or the default templates of XSLT. We initiate a discussion of the relationships between such XML idioms and generic functional programming, while focusing on the (Haskellbased) 'Scrap your boilerplate' style of generic programming (SYB). This work gives insight into mechanisms for traversal and selection. We compare SYB and XSLT. We approximate XPath in SYB. We make a case for SYB's programmability, when compared to XPath's fixed combinators. We allude to strengthened type checking for SYB traversals so as to reject certain, trivial behaviors.
p3357
aVThis paper proposes a lightweight fusion method for general recursive function definitions. Compared with existing proposals, our method has several significant practical features: it works for general recursive functions on general algebraic data types; it does not produce extra runtime overhea (except for possible code size increase due to the success of fusion); and it is readily incorporated in standard inlining optimization. This is achieved by extending the ordinary inlining process with a new fusion law that transforms a term of the form f o (fixg\u03bbx.E) to a new fixed point term fixh\u03bbx.E\u2032 by promoting the function f through the fixed point operator. This is a sound syntactic transformation rule that is not sensitive to the types of f and g. This property makes our method applicable to wide range of functions including those with multiparameters in both curried and uncurried forms. Although this method does not guarantee any form of completeness, it fuses typical examples discussed in the literature and others that involve accumulating parameters, either in the tt foldllike specific forms or in general recursive forms, without any additional machinery. In order to substantiate our claim, we have implemented our method in a compiler. Although it is preliminary, it demonstrates practical feasibility of this method.
p3358
aVA method is presented for computing all higherorder partial derivatives of a multivariate function R n \u2192 R. This method works by evaluating the function under a nonstandard interpretation, lifting reals to multivariate power series. Multivariate power series, with potentially an infinite number of terms with nonzero coefficients, are represented using a lazy data structure constructed out of linear terms. A complete implementation of this method in Scheme is presented, along with a straightforward exposition, based on Taylor expansions, of the method's correctness.
p3359
aVInterlanguage interoperability is big business, as the success of Microsoft's .NET and COM and Sun's JVM show. Programming language designers are designing programming languages that reflect that fact   SML#, Mondrian, and Scala, to name just a few examples, all treat interoperability with other languages as a central design feature. Still, current multilanguage research tends not to focus on the semantics of interoperation features, but only on how to implement them efficiently. In this paper, we take first steps toward higherlevel models of interoperating systems. Our technique abstracts away the lowlevel details of interoperability like garbage collection and representation coherence, and lets us focus on semantic properties like typesafety and observable equivalence.Beyond giving simple expressive models that are natural compositions of singlelanguage models, our studies have uncovered several interesting facts about interoperability. For example, higherorder contracts naturally emerge as the glue to ensure that interoperating languages respect each other's type systems. While we present our results in an abstract setting, they shed light on real multilanguage systems and tools such as the JNI, SWIG, and Haskell's stable pointers.
p3360
aVWe present a new coinductive syntactic theory, eager normal form bisimilarity, for the untyped callbyvalue lambda calculus extended with continuations and mutable references.We demonstrate that the associated bisimulation proof principle is easy to use and that it is a powerful tool for proving equivalences between recursive imperative higherorder programs.The theory is modular in the sense that eager normal form bisimilarity for each of the calculi extended with continuations and/or mutable references is a fully abstract extension of eager normal form bisimilarity for its subcalculi. For each calculus, we prove that eager normal form bisimilarity is a congruence and is sound with respect to contextual equivalence. Furthermore, for the calculus with both continuations and mutable references, we show that eager normal form bisimilarity is complete: it coincides with contextual equivalence.
p3361
aVWe present an internal language with equivalent expressive power to Standard ML, and discuss its formalization in LF and the machinechecked verification of its type safety in Twelf. The internal language is intended to serve as the target of elaboration in an elaborative semantics for Standard ML in the style of Harper and Stone. Therefore, it includes all the programming mechanisms necessary to implement Standard ML, including translucent modules, abstraction, polymorphism, higher kinds, references, exceptions, recursive types, and recursive functions. Our successful formalization of the proof involved a careful interplay between the precise formulations of the various mechanisms, and required the invention of new representation and proof techniques of general interest.
p3362
aVThis work presents a framework for fusing flow analysis and theorem proving called logicflow analysis (LFA). The framework itself is the reduced product of two abstract interpretations: (1) an abstract state machine and (2) a set of propositions in a restricted firstorder logic. The motivating application for LFA is the safe removal of implicit arraybounds checks without type information, user interaction or program annotation. LFA achieves this by delegating a given task to either the prover or the flow analysis depending on which is best suited to discharge it. Described within are a concrete semantics for continuationpassing style; a restricted, firstorder logic; a woven product of two abstract interpretations; proofs of correctness; and a worked example.
p3363
aVTransparent persistence promises to integrate programming languages and databases by allowing procedural programs to access persistent data with the same ease as nonpersistent data. When the data is stored in a relational database, however, transparent persistence does not naturally leverage the performance benefits of relational query optimization. We present a program analysis that combines the benefits of both approaches by extracting database queries from programs with transparent access to persistent data. The analysis uses a sound abstract interpretation of the original program to approximate the data traversal paths in the program and the conditions under which the paths are used. The resulting paths are then converted into a query, and the program is simplified by removing redundant tests. We study an imperative kernel language with readonly access to persistent data and identify the conditions under which the transformations can be applied. This analysis approach promises to combine the software engineering benefits of transparent data persistence with the performance benefits of database query optimization.
p3364
aVAn invariance assertion for a program location l is a statement that always holds at l during execution of the program. Program invariance analyses infer invariance assertions that can be useful when trying to prove safety properties. We use the term variance assertion to mean a statement that holds between any state at l and any previous state that was also at l. This paper is concerned with the development of analyses for variance assertions and their application to proving termination and liveness properties. We describe a method of constructing program variance analyses from invariance analyses. If we change the underlying invariance analysis, we get a different variance analysis. We describe several applications of the method, including variance analyses using linear arithmetic and shape analysis. Using experimental results we demonstrate that these variance analyses give rise to a new breed of termination provers which are competitive with and sometimes better than today's stateoftheart termination provers.
p3365
aVThere is a clear intuitive connection between the notion of leakage of information in a program and concepts from information theory. This intuition has not been satisfactorily pinned down, until now. In particular, previous informationtheoretic models of programs are imprecise, due to their overly conservative treatment of looping constructs. In this paper we provide the first precise informationtheoretic semantics of looping constructs. Our semantics describes both the amount and rate of leakage; if either is small enough, then a program might be deemed "secure". Using the semantics we provide an investigation and classification of bounded and unbounded covert channels.
p3366
aVIt is well recognized that JavaScript can be exploited to launch browserbased security attacks. We propose to battle such attacks using program instrumentation. Untrusted JavaScript code goes through a rewriting process which identifies relevant operations, modifies questionable behaviors, and prompts the user (a web page viewer) for decisions on how to proceed when appropriate. Our solution is parametric with respect to the security policythe policy is implemented separately from the rewriting, and the same rewriting process is carried out regardless of which policy is in use. Besides providing a rigorous account of the correctness of our solution, we also discuss practical issues including policy management and prototype experiments. A useful byproduct of our work is an operational semantics of a core subset of JavaScript, where code embedded in (HTML) documents may generate further document pieces (with new code embedded) at runtime, yielding a form of selfmodifying code.
p3367
aVThe challenges hidden in the implementation of highlevel process calculi into lowlevel environments are well understood [3]. This paper develops a secure implementation of a typed pi calculus, in which capability types are employed to realize the policies for the access to communication channels. Our implementation compiles highlevel processes of the picalculus into lowlevel principals of a cryptographic process calculus based on the appliedpi calculus [1]. In this translation, the highlevel type capabilities are implemented as term capabilities protected by encryption keys only known to the intended receivers. As such, the implementation is effective even when the compiled, lowlevel principals are deployed in open contexts for which no assumption on trust and behavior may be made. Our technique and results draw on, and extend, previous work on secure implementation of channel abstractions in a dialect of the join calculus [2]. In particular, our translation preserves the forward secrecy of communications in a calculus that includes matching and supports the dynamic exchange of write and read accessrights among processes. We establish the adequacy and full abstraction of the implementation by contrasting the untyped equivalences of the lowlevel cryptographic calculus, with the typed equivalences of the highlevel source calculus.
p3368
aVEnterprise software systems automate the business processes of most nontrivial organizations in the world economy. These systems are immensely complex, and their function is critical to our living standards and everyday lives. Their design, implementation, and maintenance occupies many thousands of programmers and engineers, who work in what are aptly called the "COBOL dungeons"1 of the IT sector. These systems have persisted, growing by accretion   some for decades; there are enterprise systems in existence today whose original and even subsequent authors are retired or deceased. Such extraordinarly old, multilayered systems might appear to be the last place to apply avantegarde techniques, but in fact, they are extremely promising candidates, and for reasons directly connected to their history and structure.In this talk we take a tour of several deployed enterprise software systems, and demonstrate that the appropriate application of methods from functional programming can and does in fact yield dramatic performance improvements and thus commercial advantage in the design and implementation of enterprise software. This concrete application is an instance of a general plan for the application of advanced programming language design and analysis methods, to the problem of improving enterprise software. It is the thesis of this talk that to a great extent, it is in enterprise software that advanced PL techniques can find their most advantageous application. This talk literally breaks no new ground in PL research: every technique discussed is nearly two decades old, and our goal is to introduce PL researchers to what we feel is an ideal target for their work.
p3369
aVIn recent years we have seen great progress made in the area of automatic sourcelevel static analysis tools. However, most of today's program verification tools are limited to properties that guarantee the absence of bad events (safety properties). Until now no formal software analysis tool has provided fully automatic support for proving properties that ensure that good events eventually happen (liveness properties). In this paper we present such a tool, which handles liveness properties of large systems written in C. Liveness properties are described in an extension of the specification language used in the SDV system. We have used the tool to automatically prove critical liveness properties of Windows device drivers and found several previously unknown liveness bugs.
p3370
aVIn aspectoriented programming, one can intercept events by writing patterns called pointcuts. The pointcut language of the most popular aspectoriented programming language, AspectJ, allows the expression of highly complex properties of the static program structure.We present the first rigorous semantics of the AspectJ pointcut language, by translating static patterns into safe ( i.e. rangerestricted and stratified) Datalog queries. Safe Datalog is a logic language like Prolog, but it does not have data structures; consequently it has a straightforward least fixpoint semantics and all queries terminate.The translation from pointcuts to safe Datalog consists of a set of simple conditional rewrite rules, implemented using the Stratego system. The resulting queries are themselves executable with the CodeQuest system. We present experiments indicating that direct execution of our semantics is not prohibitively expensive.
p3371
aVIn this paper, we propose a new algorithm for proving the validity or invalidity of a pre/postcondition pair for a program. The algorithm is motivated by the success of the algorithms for probabilistic inference developed in the machine learning community for reasoning in graphical models. The validity or invalidity proof consists of providing an invariant at each program point that can be locally verified. The algorithm works by iteratively randomly selecting a program point and updating the current abstract state representation to make it more locally consistent (with respect to the abstractions at the neighboring points). We show that this simple algorithm has some interesting aspects: (a) It brings together the complementary powers of forward and backward analyses; (b) The algorithm has the ability to recover itself from excessive underapproximation or overapproximation that it may make. (Because the algorithm does not distinguish between the forward and backward information, the information could get both underapproximated and overapproximated at any step.) (c) The randomness in the algorithm ensures that the correct choice of updates is eventually made as there is no single deterministic strategy that would provably work for any interesting class of programs. In our experiments we use this algorithm to produce the proof of correctness of a small (but nontrivial) example. In addition, we empirically illustrate several important properties of the algorithm.
p3372
aVWe introduce lock allocation, an automatic technique that takes a multithreaded program annotated with atomic sections (that must be executed atomically), and infers a lock assignment from global variables to locks and a lock instrumentation that determines where each lock should be acquired and released such that the resulting instrumented program is guaranteed to preserve atomicity and deadlock freedom (provided all shared state is accessed only within atomic sections). Our algorithm works in the presence of pointers and procedures, and sets up the lock allocation problem as a 01 ILP which minimizes the conflict cost between atomic sections while simultaneously minimizing the number of locks. We have implemented our algorithm for both C with pthreads and Java, and have applied it to infer locks in 15K lines of AOLserver code. Our automatic allocation produces the same results as hand annotations for most of this code, while solving the optimization instances within a second for most programs.
p3373
aVThis paper contributes to the development of techniques for the modular proof of programs that include concurrent algorithms. We present a proof of a nonblocking concurrent algorithm, which provides a shared stack. The interthread interference, which is essential to the algorithm, is confined in the proof and the specification to the modular operations, which perform push and pop on the stack. This is achieved by the mechanisms of separation logic. The effect is that interthread interference does not pollute specification or verification of clients of the stack.
p3374
aVPushdown Systems (PDSs) have become an important paradigm for program analysis. Indeed, recent work has shown a deep connection between interprocedural dataflow analysis for sequential programs and the model checking problem for PDSs. A natural extension of this framework to the concurrent domain hinges on the, somewhat less studied, problem of model checking Interacting Pushdown Systems. In this paper, we therefore focus on the model checking of Interacting Pushdown Systems synchronizing via the standard primitives  locks, rendezvous and broadcasts, for rich classes of temporal properties  both linear and branching time. We formulate new algorithms for model checking interacting PDSs for important fragments of LTL and the MuCalculus. Additionally, we also delineate precisely the decidability boundary for each of the standard synchronization primitives.
p3375
aVConcurrent ML (CML) is a staticallytyped higherorder concurrent language that is embedded in Standard ML. Its most notable feature is its support for firstclass synchronous operations. This mechanism allows programmers to encapsulate complicated communication and synchronization protocols as firstclass abstractions, which encourages a modular style of programming where the underlying channels used to communicate with a given thread are hidden behind data and type abstraction.While CML has been in active use for well over a decade, little attention has been paid to optimizing CML programs. In this paper, we present a new program analysis for staticallytyped higherorder concurrent languages that enables the compiletime specialization of communication operations. This specialization is particularly important in a multiprocessor or multicore setting, where the synchronization overhead for generalpurpose operations are high. Preliminary results from a prototype that we have built demonstrate that specialized channel operations are much faster than the generalpurpose operations.Our analysis technique is modular (i.e.,, it analyzes and optimizes a single unit of abstraction at a time), which plays to the modular style of many CML programs. The analysis consists of three steps: the first is a typesensitive controlflow analysis that uses the program's typeabstractions to compute more precise results. The second is the construction of an extended controlflow graph using the results of the CFA. The last step is an iterative analysis over the graph that approximates the usage patterns of known channels. Our analysis is designed to detect special patterns of use, such as oneshot channels, fanin channels, and fanout channels. We have proven the safety of our analysis and state those results.
p3376
aVRace detection algorithms for multithreaded programs using the common lockbased synchronization idiom must correlate locks with the memory locations they guard. The heart of a proof of race freedom is showing that if two locks are distinct, then the memory locations they guard are also distinct. This is an example of a general property we call conditional must not aliasing: Under the assumption that two objects are not aliased, prove that two other objects are not aliased. This paper introduces and gives an algorithm for conditional must not alias analysis and discusses experimental results for sound race detection of Java programs.
p3377
aVAn asynchronous program is one that contains procedure calls which are not immediately executed from the callsite, but stored and "dispatched" in a nondeterministic order by an external scheduler at a later point. We formalize the problem of interprocedural dataflow analysis for asynchronous programs as AIFDS problems, a generalization of the IFDS problems for interprocedural dataflow analysis. We give an algorithm for computing the precise meetovervalidpaths solution for any AIFDS instance, as well as a demanddriven algorithm for solving the corresponding demand AIFDS instances. Our algorithm can be easily implemented on top of any existing interprocedural dataflow analysis framework. We have implemented the algorithm on top of BLAST, thereby obtaining the first safety verification tool for unbounded asynchronous programs. Though the problem of solving AIFDS instances is EXPSPACEhard, we find that in practice our technique can efficiently analyze programs by exploiting standard optimizations of interprocedural dataflow analyses.
p3378
aVPath profiles provide a more accurate characterization of a program's dynamic behavior than basic block or edge profiles, but are relatively more expensive to collect. This has limited their use in practice despite demonstrations of their advantages over edge profiles for a wide variety of applications.We present a new algorithm called preferential path profiling (PPP), that reduces the overhead of path profiling. PPP leverages the observation that most consumers of path profiles are only interested in a subset of all program paths. PPP achieves low overhead by separating interesting paths from other paths and assigning a set of unique and compact numbers to these interesting paths. We draw a parallel between arithmetic coding and path numbering, and use this connection to prove an optimality result for the compactness of path numbering produced by PPP. This compact path numbering enables our PPP implementation to record path information in an array instead of a hash table. Our experimental results indicate that PPP reduces the runtime overhead of profiling paths exercised by the largest (ref) inputs of the SPEC CPU2000 benchmarks from 50% on average (maximum of 132%) to 15% on average (maximum of 26%) as compared to a stateoftheart path profiler.
p3379
aVWe propose a new technique for hardware synthesis from higherorder functional languages with imperative features based on Reynolds's Syntactic Control of Interference. The restriction on contraction in the type system is useful for managing the thorny issue of sharing of physical circuits. We use a semantic model inspired by game semantics and the geometry of interaction, and express it directly as a certain class of digital circuits that form a cartesian, monoidalclosed category. A soundness result is given, which is also a correctness result for the compilation technique.
p3380
aVTypepreserving compilation can improve software reliability by generating code that can be verified independently of the compiler. Practical type preserving compilation does not exist for languages with multiple inheritance. This paper presents Emi, the first typed intermediate language to support practical compilation of a programming language with fully general multiple inheritance. The paper demonstrates the practicality of Emi by showing that Emi can be used to faithfully model standard implementation strategies of multiple inheritance for C++, the most widelyused programming language with general multiple inheritance.
p3381
aVA memory leak in a garbagecollected program occurs when the program inadvertently maintains references to objects that it no longer needs. Memory leaks cause systematic heap growth, degrading performance and resulting in program crashes after perhaps days or weeks of execution. Prior approaches for detecting memory leaks rely on heap differencing or detailed object statistics which store state proportional to the number of objects in the heap. These overheads preclude their use on the same processor for deployed longrunning applications.This paper introduces a dynamic heapsummarization technique based on type that accurately identifies leaks, is space efficient (adding less than 1% to the heap), and is time efficient (adding 2.3% on average to total execution time). We implement this approach in Cork which utilizes dynamic type information and garbage collection to summarize the live objects in a type pointsfrom graph (TPFG) whose nodes (types) and edges (references between types) are annotated with volume. Cork compares TPFGs across multiple collections, identifies growing data structures, and computes a type slice for the user. Cork is accurate: it identifies systematic heap growth with no false positives in 4 of 15 benchmarks we tested. Cork's slice report enabled us (nonexperts) to quickly eliminate growing data structures in SPECjbb2000 and Elipse, something their developers had not previously done. Cork is accurate, scalable, and efficient enough to consider using online.
p3382
aVC programs can be difficult to debug due to lax type enforcement and lowlevel access to memory. We present a dynamic analysis for C that checks heap snapshots for consistency with program types. Our approach builds on ideas from physical subtyping and conservative garbage collection. We infer a programdefined type for each allocated storage location or identify "untypable" blocks that reveal heap corruption or type safety violations. The analysis exploits symbolic debug information if present, but requires no annotation or recompilation beyond a list of defined program types and allocated heap blocks. We have integrated our analysis into the GNU Debugger (gdb), and describe our initial experience using this tool with several small to mediumsized programs.
p3383
aVDynamic test generation is a form of dynamic program analysis that attempts to compute test inputs to drive a program along a specific program path. Directed Automated Random Testing, or DART for short, blends dynamic test generation with model checking techniques with the goal of systematically executing all feasible program paths of a program while detecting various types of errors using runtime checking tools (like Purify, for instance). Unfortunately, systematically executing all feasible program paths does not scale to large, realistic programs.This paper addresses this major limitation and proposes to perform dynamic test generation compositionally, by adapting known techniques for interprocedural static analysis. Specifically, we introduce a new algorithm, dubbed SMART for Systematic Modular Automated Random Testing, that extends DART by testing functions in isolation, encoding test results as function summaries expressed using input preconditions and output postconditions, and then reusing those summaries when testing higherlevel functions. We show that, for a fixed reasoning capability, our compositional approach to dynamic test generation (SMART) is both sound and complete compared to monolithic dynamic test generation (DART). In other words, SMART can perform dynamic test generation compositionally without any reduction in program path coverage. We also show that, given a bound on the maximum number of feasible paths in individual program functions, the number of program executions explored by SMART is linear in that bound, while the number of program executions explored by DART can be exponential in that bound. We present examples of C programs and preliminary experimental results that illustrate and validate empirically these properties.
p3384
aVReuse distance (i.e. LRU stack distance) precisely characterizes program locality and has been a basic tool for memory system research since the 1970s. However, the high cost of measuring has restricted its practical uses in performance debugging, locality analysis and optimizations of longrunning applications.In this work, we improve the efficiency by exploring the connection between time and locality. We propose a statistical model that converts cheaply obtained time distance to the more costly reuse distance. Compared to the stateoftheart technique, this approach reduces measuring time by a factor of 17, and approximates cache line reuses with over 99% accuracy and the cache miss rate with less than 0.4% average error for 12 SPEC 2000 integer and floatingpoint benchmarks. By exploiting the strong correlations between time and locality, this work makes precise locality as easy to obtain as data access frequency, and opens new opportunities for program optimizations.
p3385
aVML modules and Haskell type classes have proven to be highly effective tools for program structuring. Modules emphasize explicit configuration of program components and the use of data abstraction. Type classes emphasize implicit program construction and ad hoc polymorphism. In this paper, we show how the implicitlytyped style of type class programming may be supported within the framework of an explicitlytyped module language by viewing type classes as a particular mode of use of modules. This view offers a harmonious integration of modules and type classes, where type class features, such as class hierarchies and associated types, arise naturally as uses of existing modulelanguage constructs, such as module hierarchies and type components. In addition, programmers have explicit control over which type class instances are available for use by type inference in a given scope. We formalize our approach as a HarperStonestyle elaboration relation, and provide a sound type inference algorithm as a guide to implementation.
p3386
aVMachinechecked proofs of properties of programming languages have become acritical need, both for increased confidence in large and complex designsand as a foundation for technologies such as proofcarrying code. However, constructing these proofs remains a black art, involving many choices in the formulation of definitions and theorems that make a huge cumulative difference in the difficulty of carrying out large formal developments. There presentation and manipulation of terms with variable binding is a key issue. We propose a novel style for formalizing metatheory, combining locally nameless representation of terms and cofinite quantification of free variable names in inductivedefinitions of relations on terms (typing, reduction, ...). The key technical insight is that our use of cofinite quantification obviates the need for reasoning about equivariance (the fact that free names can be renamed in derivations); in particular, the structural induction principles of relations defined using cofinite quantification are strong enough for metatheoretic reasoning, and need not be explicitly strengthened. Strong inversion principles follow (automatically, in Coq) from the induction principles. Although many of the underlying ingredients of our technique have been used before, their combination here yields a significant improvement over other methodologies using firstorder representations, leading to developments that are faithful to informal practice, yet require noexternal tool support and little infrastructure within the proof assistant. We have carried out several large developments in this style using the Coq proof assistant and have made them publicly available. Our developments include type soundness for System F sub; and core ML (with references, exceptions, datatypes, recursion, and patterns) and subject reduction for the Calculus of Constructions. Not only do these developments demonstrate the comprehensiveness of our approach; they have also been optimized for clarity and robustness, making them good templates for future extension.
p3387
aVThe proofs of major results of Computability Theory like Rice, RiceShapiro or Kleene's fixed point theorem hidemore information of what is usually expressed in theirrespective statements. We make this information explicit, allowing to state stronger, complexity theoreticversions of all these theorems. In particular, we replace the notion of extensional set of indices of programs, by a set of indices of programs having not only the same extensional behavior but also similar complexity (Complexity Clique). We prove, under very weak complexity assumptions, that any recursive Complexity Clique is trivial, and any r.e. Complexity Clique is an extensional set (and thus satisfies RiceShapiro conditions). This allows, for instance, to use Rice's argument to prove that the property of having polynomial complexity is not decidable, and to use RiceShapiro to conclude that it is not even semidecidable. We conclude the paper with a discussion of "complexitytheoretic" versions of Kleene's Fixed Point Theorem.
p3388
aVWe propose a characterization of PSPACE by means of atype assignment for an extension of lambda calculus with a conditional construction. The type assignment STAB is an extension of STA, a type assignment for lambdacalculus inspired by Lafont's Soft Linear Logic. We extend STA by means of a ground type and terms for booleans. The key point is that the elimination rule for booleans is managed in an additive way. Thus, we are able to program polynomial time Alternating Turing Machines. Conversely, we introduce a callbyname evaluation machine in order tocompute programs in polynomial space. As far as we know, this is the first characterization of PSPACE which is based on lambda calculusand light logics.
p3389
aVOkasaki and others have demonstrated how purely functional data structures that are efficient even in the presence of persistence can be constructed. To achieve good time bounds essential use is often made of laziness. The associated complexity analysis is frequently subtle, requiring careful attention to detail, and hence formalising it is valuable. This paper describes a simple library which can be used to make the analysis of a class of purely functional data structures and algorithms almost fully formal. The basic idea is to use the type system to annotate every function with the time required to compute its result. An annotated monad is used to combine time complexity annotations. The library has been used to analyse some existing data structures, for instance the deque operations of Hinze and Paterson's finger trees.
p3390
aVThe search for proof and the search for counterexamples (bugs) are complementary activities that need to be pursued concurrently in order to maximize the practical success rate of verification tools.While this is wellunderstood in safety verification, the current focus of liveness verification has been almost exclusively on the search for termination proofs. A counterexample to termination is an infinite programexecution. In this paper, we propose a method to search for such counterexamples. The search proceeds in two phases. We first dynamically enumerate lassoshaped candidate paths for counterexamples, and then statically prove their feasibility. We illustrate the utility of our nontermination prover, called TNT, on several nontrivial examples, some of which require bitlevel reasoning about integer representations.
p3391
aVWe show that the reachability problem for recursive state machines (or equivalently, pushdown systems), believed for long to have cubic worstcase complexity, can be solved in slightly subcubic time. All that is necessary for the new bound is a simple adaptation of a known technique. We also show that a better algorithm exists if the input machine does not have infinite recursive loops.
p3392
aVThis paper takes a fresh look at the problem of precise verification of heapmanipulating programs using firstorder SatisfiabilityModuloTheories (SMT) solvers. We augment the specification logic of such solvers by introducing the Logic of Interpreted Sets and Bounded Quantification for specifying properties of heapmanipulating programs. Our logic is expressive, closed under weakest preconditions, and efficiently implementable on top of existing SMT solvers. We have created a prototype implementation of our logic over the solvers Simplify and Z3 and used our prototype to verify many programs. Our preliminary experience is encouraging; the completeness and the efficiency of the decisionprocedure is clearly evident in practice and has greatly improved the user experience of the verifier.
p3393
aVJava programmers can document that the relationship between two objects is unchanging by declaring the field that encodes that relationship to be final. This information can be used in program understanding and detection of errors in new code additions. Unfortunately, few fields in programs are actually declared final. Programs often contain fields that could be final, but are not declared so. Moreover, the definition of final has restrictions on initializationthat limit its applicability. We introduce stationary fields as a generalization of final. A field in a program is stationary if, for every object that contains it, all writes to the field occur before all the reads. Unlike the definition of final fields, there can be multiple writes during initialization, and initialization can span multiple methods. We have developed an efficient algorithm for inferring which fields are stationary in a program, based on the observation that many fields acquire their value very close to object creation. We presume that an object's initialization phase has concluded when its reference is saved in some heap object. We perform precise analysis only regarding recently created objects. Applying our algorithm to realworld Java programs demonstrates that stationary fields are more common than final fields: 4459% vs. 1117% respectively in our benchmarks. These surprising results have several significant implications. First, substantial portions of Java programs appear to be written in a functional style. Second, initialization of these fields occurs very close to object creation, when very good alias information is available. These results open the door for more accurate and efficient pointer alias analysis.
p3394
aVThis paper presents a demanddriven, flowinsensitive analysisalgorithm for answering mayalias queries. We formulate thecomputation of alias queries as a CFLreachability problem, and use this formulation to derive a demanddriven analysis algorithm. The analysis uses a worklist algorithm that gradually explores the program structure and stops as soon as enough evidence is gathered to answer the query. Unlike existing techniques, our approach does not require building or intersecting pointsto sets. Experiments show that our technique is effective at answering alias queries accurately and efficiently in a demanddriven fashion. For a set of alias queries from the SPEC2000 benchmarks, an implementation of our analysis is able to accurately answer 96% of the queries in 0.5 milliseconds per query on average, using only 65 KB of memory. Compared to a demanddriven pointsto analysis that constructs and intersects pointsto sets on the fly, our alias analysis can achieve better accuracy while running more than 30 times faster. The low runtime cost and low memory demands of the analysis make it a very good candidate not only for compilers, but also for interactive tools, such as program understanding tools or integrated development environments (IDEs).
p3395
aVThe C language definition leaves the sizes and layouts of types partially unspecified. When a C program makes assumptions about type layout, its semantics is defined only on platforms (C compilers and the underlying hardware) on which those assumptions hold. Previous work on formalizing Clike languages has ignored this issue, either by assuming that programs do not make such assumptions or by assuming that all valid programs target only one platform. In the latter case, the platform's choices are hardwired in the language semantics. In this paper, we present a practicallymotivated model for a Clike language in which the memory layouts of types are left largely unspecified. The dynamic semantics is parameterized by a platform's layout policy and makes manifest the consequence of platformdependent (i.e., unspecified) steps. A typeandeffect system produces a layout constraint: a logic formula encoding layout conditions under which the program is memorysafe. We prove that if a program typechecks, it is memorysafe on all platforms satisfying its constraint. Based on our theory, we have implemented a tool that discovers unportable layout assumptions in C programs. Our approach should generalize to other kinds of platformdependent assumptions.
p3396
aVWe present a framework for generating procedure summaries that are (a) precise  applying the summary in a given context yields the same result as reanalyzing the procedure in that context, and(b) concise  the summary exploits the commonalitiesin the ways the procedure manipulates abstract values, and does not contain superfluous context information. The use of a precise and concise procedure summary inmodular analyses provides a way to capture infinitely many possible contexts in a finite way; in interprocedural analyses, it provides a compact representation of an explicit inputoutput summary table without loss of precision. We define a class of abstract domains and transformers for which precise and concise summaries can be efficiently generated using our framework. Our framework is rich enough to encode a wide range of problems, including all IFDS and IDE problems. In addition, we show how the framework is instantiated to provide novel solutions to two hard problems: modular linear constant propagation and modular typestate verification, both in the presence of aliasing. We implemented a prototype of our framework that computes summaries for the typestate domain, and report on preliminary experimental results.
p3397
aVTranslation validation consists of transforming a program and a posteriori validating it in order to detect a modification of itssemantics. This approach can be used in a verified compiler, provided that validation is formally proved to be correct. We present two such validators and their Coq proofs of correctness. The validators are designed for two instruction scheduling optimizations: list scheduling and trace scheduling.
p3398
aVWe describe a general technique for building abstract interpreters over powerful universally quantified abstract domains that leverage existing quantifierfree domains. Our quantified abstract domain can represent universally quantified facts like \u2200i(0 \u2264 i < n \u21d2 \u03b1[i] = 0). The principal challenge in this effort is that, while most domains supply overapproximations of operations like join, meet, and variable elimination, working with the guards of quantified facts requires underapproximation. We present an automatic technique to convert the standard overapproximation operations provided with all domains into sound underapproximations. We establish the correctness of our abstract interpreters by identifying two lattices one that establishes the soundness of the abstract interpreter and another that defines its precision, or completeness. Our experiments on a variety of programs using arrays and pointers (including several sorting algorithms) demonstrate the feasibility of the approach on challenging examples.
p3399
aVShape analyses are concerned with precise abstractions of the heap to capture detailed structural properties. To do so, they need to build and decompose summaries of disjoint memory regions. Unfortunately, many data structure invariants require relations be tracked across disjoint regions, such as intricate numerical data invariants or structural invariants concerning back and cross pointers. In this paper, we identify issues inherent to analyzing relational structures and design an abstract domain that is parameterized both by an abstract domain for pure data properties and by usersupplied specifications of the data structure invariants to check. Particularly, it supports hybrid invariants about shape and data and features a generic mechanism for materializing summaries at the beginning, middle, or end of inductive structures. Around this domain, we build a shape analysis whose interesting components include a preanalysis on the usersupplied specifications that guides the abstract interpretation and a widening operator over the combined shape and data domain. We then demonstrate our techniques on the proof of preservation of the redblack tree invariants during insertion.
p3400
aVContracts are behavioural descriptions of Web services. We devise a theory of contracts that formalises the compatibility of a client to a service, and the safe replacement of a service with another service. The use of contracts statically ensures the successful completion of every possible interaction between compatible clients and services. The technical device that underlies the theory is the definition of filters, which are explicit coercions that prevent some possible behaviours of services and, in doing so, they make services compatible with different usage scenarios. We show that filters can be seen as proofs of a sound and complete subcontracting deduction system which simultaneously refines and extends Hennessy's classical axiomatisation of the must testing preorder. The relation is decidable and the decision algorithm is obtained via a cutelimination process that proves the coherence of subcontracting as a logical system. Despite the richness of the technical development, the resulting approach is based on simple ideas and basic intuitions. Remarkably, its application is mostly independent of the language used to program the services or the clients. We also outline the possible practical impact of such a work and the perspectives of future research it opens.
p3401
aVCommunication is becoming one of the central elements in software development. As a potential typed foundation for structured communicationcentred programming, session types have been studied over the last decade for a wide range of process calculi and programming languages, focussing on binary (twoparty) sessions. This work extends the foregoing theories of binary session types to multiparty, asynchronous sessions, which often arise in practical communicationcentred applications. Presented as a typed calculus for mobile processes, the theory introduces a new notion of types in which interactions involving multiple peers are directly abstracted as a global scenario. Global types retain a friendly type syntax of binary session types while capturing complex causal chains of multiparty asynchronous interactions. A global type plays the role of a shared agreement among communication peers, and is used as a basis of efficient type checking through its projection onto individual peers. The fundamental properties of the session type discipline such as communication safety, progress and session fidelity are established for generalnparty asynchronous interactions.
p3402
aVThis paper introduces a small but useful generalisation to the 'derivative' operation on datatypes underlying Huet's notion of 'zipper', giving a concrete representation to onehole contexts in data which is undergoing transformation. This operator, 'dissection', turns a containerlike functor into a bifunctor representing a onehole context in which elements to the left of the hole are distinguished in type from elements to its right. I present dissection here as a generic program, albeit for polynomial functors only. The notion is certainly applicable more widely, but here I prefer to concentrate on its diverse applications. For a start, maplike operations over the functor and foldlike operations over the recursive data structure it induces can be expressed by tail recursion alone. Further, the derivative is readily recovered from the dissection. Indeed, it is the dissection structure which delivers Huet's operations for navigating zippers. The original motivation for dissection was to define 'division', capturing the notion of leftmost hole, canonically distinguishing values with no elements from those with at least one. Division gives rise to an isomorphism corresponding to the remainder theorem in algebra. By way of a larger example, division and dissection are exploited to give a relatively efficient generic algorithm for abstracting all occurrences of one term from another in a firstorder syntax. The source code for the paper is available online and compiles with recent extensions to the Glasgow Haskell Compiler.
p3403
aVGADTs are at the cutting edge of functional programming and becomemore widely used every day. Nevertheless, the semantic foundations underlying GADTs are not well understood. In this paper we solve this problem by showing that the standard theory of data types as carriers of initial algebras of functors can be extended from algebraic and nested data types to GADTs. We then use this observation to derivean initial algebra semantics for GADTs, thus ensuring that all of the accumulated knowledge about initial algebras can be brought to bear on them. Next, we use our initial algebra semantics for GADTs to derive expressive and principled tools   analogous to the wellknown and widelyused ones for algebraic and nested data types for reasoning about, programming with, and improving the performance of programs involving, GADTs; we christen such a collection of tools for a GADT an initial algebra package. Along the way, we give a constructive demonstration that every GADT can be reduced to one which uses only the equality GADT and existential quantification. Although other such reductions exist in the literature, ours is entirely local, is independent of any particular syntactic presentation of GADTs, and can be implemented in the host language, rather than existing solely as a metatheoretical artifact. The main technical ideas underlying our approach are (i) to modify the notion of a higherorder functor so that GADTs can be seen as carriers of initial algebras of higherorder functors, and (ii) to use left Kan extensions to trade arbitrary GADTs for simplerbutequivalent ones for which initial algebra semantics can bederived.
p3404
aVSelfadjusting computation enables writing programs that can automatically and efficiently respond to changes to their data (e.g., inputs). The idea behind the approach is to store all data that can change over time in modifiable references and to let computations construct traces that can drive change propagation. After changes have occurred, change propagation updates the result of the computation by reevaluating only those expressions that depend on the changed data. Previous approaches to selfadjusting computation require that modifiable references be written at most once during execution this makes the model applicable only in a purely functional setting. In this paper, we present techniques for imperative selfadjusting computation where modifiable references can be written multiple times. We define a language SAIL (SelfAdjusting Imperative Language) and prove consistency, i.e., that change propagation and fromscratch execution are observationally equivalent. Since SAIL programs are imperative, they can create cyclic data structures. To prove equivalence in the presence of cycles in the store, we formulate and use an untyped, stepindexed logical relation, where step indices are used to ensure wellfoundedness. We show that SAIL accepts an asymptotically efficient implementation by presenting algorithms and data structures for its implementation. When the number of operations (reads and writes) per modifiable is bounded by a constant, we show that change propagation becomes as efficient as in the nonimperative case. The general case incurs a slowdown that is logarithmic in the maximum number of such operations. We describe a prototype implementation of SAIL as a Standard ML library.
p3405
aVIn languagebased security, confidentiality and integrity policies conveniently specify the permitted flows of information between different parts of a program with diverse levels of trust. These policies enable a simple treatment of security, and they can often be verified by typing. However, their enforcement in concrete systems involves delicate compilation issues. We consider cryptographic enforcement mechanisms for imperative programs with untrusted components. Such programs may represent, for instance, distributed systems connected by some untrusted network. In source programs, security depends on an abstract accesscontrol policy for reading and writing the shared memory. In their implementations, shared memory is unprotected and security depends instead on encryption and signing. We build a translation from welltyped source programs and policies to cryptographic implementations. To establish its correctness, we develop a type system for the target language. Our typing rules enforce a correct usage of cryptographic primitives against active adversaries; from an informationflow viewpoint, they capture controlled forms of robust declassification and endorsement. We showtype soundness for a variant of the noninterference property, then show that our translation preserves typability. We rely on concrete primitives and hypotheses for cryptography, stated in terms of probabilistic polynomialtime algorithms and games. We model these primitives as commands in our target language. Thus, we develop a uniform languagebased model of security, ranging from computational noninterference for probabilistic programs down to standard cryptographic hypotheses.
p3406
aVTo speak about the security of information flow in programs employing cryptographic operations, definitions based on computational indistinguish ability of distributions over program states have to be used. These definitions, as well as the accompanying analysis tools, are complex and errorprone to argue about. Cryptographically masked flows, proposed by Askarov, Hedin and Sabelfeld, are an abstract execution model and security definition that attempt to abstract away the details of computational security. This abstract model is useful because analysis of programs can be conducted using the usual techniques for enforcing noninterference. In this paper we investigate under which conditions this abstract model is computationally sound, i.e. when does the security of a program in their model imply the computational security of this program. This paper spells out a reasonable set of conditions and then proposes a simpler abstract model that is nevertheless no more restrictive than the cryptographically masked flows together with these conditions for soundness.
p3407
aVThe subtyping test consists of checking whether a type t is a descendant of a type r (Agrawal et al. 1989). We study how to perform such a test efficiently, assuming a dynamic hierarchy when new types are inserted at runtime. The goal is to achieve time and space efficiency, even as new types are inserted. We propose an extensible scheme, named ESE, that ensures (1) efficient insertion of new types, (2) efficient subtyping tests, and (3) small space usage. On the one hand ESE provides comparable test times to the most efficient existing static schemes (e.g.,Zibin et al. (2001)). On the other hand, ESE has comparable insertion times to the most efficient existing dynamic scheme (Baehni et al. 2007), while ESE outperforms it by a factor of 23 times in terms of space usage.
p3408
aVThis pearl develops a statement about parallel prefix computation in the spirit of Knuth's 01Principle for oblivious sorting algorithms. It turns out that 01 is not quite enough here. The perfect hammer for the nails we are going to drive in is relational parametricity.
p3409
aVFocusing is a proofsearch strategy, originating in linear logic, that elegantly eliminates inessential nondeterminism, with one byproduct being a correspondence between focusing proofs and programs with explicit evaluation order. Higherorder abstract syntax (HOAS) is a technique for representing higherorder programming language constructs (e.g., \u03bb's) by higherorder terms at the"metalevel", thereby avoiding some of the bureaucratic headaches of firstorder representations (e.g., captureavoiding substitution). This paper begins with a fresh, judgmental analysis of focusing for intuitionistic logic (with a full suite of propositional connectives), recasting the "derived rules" of focusing as iterated inductive definitions. This leads to a uniform presentation, allowing concise, modular proofs of the identity and cut principles. Then we show how this formulation of focusing induces, through the CurryHoward isomorphism, a new kind of higherorder encoding of abstract syntax: functions are encoded by maps from patterns to expressions. Dually, values are encoded as patterns together with explicit substitutions. This gives us patternmatching "for free", and lets us reason about a rich type system with minimal syntactic overhead. We describe how to translate the language and proof of type safety almost directly into Coq using HOAS, and finally, show how the system's modular design pays off in enabling a very simple extension with recursion and recursive types.
p3410
aVHigherorder abstract syntax (HOAS) is a simple, powerful technique for implementing object languages, since it directly supports common and tricky routines dealing with variables, such as captureavoiding substitution and renaming. This is achieved by representing binders in the objectlanguage via binders in the metalanguage. However, enriching functional programming languages with direct support for HOAS has been a major challenge, because recursion over HOAS encodings requires one to traverse lambdaabstractions and necessitates programming with open objects. We present a novel typetheoretic foundation based on contextual modal types which allows us to recursively analyze open terms via higherorder pattern matching. By design, variables occurring in open terms can never escape their scope. Using several examples, we demonstrate that our framework provides a namesafe foundation to operations typically found in nominal systems. In contrast to nominal systems however, we also support captureavoiding substitution operations and even provide firstclass substitutions to the programmer. The main contribution of this paper is a syntaxdirected bidirectional type system where we distinguish between the data language and the computation language together with the progress and preservation proof for our language.
p3411
aVWe show that a variant of Parigot's \u03bb\u03bccalculus, originally due to de Groote and proved to satisfy Boehm's theorem by Saurin, is canonically interpretable as a callbyname calculus of delimited control. This observation is expressed using Ariola et al's callbyvalue calculus of delimited control, an extension of \u03bb\u03bccalculus with delimited control known to be equationally equivalent to Danvy and Filinski's calculus with shift and reset. Our main result then is that de Groote and Saurin's variant of \u03bb\u03bccalculus is equivalent to a canonical callbyname variant of Ariola et al's calculus. The rest of the paper is devoted to a comparative study of the callbyname and callbyvalue variants of Ariola et al's calculus, covering in particular the questions of simple typing, operational semantics, and continuationpassingstyle semantics. Finally, we discuss the relevance of Ariola et al's calculus as a uniform framework for representing different calculi of delimited continuations, including "lazy" variants such as Sabry's shift and lazy reset calculus.
p3412
aVWhen scripts in untyped languages grow into large programs, maintaining them becomes difficult. A lack of types in typical scripting languages means that programmers must (re)discover critical pieces of design information every time they wish to change a program. This analysis step both slows down the maintenance process and may even introduce mistakes due to the violation of undiscovered invariants. This paper presents Typed Scheme, an explicitly typed extension of an untyped scripting language. Its type system is based on the novel notion of occurrence typing, which we formalize and mechanically prove sound. The implementation of Typed Scheme additionally borrows elements from a range of approaches, including recursive types, true unions and subtyping, plus polymorphism combined with a modicum of local inference. Initial experiments with the implementation suggest that Typed Scheme naturally accommodates the programming style of the underlying scripting language, at least for the first few thousand lines of ported code.
p3413
aVA lens is a bidirectional program. When read from left toright, it denotes an ordinary function that maps inputs to outputs. When read from right to left, it denotes an ''update translator'' that takes an input together with an updated output and produces a new input that reflects the update. Many variants of this idea have been explored in the literature, but none deal fully with ordered data. If, for example, an update changes the order of a list in theoutput, the items in the output list and the chunks of the input that generated them can be misaligned, leading to lost or corrupted data. We attack this problem in the context of bidirectional transformations over strings, the primordial ordered data type. We first propose a collection of bidirectional string lens combinators, based on familiar operations on regular transducers (union, concatenation, Kleenestar) and with a type system based on regular expressions. We then design anew semantic space of dictionary lenses, enriching the lenses of Foster et al. (2007) with support for two additional combinators for marking ''reorderable chunks'' andtheir keys. To demonstrate the effectiveness of these primitives, we describe the design and implementation of Boomerang, a fullblown bidirectional programming language with dictionary lenses at its core. We have used Boomerang to build transformers for complex realworld data format sincluding the SwissProt genomic database. We formalize the essential property of resourcefulnessthe correct use of keys to associate chunks in the input and outputby defining a refined semantic space of quasioblivious lenses. Several previously studied properties of lenses turn out to have compact characterizations in this space.
p3414
aVAn ad hoc data source is any semistructured data source for which useful data analysis and transformation tools are not readily available. Such data must be queried, transformed and displayed by systems administrators, computational biologists, financial analysts and hosts of others on a regular basis. In this paper, we demonstrate that it is possible to generate a suite of useful data processing tools, including a semistructured query engine, several format converters, a statistical analyzer and data visualization routines directly from the ad hoc data itself, without any human intervention. The key technical contribution of the work is a multiphase algorithm that automatically infers the structure of an ad hoc data source and produces a format specification in the PADS data description language. Programmers wishing to implement custom data analysis tools can use such descriptions to generate printing and parsing libraries for the data. Alternatively, our software infrastructure will push these descriptions through the PADS compiler, creating formatdependent modules that, when linked with formatindependent algorithms for analysis and transformation, result infully functional tools. We evaluate the performance of our inference algorithm, showing it scales linearlyin the size of the training data  completing in seconds, as opposed to the hours or days it takes to write a description by hand. We also evaluate the correctness of the algorithm, demonstrating that generating accurate descriptions often requires less than 5% of theavailable data.
p3415
aVModels will play a central role in the representation, storage, manipulation, and communication of knowledge in systems biology. Models capable of fulfilling such a role will likely differ from the all familiar styles deployed with great success in the physical sciences. Molecular systems at the basis of cellular decision processes are concurrent and combinatorial. Their behavior is as much constrained by relationships of causality between molecular interactions as it is by chemical kinetics. Understanding how such systems give rise to coherent behavior and designing effective interventions to fight disease will require a notion of model that is akin to the concept of program in computer science. I will discuss recent progress in implementing a platform and tools for formal analysis that bring us closer to this vision. Protein interactions are represented by means of rules expressed in a formal language that captures a very simple, yet effective and biologically meaningful level of abstraction. Models, then, are collections of rules operating on an initial set of agents, in complete analogy to rules of organic chemical reactions. I will describe tools for analyzing and navigating rule collections as well as exploring their dynamics. We draw on concepts familiar to computer science, especially event structures, and adapt them to biological needs with the goal of formalizing the notion of "pathway". The challenges are many, but a road map for the future is discernible. Computer science will play a central role in providing an additional foundational layer, both theoretical and practical, that neither physics nor chemistry can offer on their own in the future definition of the biological sciences.
p3416
aVRelevance heuristics allow us to tailor a program analysis to a particular property to be verified. This in turn makes it possible to improve the precision of the analysis where needed, while maintaining scalability. In this talk I will discuss the principles by which SAT solvers and other decision procedures decide what information is relevant to a given proof. Then we will see how these ideas can be exploited in program verification using the method of Craig interpolation. The result is an analysis that is finely tuned to prove a given property of a program. At the end of the talk, I will cover some recent research in this area, including the use of interpolants for verifying heapmanipulating programs.
p3417
aVThis paper presents a generalization of standard effect systems that we call contextual effects. A traditional effect system computes the effect of an expression e. Our system additionally computes the effects of the computational context in which e occurs. More specifically, we computethe effect of the computation that has already occurred(the prior effect) and the effect of the computation yet to take place (the future effect). Contextual effects are useful when the past or future computation of the program is relevant at various program points. We present two substantial examples. First, we show how prior and future effects can be used to enforce transactional version consistency(TVC), a novel correctness property for dynamic software updates. TV Censures that programmerdesignated transactional code blocks appear to execute entirely at the same code version, even if a dynamic update occurs in the middle of the block. Second, we show how future effects can be used in the analysis of multithreaded programs to find threadshared locations. This is an essential step in applications such as data race detection.
p3418
aVSoftware transactions have received significant attention as a way to simplify sharedmemory concurrent programming, but insufficient focus has been given to the precise meaning of software transactions or their interaction with other language features. This work begins to rectify that situation by presenting a family of formal languages that model a wide variety of behaviors for software transactions. These languages abstract away implementation details of transactional memory, providing highlevel definitions suitable for programming languages. We use smallstep semantics in order to represent explicitly the interleaved execution of threads that is necessary to investigate pertinent issues. We demonstrate the value of our core approach to modeling transactions by investigating two issues in depth. First, we consider parallel nesting, in which parallelism and transactions can nest arbitrarily. Second, we present multiple models for weak isolation, in which nontransactional code can violate the isolation of a transaction. For both, typeandeffect systems let us soundly and statically restrict what computation can occur inside or outside a transaction. We prove some key languageequivalence theorems to confirm that under sufficient static restrictions, in particular that each mutable memory location is used outside transactions or inside transactions (but not both), no program can determine whether the language implementation uses weak isolation or strong isolation.
p3419
aVSoftware Transactional Memory (STM) is an attractive basis for the development of language features for concurrent programming. However, the semantics of these features can be delicate and problematic. In this paper we explore the tradeoffs between semantic simplicity, the viability of efficient implementation strategies, and the flexibilityof language constructs. Specifically, we develop semantics and type systems for the constructs of the Automatic Mutual Exclusion (AME) programming model; our results apply also to other constructs, such as atomic blocks. With this semantics as a point of reference, we study several implementation strategies. We model STM systems that use inplace update, optimistic concurrency, lazy conflict detection, and rollback. These strategies are correct only under nontrivial assumptions that we identify and analyze. One important source of errors is that some efficient implementations create dangerous 'zombie' computations where a transaction keeps running after experiencing a conflict; the assumptions confine the effects of these computations.
p3420
aVInheritance is a fundamental concept in objectoriented programming, allowing new classes to be defined in terms of old classes. When used with care, inheritance is an essential tool for objectoriented programmers. Thus, for those interested in developing formal verification techniques, the treatment of inheritance is of paramount importance. Unfortunately, inheritance comes in a number of guises, all requiring subtle techniques. To address these subtleties, most existing verification methodologies typically adopt one of two restrictions to handle inheritance: either (1) they prevent a derived class from restricting the behaviour of its base class (typically by syntactic means) to trivialize the proof obligations; or (2) they allow a derived class to restrict the behaviour of its base class, but require that every inherited method must be reverified. Unfortunately, this means that typical inheritancerich code either cannot be verified or results in an unreasonable number of proof obligations. In this paper, we develop a separation logic for a core objectoriented language. It allows derived classes which override the behaviour of their base class, yet supports the inheritance of methods without reverification where this is safe. For each method, we require two specifications: a static specification that is used to verify the implementation and direct method calls (in Java this would be with a super call); and a dynamic specification that is used for calls that are dynamically dispatched; along with a simple relationship between the two specifications. Only the dynamic specification is involved with behavioural subtyping. This simple separation of concerns leads to a powerful system that supports all forms of inheritance with low proofobligation overheads. We both formalize our methodology and demonstrate its power with a series of inheritance examples.
p3421
aVConventional specifications for objectoriented (OO) programs must adhere to behavioral subtyping in support of class inheritance and method overriding. However, this requirement inherently weakens the specifications of overridden methods in superclasses, leading to imprecision during program reasoning. To address this, we advocate a fresh approach to OO verification that focuses on the distinction and relation between specifications that cater to calls with static dispatching from those for calls with dynamic dispatching. We formulate a novel specification subsumption that can avoid code reverification, where possible. Using a predicate mechanism, we propose a flexible scheme for supporting class invariant and lossless casting. Our aim is to lay the foundation for a practical verification system that is precise, concise and modular for sequential OO programs. We exploit the separation logic formalism to achieve this.
p3422
aVWe propose a novel approach to proving the termination of heapmanipulating programs, which combines separation logic with cyclic proof within a Hoarestyle proof system.Judgements in this system express (guaranteed) termination of the program when started from a given line in the program and in a state satisfying a given precondition, which is expressed as a formula of separation logic. The proof rules of our system are of two types: logical rules that operate on preconditions; and symbolic execution rules that capture the effect of executing program commands. Our logical preconditions employ inductively defined predicates to describe heap properties, and proofs in our system are cyclic proofs: cyclic derivations in which some inductive predicate is unfolded infinitely often along every infinite path, thus allowing us to discard all infinite paths in the proof by an infinite descent argument. Moreover, the use of this soundness condition enables us to avoid the explicit construction and use of ranking functions for termination. We also give a completeness result for our system, which is relative in that it relies upon completeness of a proof system for logical implications in separation logic. We give examples illustrating our approach, including one example for which thecorresponding ranking function is nonobvious: termination of the classical algorithm for inplace reversal of a (possibly cyclic) linked list.
p3423
aVBuilding concurrent sharedmemory data structures is a notoriously difficult problem, and so the widespread move to multicore and multiprocessor hardware has led to increasing interest in language constructs that may make concurrent programming easier. One technique that has been studied widely is the use of atomic blocks built over transactional memory (TM): the programmer marks a section of code as atomic, and the language implementation speculatively executes it using transactions. Transactions can run in parallel so long as they access different data. In this talk I'll introduce some of the challenges that I've seen in building robust implementations of this idea. What are the language design choices that exist? What language features can be used inside atomic blocks, and where can atomic blocks occur? Which uses of atomic blocks should be considered correct, and which uses should be considered "racy"? What are the likely impacts of different design choices on performance? What are the impacts on flexibility for the language implementer, and what are the impacts on flexibility to the programmer using these constructs? I'll argue that one way of trying to resolve these questions is to be rigorous about keeping the ideas of atomic blocks and TM separate; in practice they've often been conflated (not least in languages that I've worked on). I'll argue that, when thinking about atomic blocks, we should keep a wide range of possible implementations in mind (for example, TM, lock inference, or simply control over preemption). Similarly, when thinking about TM, we should recognize that it can be exposed to programmers through a wide range of abstractions and language constructs.
p3424
aVAsynchronous or 'eventdriven' programming is a popular technique to efficiently and flexibly manage concurrent interactions. In these programs, the programmer can post tasks that get stored in a task buffer and get executed atomically by a nonpreemptive scheduler at a future point. We give a decision procedure for the fair termination property of asynchronous programs. The fair termination problem asks, given an asynchronous program and a fairness condition on its executions, does the program always terminate on fair executions? The fairness assumptions rule out certain undesired bad behaviors, such as where the scheduler ignores a set of posted tasks forever, or where a nondeterministic branch is always chosen in one direction. Since every liveness property reduces to a fair termination property, our decision procedure extends to liveness properties of asynchronous programs. Our decision procedure for the fair termination of asynchronous programs assumes all variables are finitestate. Even though variables are finitestate, asynchronous programs can have an unbounded stack from recursive calls made by tasks, as well as an unbounded task buffer of pending tasks. We show a reduction from the fair termination problem for asynchronous programs to fair termination problems on Petri Nets, and our main technical result is a reduction of the latter problem to Presburger satisfiability. Our decidability result is in contrast to multithreaded recursive programs, for which liveness properties are undecidable. While we focus on fair termination, we show our reduction to Petri Nets can be used to prove related properties such as fair nonstarvation (every posted task is eventually executed) and safety properties such as boundedness (find a bound on the maximum number of posted tasks that can be in the task buffer at any point).
p3425
aVReasoning about program controlflow paths is an important functionality of a number of recent program matching languages and associated searching and transformation tools. Temporal logic provides a welldefined means of expressing properties of controlflow paths in programs, and indeed an extension of the temporal logic CTL has been applied to the problem of specifying and verifying the transformations commonly performed by optimizing compilers. Nevertheless, in developing the Coccinelle program transformation tool for performing Linux collateral evolutions in systems code, we have found that existing variants of CTL do not adequately support rules that transform subterms other than the ones matching an entire formula. Being able to transform any of the subterms of a matched term seems essential in the domain targeted by Coccinelle.  In this paper, we propose an extension to CTL named CTLVW (CTL with variables and witnesses) that is a suitable basis for the semantics and implementation of the Coccinelles program matching language. Our extension to CTL includes existential quantification over program fragments, which allows metavariables in the program matching language to range over different values within different controlflow paths, and a notion of witnesses that record such existential bindings for use in the subsequent program transformation process. We formalize CTLVW and describe its use in the context of Coccinelle. We then assess the performance of the approach in practice, using a transformation rule that fixes several reference count bugs in Linux code.
p3426
aVThis paper describes an interprocedural technique for computing symbolic bounds on the number of statements a procedure executes in terms of its scalar inputs and userdefined quantitative functions of input datastructures. Such computational complexity bounds for even simple programs are usually disjunctive, nonlinear, and involve numerical properties of heaps. We address the challenges of generating these bounds using two novel ideas. We introduce a proof methodology based on multiple counter instrumentation (each counter can be initialized and incremented at potentially multiple program locations) that allows a given linear invariant generation tool to compute linear bounds individually on these counter variables. The bounds on these counters are then composed together to generate total bounds that are nonlinear and disjunctive. We also give an algorithm for automating this proof methodology. Our algorithm generates complexity bounds that are usually precise not only in terms of the computational complexity, but also in terms of the constant factors. Next, we introduce the notion of userdefined quantitative functions that can be associated with abstract datastructures, e.g., length of a list, height of a tree, etc. We show how to compute bounds in terms of these quantitative functions using a linear invariant generation tool that has support for handling uninterpreted functions. We show application of this methodology to commonly used datastructures (namely lists, list of lists, trees, bitvectors) using examples from Microsoft product code. We observe that a few quantitative functions for each datastructure are usually sufficient to allow generation of symbolic complexity bounds of a variety of loops that iterate over these datastructures, and that it is straightforward to define these quantitative functions. The combination of these techniques enables generation of precise computational complexity bounds for realworld examples (drawn from Microsoft product code and C++ STL library code) for some of which it is nontrivial to even prove termination. Such automatically generated bounds are very useful for early detection of egregious performance problems in large modular codebases that are constantly being changed by multiple developers who make heavy use of code written by others without a good understanding of their implementation complexity.
p3427
aVWe propose a method for automatically generating abstract transformers for static analysis by abstract interpretation. The method focuses on linear constraints on programs operating on rational, real or floatingpoint variables and containing linear assignments and tests. In addition to loopfree code, the same method also applies for obtaining least fixed points as functions of the precondition, which permits the analysis of loops and recursive functions. Our algorithms are based on new quantifier elimination and symbolic manipulation techniques. Given the specification of an abstract domain, and a program block, our method automatically outputs an implementation of the corresponding abstract transformer. It is thus a form of program transformation. The motivation of our work is dataflow synchronous programming languages, used for building controlcommand embedded systems, but it also applies to imperative and functional programming.
p3428
aVLinguists seek to understand the semantics of expressions in human languages. Certainly there are many natural language expressions operators in the wild, so to speak that control evaluation in ways that are familiar from programming languages: just think of the naturallanguage counterparts of if, unless, while, etc. But in general, how wellbehaved are control operators found in the wild? Can we always understand them in terms of familiar programming constructs, or do they go significantly beyond the expressive power of programming languages? As an example where operators from a programming langauge can provide an insightful analysis of a natural language construction, consider the difference in meaning between the following two sentences: (1) a. & John only drinks PERRIER. (emphasis on Perrier) b. John only DRINKS Perrier. (emphasis on drinks) The first sentence entails that John never drinks, say, Evian, but the second sentence entails instead that John never does anything (relevant) with Perrier except drink it. I will suggest that we can understand this difference by expressing the meanings of these sentences in terms of Sitaram's fcontrol and run operators (variants on throw and catch). But not all wild operators are so easily captured. I will discuss in some detail the meaning of the word same in English as it occurs in the following sentence: (2) & John and Bill read the same book. This sentence has a prominent interpretation on which it means (roughly) 'there is some book x such that John read x and Bill read x. I provide a preliminary analysis based on Danvy and Filinski's shift and reset. However, the shift reset approach does not generalize to the full range of related sentences in English. I give a more general solution expressed in a Type Logical Grammar (a certain kind of substructural logic) with explicit continuations. But even this is inadequate: I go on to discuss additional, only slightly less ordinary uses of same that remain untamed by any known compositional semantics.
p3429
aVThe callbyneed lambda calculus provides an equational framework for reasoning syntactically about lazy evaluation. This paper examines its operational characteristics. By a series of reasoning steps, we systematically unpack the standardorder reduction relation of the calculus and discover a novel abstract machine definition which, like the calculus, goes "under lambdas." We prove that machine evaluation is equivalent to standardorder evaluation. Unlike traditional abstract machines, delimited control plays a significant role in the machine's behavior. In particular, the machine replaces the manipulation of a heap using storebased effects with disciplined management of the evaluation stack using controlbased effects. In short, state is replaced with control. To further articulate this observation, we present a simulation of callbyneed in a callbyvalue language using delimited control operations.
p3430
aVA bidirectional transformation consists of a function get that takes a source (document or value) to a view and a function put that takes an updated view and the original source back to an updated source, governed by certain consistency conditions relating the two functions. Both the database and programming language communities have studied techniques that essentially allow a user to specify only one of get and put and have the other inferred automatically. All approaches so far to this bidirectionalization task have been syntactic in nature, either proposing a domainspecific language with limited expressiveness but builtin (and composable) backward components, or restricting get to a simple syntactic form from which some algorithm can synthesize an appropriate definition for put. Here we present a semantic approach instead. The idea is to take a generalpurpose language, Haskell, and write a higherorder function that takes (polymorphic) getfunctions as arguments and returns appropriate putfunctions. All this on the level of semantic values, without being willing, or even able, to inspect the definition of get, and thus liberated from syntactic restraints. Our solution is inspired by relational parametricity and uses free theorems for proving the consistency conditions. It works beautifully.
p3431
aVParallel programs on lists have been intensively studied. It is well known that associativity provides a good characterization for divideandconquer parallel programs. In particular, the third homomorphism theorem is not only useful for systematic development of parallel programs on lists, but it is also suitable for automatic parallelization. The theorem states that if two sequential programs iterate the same list leftward and rightward, respectively, and compute the same value, then there exists a divideandconquer parallel program that computes the same value as the sequential programs. While there have been many studies on lists, few have been done for characterizing and developing of parallel programs on trees. Naive divideandconquer programs, which divide a tree at the root and compute independent subtrees in parallel, take time that is proportional to the height of the input tree and have poor scalability with respect to the number of processors when the input tree is illbalanced. In this paper, we develop a method for systematically constructing scalable divideandconquer parallel programs on trees, in which two sequential programs lead to a scalable divideandconquer parallel program. We focus on paths instead of trees so as to utilize rich results on lists and demonstrate that associativity provides good characterization for scalable divideandconquer parallel programs on trees. Moreover, we generalize the third homomorphism theorem from lists to trees.We demonstrate the effectiveness of our method with various examples. Our results, being generalizations of known results for lists, are generic in the sense that they work well for all polynomial data structures.
p3432
aVSelfadjusting computation is an evaluation model in which programs can respond efficiently to small changes to their input data by using a changepropagation mechanism that updates computation by rebuilding only the parts affected by changes. Previous work has proposed language techniques for selfadjusting computation and showed the approach to be effective in a number of application areas. However, due to the complex semantics of change propagation and the indirect nature of previously proposed language techniques, it remains difficult to reason about the efficiency of selfadjusting programs and change propagation. In this paper, we propose a cost semantics for selfadjusting computation that enables reasoning about its effectiveness. As our source language, we consider a directstyle \u03bbcalculus with firstclass mutable references and develop a notion of trace distance for source programs. To facilitate asymptotic analysis, we propose techniques for composing and generalizing concrete distances via trace contexts (traces with holes). We then show how to translate the source language into a selfadjusting target language such that the translation (1) preserves the extensional semantics of the source programs and the cost of fromscratch runs, and (2) ensures that change propagation between two evaluations takes time bounded by their relative distance. We consider several examples and analyze their effectiveness by considering upper and lower bounds.
p3433
aVPHP is a popular language for serverside applications. In PHP, assignment to variables copies the assigned values, according to its socalled copyonassignment semantics. In contrast, a typical PHP implementation uses a copyonwrite scheme to reduce the copy overhead by delaying copies as much as possible. This leads us to ask if the semantics and implementation of PHP coincide, and actually this is not the case in the presence of sharings within values. In this paper, we describe the copyonassignment semantics with three possible strategies to copy values containing sharings. The current PHP implementation has inconsistencies with these semantics, caused by its nave use of copyonwrite. We fix this problem by the novel mostly copyonwrite scheme, making the copyonwrite implementations faithful to the semantics. We prove that our copyonwrite implementations are correct, using bisimulation with the copyonassignment semantics.
p3434
aVWe present a proof calculus and method for the static verification of assertions and procedure specifications in sharedmemory concurrent programs. The key idea in our approach is to use atomicity as a proof tool and to simplify the verification of assertions by rewriting programs to consist of larger atomic actions. We propose a novel, iterative proof style in which alternating use of abstraction and reduction is exploited to compute larger atomic code blocks in a sound manner. This makes possible the verification of assertions in the transformed program by simple sequential reasoning within atomic blocks, or significantly simplified application of existing concurrent program verification techniques such as the OwickiGries or relyguarantee methods. Our method facilitates a clean separation of concerns where at each phase of the proof, the user worries only about only either the sequential properties or the concurrency control mechanisms in the program. We implemented our method in a tool called QED. We demonstrate the simplicity and effectiveness of our approach on a number of benchmarks including ones with intricate concurrency protocols.
p3435
aVSpeed improvements in today's processors have largely been delivered in the form of multiple cores, increasing the importance of abstractions that ease parallel programming. Software transactional memory (STM) addresses many of the complications of concurrency by providing a simple and composable model for safe access to shared data structures. Software transactions extend a language with an atomic primitive that declares that the effects of a block of code should not be interleaved with actions executing concurrently on other threads. Adding barriers to shared memory accesses provides atomicity, consistency and isolation. Strongly isolated STMs preserve the safety properties of transactions for all memory operations in a program, not just those inside an atomic block. Isolation barriers are added to nontransactional loads and stores in such a system to prevent those accesses from observing or corrupting a partially completed transaction. Strong isolation is especially important when integrating transactions into an existing language and memory model. Isolation barriers have a prohibitive performance overhead, however, so most STM proposals have chosen not to provide strong isolation. In this paper we reduce the costs of strong isolation by customizing isolation barriers for their observed usage. The customized barriers provide accelerated execution by blocking threads whose accesses do not follow the expected pattern. We use hot swap to tighten or loosen the hypothesized pattern, while preserving strong isolation. We introduce a family of optimization hypotheses that balance verification cost against generality. We demonstrate the feasibility of dynamic barrier optimization by implementing it in a bytecoderewriting Java STM. Feedbackdirected customization reduces the overhead of strong isolation from 505% to 38% across 11 nontransactional benchmarks; persistent feedback data further reduces the overhead to 16%. Dynamic optimization accelerates a multithreaded transactional benchmark by 31% for weaklyisolated execution and 34% for stronglyisolated execution.
p3436
aVPointer analysis is a prerequisite for many program analyses, and the effectiveness of these analyses depends on the precision of the pointer information they receive. Two major axes of pointer analysis precision are flowsensitivity and contextsensitivity, and while there has been significant recent progress regarding scalable contextsensitive pointer analysis, relatively little progress has been made in improving the scalability of flowsensitive pointer analysis. This paper presents a new interprocedural, flowsensitive pointer analysis algorithm that combines two ideassemisparse analysis and a novel use of BDDsthat arise from a careful understanding of the unique challenges that face flowsensitive pointer analysis. We evaluate our algorithm on 12 C benchmarks ranging from 11K to 474K lines of code. Our fastest algorithm is on average 197x faster and uses 4.6x less memory than the state of the art, and it can analyze programs that are an order of magnitude larger than the previous state of the art.
p3437
aVWe describe an abstract interpretation based framework for proving relationships between sizes of memory partitions. Instances of this framework can prove traditional properties such as memory safety and program termination but can also establish upper bounds on usage of dynamically allocated memory. Our framework also stands out in its ability to prove properties of programs manipulating both heap and arrays which is considered a difficult task. Technically, we define an abstract domain that is parameterized by an abstract domain for tracking memory partitions (sets of memory locations) and by a numerical abstract domain for tracking relationships between cardinalities of the partitions. We describe algorithms to construct the transfer functions for the abstract domain in terms of the corresponding transfer functions of the parameterized abstract domains. A prototype of the framework was implemented and used to prove interesting properties of realistic programs, including programs that could not have been automatically analyzed before.
p3438
aVDeadlock in multithreaded programs is an increasingly important problem as ubiquitous multicore architectures force parallelization upon an ever wider range of software. This paper presents a theoretical foundation for dynamic deadlock avoidance in concurrent programs that employ conventional mutual exclusion and synchronization primitives (e.g., multithreaded C/Pthreads programs). Beginning with control flow graphs extracted from program source code, we construct a formal model of the program and then apply Discrete Control Theory to automatically synthesize deadlockavoidance control logic that is implemented by program instrumentation. At run time, the control logic avoids deadlocks by postponing lock acquisitions. Discrete Control Theory guarantees that the program instrumented with our synthesized control logic cannot deadlock. Our method furthermore guarantees that the control logic is maximally permissive: it postpones lock acquisitions only when necessary to prevent deadlocks, and therefore permits maximal runtime concurrency. Our prototype for C/Pthreads scales to real software including Apache, OpenLDAP, and two kinds of benchmarks, automatically avoiding both injected and naturally occurring deadlocks while imposing modest runtime overheads.
p3439
aVOptimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.
p3440
aVPrevious deforestation and supercompilation algorithms may introduce accidental termination when applied to callbyvalue programs. This hides looping bugs from the programmer, and changes the behavior of a program depending on whether it is optimized or not. We present a supercompilation algorithm for a higherorder callbyvalue language and we prove that the algorithm both terminates and preserves termination properties. This algorithm utilizes strictness information for deciding whether to substitute or not and compares favorably with previous callbyname transformations.
p3441
aVThis paper describes a compositional shape analysis, where each procedure is analyzed independently of its callers. The analysis uses an abstract domain based on a restricted fragment of separation logic, and assigns a collection of Hoare triples to each procedure; the triples provide an overapproximation of data structure usage. Compositionality brings its usual benefits   increased potential to scale, ability to deal with unknown calling contexts, graceful way to deal with imprecision   to shape analysis, for the first time. The analysis rests on a generalized form of abduction (inference of explanatory hypotheses) which we call biabduction. Biabduction displays abduction as a kind of inverse to the frame problem: it jointly infers antiframes (missing portions of state) and frames (portions of state not touched by an operation), and is the basis of a new interprocedural analysis algorithm. We have implemented our analysis algorithm and we report case studies on smaller programs to evaluate the quality of discovered specifications, and larger programs (e.g., an entire Linux distribution) to test scalability and graceful imprecision.
p3442
aVI shall present an extension of Moggi's computational metalanguage with primitives from linear logic, the enriched effectcalculus. Illustrative applications to side effects, continuations, nondeterminism and polymorphism will be considered. The talk is based on joint work with Jeff Egger and Rasmus Mogelberg.
p3443
aVWe present a unified approach to type checking and property checking for lowlevel code. Type checking for lowlevel code is challenging because type safety often depends on complex, programspecific invariants that are difficult for traditional type checkers to express. Conversely, property checking for lowlevel code is challenging because it is difficult to write concise specifications that distinguish between locations in an untyped program's heap. We address both problems simultaneously by implementing a type checker for lowlevel code as part of our property checker. We present a lowlevel formalization of a C program's heap and its types that can be checked with an SMT solver, and we provide a decision procedure for checking type safety. Our type system is flexible enough to support a combination of nominal and structural subtyping for C, on a perstructure basis. We discuss several case studies that demonstrate the ability of this tool to express and check complex type invariants in lowlevel C code, including several small Windows device drivers.
p3444
aVRelyGuarantee reasoning is a wellknown method for verification of sharedvariable concurrent programs. However, it is difficult for users to define rely/guarantee conditions, which specify threads' behaviors over the whole program state. Recent efforts to combine Separation Logic with RelyGuarantee reasoning have made it possible to hide threadlocal resources, but the shared resources still need to be globally known and specified. This greatly limits the reuse of verified program modules. In this paper, we propose LRG, a new RelyGuaranteebased logic that brings local reasoning and information hiding to concurrency verification. Our logic, for the first time, supports a frame rule over rely/guarantee conditions so that specifications of program modules only need to talk about the resources used locally, and the verified modules can be reused in different threads without redoing the proof. Moreover, we introduce a new hiding rule to hide the resources shared by a subset of threads from the rest in the system. The support of information hiding not only improves the modularity of RelyGuarantee reasoning, but also enables the sharing of dynamically allocated resources, which requires adjustment of rely/guarantee conditions.
p3445
aVA concurrent datastructure implementation is considered nonblocking if it meets one of three following liveness criteria: waitfreedom, lockfreedom, or obstructionfreedom. Developers of nonblocking algorithms aim to meet these criteria. However, to date their proofs for nontrivial algorithms have been only manual pencilandpaper semiformal proofs. This paper proposes the first fully automatic tool that allows developers to ensure that their algorithms are indeed nonblocking. Our tool uses relyguarantee reasoning while overcoming the technical challenge of sound reasoning in the presence of interdependent liveness properties.
p3446
aVWe show how to extend O'Hearn and Pym's logic of bunched implications, BI, to classical BI (CBI), in which both the additive and the multiplicative connectives behave classically. Specifically, CBI is a nonconservative extension of (propositional) Boolean BI that includes multiplicative versions of falsity, negation and disjunction. We give an algebraic semantics for CBI that leads us naturally to consider resource models of CBI in which every resource has a unique dual. We then give a cuteliminating proof system for CBI, based on Belnap's display logic, and demonstrate soundness and completeness of this proof system with respect to our semantics.
p3447
aVMitchell's notion of representation independence is a particularly useful application of Reynolds' relational parametricity   two different implementations of an abstract data type can be shown contextually equivalent so long as there exists a relation between their type representations that is preserved by their operations. There have been a number of methods proposed for proving representation independence in various pure extensions of System F (where data abstraction is achieved through existential typing), as well as in Algol or Javalike languages (where data abstraction is achieved through the use of local mutable state). However, none of these approaches addresses the interaction of existential type abstraction and local state. In particular, none allows one to prove representation independence results for generative ADTs   i.e. ADTs that both maintain some local state and define abstract types whose internal representations are dependent on that local state. In this paper, we present a syntactic, logicalrelationsbased method for proving representation independence of generative ADTs in a language supporting polymorphic types, existential types, general recursive types, and unrestricted MLstyle mutable references. We demonstrate the effectiveness of our method by using it to prove several interesting contextual equivalences that involve a close interaction between existential typing and local state, as well as some wellknown equivalences from the literature (such as Pitts and Stark's "awkward" example) that have caused trouble for previous logicalrelationsbased methods. The success of our method relies on two key technical innovations. First, in order to handle generative ADTs, we develop a possibleworlds model in which relational interpretations of types are allowed to grow over time in a manner that is tightly coupled with changes to some local state. Second, we employ a stepindexed stratification of possible worlds, which facilitates a simplified account of mutable references of higher type.
p3448
aVWe propose Fzip, a calculus of open existential types that is an extension of System F obtained by decomposing the introduction and elimination of existential types into more atomic constructs. Open existential types model modular type abstraction as done in module systems. The static semantics of Fzip adapts standard techniques to deal with linearity of typing contexts, its dynamic semantics is a smallstep reduction semantics that performs extrusion of type abstraction as needed during reduction, and the two are related by subject reduction and progress lemmas. Applying the CurryHoward isomorphism, Fzip can be also read back as a logic with the same expressive power as secondorder logic but with more modular ways of assembling partial proofs. We also extend the core calculus to handle the double vision problem as well as typelevel and termlevel recursion. The resulting language turns out to be a new formalization of (a minor variant of) Dreyer's internal language for recursive and mixin modules.
p3449
aVIn this paper, we show how pattern matching can be seen to arise from a proof term assignment for the focused sequent calculus. This use of the CurryHoward correspondence allows us to give a novel coverage checking algorithm, and makes it possible to give a rigorous correctness proof for the classical pattern compilation strategy of building decision trees via matrices of patterns.
p3450
aVMultiprocessors are now dominant, but real multiprocessors do not provide the sequentially consistent memory that is assumed by most work on semantics and verification. Instead, they have subtle relaxed (or weak) memory models, usually described only in ambiguous prose, leading to widespread confusion. We develop a rigorous and accurate semantics for x86 multiprocessor programs, from instruction decoding to relaxed memory model, mechanised in HOL. We test the semantics against actual processors and the vendor litmustest examples, and give an equivalent abstractmachine characterisation of our axiomatic memory model. For programs that are (in some precise sense) datarace free, we prove in HOL that their behaviour is sequentially consistent. We also contrast the x86 model with some aspects of Power and ARM behaviour. This provides a solid intuition for lowlevel programming, and a sound foundation for future work on verification, static analysis, and compilation of lowlevel concurrent code.
p3451
aVMemory models define an interface between programs written in some language and their implementation, determining which behaviour the memory (and thus a program) is allowed to have in a given model. A minimal guarantee memory models should provide to the programmer is that wellsynchronized, that is, datarace free code has a standard semantics. Traditionally, memory models are defined axiomatically, setting constraints on the order in which memory operations are allowed to occur, and the programming language semantics is implicit as determining some of these constraints. In this work we propose a new approach to formalizing a memory model in which the model itself is part of a weak operational semantics for a (possibly concurrent) programming language. We formalize in this way a model that allows write operations to the store to be buffered. This enables us to derive the ordering constraints from the weak semantics of programs, and to prove, at the programming language level, that the weak semantics implements the usual interleaving semantics for datarace free programs, hence in particular that it implements the usual semantics for sequential code.
p3452
aVTransactional memory (TM) is a promising paradigm for concurrent programming. Whereas the number of TM implementations is growing, however, little research has been conducted to precisely define TM semantics, especially their progress guarantees. This paper is the first to formally define the progress semantics of lockbased TMs, which are considered the most effective in practice. We use our semantics to reduce the problems of reasoning about the correctness and computability power of lockbased TMs to those of simple trylock objects. More specifically, we prove that checking the progress of any set of transactions accessing an arbitrarily large set of shared variables can be reduced to verifying a simple property of each individual (logical) trylock used by those transactions. We use this theorem to determine the correctness of stateoftheart lockbased TMs and highlight various configuration ambiguities. We also prove that lockbased TMs have consensus number 2. This means that, on the one hand, a lockbased TM cannot be implemented using only readwrite memory, but, on the other hand, it does not need very powerful instructions such as the commonly used compareandswap. We finally use our semantics to formally capture an inherent tradeoff in the performance of lockbased TM implementations. Namely, we show that the space complexity of every lockbased software TM implementation that uses invisible reads is at least exponential in the number of objects accessible to transactions.
p3453
aVWe propose a new verification method for temporal properties of higherorder functional programs, which takes advantage of Ong's recent result on the decidability of the modelchecking problem for higherorder recursion schemes (HORS's). A program is transformed to an HORS that generates a tree representing all the possible event sequences of the program, and then the HORS is modelchecked. Unlike most of the previous methods for verification of higherorder programs, our verification method is sound and complete. Moreover, this new verification framework allows a smooth integration of abstract model checking techniques into verification of higherorder programs. We also present a typebased verification algorithm for HORS's. The algorithm can deal with only a fragment of the properties expressed by modal mucalculus, but the algorithm and its correctness proof are (arguably) much simpler than those of Ong's gamesemanticsbased algorithm. Moreover, while the HORS model checking problem is nEXPTIME in general, our algorithm is linear in the size of HORS, under the assumption that the sizes of types and specification formulas are bounded by a constant.
p3454
aVThis work develops an integrated approach to the verification of behaviourally rich programs, founded directly on operational semantics. The power of the approach is demonstrated with a stateoftheart verification of a core piece of distributed infrastructure, involving networking, a filesystem, and concurrent OCaml code. The formalization is in higherorder logic and proof support is provided by the HOL4 theorem prover. Difficult verification problems demand a wide range of techniques. Here these include ground and symbolic evaluation, local reasoning, separation, invariants, Hoarestyle assertional reasoning, rely/guarantee, inductive reasoning about protocol correctness, multiple refinement, and linearizability. While each of these techniques is useful in isolation, they are even more so in combination. The first contribution of this paper is to present the operational approach and describe how existing techniques, including all those mentioned above, may be cleanly and precisely integrated in this setting. The second contribution is to show how to combine verifications of individual library functions with arbitrary and unknown user code in a compositional manner, focusing on the problems of private state and encapsulation. The third contribution is the example verification itself. The infrastructure must behave correctly under arbitrary patterns of host and network failure, whilst for performance reasons the code also includes data races on shared state. Both features make the verification particularly challenging.
p3455
aVWe develop a model of concurrent imperative programming with threads. We focus on a small imperative language with cooperative threads which execute without interruption until they terminate or explicitly yield control. We define and study a tracebased denotational semantics for this language; this semantics is fully abstract but mathematically elementary. We also give an equational theory for the computational effects that underlie the language, including thread spawning. We then analyze threads in terms of the free algebra monad for this theory.
p3456
aVProgram errors are hard to detect and are costly both to programmers who spend significant efforts in debugging, and for systems that are guarded by runtime checks. Static verification techniques have been applied to imperative and objectoriented languages, like Java and C#, but few have been applied to a higherorder lazy functional language, like Haskell. In this paper, we describe a sound and automatic static verification framework for Haskell, that is based on contracts and symbolic execution. Our approach is modular and gives precise blame assignments at compiletime in the presence of higherorder functions and laziness.
p3457
aVThis paper presents a typebased solution to the longstanding problem of object initialization. Constructors, the conventional mechanism for object initialization, have semantics that are surprising to programmers and that lead to bugs. They also contribute to the problem of nullpointer exceptions, which make software less reliable. Masked types are a new typestate mechanism that explicitly tracks the initialization state of objects and prevents reading from uninitialized fields. In the resulting language, constructors are ordinary methods that operate on uninitialized objects, and no special default value (null) is needed in the language. Initialization of cyclic data structures is achieved with the use of conditionally masked types. Masked types are modular and compatible with data abstraction. The type system is presented in a simplified object calculus and is proved to soundly prevent reading from uninitialized fields. Masked types have been implemented as an extension to Java, in which compilation simply erases extra type information. Experience using the extended language suggests that masked types work well on real code.
p3458
aVWe present HML, a type inference system that supports full firstclass polymorphism where few annotations are needed: only function parameters with a polymorphic type need to be annotated. HML is a simplification of MLF where only flexibly quantified types are used. This makes the types easier to work with from a programmers perspective, and simplifies the implementation of the type inference algorithm. Still, HML retains much of the expressiveness of MLF, it is robust with respect to small program transformations, and has a simple specification of the type rules with an effective type inference algorithm that infers principal types. A small reference implementation with many examples is available at: http://research.microsoft.com/users/daan/pubs.html.
p3459
aVWe study modular, automatic code generation from hierarchical block diagrams with synchronous semantics. Such diagrams are the fundamental model behind widespread tools in the embedded software domain, such as Simulink and SCADE. Code is modular in the sense that it is generated for a given composite block independently from context (i.e., without knowing in which diagrams the block is to be used) and using minimal information about the internals of the block. In previous work, we have shown how modular code can be generated by computing a set of interface functions for each block and a set of dependencies between these functions that is exported along with the interface. We have also introduced a quantified notion of modularity in terms of the number of interface functions generated per block, and showed how to minimize this number, which is essential for scalability. Finally, we have exposed the fundamental tradeoff between modularity and reusability (set of diagrams the block can be used in). In this paper we explore another tradeoff: modularity vs. code size. We show that our previous technique, although it achieves maximal reusability and is optimal in terms of modularity, may result in code replication and therefore large code sizes, something often unacceptable in an embedded system context. We propose to remedy this by generating code with no replication, and show that this generally results in some loss of modularity. We show that optimizing modularity while maintaining maximal reusability and zero replication is an intractable problem (NPcomplete). We also show that this problem can be solved using a simple iterative procedure that checks satisfiability of a sequence of propositional formulas. We report on a new prototype implementation and experimental results. The latter demonstrate the practical interest in our methods.
p3460
aVAs cryptographic proofs have become essentially unverifiable, cryptographers have argued in favor of developing techniques that help tame the complexity of their proofs. Gamebased techniques provide a popular approach in which proofs are structured as sequences of games and in which proof steps establish the validity of transitions between successive games. Codebased techniques form an instance of this approach that takes a codecentric view of games, and that relies on programming language theory to justify proof steps. While codebased techniques contribute to formalize the security statements precisely and to carry out proofs systematically, typical proofs are so long and involved that formal verification is necessary to achieve a high degree of confidence. We present Certicrypt, a framework that enables the machinechecked construction and verification of codebased proofs. Certicrypt is built upon the generalpurpose proof assistant Coq, and draws on many areas, including probability, complexity, algebra, and semantics of programming languages. Certicrypt provides certified tools to reason about the equivalence of probabilistic programs, including a relational Hoare logic, a theory of observational equivalence, verified program transformations, and gamebased techniques such as reasoning about failure events. The usefulness of Certicrypt is demonstrated through various examples, including a proof of semantic security of OAEP (with a bound that improves upon existing published results), and a proof of existential unforgeability of FDH signatures. Our work provides a first yet significant step towards Halevi's ambitious programme of providing tool support for cryptographic proofs.
p3461
aVComputer science has served to insulate programs and programmers from knowledge of the underlying mechanisms used to manipulate information, however this fiction is increasingly hard to maintain as computing devices decrease in size and systems increase in complexity. Manifestations of these limits appearing in computers include scaling issues in interconnect, dissipation, and coding. Reconfigurable Asynchronous Logic Automata (RALA) is an alternative formulation of computation that seeks to align logical and physical descriptions by exposing rather than hiding this underlying reality. Instead of physical units being represented in computer programs only as abstract symbols, RALA is based on a lattice of cells that asynchronously pass state tokens corresponding to physical resources. We introduce the design of RALA, review its relationships to its many progenitors, and discuss its benefits, implementation, programming, and extensions
p3462
aVThis paper presents a method for creating formally correct justintime (JIT) compilers. The tractability of our approach is demonstrated through, what we believe is the first, verification of a JIT compiler with respect to a realistic semantics of selfmodifying x86 machine code. Our semantics includes a model of the instruction cache. Two versions of the verified JIT compiler are presented: one generates all of the machine code at once, the other one is incremental i.e. produces code ondemand. All proofs have been performed inside the HOL4 theorem prover.
p3463
aVMotivated by recent research in abstract model checking, we present a new approach to inferring dependent types. Unlike many of the existing approaches, our approach does not rely on programmers to supply the candidate (or the correct) types for the recursive functions and instead does counterexampleguided refinement to automatically generate the set of candidate dependent types. The main idea is to extend the classical fixedpoint type inference routine to return a counterexample if the program is found untypable with the current set of candidate types. Then, an interpolating theorem prover is used to validate the counterexample as a real type error or generate additional candidate dependent types to refute the spurious counterexample. The process is repeated until either a real type error is found or sufficient candidates are generated to prove the program typable. Our system makes nontrivial use of "linear" intersection types in the refinement phase. The paper presents the type inference system and reports on the experience with a prototype implementation that infers dependent types for a subset of the Ocaml language. The implementation infers dependent types containing predicates from the quantifierfree theory of linear arithmetic and equality with uninterpreted function symbols.
p3464
aVWe present LowLevel Liquid Types , a refinement type system for C based on Liquid Types . LowLevel Liquid Types combine refinement types with three key elements to automate verification of critical safety properties of lowlevel programs: First, by associating refinement types with individual heap locations and precisely tracking the locations referenced by pointers, our system is able to reason about complex invariants of inmemory data structures and sophisticated uses of pointer arithmetic. Second, by adding constructs which allow strong updates to the types of heap locations, even in the presence of aliasing, our system is able to verify properties of inmemory data structures in spite of temporary invariant violations. By using this strong update mechanism, our system is able to verify the correct initialization of newlyallocated regions of memory. Third, by using the abstract interpretation framework of Liquid Types, we are able to use refinement type inference to automatically verify important safety properties without imposing an onerous annotation burden. We have implemented our approach in CSOLVE, a tool for LowLevel Liquid Type inference for C programs. We demonstrate through several examples that CSOLVE is able to precisely infer complex invariants required to verify important safety properties, like the absence of array bounds violations and nulldereferences, with a minimal annotation overhead.
p3465
aVType inference for Datalog can be understood as the problem of mapping programs to a sublanguage for which containment is decidable. To wit, given a program in Datalog, a schema describing the types of extensional relations, and a usersupplied set of facts about the basic types (stating conditions such as disjointness, implication or equivalence), we aim to infer an overapproximation of the semantics of the program, which should be expressible in a suitable sublanguage of Datalog. We argue that Datalog with monadic extensionals is an appropriate choice for that sublanguage of types, and we present an inference algorithm. The inference algorithm is proved sound, and we also show that it infers the tightest possible overapproximation for a large class of Datalog programs. Furthermore, we present a practical containment check for a large subset of our type language. The crux of that containment check is a novel generalisation of Quine's procedure for computing prime implicants. The type system has been implemented in a stateoftheart industrial database system, and we report on experiments with this implementation.
p3466
aVClassical formalizations of systems and properties are boolean: given a system and a property, the property is either true or false of the system. Correspondingly, classical methods for system analysis determine the truth value of a property, preferably giving a proof if the property is true, and a counterexample if the property is false; classical methods for system synthesis construct a system for which a property is true; classical methods for system transformation, composition, and abstraction aim to preserve the truth of properties. The boolean view is prevalent even if the system, the property, or both refer to numerical quantities, such as the times or probabilities of events. For example, a timed automaton either satisfies or violates a formula of a realtime logic; a stochastic process either satisfies or violates a formula of a probabilistic logic. The classical blackandwhite view partitions the world into "correct" and "incorrect" systems, offering few nuances. In reality, of several systems that satisfy a property in the boolean sense, often some are more desirable than others, and of the many systems that violate a property, usually some are less objectionable than others. For instance, among the systems that satisfy the response property that every request be granted, we may prefer systems that grant requests quickly (the quicker, the better), or we may prefer systems that issue few unnecessary grants (the fewer, the better); and among the systems that violate the response property, we may prefer systems that serve many initial requests (the more, the better), or we may prefer systems that serve many requests in the long run (the greater the fraction of served to unserved requests, the better). Formally, while a boolean notion of correctness is given by a preorder on systems and properties, a quantitative notion of correctness is defined by a directed metric on systems and properties, where the distance between a system and a property provides a measure of "fit" or "desirability." There are many ways how such distances can be defined. In a lineartime framework, one assigns numerical values to individual behaviors before assigning values to systems and properties, which are sets of behaviors. For example, the value of a single behavior may be a discounted value, which is largely determined by a prefix of the behavior, e.g., by the number of requests that are granted before the first request that is not granted; or a limit value, which is independent of any finite prefix. A limit value may be an average, such as the average response time over an infinite sequence of requests and grants, or a supremum, such as the worstcase response time. Similarly, the value of a set of behaviors may be an extremum or an average across the values of all behaviors in the set: in this way one can measure the worst of all possible averagecase response times, or the average of all possible worstcase response times, etc. Accordingly, the distance between two sets of behaviors may be defined as the worst or average difference between the values of corresponding behaviors. In summary, we propagate replacing boolean specifications for the correctness of systems with quantitative measures for the desirability of systems. In quantitative analysis, the aim is to compute the distance between a system and a property (or between two systems, or two properties); in quantitative synthesis, the objective is to construct a system that has minimal distance from a given property. Multiple quantitative measures can be prioritized (e.g., combined lexicographically into a single measure) or studied along the Pareto curve. Quantitative transformations, compositions, and abstractions of systems are useful if they allow us to bound the induced change in distance from a property. We present some initial results in some of these directions. We also give some potential applications, which not only generalize tradiditional correctness concerns in the functional, timed, and probabilistic domains, but also capture such system measures as resource use, performance, cost, reliability, and robustness.
p3467
aVThis paper introduces a new recursion principle for inductive data modulo \u03b1equivalence of bound names. It makes use of Oderskystyle local names when recursing over bound names. It is formulated in an extension of Gdel's System T with names that can be tested for equality, explicitly swapped in expressions and restricted to a lexical scope. The new recursion principle is motivated by the nominal sets notion of "\u03b1structural recursion", whose use of names and associated freshness sideconditions in recursive definitions formalizes common practice with binders. The new Nominal System T  presented here provides a calculus of total functions that is shown to adequately represent \u03b1structural recursion while avoiding the need to verify freshness sideconditions in definitions and computations. Adequacy is proved via a normalizationbyevaluation argument that makes use of a new semantics of local names in GabbayPitts nominal sets.
p3468
aVBuilding semantic models that account for various kinds of indirect reference has traditionally been a difficult problem. Indirect reference can appear in many guises, such as heap pointers, higherorder functions, object references, and sharedmemory mutexes. We give a general method to construct models containing indirect reference by presenting a "theory of indirection". Our method can be applied in a wide variety of settings and uses only simple, elementary mathematics. In addition to various forms of indirect reference, the resulting models support powerful features such as impredicative quantification and equirecursion; moreover they are compatible with the kind of powerful substructural accounting required to model (higherorder) separation logic. In contrast to previous work, our model is easy to apply to new settings and has a simple axiomatization, which is complete in the sense that all models of it are isomorphic. Our proofs are machinechecked in Coq.
p3469
aVThe method of logical relations is a classic technique for proving the equivalence of higherorder programs that implement the same observable behavior but employ different internal data representations. Although it was originally studied for pure, strongly normalizing languages like System F, it has been extended over the past two decades to reason about increasingly realistic languages. In particular, Appel and McAllester's idea of stepindexing has been used recently to develop syntactic Kripke logical relations for MLlike languages that mix functional and imperative forms of data abstraction. However, while stepindexed models are powerful tools, reasoning with them directly is quite painful, as one is forced to engage in tedious stepindex arithmetic to derive even simple results. In this paper, we propose a logic LADR for equational reasoning about higherorder programs in the presence of existential type abstraction, general recursive types, and higherorder mutable state. LADR exhibits a novel synthesis of features from PlotkinAbadi logic, GdelLb logic, S4 modal logic, and relational separation logic. Our model of LADR is based on Ahmed, Dreyer, and Rossberg's stateoftheart stepindexed Kripke logical relation, which was designed to facilitate proofs of representation independence for "statedependent" ADTs. LADR enables one to express such proofs at a much higher level, without counting steps or reasoning about the subtle, stepstratified construction of possible worlds.
p3470
aVWe describe a family of decision procedures that extend the decision procedure for quantifierfree constraints on recursive algebraic data types (term algebras) to support recursive abstraction functions. Our abstraction functions are catamorphisms (term algebra homomorphisms) mapping algebraic data type values into values in other decidable theories (e.g. sets, multisets, lists, integers, booleans). Each instance of our decision procedure family is sound; we identify a widely applicable manytoone condition on abstraction functions that implies the completeness. Complete instances of our decision procedure include the following correctness statements: 1) a functional data structure implementation satisfies a recursively specified invariant, 2) such data structure conforms to a contract given in terms of sets, multisets, lists, sizes, or heights, 3) a transformation of a formula (or lambda term) abstract syntax tree changes the set of free variables in the specified way.
p3471
aVWe present a logic for relating heapmanipulating programs to numeric abstractions. These numeric abstractions are expressed as simple imperative programs over integer variables and have the property that termination and safety of the numeric program ensures termination and safety of the original, heapmanipulating program. We have implemented an automated version of this abstraction process and present experimental results for programs involving a variety of data structures.
p3472
aVWe address the verification problem of finitestate concurrent programs running under weak memory models. These models capture the reordering of program (read and write) operations done by modern multiprocessor architectures for performance. The verification problem we study is crucial for the correctness of concurrency libraries and other performancecritical system services employing lockfree synchronization, as well as for the correctness of compiler backends that generate code targeted to run on such architectures. We consider in this paper combinations of three wellknown program order relaxations. We consider first the "write to read" relaxation, which corresponds to the TSO (Total Store Ordering) model. This relaxation is used in most hardware architectures available today. Then, we consider models obtained by adding either (1) the "write to write" relaxation, leading to a model which is essentially PSO (Partial Store Ordering), or (2) the "read to read/write" relaxation, or (3) both of them, as it is done in the RMO (Relaxed Memory Ordering) model for instance. We define abstract operational models for these weak memory models based on state machines with (potentially unbounded) FIFO buffers, and we investigate the decidability of their reachability and their repeated reachability problems. We prove that the reachability problem is decidable for the TSO model, as well as for its extension with "write to write" relaxation (PSO). Furthermore, we prove that the reachability problem becomes undecidable when the "read to read/write" relaxation is added to either of these two memory models, and we give a condition under which this addition preserves the decidability of the reachability problem. We show also that the repeated reachability problem is undecidable for all the considered memory models.
p3473
aVWe describe a new automatic  static analysis for determining upperbound functions on the use of quantitative resources for strict, higherorder, polymorphic, recursive programs dealing with possiblyaliased data. Our analysis is a variant of Tarjan's manual amortised cost analysis  technique. We use a typebased approach, exploiting linearity to allow inference, and place a new emphasis on the number of references to a data object. The bounds we infer depend on the sizes of the various inputs to a program. They thus expose the impact of specific inputs on the overall cost behaviour. The key novel aspect of our work is that it deals directly with polymorphic higherorder functions without requiring sourcelevel transformations that could alter resource usage . We thus obtain safe  and accurate  compiletime bounds. Our work is generic  in that it deals with a variety of quantitative resources. We illustrate our approach with reference to dynamic memory allocations/deallocations, stack usage, and worstcase execution time, using metrics taken from a real implementation on a simple microcontroller platform that is used in safetycritical automotive applications.
p3474
aVWe report on our experience implementing a lightweight, fully verified relational database management system (RDBMS). The functional specification of RDBMS behavior, RDBMS implementation, and proof that the implementation meets the specification are all written and verified in Coq. Our contributions include: (1) a complete specification of the relational algebra in Coq; (2) an efficient realization of that model (B+ trees) implemented with the Ynot extension to Coq; and (3) a set of simple query optimizations proven to respect both semantics and runtime cost. In addition to describing the design and implementation of these artifacts, we highlight the challenges we encountered formalizing them, including the choice of representation for finite relations of typed tuples and the challenges of reasoning about data structures with complex sharing. Our experience shows that though many challenges remain, building fullyverified systems software in Coq is within reach.
p3475
aVThe automated inference of quantified invariants is considered one of the next challenges in software verification. The question of the right precisionefficiency tradeoff for the corresponding program analyses here boils down to the question of the right treatment of disjunction below and above the universal quantifier. In the closely related setting of shape analysis one uses the focus operator in order to adapt the treatment of disjunction (and thus the efficiencyprecision tradeoff) to the individual program statement. One promising research direction is to design parameterized versions of the focus operator which allow the user to finetune the focus operator not only to the individual program statements but also to the specific verification task. We carry this research direction one step further. We finetune the focus operator to each individual step of the analysis (for a specific verification task). This finetuning must be done automatically. Our idea is to use counterexamples for this purpose. We realize this idea in a tool that automatically infers quantified invariants for the verification of a variety of heapmanipulating programs.
p3476
aVMost systems based on separation logic consider only restricted forms of implication or nonseparating conjunction, as full support for these connectives requires a nontrivial notion of variable context, inherited from the logic of bunched implications (BI). We show that in an expressive type theory such as Coq, one can avoid the intricacies of BI, and support full separation logic very efficiently, using the native structuring primitives of the type theory. Our proposal uses reflection to enable equational reasoning about heaps, and Hoare triples with binary postconditions to further facilitate it. We apply these ideas to Hoare Type Theory, to obtain a new proof technique for verification of higherorder imperative programs that is general, extendable, and supports very short proofs, even without significant use of automation by tactics. We demonstrate the usability of the technique by verifying the fast congruence closure algorithm of Nieuwenhuis and Oliveras, employed in the stateoftheart Barcelogic SAT solver.
p3477
aVThe definition of type equivalence is one of the most important design issues for any typed language. In dependently typed languages, because terms appear in types, this definition must rely on a definition of term equivalence. In that case, decidability of type checking requires decidability for the term equivalence relation. Almost all dependentlytyped languages require this relation to be decidable. Some, such as Coq, Epigram or Agda, do so by employing analyses to force all programs to terminate. Conversely, others, such as DML, ATS, \u03a9mega, or Haskell, allow nonterminating computation, but do not allow those terms to appear in types. Instead, they identify a terminating index language and use singleton types to connect indices to computation. In both cases, decidable type checking comes at a cost, in terms of complexity and expressiveness. Conversely, the benefits to be gained by decidable type checking are modest. Termination analyses allow dependently typed programs to verify total correctness properties. However, decidable type checking is not a prerequisite for type safety. Furthermore, decidability does not imply tractability. A decidable approximation of program equivalence may not be useful in practice. Therefore, we take a different approach: instead of a fixed notion for term equivalence, we parameterize our type system with an abstract relation that is not necessarily decidable. We then design a novel set of typing rules that require only weak properties of this abstract relation in the proof of the preservation and progress lemmas. This design provides flexibility: we compare valid instantiations of term equivalence which range from betaequivalence, to contextual equivalence, to some exotic equivalences.
p3478
aVThis paper introduces a new approach to type theory called pure subtype systems . Pure subtype systems differ from traditional approaches to type theory (such as pure type systems) because the theory is based on subtyping, rather than typing. Proper types and typing are completely absent from the theory; the subtype relation is defined directly over objects. The traditional typing relation is shown to be a special case of subtyping, so the loss of types comes without any loss of generality. Pure subtype systems provide a uniform framework which seamlessly integrates subtyping with dependent and singleton types. The framework was designed as a theoretical foundation for several problems of practical interest, including mixin modules, virtual classes, and featureoriented programming. The cost of using pure subtype systems is the complexity of the metatheory. We formulate the subtype relation as an abstract reduction system, and show that the theory is sound if the underlying reductions commute. We are able to show that the reductions commute locally, but have thus far been unable to show that they commute globally. Although the proof is incomplete, it is ``close enough'' to rule out obvious counterexamples. We present it as an open problem in type theory.
p3479
aVSession types allow communication protocols to be specified typetheoretically so that protocol implementations can be verified by static typechecking. We extend previous work on session types for distributed objectoriented languages in three ways. (1) We attach a session type to a class definition, to specify the possible sequences of method calls. (2) We allow a session type (protocol) implementation to be modularized , i.e. partitioned into separatelycallable methods. (3) We treat sessiontyped communication channels as objects, integrating their session types with the session types of classes. The result is an elegant unification of communication channels and their session types, distributed objectoriented programming, and a form of typestates supporting nonuniform objects, i.e. objects that dynamically change the set of available methods. We define syntax, operational semantics, a sound type system, and a correct and complete type checking algorithm for a small distributed classbased objectoriented language. Static typing guarantees that both sequences of messages on channels, and sequences of method calls on objects, conform to typetheoretic specifications, thus ensuring typesafety. The language includes expected features of session types, such as delegation, and expected features of objectoriented programming, such as encapsulation of local state. We also describe a prototype implementation as an extension of Java.
p3480
aVThis paper describes a novel technique for the synthesis of imperative programs. Automated program synthesis has the potential to make programming and the design of systems easier by allowing programs to be specified at a higherlevel than executable code. In our approach, which we call prooftheoretic synthesis, the user provides an inputoutput functional specification, a description of the atomic operations in the programming language, and a specification of the synthesized program's looping structure, allowed stack space, and bound on usage of certain operations. Our technique synthesizes a program, if there exists one, that meets the inputoutput specification and uses only the given resources. The insight behind our approach is to interpret program synthesis as generalized program verification, which allows us to bring verification tools and techniques to program synthesis. Our synthesis algorithm works by creating a program with unknown statements, guards, inductive invariants, and ranking functions. It then generates constraints that relate the unknowns and enforces three kinds of requirements: partial correctness, loop termination, and wellformedness conditions on program guards. We formalize the requirements that program verification tools must meet to solve these constraint and use tools from prior work as our synthesizers. We demonstrate the feasibility of the proposed approach by synthesizing programs in three different domains: arithmetic, sorting, and dynamic programming. Using verification tools that we previously built in the VS3 project we are able to synthesize programs for complicated arithmetic algorithms including Strassen's matrix multiplication and Bresenham's line drawing; several sorting algorithms; and several dynamic programming algorithms. For these programs, the median time for synthesis is 14 seconds, and the ratio of synthesis to verification time ranges between 1x to 92x (with an median of 7x), illustrating the potential of the approach.
p3481
aVWe present a novel framework for automatic inference of efficient synchronization in concurrent programs, a task known to be difficult and errorprone when done manually. Our framework is based on abstract interpretation and can infer synchronization for infinite state programs. Given a program, a specification, and an abstraction, we infer synchronization that avoids all (abstract) interleavings that may violate the specification, but permits as many valid interleavings as possible. Combined with abstraction refinement, our framework can be viewed as a new approach for verification where both the program and the abstraction can be modified onthefly during the verification process. The ability to modify the program, and not only the abstraction, allows us to remove program interleavings not only when they are known to be invalid, but also when they cannot be verified using the given abstraction. We implemented a prototype of our approach using numerical abstractions and applied it to verify several interesting programs.
p3482
aVAngelic nondeterminism can play an important role in program development. It simplifies specifications, for example in deriving programs with a refinement calculus; it is the formal basis of regular expressions; and Floyd relied on it to concisely express backtracking algorithms such as Nqueens. We show that angelic nondeterminism is also useful during the development of deterministic programs. The semantics of our angelic operator are the same as Floyd's but we use it as a substitute for yettobewritten deterministic code; the final program is fully deterministic. The angelic operator divines a value that makes the program meet its specification, if possible. Because the operator is executable, it allows the programmer to test incomplete programs: if a program has no safe execution, it is already incorrect; if a program does have a safe execution, the execution may reveal an implementation strategy to the programmer. We introduce refinementbased angelic programming, describe our embedding of angelic operators into Scala, report on our implementation with bounded model checking, and describe our experience with two case studies. In one of the studies, we use angelic operators to modularize the DeutschSchorrWaite (DSW) algorithm. The modularization is performed with the notion of a parasitic stack, whose incomplete specification was instantiated for DSW with angelic nondeterminism.
p3483
aVTraditional transactional memory systems suffer from overly conservative conflict detection, yielding socalled false conflicts, because they are based on finegrained, lowlevel read/write conflicts. In response, the recent trend has been toward integrating various abstract datatype libraries using adhoc methods of highlevel conflict detection. These proposals have led to improved performance but a lack of a unified theory has led to confusion in the literature. We clarify these recent proposals by defining a generalization of transactional memory in which a transaction consists of coarsegrained (abstract datatype) operations rather than simple memory read/write operations. We provide semantics for both pessimistic (e.g. transactional boosting) and optimistic (e.g. traditional TMs and recent alternatives) execution. We show that both are included in the standard atomic semantics, yet find that the choice imposes different requirements on the coarsegrained operations: pessimistic requires operations be leftmovers, optimistic requires rightmovers. Finally, we discuss how the semantics applies to numerous TM implementation details discussed widely in the literature.
p3484
aVSince Findler and Felleisen introduced higherorder contracts , many variants have been proposed. Broadly, these fall into two groups: some follow Findler and Felleisen in using latent  contracts, purely dynamic checks that are transparent to the type system; others use manifest  contracts, where refinement  types record the most recent check that has been applied to each value. These two approaches are commonly assumed to be equivalent different ways of implementing the same idea, one retaining a simple type system, and the other providing more static information. Our goal is to formalize and clarify this folklore understanding. Our work extends that of Gronski and Flanagan, who defined a latent calculus \u03bb C  and a manifest calculus \u03bb H , gave a translation \u03c6 from \u03bb C  to \u03bb H , and proved that, if a \u03bb C  term reduces to a constant, then so does its \u03c6image. We enrich their account with a translation \u03a8 from \u03bb H  to \u03bb C  and prove an analogous theorem. We then generalize the whole framework to dependent contracts , whose predicates can mention free variables. This extension is both pragmatically crucial, supporting a much more interesting range of contracts, and theoretically challenging. We define dependent versions of \u03bb H  and two dialects ("lax" and "picky") of \u03bb C , establish type soundness a substantial result in itself, for \u03bb H  and extend \u03c6 and \u03a8 accordingly. Surprisingly, the intuition that the latent and manifest systems are equivalent now breaks down: the extended translations preserve behavior in one direction but, in the other, sometimes yield terms that blame more.
p3485
aVHow to integrate static and dynamic types? Recent work focuses on casts to mediate between the two. However, adding casts may degrade tail calls into a nontail calls, increasing space consumption from constant to linear in the depth of calls. We present a new solution to this old problem, based on the notion of a threesome. A cast is specified by a source and a target type a twosome. Any twosome factors into a downcast from the source to an intermediate type, followed by an upcast from the intermediate to the target a threesome. Any chain of threesomes collapses to a single threesome, calculated by taking the greatest lower bound of the intermediate types. We augment this solution with blame labels to map any failure of a threesome back to the offending twosome in the source program. Herman, Tomb, and Flanagan (2007) solve the space problem by representing casts with the coercion calculus of Henglein (1994). While they provide a theoretical limit on the space overhead, there remains the practical question of how best to implement coercion reduction. The threesomes presented in this paper provide a streamlined data structure and algorithm for representing and normalizing coercions. Furthermore, threesomes provide a typedbased explanation of coercion reduction.
p3486
aVMany large software systems originate from untyped scripting language code. While good for initial development, the lack of static type annotations can impact codequality and performance in the long run. We present an approach for integrating untyped code and typed code in the same system to allow an initial prototype to smoothly evolve into an efficient and robust program. We introduce like types , a novel intermediate point between dynamic and static typing. Occurrences of like types variables are checked statically within their scope but, as they may be bound to dynamic values, their usage is checked dynamically. Thus like types provide some of the benefits of static typing without decreasing the expressiveness of the language. We provide a formal account of like types in a core object calculus and evaluate their applicability in the context of a new scripting language.
p3487
aVWe present an automated technique for generating compiler optimizations from examples of concrete programs before and after improvements have been made to them. The key technical insight of our technique is that a proof of equivalence between the original and transformed concrete programs informs us which aspects of the programs are important and which can be discarded. Our technique therefore uses these proofs, which can be produced by translation validation or a proofcarrying compiler, as a guide to generalize the original and transformed programs into broadly applicable optimization rules. We present a categorytheoretic formalization of our proof generalization technique. This abstraction makes our technique applicable to logics besides our own. In particular, we demonstrate how our technique can also be used to learn query optimizations for relational databases or to aid programmers in debugging type errors. Finally, we show experimentally that our technique enables programmers to train a compiler with applicationspecific optimizations by providing concrete examples of original programs and the desired transformed programs. We also show how it enables a compiler to learn efficienttorun optimizations from expensivetorun superoptimizers.
p3488
aVDespite years of work on retargetable compilers, creating a good, reliable back end for an optimizing compiler still entails a lot of hard work. Moreover, a critical component of the back end the instruction selector must be written by a person who is expert in both the compiler's intermediate code and the target machine's instruction set. By generating  the instruction selector from declarative machine descriptions we have (a) made it unnecessary for one person to be both a compiler expert and a machine expert, and (b) made creating an optimizing back end easier than ever before. Our achievement rests on two new results. First, finding a mapping from intermediate code to machine code is an undecidable problem. Second, using heuristic search, we can find mappings for machines of practical interest in at most a few minutes of CPU time.  Our most significant new idea is that heuristic search should be controlled by algebraic laws. Laws are used not only to show when a sequence of instructions implements part of an intermediate code, but also to limit the search: we drop a sequence of instructions not when it gets too long or when it computes too complicated a result, but when too much reasoning  will be required to show that the result computed might be useful.
p3489
aVWe present the design and theory of a new parsing engine, YAKKER, capable of satisfying the many needs of modern programmers and modern data processing applications. In particular, our new parsing engine handles (1) full scannerless contextfree grammars with (2) regular expressions as righthand sides for defining nonterminals. YAKKER also includes (3) facilities for binding variables to intermediate parse results and (4) using such bindings within arbitrary constraints to control parsing. These facilities allow the kind of datadependent parsing commonly needed in systems applications, particularly those that operate over binary data. In addition, (5) nonterminals may be parameterized by arbitrary values, which gives the system good modularity and abstraction properties in the presence of datadependent parsing. Finally, (6) legacy parsing libraries,such as sophisticated libraries for dates and times, may be directly incorporated into parser specifications. We illustrate the importance and utility of this rich collection of features by presenting its use on examples ranging from difficult programming language grammars to web server logs to binary data specification. We also show that our grammars have important compositionality properties and explain why such properties areimportant in modern applications such as automatic grammar induction. In terms of technical contributions, we provide a traditional highlevel semantics for our new grammar formalization and show how to compile grammars into non deterministic automata. These automata are stackbased, somewhat like conventional pushdown automata,but are also equipped with environments to track datadependent parsing state. We prove the correctness of our translation of datadependent grammars into these new automata and then show how to implement the automata efficiently using a variation of Earley's parsing algorithm.
p3490
aVThis paper presents Paralocks, a language for building expressive but statically verifiable finegrained information flow policies. Paralocks combine the expressive power of Flow Locks (Broberg & Sands, ESOP'06) with the ability to express policies involving runtime principles, roles (in the style of rolebased access control), and relations (such as "actsfor" in discretionary access control). We illustrate the Paralocks policy language by giving a simple encoding of Myers and Liskov's Decentralized Label Model (DLM). Furthermore  and unlike the DLM  we provide an information flow semantics for full Paralock policies. Lastly we illustrate how Paralocks can be statically verified by providing a simple programming language incorporating Paralock policy specifications, and a static type system which soundly enforces information flow security according to the Paralock semantics.
p3491
aVWe propose a method for verifying the security of protocol implementations. Our method is based on declaring and enforcing invariants on the usage of cryptography. We develop cryptographic libraries that embed a logic model of their cryptographic structures and that specify preconditions and postconditions on their functions so as to maintain their invariants. We present a theory to justify the soundness of modular code verification via our method. We implement the method for protocols coded in F# and verified using F7, our SMTbased typechecker for refinement types, that is, types carrying formulas to record invariants. As illustrated by a series of programming examples, our method can flexibly deal with a range of different cryptographic constructions and protocols. We evaluate the method on a series of larger case studies of protocol code, previously checked using wholeprogram analyses based on ProVerif, a leading verifier for cryptographic protocols. Our results indicate that compositional verification by typechecking with refinement types is more scalable than the best domainspecific analysis currently available for cryptographic code.
p3492
aVConcurrent programming errors arise when threads share data incorrectly. Programmers often avoid these errors by using synchronization to enforce a simple ownership policy: data is either owned exclusively  by a thread that can read or write the data, or it is read owned  by a set of threads that can read but not write the data. Unfortunately, incorrect synchronization often fails to enforce these policies and memory errors in languages like C and C++ can violate these policies even when synchronization is correct. In this paper, we present a dynamic analysis for checking ownership policies in concurrent C and C++ programs despite memory errors. The analysis can be used to find errors in commodity multithreaded programs and to prevent attacks that exploit these errors. We require programmers to write ownership assertions that describe the sharing policies used by different parts of the program. These policies may change over time, as may the policies' means of enforcement, whether it be locks, barriers, thread joins, etc. Our compiler inserts checks in the program that signal an error if these policies are violated at runtime. We evaluated our tool on several benchmark programs. The runtime overhead was reasonable: between 0 and 49% with an average of 26%. We also found the tool easy to use: the total number of ownership assertions is small, and the asserted specification and implementation can be debugged together by running the instrumented program and addressing the errors that arise. Our approach enjoys a pleasing modular soundness property: if a thread executes a sequence of statements on variables it owns, the statements are serializable within a valid execution, and thus their effects can be reasoned about in isolation from other threads in the program.
p3493
aVIn this paper, we explore the potential of the theory of nested words for partial correctness proofs of recursive programs. Our conceptual contribution is a simple framework that allows us to shine a new light on classical concepts such as Floyd/Hoare proofs and predicate abstraction in the context of recursive programs. Our technical contribution is an interpolantbased software model checking method for recursive programs. The method avoids the costly construction of the abstract transformer by constructing a nested word automaton from an inductive sequence of `nested interpolants' (i.e., interpolants for a nested word which represents an infeasible error trace).
p3494
aVSerializability is a commonly used correctness condition in concurrent programming. When a concurrent module is serializable, certain other properties of the module can be verified by considering only its sequential executions. In many cases, concurrent modules guarantee serializability by using standard locking protocols, such as tree locking or twophase locking. Unfortunately, according to the existing literature, verifying that a concurrent module adheres to these protocols requires considering concurrent interleavings. In this paper, we show that adherence to a large class of locking protocols (including tree locking and twophase locking) can be verified by considering only sequential  executions. The main consequence of our results is that in many cases, the (manual or automatic) verification of serializability can itself be done using sequential reasoning .
p3495
aVIn functional programming, monadic characterizations of computational effects are normally understood denotationally: they describe how an effectful program can be systematically expanded or translated into a larger, pure program, which can then be evaluated according to an effectfree semantics. Any effectspecific operations expressible in the monad are also given purely functional definitions, but these definitions are only directly executable in the context of an already translated program. This approach thus takes an inherently Churchstyle view of effects: the nominal meaning of every effectful term in the program depends crucially on its type. We present here a complementary, operational view of monadic effects, in which an effect definition directly induces an imperative behavior of the new operations expressible in the monad. This behavior is formalized as additional operational rules for only the new constructs; it does not require any structural changes to the evaluation judgment. Specifically, we give a smallstep operational semantics of a prototypical functional language supporting programmerdefinable, layered effects, and show how this semantics naturally supports reasoning by familiar syntactic techniques, such as showing soundness of a Currystyle effecttype system by the progress+preservation method.
p3496
aVProgram analysis tools typically compute two types of information: (1) may  information that is true of all  program executions and is used to prove the absence of bugs in the program, and (2) must  information that is true of some  program executions and is used to prove the existence of bugs in the program. In this paper, we propose a new algorithm, dubbed SMASH, which computes both may and must information compositionally . At each procedure boundary, may and must information is represented and stored as may and must summaries, respectively. Those summaries are computed in a demand driven manner and possibly using summaries of the opposite type. We have implemented SMASH using predicate abstraction (as in SLAM) for the may part and using dynamic test generation (as in DART) for the must part. Results of experiments with 69 Microsoft Windows 7 device drivers show that SMASH can significantly outperform mayonly, mustonly and noncompositional maymust algorithms. Indeed, our empirical results indicate that most complex code fragments in large programs are actually often either easy to prove irrelevant to the specific property of interest using may analysis or  easy to traverse using directed testing. The finegrained coupling and alternation  of may (universal) and must (existential) summaries allows SMASH to easily navigate through these code fragments while traditional mayonly, mustonly or noncompositional maymust algorithms are stuck in their specific analyses.
p3497
aVWe present an analysis to automatically determine if a program represents a continuous function, or equivalently, if infinitesimal changes to its inputs can only cause infinitesimal changes to its outputs. The analysis can be used to verify the robustness  of programs whose inputs can have small amounts of error and uncertainty e.g., embedded controllers processing slightly unreliable sensor data, or handheld devices using slightly stale satellite data. Continuity is a fundamental notion in mathematics. However, it is difficult to apply continuity proofs from real analysis to functions that are coded as imperative programs, especially when they use diverse data types and features such as assignments, branches, and loops. We associate data types with metric spaces as opposed to just sets of values, and continuity of typed programs is phrased in terms of these spaces. Our analysis reduces questions about continuity to verification conditions that do not refer to infinitesimal changes and can be discharged using offtheshelf SMT solvers. Challenges arise in proving continuity of programs with branches and loops, as a small perturbation in the value of a variable often leads to divergent controlflow that can lead to large changes in values of variables. Our proof rules identify appropriate ``synchronization points'' between executions and their perturbed counterparts, and establish that values of certain variables converge back to the original results in spite of temporary divergence. We prove our analysis sound with respect to the traditional epsilondelta definition of continuity. We demonstrate the precision of our analysis by applying it to a range of classic algorithms, including algorithms for array sorting, shortest paths in graphs, minimum spanning trees, and combinatorial optimization. A prototype implementation based on the Z3 SMTsolver is also presented.
p3498
aVPathsensitivity is often a crucial requirement for verifying safety properties of programs. As it is infeasible to enumerate and analyze each path individually, analyses compromise by soundly merging information about executions along multiple paths. However, this frequently results in a loss of precision. We present a program analysis technique that we call Satisfiability Modulo Path Programs  (SMPP), based on a pathbased decomposition of a program. It is inspired by insights that have driven the development of modern SMT(Satisfiability Modulo Theory) solvers. SMPP symbolically enumerates path programs using a SAT formula over control edges in the program. Each enumerated path program is verified using an oracle, such as abstract interpretation or symbolic execution, to either find a proof of correctness or report a potential violation. If a proof is found, then SMPP extracts a sufficient set of control edges and corresponding interference edges, as a form of proofbased learning. Blocking clauses derived from these edges are added back to the SAT formula to avoid enumeration of other path programs guaranteed to be correct, thereby improving performance and scalability. We have applied SMPP in the FSoft program verification framework, to verify properties of realworld C programs that require pathsensitive reasoning. Our results indicate that the precision from analyzing individual path programs, combined with their efficient enumeration by SMPP, can prove properties as well as indicate potential violations in the large.
p3499
aVSoftware pipelining is a loop optimization that overlaps the execution of several iterations of a loop to expose more instructionlevel parallelism. It can result in firstclass performance characteristics, but at the cost of significant obfuscation of the code, making this optimization difficult to test and debug. In this paper, we present a translation validation algorithm that uses symbolic evaluation to detect semantics discrepancies between a loop and its pipelined version. Our algorithm can be implemented simply and efficiently, is provably sound, and appears to be complete with respect to most modulo scheduling algorithms. A conclusion of this case study is that it is possible and effective to use symbolic evaluation to reason about loop transformations.
p3500
aVWe present a verified compiler to an idealized assembly language from a small, untyped functional language with mutable references and exceptions. The compiler is programmed in the Coq proof assistant and has a proof of total correctness with respect to bigstep operational semantics for the source and target languages. Compilation is staged and includes standard phases like translation to continuationpassing style and closure conversion, as well as a common subexpression elimination optimization. In this work, our focus has been on discovering and using techniques that make our proofs easy to engineer and maintain. While most programming language work with proof assistants uses very manual proof styles, all of our proofs are implemented as adaptive programs in Coq's tactic language, making it possible to reuse proofs unchanged as new language features are added. In this paper, we focus especially on phases of compilation that rearrange the structure of syntax with nested variable binders. That aspect has been a key challenge area in past compiler verification projects, with much more effort expended in the statement and proof of binderrelated lemmas than is found in standard pencilandpaper proofs. We show how to exploit the representation technique of parametric higherorder abstract syntax to avoid the need to prove any of the usual lemmas about binder manipulation, often leading to proofs that are actually shorter than their pencilandpaper analogues. Our strategy is based on a new approach to encoding operational semantics which delegates all concerns about substitution to the meta language, without using features incompatible with generalpurpose type theories like Coq's logic.
p3501
aVThe formal verification of programs has progressed tremendously in the last decade. In this talk, I review some of the obstacles that [6, 8, 15, 18] remain to be lifted before sourcelevel verification tools can be taken really seriously in the critical software industry. A direction I advocate is the systematic formal verification of the development tools that participate in the production and verification of critical software.
p3502
aVWe introduce FunArray, a parametric segmentation abstract domain functor for the fully automatic and scalable analysis of array content properties. The functor enables a natural, painless and efficient lifting of existing abstract domains for scalar variables to the analysis of uniform compound datastructures such as arrays and collections. The analysis automatically and semantically divides arrays into consecutive nonoverlapping possibly empty segments. Segments are delimited by sets of bound expressions and abstracted uniformly. All symbolic expressions appearing in a bound set are equal in the concrete. The FunArray can be naturally combined via reduced product with any existing analysis for scalar variables. The analysis is presented as a general framework parameterized by the choices of bound expressions, segment abstractions and the reduction operator. Once the functor has been instantiated with fixed parameters, the analysis is fully automatic. We first prototyped FunArray in Arrayal to adjust and experiment with the abstractions and the algorithms to obtain the appropriate precision/ratio cost. Then we implemented it into Clousot, an abstract interpretationbased static contract checker for .NET. We empirically validated the precision and the performance of the analysis by running it on the main libraries of .NET and on its own code. We were able to infer thousands of nontrivial invariants and verify the implementation with a modest overhead (circa 1%). To the best of our knowledge this is the first analysis of this kind applied to such a large code base, and proven to scale.
p3503
aVOver the last decade, there has been extensive research on modelling challenging features in programming languages and program logics, such as higherorder store and storable resource invariants. A recent line of work has identified a common solution to some of these challenges: Kripke models over worlds that are recursively defined in a category of metric spaces. In this paper, we broaden the scope of this technique from the original domaintheoretic setting to an elementary, operational one based on step indexing. The resulting method is widely applicable and leads to simple, succinct models of complicated language features, as we demonstrate in our semantics of Charguraud and Pottier's typeandcapability system for an MLlike higherorder language. Moreover, the method provides a highlevel understanding of the essence of recent approaches based on step indexing.
p3504
aVThere has recently been great progress in proving the correctness of compilers for increasingly realistic languages with increasingly realistic runtime systems. Most work on this problem has focused on proving the correctness of a particular compiler, leaving open the question of how to verify the correctness of assembly code that is handoptimized or linked together from the output of multiple compilers. This has led Benton and other researchers to propose more abstract, compositional notions of when a lowlevel program correctly realizes a highlevel one. However, the state of the art in socalled "compositional compiler correctness" has only considered relatively simple highlevel and lowlevel languages. In this paper, we propose a novel, extensional, compilerindependent notion of equivalence between highlevel programs in an expressive, impure MLlike \u03bbcalculus and lowlevel programs in an (only slightly) idealized assembly language. We define this equivalence by means of a biorthogonal, stepindexed, Kripke logical relation, which enables us to reason quite flexibly about assembly code that uses local state in a different manner than the highlevel code it implements (e.g. selfmodifying code). In contrast to prior work, we factor our relation in a symmetric, languagegeneric fashion, which helps to simplify and clarify the formal presentation, and we also show how to account for the presence of a garbage collector. Our approach relies on recent developments in Kripke logical relations for MLlike languages, in particular the idea of possible worlds as state transition systems.
p3505
aVWe present a storepassing translation of System F with general references into an extension of System F\u03c9 with certain wellbehaved recursive kinds. This seems to be the first typepreserving storepassing translation for general references. It can be viewed as a purely syntactic account of a possible worlds model.
p3506
aVComputations on unstructured graphs are challenging to parallelize because dependences in the underlying algorithms are usually complex functions of runtime data values, thwarting static parallelization. One promising generalpurpose parallelization strategy for these algorithms is optimistic parallelization. This paper identifies the optimization of optimistically parallelized graph programs as a new application area, and develops the first shape analysis for addressing this problem. Our shape analysis identifies failsafe points in the program after which the execution is guaranteed not to abort and backup copies of modified data are not needed; additionally, the analysis can be used to eliminate redundant conflict checking. It uses two key ideas: a novel topdown heap abstraction that controls state space explosion, and a strategy for predicate discovery that exploits common patterns of data structure usage. We implemented the shape analysis in TVLA, and used it to optimize benchmarks from the Lonestar suite. The optimized programs were executed on the Galois system. The analysis was successful in eliminating all costs related to rollback logging for our benchmarks. Additionally, it reduced the number of lock acquisitions by a factor ranging from 10x to 50x, depending on the application and the number of threads. These optimizations were effective in reducing the running times of the benchmarks by factors of 2x to 12x.
p3507
aVInterprocedural program analysis is often performed by computing procedure summaries. While possible, computing adequate summaries is difficult, particularly in the presence of recursive procedures. In this paper, we propose a complementary framework for interprocedural analysis based on a direct abstraction of the calling context. Specifically, our approach exploits the inductive structure of a calling context by treating it directly as a stack of activation records. We then build an abstraction based on separation logic with inductive definitions. A key element of this abstract domain is the use of parameters to refine the meaning of such call stack summaries and thus express relations across activation records and with the heap. In essence, we define an abstract interpretationbased analysis framework for recursive programs that permits a fluid per call site abstraction of the call stack much like how shape analyzers enable a fluid per program point abstraction of the heap.
p3508
aVContainers are generalpurpose data structures that provide functionality for inserting, reading, removing, and iterating over elements. Since many applications written in modern programming languages, such as C++ and Java, use containers as standard building blocks, precise analysis of many programs requires a fairly sophisticated understanding of container contents. In this paper, we present a sound, precise, and fully automatic technique for static reasoning about contents of containers. We show that the proposed technique adds useful precision for verifying real C++ applications and that it scales to applications with over 100,000 lines of code.
p3509
aVSeveral programming languages are beginning to integrate static and dynamic typing, including Racket (formerly PLT Scheme), Perl 6, and C# 4.0 and the research languages Sage (Gronski, Knowles, Tomb, Freund, and Flanagan, 2006) and Thorn (Wrigstad, Eugster, Field, Nystrom, and Vitek, 2009). However, an important open question remains, which is how to add parametric polymorphism to languages that combine static and dynamic typing. We present a system that permits a value of dynamic type to be cast to a polymorphic type and vice versa, with relational parametricity enforced by a kind of dynamic sealing along the lines proposed by Matthews and Ahmed (2008) and Neis, Dreyer, and Rossberg (2009). Our system includes a notion of blame, which allows us to show that when casting between a moreprecise type and a lessprecise type, any cast failures are due to the lesspreciselytyped portion of the program. We also show that a cast from a subtype to its supertype cannot fail.
p3510
aVBehavioral software contracts supplement interface information with logical assertions. A rigorous enforcement of contracts provides useful feedback to developers if it signals contract violations as soon as they occur and if it assigns blame to violators with preciseexplanations. Correct blame assignment gets programmers started with the debugging process and can significantly decrease the time needed to discover and fix bugs. Sadly the literature on contracts lacks a framework for making statements about the correctness of blame assignment and for validating such statements. This paper fills the gap and uses the framework to demonstrate how one of the proposed semantics for higherorder contracts satisfies this criteria and another semantics occasionally assigns blame to the wrong module. Concretely, the paper applies the framework to the lax enforcement of dependent higherorder contracts and the picky one. A higherorder dependent contract specifies constraints for the domain and range of higherorder functions and also relates arguments and results in auxiliary assertions. The picky semantics ensures that the use of arguments in the auxiliary assertion satisfies the domain contracts and the lax one does not. While the picky semantics discovers more contract violations than the lax one, it occasionally blames the wrong module. Hence the paper also introduces a third semantics, dubbed indy, which fixes the problems of the picky semantics without giving up its advantages.
p3511
aVModular languages support generative type abstraction, ensuring that an abstract type is distinct from its representation, except inside the implementation where the two are synonymous. We show that this wellestablished feature is in tension with the nonparametric features of newer type systems, such as indexed type families and GADTs. In this paper we solve the problem by using kinds to distinguish between parametric and nonparametric contexts. The result is directly applicable to Haskell, which is rapidly developing support for typelevel computation, but the same issues should arise whenever generativity and nonparametric features are combined.
p3512
aVThis paper explores a sweet spot between flowinsensitive and flowsensitive subsetbased pointsto analysis. Flowinsensitive analysis is efficient: it has been applied to millionline programs and even its worstcase requirements are quadratic space and cubic time. Flowsensitive analysis is precise because it allows strong updates, so that pointsto relationships holding in one program location can be removed from the analysis when they no longer hold in other locations. We propose a "Strong Update" analysis combining both features: it is efficient like flowinsensitive analysis, with the same worstcase bounds, yet its precision benefits from strong updates like flowsensitive analysis. The key enabling insight is that strong updates are applicable when the dereferenced pointsto set is a singleton, and a singleton set is cheap to analyze. The analysis therefore focuses flow sensitivity on singleton sets. Larger sets, which will not lead to strong updates, are modelled flow insensitively to maintain efficiency. We have implemented and evaluated the analysis as an extension of the standard flowinsensitive pointsto analysis in the LLVM compiler infrastructure.
p3513
aVKodu is a relatively new programming language designed specifically for young children to learn through independent exploration. Kodu seeks to lower the barrier to entry for new programmers by presenting a radically simplified programming model which nevertheless has significant expressive power. Kodu is integrated in a realtime 3D gaming environment and is designed to compete with modern console games in terms of intuitive user interface and graphical production values. In this paper we will review key tradeoffs made in the design of the programming language and illustrate how it is one of very few languages designed using user interface design principles and methodologies, to the extent that the blend of subjective and objective factors considered in the language design have succeeded in presenting a model of programming which is uniquely approachable and creatively empowering for nontechnical users.
p3514
aVFinegrained concurrent data structures are crucial for gaining performance from multiprocessing, but their design is a subtle art. Recent literature has made large strides in verifying these data structures, using either atomicity refinement or separation logic with relyguarantee reasoning. In this paper we show how the ownership discipline of separation logic can be used to enable atomicity refinement, and we develop a new relyguarantee method that is localized to the definition of a data structure. We present the first semantics of separation logic that is sensitive to atomicity, and show how to control this sensitivity through ownership. The result is a logic that enables compositional reasoning about atomicity and interference, even for programs that use finegrained synchronization and dynamic memory allocation.
p3515
aVWeaving a concurrency control protocol into a program is difficult and errorprone. One way to alleviate this burden is deterministic parallelism. In this wellstudied approach to parallelisation, a sequential program is annotated with sections that can execute concurrently, with automatically injected control constructs used to ensure observable behaviour consistent with the original program. This paper examines the formal specification and verification of these constructs. Our highlevel specification defines the conditions necessary for correct execution; these conditions reflect program dependencies necessary to ensure deterministic behaviour. We connect the highlevel specification used by clients of the library with the lowlevel library implementation, to prove that a client's requirements for determinism are enforced. Significantly, we can reason about program and library correctness without breaking abstraction boundaries. To achieve this, we use concurrent abstract predicates, based on separation logic, to encapsulate racy behaviour in the library's implementation. To allow generic specifications of libraries that can be instantiated by client programs, we extend the logic with higherorder parameters and quantification. We show that our highlevel specification abstracts the details of deterministic parallelism by verifying two different lowlevel implementations of the library.
p3516
aVCompared to coarsegrained external synchronization of operations on data structures shared between concurrent threads, finegrained, internal synchronization can offer stronger progress guarantees and better performance. However, fully specifying operations that perform internal synchronization modularly is a hard, open problem. The state of the art approaches, based on linearizability or on concurrent abstract predicates, have important limitations on the expressiveness of specifications. Linearizability does not support ownership transfer, and the concurrent abstract predicatesbased specification approach requires hardcoding a particular usage protocol. In this paper, we propose a novel approach that lifts these limitations and enables fully general specification of finegrained concurrent data structures. The basic idea is that clients pass the ghost code required to instantiate an operation's specification for a specific client scenario into the operation in a simple form of higherorder programming. We machinechecked the theory of the paper using the Coq proof assistant. Furthermore, we implemented the approach in our program verifier VeriFast and used it to verify two challenging finegrained concurrent data structures from the literature: a multiplecompareandswap algorithm and a lockcoupling list.
p3517
aVWe propose a generalization of results on the decidability of emptiness for several restricted classes of sequential and distributed automata with auxiliary storage (stacks, queues) that have recently been proved. Our generalization relies on reducing emptiness of these automata to finitestate graph automata (without storage) restricted to monadic secondorder (MSO) definable graphs of bounded treewidth, where the graph structure encodes the mechanism provided by the auxiliary storage. Our results outline a uniform mechanism to derive emptiness algorithms for automata, explaining and simplifying several existing results, as well as proving new decidability results.
p3518
aVWhat is a basic automatatheoretic model of computation with names and freshname generation? We introduce FreshRegister Automata (FRA), a new class of automata which operate on an infinite alphabet of names and use a finite number of registers to store fresh names, and to compare incoming names with previously stored ones. These finite machines extend Kaminski and Francez's FiniteMemory Automata by being able to recognise globally fresh inputs, that is, names fresh in the whole current run. We examine the expressivity of FRA's both from the aspect of accepted languages and of bisimulation equivalence. We establish primary properties and connections between automata of this kind, and answer key decidability questions. As a demonstrating example, we express the theory of the picalculus in FRA's and characterise bisimulation equivalence by an appropriate, and decidable in the finitary case, notion in these automata.
p3519
aVThe reachability problem for Vector Addition Systems (VASs) is a central problem of net theory. The general problem is known decidable by algorithms exclusively based on the classical KosarajuLambertMayrSacerdoteTenney decomposition (KLMTS decomposition). Recently from this decomposition, we deduced that a final configuration is not reachable from an initial one if and only if there exists a Presburger inductive invariant that contains the initial configuration but not the final one. Since we can decide if a Preburger formula denotes an inductive invariant, we deduce from this result that there exist checkable certificates of nonreachability in the Presburger arithmetic. In particular, there exists a simple algorithm for deciding the general VAS reachability problem based on two semialgorithms. A first one that tries to prove the reachability by enumerating finite sequences of actions and a second one that tries to prove the nonreachability by enumerating Presburger formulas. In this paper we provide the first proof of the VAS reachability problem that is not based on the KLMST decomposition. The proof is based on the notion of production relations inspired from Hauschildt that directly provides the existence of Presburger inductive invariants.
p3520
aVWe describe the design of a string programming/expression language that supports restricted forms of regular expressions, conditionals and loops. The language is expressive enough to represent a wide variety of string manipulation tasks that endusers struggle with. We describe an algorithm based on several novel concepts for synthesizing a desired program in this language from inputoutput examples. The synthesis algorithm is very efficient taking a fraction of a second for various benchmark examples. The synthesis algorithm is interactive and has several desirable features: it can rank multiple solutions and has fast convergence, it can detect noise in the user input, and it supports an active interaction model wherein the user is prompted to provide outputs on inputs that may have multiple computational interpretations. The algorithm has been implemented as an interactive addin for Microsoft Excel spreadsheet system. The prototype tool has met the golden test  it has synthesized part of itself, and has been used to solve problems beyond author's imagination.
p3521
aVAutomated verification of multithreaded programs requires explicit identification of the interplay between interacting threads, socalled environment transitions, to enable scalable, compositional reasoning. Once the environment transitions are identified, we can prove program properties by considering each program thread in isolation, as the environment transitions keep track of the interleaving with other threads. Finding adequate environment transitions that are sufficiently precise to yield conclusive results and yet do not overwhelm the verifier with unnecessary details about the interleaving with other threads is a major challenge. In this paper we propose a method for safety verification of multithreaded programs that applies (transition) predicate abstractionbased discovery of environment transitions, exposing a minimal amount of information about the thread interleaving. The crux of our method is an abstraction refinement procedure that uses recursionfree Horn clauses to declaratively state abstraction refinement queries. Then, the queries are resolved by a corresponding constraint solving algorithm. We present preliminary experimental results for mutual exclusion protocols and multithreaded device drivers.
p3522
aVGeometry of Synthesis is a technique for compiling higherlevel programming languages into digital circuits via their game semantic model. Ghica (2007) first presented the key idea, then Ghica and Smith (2010) gave a provably correct compiler into asynchronous circuits for Syntactic Control of Interference (SCI), an affinetyped version of Reynolds's Idealized Algol. Affine typing has the dual benefits of ruling out race conditions through the type system and having a finitestate gamesemantic model for any term, which leads to a natural circuit representation and simpler correctness proofs. In this paper we go beyond SCI to full Idealized Algol, enhanced with sharedmemory concurrency and semaphores. Compiling ICA proceeds in three stages. First, an intermediate type system called Syntactic Control of Concurrency (SCC), is used to statically determine "concurrency bounds" on all identifiers in the program. Then, a program transformation called serialization is applied to the program to translate it into an equivalent SCC program in which all concurrency bounds are set to the unit. Finally, the resulting program can be then compiled into asynchronous circuits using a slightly enhanced version of the GoS II compiler, which can handle assignable variables used in nonsequential contexts.
p3523
aVObjectsensitivity has emerged as an excellent context abstraction for pointsto analysis in objectoriented languages. Despite its practical success, however, objectsensitivity is poorly understood. For instance, for a context depth of 2 or higher, past scalable implementations deviate significantly from the original definition of an objectsensitive analysis. The reason is that the analysis has many degrees of freedom, relating to which context elements are picked at every method call and object creation. We offer a clean model for the analysis design space, and discuss a formal and informal understanding of objectsensitivity and of how to create good objectsensitive analyses. The results are surprising in their extent. We find that past implementations have made a suboptimal choice of contexts, to the severe detriment of precision and performance. We define a "fullobjectsensitive" analysis that results in significantly higher precision, and often performance, for the exact same context depth. We also introduce "typesensitivity" as an explicit approximation of objectsensitivity that preserves high context quality at substantially reduced cost. A typesensitive pointsto analysis makes an unconventional use of types as context: the context types are not dynamic types of objects involved in the analysis, but instead upper bounds on the dynamic types of their allocator objects. Our results expose the influence of context choice on the quality of pointsto analysis and demonstrate typesensitivity to be an idea with major impact: It decisively advances the stateoftheart with a spectrum of analyses that simultaneously enjoy speed (several times faster than an analogous objectsensitive analysis), scalability (comparable to analyses with much less contextsensitivity), and precision (comparable to the best objectsensitive analysis with the same context depth).
p3524
aVWe study the problem of automatically analyzing the worstcase resource usage of procedures with several arguments. Existing automatic analyses based on amortization, or sized types bound the resource usage or result size of such a procedure by a sum of unary functions of the sizes of the arguments. In this paper we generalize this to arbitrary multivariate polynomial functions thus allowing bounds of the form mn which had to be grossly overestimated by m2+n2 before. Our framework even encompasses bounds like \u2217i,j\u2264n m_i mj where the mi are the sizes of the entries of a list of length n. This allows us for the first time to derive useful resource bounds for operations on matrices that are represented as lists of lists and to considerably improve bounds on other superlinear operations on lists such as longest common subsequence and removal of duplicates from lists of lists. Furthermore, resource bounds are now closed under composition which improves accuracy of the analysis of composed programs when some or all of the components exhibit superlinear resource or size behavior. The analysis is based on a novel multivariate amortized resource analysis. We present it in form of a type system for a simple firstorder functional language with lists and trees, prove soundness, and describe automatic type inference based on linear programming. We have experimentally validated the automatic analysis on a wide range of examples from functional programming with lists and trees. The obtained bounds were compared with actual resource consumption. All bounds were asymptotically tight, and the constants were close or even identical to the optimal ones.
p3525
aVLenses bidirectional transformations between pairs of connected structures have been extensively studied and are beginning to find their way into industrial practice. However, some aspects of their foundations remain poorly understood. In particular, most previous work has focused on the special case of asymmetric lenses, where one of the structures is taken as primary and the other is thought of as a projection, or view. A few studies have considered symmetric variants, where each structure contains information not present in the other, but these all lack the basic operation of composition. Moreover, while many domainspecific languages based on lenses have been designed, lenses have not been thoroughly explored from an algebraic perspective. We offer two contributions to the theory of lenses. First, we present a new symmetric formulation, based on complements, an old idea from the database literature. This formulation generalizes the familiar structure of asymmetric lenses, and it admits a good notion of composition. Second, we explore the algebraic structure of the space of symmetric lenses. We present generalizations of a number of known constructions on asymmetric lenses and settle some longstanding questions about their properties in particular, we prove the existence of (symmetric monoidal) tensor products and sums and the nonexistence of full categorical products or sums in the category of symmetric lenses. We then show how the methods of universal algebra can be applied to build iterator lenses for structured data such as lists and trees, yielding lenses for operations like mapping, filtering, and concatenation from first principles. Finally, we investigate an even more general technique for constructing mapping combinators, based on the theory of containers.
p3526
aVWe present a new sound and complete axiomatization of regular expression containment. It consists of the conventional axiomatization of concatenation, alternation, empty set and (the singleton set containing) the empty string as an idempotent semiring, the fixed point rule E* = 1 + E  E* for Kleenestar, and a general coinduction rule as the only additional rule. Our axiomatization gives rise to a natural computational interpretation of regular expressions as simple types that represent parse trees, and of containment proofs as coercions. This gives the axiom atization a CurryHowardstyle constructive interpretation: Containment proofs do not only certify a languagetheoretic contain ment, but, under our computational interpretation, constructively transform a membership proof of a string in one regular expression into a membership proof of the same string in another regular expression. We show how to encode regular expression equivalence proofs in Salomaa's, Kozen's and Grabmayer's axiomatizations into our containment system, which equips their axiomatizations with a computational interpretation and implies completeness of our axiomatization. To ensure its soundness, we require that the computational interpretation of the coinduction rule be a hereditarily total function. Hereditary totality can be considered the mother of syn tactic side conditions: it "explains" their soundness, yet cannot be used as a conventional side condition in its own right since it turns out to be undecidable. We discuss application of regular expressions as types to bit coding of strings and hint at other applications to the widespread use of regular expressions for substring matching, where classical automatatheoretic techniques are a priori inapplicable. Neither regular expressions as types nor subtyping interpreted coercively are novel per se. Somewhat surprisingly, this seems to be the first investigation of a general prooftheoretic framework for the latter in the context of the former, however.
p3527
aVWe describe a new algorithm for proving temporal properties expressed in LTL of infinitestate programs. Our approach takes advantage of the fact that LTL properties can often be proved more efficiently using techniques usually associated with the branchingtime logic CTL than they can with native LTL algorithms. The caveat is that, in certain instances, nondeterminism in the system's transition relation can cause CTL methods to report counter examples that are spurious with respect to the original LTL formula. To address this problem we describe an algorithm that, as it attempts to apply CTL proof methods, finds and then removes problematic nondeterminism via an analysis on the potentially spurious counterexamples. Problematic nondeterminism is characterized using decision predicates, and removed using a partial, symbolic determinization procedure which introduces new prophecy variables to predict the future outcome of these choices. We demonstrate using examples taken from the PostgreSQL database server, Apache web server, and Windows OS kernel that our method can yield enormous performance improvements in comparison to known tools, allowing us to automatically prove properties of programs where we could not prove them before.
p3528
aVWe provide a new characterization of scheduling nondeterminism by allowing deterministic schedulers to delay their nextscheduled task. In limiting the delays an otherwisedeterministic scheduler is allowed, we discover concurrency bugs efficiently by exploring few schedules and robustly i.e., independent of the number of tasks, context switches, or buffered events. Our characterization elegantly applies to any systematic exploration (e.g., testing, model checking) of concurrent programs with dynamic taskcreation. Additionally, we show that certain delaying schedulers admit efficient reductions from concurrent to sequential program analysis.
p3529
aVInterference is the bane of both concurrent programming and analysis. To avoid considering all possible interferences between concurrent threads, most automated static analysis employ techniques to approximate interference, e.g., by restricting the thread scheduler choices or by approximating the transition relations or reachable states of the program. However, none of these methods are able to reason about interference directly. In this paper, we introduce the notion of interference abstractions (IAs), based on the models of shared memory consistency, to reason about interference efficiently. IAs differ from the known abstractions for concurrent programs and cannot be directly modeled by these abstractions. Concurrency bugs typically involve a small number of unexpected interferences and therefore can be captured by small IAs. We show how IAs, in the form of both over and underapproximations of interference, can be obtained syntactically from the axioms of sequential consistency. Further, we present an automatic method to synthesize IAs suitable for checking safety properties. Our experimental results show that small IAs are often sufficient to check properties in realistic applications, and drastically improve the scalability of concurrent program analysis in these applications.
p3530
aVMultiparty session types enforce structured safe communications between several participants, as long as their number is fixed when the session starts. In order to handle common distributed interaction patterns such as peertopeer protocols or cloud algorithms, we propose a new rolebased multiparty session type theory where roles are defined as classes of local behaviours that an arbitrary number of participants can dynamically join and leave. We offer programmers a polling operation that gives access to the current set of a role's participants in order to fork processes. Our type system with universal types for polling can handle this dynamism and retain type safety. A multiparty locking mechanism is introduced to provide communication safety, but also to ensure a stronger progress property for joining participants that has never been guaranteed in previous systems. Finally, we present some implementation mechanisms used in our prototype extension of ML.
p3531
aVAlms is a generalpurpose programming language that supports practical affine types. To offer the expressiveness of Girard's linear logic while keeping the type system light and convenient, Alms uses expressive kinds that minimize notation while maximizing polymorphism between affine and unlimited types. A key feature of Alms is the ability to introduce abstract affine types via MLstyle signature ascription. In Alms, an interface can impose stiffer resource usage restrictions than the principal usage restrictions of its implementation. This form of sealing allows the type system to naturally and directly express a variety of resource management protocols from specialpurpose type systems. We present two pieces of evidence to demonstrate the validity of our design goals. First, we introduce a prototype implementation of Alms and discuss our experience programming in the language. Second, we establish the soundness of the core language. We also use the core model to prove a principal kinding theorem.
p3532
aVThere have been several efforts to bring static type inference to objectoriented dynamic languages such as Ruby, Python, and Perl. In our experience, however, such type inference systems are extremely difficult to develop, because dynamic languages are typically complex, poorly specified, and include features, such as eval and reflection, that are hard to analyze. In this paper, we introduce constraintbased dynamic type inference, a technique that infers static types based on dynamic program executions. In our approach, we wrap each runtime value to associate it with a type variable, and the wrapper generates constraints on this type variable when the wrapped value is used. This technique avoids many of the often overly conservative approximations of static tools, as constraints are generated based on how values are used during actual program runs. Using wrappers is also easy to implement, since we need only write a constraint resolution algorithm and a transformation to introduce the wrappers. The best part is that we can eat our cake, too: our algorithm will infer sound types as long as it observes every path through each method body note that the number of such paths may be dramatically smaller than the number of paths through the program as a whole. We have developed Rubydust, an implementation of our algorithm for Ruby. Rubydust takes advantage of Ruby's dynamic features to implement wrappers as a language library. We applied Rubydust to a number of small programs and found it to be both easy to use and useful: Rubydust discovered 1 real type error, and all other inferred types were correct and readable.
p3533
aVStatic analyses are generally parametrized by an abstraction which is chosen from a family of abstractions. We are interested in flexible families of abstractions with many parameters, as these families can allow one to increase precision in ways tailored to the client without sacrificing scalability. For example, we consider klimited pointsto analyses where each call site and allocation site in a program can have a different k value. We then ask a natural question in this paper: What is the minimal (coarsest) abstraction in a given family which is able to prove a set of queries? In addressing this question, we make the following two contributions: (i) We introduce two machine learning algorithms for efficiently finding a minimal abstraction; and (ii) for a static race detector backed by a klimited pointsto analysis, we show empirically that minimal abstractions are actually quite coarse: It suffices to provide context/object sensitivity to a very small fraction (0.42.3%) of the sites to yield equally precise results as providing context/object sensitivity uniformly to all sites.
p3534
aVDynamic memory allocation is ubiquitous in today's runtime environments. Allocation and deallocation of objects during program execution may cause fragmentation and foil the program's ability to allocate objects. Robson has shown that a worst case scenario can create a space overhead within a factor of log(n) of the space that is actually required by the program, where n is the size of the largest possible object. Compaction can eliminate fragmentation, but is too costly to be run frequently. Many runtime systems employ partial compaction, in which only a small fraction of the allocated objects are moved. Partial compaction reduces some of the existing fragmentation at an acceptable cost. In this paper we study the effectiveness of partial compaction and provide the first rigorous lower and upper bounds on its effectiveness in reducing fragmentation at a low cost.
p3535
aVBuilding correct and efficient concurrent algorithms is known to be a difficult problem of fundamental importance. To achieve efficiency, designers try to remove unnecessary and costly synchronization. However, not only is this manual trialanderror process adhoc, time consuming and errorprone, but it often leaves designers pondering the question of: is it inherently impossible to eliminate certain synchronization, or is it that I was unable to eliminate it on this attempt and I should keep trying? In this paper we respond to this question. We prove that it is impossible to build concurrent implementations of classic and ubiquitous specifications such as sets, queues, stacks, mutual exclusion and readmodifywrite operations, that completely eliminate the use of expensive synchronization. We prove that one cannot avoid the use of either: i) readafterwrite (RAW), where a write to shared variable A is followed by a read to a different shared variable B without a write to B in between, or ii) atomic writeafterread (AWAR), where an atomic operation reads and then writes to shared locations. Unfortunately, enforcing RAW or AWAR is expensive on all current mainstream processors. To enforce RAW, memory ordering also called fence or barrier instructions must be used. To enforce AWAR, atomic instructions such as compareandswap are required. However, these instructions are typically substantially slower than regular instructions. Although algorithm designers frequently struggle to avoid RAW and AWAR, their attempts are often futile. Our result characterizes the cases where avoiding RAW and AWAR is impossible. On the flip side, our result can be used to guide designers towards new algorithms where RAW and AWAR can be eliminated.
p3536
aVPatternbased verification checks the correctness of the program executions that follow a given pattern, a regular expression over the alphabet of program transitions of the form w1* ... wn*. For multithreaded programs, the alphabet of the pattern is given by the synchronization operations between threads. We study the complexity of patternbased verification for abstracted multithreaded programs in which, as usual in program analysis, conditions have been replaced by nondeterminism (the technique works also for boolean programs). While unrestricted verification is undecidable for abstracted multithreaded programs with recursive procedures and PSPACEcomplete for abstracted multithreaded whileprograms, we show that patternbased verification is NPcomplete for both classes. We then conduct a multiparameter analysis in which we study the complexity in the number of threads, the number of procedures per thread, the size of the procedures, and the size of the pattern. We first show that no algorithm for patternbased verification can be polynomial in the number of threads, procedures per thread, or the size of the pattern (unless P=NP). Then, using recent results about Parikh images of regular languages and semilinear sets, we present an algorithm exponential in the number of threads, procedures per thread, and size of the pattern, but polynomial in the size of the procedures.
p3537
aVWe describe, implement and benchmark EigenCFA, an algorithm for accelerating higherorder controlflow analysis (specifically, 0CFA) with a GPU. Ultimately, our program transformations, reductions and optimizations achieve a factor of 72 speedup over an optimized CPU implementation. We began our investigation with the view that GPUs accelerate higharithmetic, dataparallel computations with a poor tolerance for branching. Taking that perspective to its limit, we reduced Shivers's abstractinterpretive 0CFA to an algorithm synthesized from linearalgebra operations. Central to this reduction were "abstract" Church encodings, and encodings of the syntax tree and abstract domains as vectors and matrices. A straightforward (densematrix) implementation of EigenCFA performed slower than a fast CPU implementation. Ultimately, sparsematrix data structures and operations turned out to be the critical accelerants. Because controlflow graphs are sparse in practice (up to 96% empty), our controlflow matrices are also sparse, giving the sparse matrix operations an overwhelming space and speed advantage. We also achieved speedups by carefully permitting data races. The monotonicity of 0CFA makes it sound to perform analysis operations in parallel, possibly using stale or even partiallyupdated data.
p3538
aVQuantum cryptographic systems have been commercially available, with a striking advantage over classical systems that their security and ability to detect the presence of eavesdropping are provable based on the principles of quantum mechanics. On the other hand, quantum protocol designers may commit much more faults than classical protocol designers since human intuition is much better adapted to the classical world than the quantum world. To offer formal techniques for modeling and verification of quantum protocols, several quantum extensions of process algebra have been proposed. One of the most serious issues in quantum process algebra is to discover a quantum generalization of the notion of bisimulation, which lies in a central position in process algebra, preserved by parallel composition in the presence of quantum entanglement, which has no counterpart in classical computation. Quite a few versions of bisimulation have been defined for quantum processes in the literature, but in the best case they are only proved to be preserved by parallel composition of purely quantum processes where no classical communications are involved. Many quantum cryptographic protocols, however, employ the LOCC (Local Operations and Classical Communications) scheme, where classical communications must be explicitly specified. So, a notion of bisimulation preserved by parallel composition in the circumstance of both classical and quantum communications is crucial for process algebra approach to verification of quantum cryptographic protocols. In this paper we introduce a novel notion of bisimulation for quantum processes and prove that it is congruent with respect to various process algebra combinators including parallel composition even when both classical and quantum communications are present. We also establish some basic algebraic laws for this bisimulation. In particular, we prove uniqueness of the solutions to recursive equations of quantum processes, which provides a powerful proof technique for verifying complex quantum protocols.
p3539
aVA number of deterministic parallel programming models with strong safety guarantees are emerging, but similar support for nondeterministic algorithms, such as branch and bound search, remains an open question. We present a language together with a type and effect system that supports nondeterministic computations with a deterministicbydefault guarantee: nondeterminism must be explicitly requested via special parallel constructs (marked nd), and any deterministic construct that does not execute any nd construct has deterministic inputoutput behavior. Moreover, deterministic parallel constructs are always equivalent to a sequential composition of their constituent tasks, even if they enclose, or are enclosed by, nd constructs. Finally, in the execution of nd constructs, interference may occur only between pairs of accesses guarded by atomic statements, so there are no data races, either between atomic statements and unguarded accesses (strong isolation) or between pairs of unguarded accesses (stronger than strong isolation alone). We enforce the guarantees at compile time with modular checking using novel extensions to a previously described effect system. Our effect system extensions also enable the compiler to remove unnecessary transactional synchronization. We provide a static semantics, dynamic semantics, and a complete proof of soundness for the language, both with and without the barrier removal feature. An experimental evaluation shows that our language can achieve good scalability for realistic parallel algorithms, and that the barrier removal techniques provide significant performance gains.
p3540
aVHighlevel loop transformations are a key instrument in mapping computational kernels to effectively exploit the resources in modern processor architectures. Nevertheless, selecting required compositions of loop transformations to achieve this remains a significantly challenging task; current compilers may be off by orders of magnitude in performance compared to handoptimized programs. To address this fundamental challenge, we first present a convex characterization of all distinct, semanticspreserving, multidimensional affine transformations. We then bring together algebraic, algorithmic, and performance analysis results to design a tractable optimization algorithm over this highly expressive space. Our framework has been implemented and validated experimentally on a representative set of benchmarks running on stateoftheart multicore platforms.
p3541
aVThe technique of tracebased justintime compilation was introduced by Bala et al. and was further developed by Gal et al. It currently enjoys success in Mozilla Firefox's JavaScript engine. A tracebased JIT compiler leverages runtime profiling to optimize frequentlyexecuted paths while enabling the optimized code to ``bail out'' to the original code when the path has been invalidated. This optimization strategy differs from those of other JIT compilers and opens the question of which trace optimizations are sound. In this paper we present a framework for reasoning about the soundness of trace optimizations, and we show that some traditional optimization techniques are sound when used in a trace compiler while others are unsound. The converse is also true: some trace optimizations are sound when used in a traditional compiler while others are unsound. So, traditional and trace optimizations form incomparable sets. Our setting is an imperative calculus for which tracing is explicitly spelled out in the semantics. We define optimization soundness via a notion of bisimulation, and we show that sound optimizations lead to confluence and determinacy of stores.
p3542
aVWe present a novel variation on the standard technique of selecting instructions by tiling an intermediatecode tree. Typical compilers use a different set of tiles for every target machine. By analyzing a formal model of machinelevel computation, we have developed a single set of tiles that is machineindependent while retaining the expressive power of machine code. Using this tileset, we reduce the number of tilers required from one per machine to one per architectural family (e.g., register architecture or stack architecture). Because the tiler is the part of the instruction selector that is most difficult to reason about, our technique makes it possible to retarget an instruction selector with significantly less effort than standard techniques. Retargeting effort is further reduced by applying an earlier result which generates the machinedependent implementation of our tileset automatically from a declarative description of instructions' semantics. Our design has the additional benefit of enabling modular reasoning about three aspects of code generation that are not typically separated: the semantics of the compiler's intermediate representation, the semantics of the target instruction set, and the techniques needed to generate good target code.
p3543
aVTypebased model checking algorithms for higherorder recursion schemes have recently emerged as a promising approach to the verification of functional programs. We introduce patternmatching recursion schemes (PMRS) as an accurate model of computation for functional programs that manipulate algebraic datatypes. PMRS are a natural extension of higherorder recursion schemes that incorporate patternmatching in the defining rules. This paper is concerned with the following (undecidable) verification problem: given a correctness property \u03c6, a functional program \u2118 (qua PMRS) and a regular input set \u2111, does every term that is reachable from \u2111 under rewriting by \u2118 satisfy \u03c6? To solve the PMRS verification problem, we present a sound semialgorithm which is based on modelchecking and counterexample guided abstraction refinement. Given a noinstance of the verification problem, the method is guaranteed to terminate. From an ordern PMRS and an input set generated by a regular tree grammar, our method constructs an ordern weak PMRS which overapproximates only the firstorder patternmatching behaviour, whilst remaining completely faithful to the higherorder control flow. Using a variation of Kobayashi's typebased approach, we show that the (trivial automaton) modelchecking problem for weak PMRS is decidable. When a violation of the property is detected in the abstraction which does not correspond to a violation in the model, the abstraction is automatically refined by `unfolding' the patternmatching rules in the program to give successively more and more accurate weak PMRS models.
p3544
aVIn this paper, we consider the semantic design and verified compilation of a Clike programming language for concurrent sharedmemory computation above x86 multiprocessors. The design of such a language is made surprisingly subtle by several factors: the relaxedmemory behaviour of the hardware, the effects of compiler optimisation on concurrent code, the need to support highperformance concurrent algorithms, and the desire for a reasonably simple programming model. In turn, this complexity makes verified (or verifying) compilation both essential and challenging. We define a concurrent relaxedmemory semantics for ClightTSO, an extension of CompCert's Clight in which the processor's memory model is exposed for highperformance code. We discuss a strategy for verifying compilation from ClightTSO to x86, which we validate with correctness proofs (building on CompCert) for the most interesting compiler phases.
p3545
aVWe introduce streaming data string transducers that map input data strings to output data strings in a single lefttoright pass in linear time. Data strings are (unbounded) sequences of data values, tagged with symbols from a finite set, over a potentially infinite data domain that supports only the operations of equality and ordering. The transducer uses a finite set of states, a finite set of variables ranging over the data domain, and a finite set of variables ranging over data strings. At every step, it can make decisions based on the next input symbol, updating its state, remembering the input data value in its data variables, and updating datastring variables by concatenating datastring variables and new symbols formed from data variables, while avoiding duplication. We establish PSPACE bounds for the problems of checking functional equivalence of two streaming transducers, and of checking whether a streaming transducer satisfies pre/post verification conditions specified by streaming acceptors over input/output datastrings. We identify a class of imperative and a class of functional programs, manipulating lists of data items, which can be effectively translated to streaming datastring transducers. The imperative programs dynamically modify a singlylinked heap by changing nextpointers of heapnodes and by adding new nodes. The main restriction specifies how the nextpointers can be used for traversal. We also identify an expressively equivalent fragment of functional programs that traverse a list using syntactically restricted recursive calls. Our results lead to algorithms for assertion checking and for checking functional equivalence of two programs, written possibly in different programming styles, for commonly used routines such as insert, delete, and reverse.
p3546
aVWe define a new logic, STRAND, that allows reasoning with heapmanipulating programs using deductive verification and SMT solvers. STRAND logic ("STRucture ANd Data" logic) formulas express constraints involving heap structures and the data they contain; they are defined over a class of pointerstructures R defined using MSOdefined relations over trees, and are of the form \u2203\u2192x\u2200\u2192y (\u2192x,\u2192) x" , where "\u03c6" is a monadic secondorder logic (MSO) formulawith additional quantification that combines structural constraints as well as dataconstraints, but where the dataconstraints are only allowed to refer to "\u2192x" and "\u2192y" The salient aspects of the logic are: (a) the logic is powerful, allowing existential and universal quantification over the nodes, and complex combinations of data and structural constraints; (b) checking Hoaretriples for linear blocks of statements with preconditions and postconditions expressed as Boolean combinations of existential and universal STRAND formulas reduces to satisfiability of a STRAND formula; (c) there are powerful decidable fragments of STRAND, one semantically defined and one syntactically defined, where the decision procedure works by combining the theory of MSO over trees and the quantifierfree theory of the underlying datalogic. We demonstrate the effectiveness and practicality of the logic by checking verification conditions generated in proving properties of several heapmanipulating programs, using a tool that combines an MSO decision procedure over trees (MONA) with an SMT solver for integer constraints (Z3).
p3547
aVSharedmemory concurrency in C and C++ is pervasive in systems programming, but has long been poorly defined. This motivated an ongoing shared effort by the standards committees to specify concurrent behaviour in the next versions of both languages. They aim to provide strong guarantees for racefree programs, together with new (but subtle) relaxedmemory atomic primitives for highperformance concurrent code. However, the current draft standards, while the result of careful deliberation, are not yet clear and rigorous definitions, and harbour substantial problems in their details. In this paper we establish a mathematical (yet readable) semantics for C++ concurrency. We aim to capture the intent of the current (`Final Committee') Draft as closely as possible, but discuss changes that fix many of its problems. We prove that a proposed x86 implementation of the concurrency primitives is correct with respect to the x86TSO model, and describe our Cppmem tool for exploring the semantics of examples, using code generated from our Isabelle/HOL definitions. Having already motivated changes to the draft standard, this work will aid discussion of any further changes, provide a correctness condition for compilers, and give a muchneeded basis for analysis and verification of concurrent C and C++ programs.
p3548
aVObject layout  the concrete inmemory representation of objects  raises many delicate issues in the case of the C++ language, owing in particular to multiple inheritance, C compatibility and separate compilation. This paper formalizes a family of C++ object layout schemes and mechanically proves their correctness against the operational semantics for multiple inheritance of Wasserrab et al. This formalization is flexible enough to account for spacesaving techniques such as empty base class optimization and tailpadding optimization. As an application, we obtain the first formal correctness proofs for realistic, optimized object layout algorithms, including one based on the popular "common vendor" Itanium C++ application binary interface. This work provides semantic foundations to discover and justify new layout optimizations; it is also a first step towards the verification of a C++ compiler frontend.
p3549
aVStatic analysis of multistaged programs is challenging because the basic assumption of conventional static analysis no longer holds: the program text itself is no longer a fixed static entity, but rather a dynamically constructed value. This article presents a semanticpreserving translation of multistaged callbyvalue programs into unstaged programs and a static analysis framework based on this translation. The translation is semanticpreserving in that every smallstep reduction of a multistaged program is simulated by the evaluation of its unstaged version. Thanks to this translation we can analyze multistaged programs with existing static analysis techniques that have been developed for conventional unstaged programs: we first apply the unstaging translation, then we apply conventional static analysis to the unstaged version, and finally we cast the analysis results back in terms of the original staged program. Our translation handles staging constructs that have been evolved to be useful in practice (typified in Lisp's quasiquotation): open code as values, unrestricted operations on references and intentional variablecapturing substitutions. This article omits references for which we refer the reader to our companion technical report.
p3550
aVWe consider programs for embedded realtime systems which use prioritydriven preemptive scheduling with task priorities adjusted dynamically according to the immediate ceiling priority protocol. For these programs, we provide static analyses for detecting data races between tasks running at different priorities as well as methods to guarantee transactional execution of procedures. Beyond that, we demonstrate how general techniques for value analyses can be adapted to this setting by developing a precise analysis of affine equalities.
p3551
aVDifferential privacy is a notion of confidentiality that protects the privacy of individuals while allowing useful computations on their private data. Deriving differential privacy guarantees for real programs is a difficult and errorprone task that calls for principled approaches and tool support. Approaches based on linear types and static analysis have recently emerged; however, an increasing number of programs achieve privacy using techniques that cannot be analyzed by these approaches. Examples include programs that aim for weaker, approximate differential privacy guarantees, programs that use the Exponential mechanism, and randomized programs that achieve differential privacy without using any standard mechanism. Providing support for reasoning about the privacy of such programs has been an open problem. We report on CertiPriv, a machinechecked framework for reasoning about differential privacy built on top of the Coq proof assistant. The central component of CertiPriv is a quantitative extension of a probabilistic relational Hoare logic that enables one to derive differential privacy guarantees for programs from first principles. We demonstrate the expressiveness of CertiPriv using a number of examples whose formal analysis is out of the reach of previous techniques. In particular, we provide the first machinechecked proofs of correctness of the Laplacian and Exponential mechanisms and of the privacy of randomized and streaming algorithms from the recent literature.
p3552
aVThe ideal software contract fully specifies the behavior of an operation. Often, in particular in the context of scripting languages, a full specification may be cumbersome to state and may not even be desired. In such cases, a partial specification, which describes selected aspects of the behavior, may be used to raise the confidence in an implementation of the operation to a reasonable level. We propose a novel kind of contract for objectbased languages that specifies the side effects of an operation with access permissions. An access permission contract uses sets of access paths to express read and write permissions for the properties of the objects accessible from the operation. We specify a monitoring semantics for access permission contracts and implement this semantics in a contract system for JavaScript. We prove soundness and stability of violation under increasing aliasing for our semantics. Applications of access permission contracts include enforcing modularity, testdriven development, program understanding, and regression testing. With respect to testing and understanding, we find that adding access permissions to contracts increases the effectiveness of error detection through contract monitoring by 613%.
p3553
aVWe develop logical mechanisms and procedures to facilitate the verification of full functional properties of inductive tree datastructures using recursion that are sound, incomplete, but terminating. Our contribution rests in a new extension of firstorder logic with recursive definitions called Dryad, a syntactical restriction on pre and postconditions of recursive imperative programs using Dryad, and a systematic methodology for accurately unfolding the footprint on the heap uncovered by the program that leads to finding simple recursive proofs using formula abstraction and calls to SMT solvers. We evaluate our methodology empirically and show that several complex tree datastructure algorithms can be checked against full functional specifications automatically, given pre and postconditions. This results in the first automatic terminating methodology for proving a wide variety of annotated algorithms on tree datastructures correct, including maxheaps, treaps, redblack trees, AVL trees, binomial heaps, and Btrees.
p3554
aVFinite automata and finite transducers are used in a wide range of applications in software engineering, from regular expressions to specification languages. We extend these classic objects with symbolic alphabets represented as parametric theories. Admitting potentially infinite alphabets makes this representation strictly more general and succinct than classical finite transducers and automata over strings. Despite this, the main operations, including composition, checking that a transducer is singlevalued, and equivalence checking for singlevalued symbolic finite transducers are effective given a decision procedure for the background theory. We provide novel algorithms for these operations and extend composition to symbolic transducers augmented with registers. Our base algorithms are unusual in that they are nonconstructive, therefore, we also supply a separate model generation algorithm that can quickly find counterexamples in the case two symbolic finite transducers are not equivalent. The algorithms give rise to a complete decidable algebra of symbolic transducers. Unlike previous work, we do not need any syntactic restriction of the formulas on the transitions, only a decision procedure. In practice we leverage recent advances in satisfiability modulo theory (SMT) solvers. We demonstrate our techniques on four case studies, covering a wide range of applications. Our techniques can synthesize string preimages in excess of 8,000 bytes in roughly a minute, and we find that our new encodings significantly outperform previous techniques in succinctness and speed of analysis.
p3555
aVWe present an extension of Scala that supports constraint programming over bounded and unbounded domains. The resulting language, Kaplan, provides the benefits of constraint programming while preserving the existing features of Scala. Kaplan integrates constraint and imperative programming by using constraints as an advanced control structure; the developers use the monadic 'for' construct to iterate over the solutions of constraints or branch on the existence of a solution. The constructs we introduce have simple semantics that can be understood as explicit enumeration of values, but are implemented more efficiently using symbolic reasoning. Kaplan programs can manipulate constraints at runtime, with the combined benefits of typesafe syntax trees and firstclass functions. The language of constraints is a functional subset of Scala, supporting arbitrary recursive function definitions over algebraic data types, sets, maps, and integers. Our implementation runs on a platform combining a constraint solver with a standard virtual machine. For constraint solving we use an algorithm that handles recursive function definitions through fair function unrolling and builds upon the stateofthe art SMT solver Z3. We evaluate Kaplan on examples ranging from enumeration of data structures to execution of declarative specifications. We found Kaplan promising because it is expressive, supporting a range of problem domains, while enabling fullspeed execution of programs that do not rely on constraint programming.
p3556
aVJavaScript has become a central technology of the web, but it is also the source of many security problems, including crosssite scripting attacks and malicious advertising code. Central to these problems is the fact that code from untrusted sources runs with full privileges. We implement information flow controls in Firefox to help prevent violations of data confidentiality and integrity. Most previous information flow techniques have primarily relied on either static type systems, which are a poor fit for JavaScript, or on dynamic analyses that sometimes get stuck due to problematic implicit flows, even in situations where the target web application correctly satisfies the desired security policy. We introduce faceted values, a new mechanism for providing information flow security in a dynamic manner that overcomes these limitations. Taking inspiration from secure multiexecution, we use faceted values to simultaneously and efficiently simulate multiple executions for different security levels, thus providing noninterference with minimal overhead, and without the reliance on the stuck executions of prior dynamic approaches.
p3557
aVThis paper shows that existing definitions of codeinjection attacks (e.g., SQLinjection attacks) are flawed. The flaws make it possible for attackers to circumvent existing mechanisms, by supplying codeinjecting inputs that are not recognized as such. The flaws also make it possible for benign inputs to be treated as attacks. After describing these flaws in conventional definitions of codeinjection attacks, this paper proposes a new definition, which is based on whether the symbols input to an application get used as (normalform) values in the application's output. Because values are already fully evaluated, they cannot be considered "code" when injected. This simple new definition of codeinjection attacks avoids the problems of existing definitions, improves our understanding of how and when such attacks occur, and enables us to evaluate the effectiveness of mechanisms for mitigating such attacks.
p3558
aVSince software systems are becoming increasingly more concurrent and distributed, modeling and analysis of interactions among their components is a crucial problem. In several application domains, messagebased communication is used as the interaction mechanism, and the communication contract among the components of the system is specified semantically as a state machine. In the serviceoriented computing domain such communication contracts are called "choreography" specifications. A choreography specification identifies allowable ordering of message exchanges in a distributed system. A fundamental question about a choreography specification is determining its realizability, i.e., given a choreography specification, is it possible to build a distributed system that communicates exactly as the choreography specifies? Checking realizability of choreography specifications has been an open problem for several years and it was not known if this was a decidable problem. In this paper we give necessary and sufficient conditions for realizability of choreographies. We implemented the proposed realizability check and our experiments show that it can efficiently determine the realizability of 1) web service choreographies, 2) Singularity OS channel contracts, and 3) UML collaboration (communication) diagrams.
p3559
aVWe propose a general formal model of isolated hierarchical parallel computations, and identify several fragments to match the concurrency constructs present in realworld programming languages such as Cilk and X10. By associating fundamental formal models (vector addition systems with recursive transitions) to each fragment, we provide a common platform for exposing the relative difficulties of algorithmic reasoning. For each case we measure the complexity of deciding statereachability for finitedata recursive programs, and propose algorithms for the decidable cases. The complexities which include PTIME, NP, EXPSPACE, and 2EXPTIME contrast with undecidable statereachability for recursive multithreaded programs.
p3560
aVToday's computer networks perform a bewildering array of tasks, from routing and access control, to traffic monitoring and load balancing. To support wireless users accessing services hosted in the cloud, enterprise and datacenter networks are under increasing pressure to support client mobility, virtualmachine migration, resource isolation between cloud services, and energyefficient operation. Yet, network administrators must configure the network through closed and proprietary interfaces to heterogeneous devices, such as routers, switches, firewalls, load balancers, network address translators, and intrusion detection systems. Not surprisingly, configuring these complex networks is expensive and errorprone, and innovation in network management proceeds at a snail's pace. During the past several years, the networking industry and research community have pushed for greater openness in networking software, and a clearer separation between networking devices and the software that controls them. This broad trend is known as Software Defined Networking (SDN). A hallmark of SDN is having an open interface for controller software running on a commodity computer to install packetprocessing rules in the underlying switches. In particular, the OpenFlow protocol (see www.openflow.org) has significant momentum. Many commercial switches support OpenFlow, and a number of campus, datacenter, and backbone networks have deployed the new technology. With the emergence of open interfaces to network devices, the time is ripe to rethink the design of network software, to put networking on a stronger foundation and foster innovation in networked services. The programming languages community can play a vital role in this transformation, by creating languages, compilers, runtime systems, and testing and verification techniques that raise the level of abstraction for programming the network. In this talk, we give an overview of Software Defined Networking, and survey the early programminglanguages research in this area. We also outline exciting opportunities for interdisciplinary research at the intersection of programming languages and computer networks.
p3561
aVSoftwaredefined networks (SDNs) are a new kind of network architecture in which a controller machine manages a distributed collection of switches by instructing them to install or uninstall packetforwarding rules and report traffic statistics. The recently formed Open Networking Consortium, whose members include Google, Facebook, Microsoft, Verizon, and others, hopes to use this architecture to transform the way that enterprise and data center networks are implemented. In this paper, we define a highlevel, declarative language, called NetCore, for expressing packetforwarding policies on SDNs. NetCore is expressive, compositional, and has a formal semantics. To ensure that a majority of packets are processed efficiently on switches instead of on the controller we present new compilation algorithms for NetCore and couple them with a new runtime system that issues rule installation commands and trafficstatistics queries to switches. Together, the compiler and runtime system generate efficient rules whenever possible and outperform the simple, manual techniques commonly used to program SDNs today. In addition, the algorithms we develop are generic, assuming only that the packetmatching capabilities available on switches satisfy some basic algebraic laws. Overall, this paper delivers a new design for a highlevel network programming language; an improved set of compiler algorithms; a new runtime system for SDN architectures; the first formal semantics and proofs of correctness in this domain; and an implementation and evaluation that demonstrates the performance benefits over traditional manual techniques.
p3562
aVPrograms written in dynamic languages make heavy use of features   runtime type tests, valueindexed dictionaries, polymorphism, and higherorder functions   that are beyond the reach of type systems that employ either purely syntactic or purely semantic reasoning. We present a core calculus, System D, that merges these two modes of reasoning into a single powerful mechanism of nested refinement types wherein the typing relation is itself a predicate in the refinement logic. System D coordinates SMTbased logical implication and syntactic subtyping to automatically typecheck sophisticated dynamic language programs. By coupling nested refinements with McCarthy's theory of finite maps, System D can precisely reason about the interaction of higherorder functions, polymorphism, and dictionaries. The addition of type predicates to the refinement logic creates a circularity that leads to unique technical challenges in the metatheory, which we solve with a novel stratification approach that we use to prove the soundness of System D.
p3563
aVProof, verification and analysis methods for termination all rely on two induction principles: (1) a variant function or induction on data ensuring progress towards the end and (2) some form of induction on the program structure. The abstract interpretation design principle is first illustrated for the design of new forward and backward proof, verification and analysis methods for safety. The safety collecting semantics defining the strongest safety property of programs is first expressed in a constructive fixpoint form. Safety proof and checking/verification methods then immediately follow by fixpoint induction. Static analysis of abstract safety properties such as invariance are constructively designed by fixpoint abstraction (or approximation) to (automatically) infer safety properties. So far, no such clear design principle did exist for termination so that the existing approaches are scattered and largely not comparable with each other. For (1), we show that this design principle applies equally well to potential and definite termination. The tracebased termination collecting semantics is given a fixpoint definition. Its abstraction yields a fixpoint definition of the best variant function. By further abstraction of this best variant function, we derive the Floyd/Turing termination proof method as well as new static analysis methods to effectively compute approximations of this best variant function. For (2), we introduce a generalization of the syntactic notion of struc tural induction (as found in Hoare logic) into a semantic structural induction based on the new semantic concept of inductive trace cover covering execution traces by segments, a new basis for formulating program properties. Its abstractions allow for generalized recursive proof, verification and static analysis methods by induction on both program structure, control, and data. Examples of particular instances include Floyd's handling of loop cutpoints as well as nested loops, Burstall's intermittent assertion total correctness proof method, and PodelskiRybalchenko transition invariants.
p3564
aVInterpolation is an important technique in verification and static analysis of programs. In particular, interpolants extracted from proofs of various properties are used in invariant generation and bounded model checking. A number of recent papers studies interpolation in various theories and also extraction of smaller interpolants from proofs. In particular, there are several algorithms for extracting of interpolants from socalled local proofs. The main contribution of this paper is a technique of minimising interpolants based on transformations of what we call the "grey area" of local proofs. Another contribution is a technique of transforming, under certain common conditions, arbitrary proofs into local ones. Unlike many other interpolation techniques, our technique is very general and applies to arbitrary theories. Our approach is implemented in the theorem prover Vampire and evaluated on a large number of benchmarks coming from firstorder theorem proving and bounded model checking using logic with equality, uninterpreted functions and linear integer arithmetic. Our experiments demonstrate the power of the new techniques: for example, it is not unusual that our proof transformation gives more than a tenfold reduction in the size of interpolants.
p3565
aVDespite recent successes, largescale proof development within proof assistants remains an arcane art that is extremely timeconsuming. We argue that this can be attributed to two profound shortcomings in the architecture of modern proof assistants. The first is that proofs need to include a large amount of minute detail; this is due to the rigidity of the proof checking process, which cannot be extended with domainspecific knowledge. In order to avoid these details, we rely on developing and using tactics, specialized procedures that produce proofs. Unfortunately, tactics are both hard to write and hard to use, revealing the second shortcoming of modern proof assistants. This is because there is no static knowledge about their expected use and behavior. As has recently been demonstrated, languages that allow typesafe manipulation of proofs, like Beluga, Delphin and VeriML, can be used to partly mitigate this second issue, by assigning rich types to tactics. Still, the architectural issues remain. In this paper, we build on this existing work, and demonstrate two novel ideas: an extensible conversion rule and support for static proof scripts. Together, these ideas enable us to support both userextensible proof checking, and sophisticated static checking of tactics, leading to a new point in the design space of future proof assistants. Both ideas are based on the interplay between a lightweight staging construct and the rich type information available.
p3566
aVFormal models serve in many roles in the programming language community. In its primary role, a model communicates the idea of a language design; the architecture of a language tool; or the essence of a program analysis. No matter which role it plays, however, a faulty model doesn't serve its purpose. One way to eliminate flaws from a model is to write it down in a mechanized formal language. It is then possible to state theorems about the model, to prove them, and to check the proofs. Over the past nine years, PLT has developed and explored a lightweight version of this approach, dubbed Redex. In a nutshell, Redex is a domainspecific language for semantic models that is embedded in the Racket programming language. The effort of creating a model in Redex is often no more burdensome than typesetting it with LaTeX; the difference is that Redex comes with tools for the semantics engineering life cycle.
p3567
aVIn this paper, we consider the problem of verifying threadstate properties of multithreaded programs in which the number of active threads cannot be statically bounded. Our approach is based on decomposing the task into two modules, where one reasons about data and the other reasons about control. The data module computes threadstate invariants (e.g., linear constraints over global variables and local variables of one thread) using the thread interference information computed by the control module. The control module computes a representation of thread interference, as an incrementally constructed data flow graph, using the data invariants provided by the data module. These invariants are used to rule out patterns of thread interference that can not occur in a real program execution. The two modules are incorporated into a feedback loop, so that the abstractions of data and interference are iteratively coarsened as the algorithm progresses (that is, they become weaker) until a fixed point is reached. Our approach is sound and terminating, and applicable to programs with infinite state (e.g., unbounded integers) and unboundedly many threads. The verification method presented in this paper has been implemented into a tool, called Duet. We demonstrate the effectiveness of our technique by verifying properties of a selection of Linux device drivers using Duet, and also compare Duet with previous work on verification of parameterized Boolean program using the Boolean abstractions of these drivers.
p3568
aVWe present an analysis which takes as its input a sequential program, augmented with annotations indicating potential parallelization opportunities, and a sequential proof, written in separation logic, and produces a correctlysynchronized parallelized program and proof of that program. Unlike previous work, ours is not an independence analysis; we insert synchronization constructs to preserve relevant dependencies found in the sequential program that may otherwise be violated by a naive translation. Separation logic allows us to parallelize finegrained patterns of resourceusage, moving beyond straightforward pointsto analysis. Our analysis works by using the sequential proof to discover dependencies between different parts of the program. It leverages these discovered dependencies to guide the insertion of synchronization primitives into the parallelized program, and to ensure that the resulting parallelized program satisfies the same specification as the original sequential program, and exhibits the same sequential behaviour. Our analysis is built using frame inference and abduction, two techniques supported by an increasing number of separation logic tools.
p3569
aVSeparation Logic has witnessed tremendous success in recent years in reasoning about programs that deal with heap storage. Its success owes to the fundamental principle that one should keep separate areas of the heap storage separate in program reasoning. However, the way Separation Logic deals with program variables continues to be based on traditional Hoare Logic without taking any benefit of the separation principle. This has led to unwieldy proof rules suffering from lack of clarity as well as questions surrounding their soundness. In this paper, we extend the separation idea to the treatment of variables in Separation Logic, especially Concurrent Separation Logic, using the system of Syntactic Control of Interference proposed by Reynolds in 1978. We extend the original system with permission algebras, making it more powerful and able to deal with the issues of concurrent programs. The result is a streamined presentation of Concurrent Separation Logic, whose rules are memorable and soundness obvious. We also include a discussion of how the new rules impact the semantics and devise static analysis techniques to infer the required permissions automatically.
p3570
aVHigherdimensional dependent type theory enriches conventional onedimensional dependent type theory with additional structure expressing equivalence of elements of a type. This structure may be employed in a variety of ways to capture rather coarse identifications of elements, such as a universe of sets considered modulo isomorphism. Equivalence must be respected by all families of types and terms, as witnessed computationally by a typegeneric program. Higherdimensional type theory has applications to code reuse for dependently typed programming, and to the formalization of mathematics. In this paper, we develop a novel judgemental formulation of a twodimensional type theory, which enjoys a canonicity property: a closed term of boolean type is definitionally equal to true or false. Canonicity is a necessary condition for a computational interpretation of type theory as a programming language, and does not hold for existing axiomatic presentations of higherdimensional type theory. The method of proof is a generalization of the NuPRL semantics, interpreting types as syntactic groupoids rather than equivalence relations.
p3571
aVFreefinement is an algorithm that constructs a sound refinement calculus from a verification system under certain conditions. In this paper, a verification system is any formal system for establishing whether an inductively defined term, typically a program, satisfies a specification. Examples of verification systems include Hoare logics and type systems. Freefinement first extends the term language to include specification terms, and builds a verification system for the extended language that is a sound and conservative extension of the original system. The extended system is then transformed into a sound refinement calculus. The resulting refinement calculus can interoperate closely with the verification system  it is even possible to reuse and translate proofs between them. Freefinement gives a semantics to refinement at an abstract level: it associates each term of the extended language with a set of terms from the original language, and refinement simply reduces this set. The paper applies freefinement to a simple type system for the lambda calculus and also to a Hoare logic.
p3572
aVWe present a general theory of Giffordstyle type and effect annotations, where effect annotations are sets of effects. Generality is achieved by recourse to the theory of algebraic effects, a development of Moggi's monadic theory of computational effects that emphasises the operations causing the effects at hand and their equational theory. The key observation is that annotation effects can be identified with operation symbols. We develop an annotated version of Levy's CallbyPushValue language with a kind of computations for every effect set; it can be thought of as a sequential, annotated intermediate language. We develop a range of validated optimisations (i.e., equivalences), generalising many existing ones and adding new ones. We classify these optimisations as structural, algebraic, or abstract: structural optimisations always hold; algebraic ones depend on the effect theory at hand; and abstract ones depend on the global nature of that theory (we give modularlycheckable sufficient conditions for their validity).
p3573
aVErasable coercions in System Feta, also known as retyping functions, are welltyped etaexpansions of the identity. They may change the type of terms without changing their behavior and can thus be erased before reduction. Coercions in Feta can model subtyping of known types and some displacement of quantifiers, but not subtyping assumptions nor certain forms of delayed type instantiation. We generalize Feta by allowing abstraction over retyping functions. We follow a general approach where computing with coercions can be seen as computing in the lambdacalculus but keeping track of which parts of terms are coercions. We obtain a language where coercions do not contribute to the reduction but may block it and are thus not erasable. We recover erasable coercions by choosing a weak reduction strategy and restricting coercion abstraction to valueforms or by restricting abstraction to coercions that are polymorphic in their domain or codomain. The latter variant subsumes Feta, Fsub, and MLF in a unified framework.
p3574
aVWe present a framework for leveraging dynamic analysis to find good abstractions for static analysis. A static analysis in our framework is parametrised. Our main insight is to directly and efficiently compute from a concrete trace, a necessary condition on the parameter configurations to prove a given query, and thereby prune the space of parameter configurations that the static analysis must consider. We provide constructive algorithms for two instance analyses in our framework: a flow and contextsensitive threadescape analysis and a flow and contextinsensitive pointsto analysis. We show the efficacy of these analyses, and our approach, on six Java programs comprising two million bytecodes: the threadescape analysis resolves 80% of queries on average, disproving 28% and proving 52%; the pointsto analysis resolves 99% of queries on average, disproving 29% and proving 70%.
p3575
aVData races are among the most reliable indicators of programming errors in concurrent software. For at least two decades, Lamport's happensbefore (HB) relation has served as the standard test for detecting races other techniques, such as locksetbased approaches, fail to be sound, as they may falsely warn of races. This work introduces a new relation, causallyprecedes (CP), which generalizes happensbefore to observe more races without sacrificing soundness. Intuitively, CP tries to capture the concept of happensbefore ordered events that must occur in the observed order for the program to observe the same values. What distinguishes CP from past predictive race detection approaches (which also generalize an observed execution to detect races in other plausible executions) is that CPbased race detection is both sound and of polynomial complexity. We demonstrate that the unique aspects of CP result in practical benefit. Applying CP to realworld programs, we successfully analyze serverlevel applications (e.g., Apache FtpServer) and show that traces longer than in past predictive race analyses can be analyzed in mere seconds to a few minutes. For these programs, CP race detection uncovers races that are hard to detect by repeated execution and HB race detection: a single run of CP race detection produces several races not discovered by 10 separate rounds of happensbefore race detection.
p3576
aVNominal sets are a different kind of set theory, with a more relaxed notion of finiteness. They offer an elegant formalism for describing lambdaterms modulo alphaconversion, or automata on data words. This paper is an attempt at defining computation in nominal sets. We present a rudimentary programming language, called Nlambda. The key idea is that it includes a native type for finite sets in the nominal sense. To illustrate the power of our language, we write short programs that process automata on data words.
p3577
aVWe show how to combine a general purpose type system for an existing language with support for programming with binders and contexts by refining the type system of ML with a restricted form of dependent types where index objects are drawn from contextual LF. This allows the user to specify formal systems within the logical framework LF and index ML types with contextual LF objects. Our language design keeps the index language generic only requiring decidability of equality of the index language providing a modular design. To illustrate the elegance and effectiveness of our language, we give programs for closure conversion and normalization by evaluation. Our three key technical contribution are: 1) We give a bidirectional type system for our core language which is centered around refinement substitutions instead of constraint solving. As a consequence, type checking is decidable and easy to trust, although constraint solving may be undecidable. 2) We give a bigstep environment based operational semantics with environments which lends itself to efficient implementation. 3) We prove our language to be type safe and have mechanized our theoretical development in the proof assistant Coq using the fresh approach to binding.
p3578
aVThe ACL2 theorem prover the current incarnation of "the" BoyerMoore theorem prover is a theorem prover for an extension of a firstorder, applicative subset of Common Lisp. The ACL2 system provides a useful specification and modeling language as well as a useful mechanical theorem proving environment. ACL2 is in use at several major microprocessor manufacturers to verify functional correctness of important components of commercial designs. This talk explores the design of ACL2 and the tradeoffs that have turned out to be pivotal to its success.
p3579
aVThis paper presents Vellvm (verified LLVM), a framework for reasoning about programs expressed in LLVM's intermediate representation and transformations that operate on it. Vellvm provides a mechanized formal semantics of LLVM's intermediate representation, its type system, and properties of its SSA form. The framework is built using the Coq interactive theorem prover. It includes multiple operational semantics and proves relations among them to facilitate different reasoning styles and proof techniques. To validate Vellvm's design, we extract an interpreter from the Coq formal semantics that can execute programs from LLVM test suite and thus be compared against LLVM reference implementations. To demonstrate Vellvm's practicality, we formalize and verify a previously proposed transformation that hardens C programs against spatial memory safety violations. Vellvm's tools allow us to extract a new, verified implementation of the transformation pass that plugs into the real LLVM infrastructure; its performance is competitive with the nonverified, adhoc original.
p3580
aVDespite the fact that approximate computations have come to dominate many areas of computer science, the field of program transformations has focused almost exclusively on traditional semanticspreserving transformations that do not attempt to exploit the opportunity, available in many computations, to acceptably trade off accuracy for benefits such as increased performance and reduced resource consumption. We present a model of computation for approximate computations and an algorithm for optimizing these computations. The algorithm works with two classes of transformations: substitution transformations (which select one of a number of available implementations for a given function, with each implementation offering a different combination of accuracy and resource consumption) and sampling transformations (which randomly discard some of the inputs to a given reduction). The algorithm produces a (1+\u03b5) randomized approximation to the optimal randomized computation (which minimizes resource consumption subject to a probabilistic accuracy specification in the form of a maximum expected error or maximum error variance).
p3581
aVVerifying program transformations usually requires proving that the resulting program (the target) refines or is equivalent to the original one (the source). However, the refinement relation between individual sequential threads cannot be preserved in general with the presence of parallel compositions, due to instruction reordering and the different granularities of atomic operations at the source and the target. On the other hand, the refinement relation defined based on fully abstract semantics of concurrent programs assumes arbitrary parallel environments, which is too strong and cannot be satisfied by many wellknown transformations. In this paper, we propose a RelyGuaranteebased Simulation (RGSim) to verify concurrent program transformations. The relation is parametrized with constraints of the environments that the source and the target programs may compose with. It considers the interference between threads and their environments, thus is less permissive than relations over sequential programs. It is compositional w.r.t. parallel compositions as long as the constraints are satisfied. Also, RGSim does not require semantics preservation under all environments, and can incorporate the assumptions about environments made by specific program transformations in the form of rely/guarantee conditions. We use RGSim to reason about optimizations and prove atomicity of concurrent objects. We also propose a general garbage collector verification framework based on RGSim, and verify the Boehm et al. concurrent marksweep GC.
p3582
aVStatic assertion checking of open programs requires setting up a precise harness to capture the environment assumptions. For instance, a library may require a file handle to be properly initialized before it is passed into it. A harness is used to set up or specify the appropriate preconditions before invoking methods from the program. In the absence of a precise harness, even the most precise automated static checkers are bound to report numerous false alarms. This often limits the adoption of static assertion checking in the hands of a user. In this work, we explore the possibility of automatically filtering away (or prioritizing) warnings that result from imprecision in the harness. We limit our attention to the scenario when one is interested in finding bugs due to concurrency. We define a warning to be an interleaved bug when it manifests on an input for which no sequential interleaving produces a warning. As we argue in the paper, limiting a static analysis to only consider interleaved bugs greatly reduces false positives during static concurrency analysis in the presence of an imprecise harness. We formalize interleaved bugs as a differential analysis between the original program and its sequential version and provide various techniques for finding them. Our implementation CBugs demonstrates that the scheme of finding interleaved bugs can alleviate the need to construct precise harnesses while checking reallife concurrent programs.
p3583
aVWe give an axiomatic presentation of sharingvialabelling for weak lambdacalculi, that makes it possible to formally compare many different approaches to fully lazy sharing, and obtain two important results. We prove that the known implementations of full laziness are all equivalent in terms of the number of betareductions performed, although they behave differently regarding the duplication of terms. We establish a link between the optimality theories of weak lambdacalculi and firstorder rewriting systems by expressing fully lazy lambdalifting in our framework, thus emphasizing the firstorder essence of weak reduction.
p3584
aVGradual typing lets programmers evolve their dynamically typed programs by gradually adding explicit type annotations, which confer benefits like improved performance and fewer runtime failures. However, we argue that such evolution often requires a giant leap, and that type inference can offer a crucial missing step. If omitted type annotations are interpreted as unknown types, rather than the dynamic type, then static types can often be inferred, thereby removing unnecessary assumptions of the dynamic type. The remaining assumptions of the dynamic type may then be removed by either reasoning outside the static type system, or restructuring the code. We present a type inference algorithm that can improve the performance of existing gradually typed programs without introducing any new runtime failures. To account for dynamic typing, types that flow in to an unknown type are treated in a fundamentally different manner than types that flow out. Furthermore, in the interests of backwardcompatibility, an escape analysis is conducted to decide which types are safe to infer. We have implemented our algorithm for ActionScript, and evaluated it on the SunSpider and V8 benchmark suites. We demonstrate that our algorithm can improve the performance of unannotated programs as well as recover most of the type annotations in annotated programs.
p3585
aVA lens is a bidirectional transformation between a pair of connected data structures, capable of translating an edit on one structure into an appropriate edit on the other. Many varieties of lenses have been studied, but none, to date, has offered a satisfactory treatment of how edits are represented. Many foundational accounts only consider edits of the form "overwrite the whole structure," leading to poor behavior in many situations by failing to track the associations between corresponding parts of the structures when elements are inserted and deleted in ordered lists, for example. Other theories of lenses do maintain these associations, either by annotating the structures themselves with change information or using auxiliary data structures, but every extant theory assumes that the entire original source structure is part of the information passed to the lens. We offer a general theory of edit lenses, which work with descriptions of changes to structures, rather than with the structures themselves. We identify a simple notion of "editable structure" a set of states plus a monoid of edits with a partial monoid action on the states and construct a semantic space of lenses between such structures, with natural laws governing their behavior. We show how a range of constructions from earlier papers on "statebased" lenses can be carried out in this space, including composition, products, sums, list operations, etc. Further, we show how to construct edit lenses for arbitrary containers in the sense of Abbott, Altenkirch, and Ghani. Finally, we show that edit lenses refine a wellknown formulation of statebased lenses, in the sense that every statebased lens gives rise to an edit lens over structures with a simple overwriteonly edit language, and conversely every edit lens on such structures gives rise to a statebased lens.
p3586
aVThe upcoming C and C++ revised standards add concurrency to the languages, for the first time, in the form of a subtle *relaxed memory model* (the *C++11 model*). This aims to permit compiler optimisation and to accommodate the differing relaxedmemory behaviours of mainstream multiprocessors, combining simple semantics for most code with highperformance *lowlevel atomics* for concurrency libraries. In this paper, we first establish two simpler but provably equivalent models for C++11, one for the full language and another for the subset without consume operations. Subsetting further to the fragment without lowlevel atomics, we identify a subtlety arising from atomic initialisation and prove that, under an additional condition, the model is equivalent to sequential consistency for racefree programs. We then prove our main result, the correctness of two proposed compilation schemes for the C++11 load and store concurrency primitives to Power assembly, having noted that an earlier proposal was flawed. (The main ideas apply also to ARM, which has a similar relaxed memory architecture.) This should inform the ongoing development of production compilers for C++11 and C1x, clarifies what properties of the machine architecture are required, and builds confidence in the C++11 and Power semantics.
p3587
aVWe present a formal operational semantics and its Coq mechanization for the C++ object model, featuring object construction and destruction, shared and repeated multiple inheritance, and virtual function call dispatch. These are key C++ language features for highlevel system programming, in particular for predictable and reliable resource management. This paper is the first to present a formal mechanized account of the metatheory of construction and destruction in C++, and applications to popular programming techniques such as "resource acquisition is initialization". We also report on irregularities and apparent contradictions in the ISO C++03 and C++11 standards.
p3588
aVThis paper describes an executable formal semantics of C. Being executable, the semantics has been thoroughly tested against the GCC torture test suite and successfully passes 99.2% of 776 test programs. It is the most complete and thoroughly tested formal definition of C to date. The semantics yields an interpreter, debugger, state space search tool, and model checker "for free". The semantics is shown capable of automatically finding program errors, both statically and at runtime. It is also used to enumerate nondeterministic behavior.
p3589
aVThere has been great interest in creating probabilistic programming languages to simplify the coding of statistical tasks; however, there still does not exist a formal language that simultaneously provides (1) continuous probability distributions, (2) the ability to naturally express custom probabilistic models, and (3) probability density functions (PDFs). This collection of features is necessary for mechanizing fundamental statistical techniques. We formalize the first probabilistic language that exhibits these features, and it serves as a foundational framework for extending the ideas to more general languages. Particularly novel are our type system for absolutely continuous (AC) distributions (those which permit PDFs) and our PDF calculation procedure, which calculates PDF s for a large class of AC distributions. Our formalization paves the way toward the rigorous encoding of powerful statistical reformulations.
p3590
aVIn objectoriented programming, unique permissions to object references are useful for checking correctness properties such as consistency of typestate and noninterference of concurrency. To be usable, unique permissions must be borrowed   for example, one must be able to read a unique reference out of a field, use it for something, and put it back. While one can null out the field and later reassign it, this paradigm is ungainly and requires unnecessary writes, potentially hurting cache performance. Therefore, in practice borrowing must occur in the type system, without requiring memory updates. Previous systems support borrowing with external alias analysis and/or explicit programmer management of fractional permissions. While these approaches are powerful, they are also awkward and difficult for programmers to understand. We present an integrated language and type system with unique, immutable, and shared permissions, together with new local permissions that say that a reference may not be stored to the heap. Our system also includes change permissions such as unique>>unique and unique>>none that describe how permissions flow in and out of method formal parameters. Together, these features support common patterns of borrowing, including borrowing multiple local permissions from a unique reference and recovering the unique reference when the local permissions go out of scope, without any explicit management of fractions in the source language. All accounting of fractional permissions is done by the type system "under the hood." We present the syntax and static and dynamic semantics of a formal core language and state soundness results. We also illustrate the utility and practicality of our design by using it to express several realistic examples.
p3591
aVJavaScript has become the most widely used language for clientside web programming. The dynamic nature of JavaScript makes understanding its code notoriously difficult, leading to buggy programs and a lack of adequate staticanalysis tools. We believe that logical reasoning has much to offer JavaScript: a simple description of program behaviour, a clear understanding of module boundaries, and the ability to verify security contracts. We introduce a program logic for reasoning about a broad subset of JavaScript, including challenging features such as prototype inheritance and "with". We adapt ideas from separation logic to provide tractable reasoning about JavaScript code: reasoning about easy programs is easy; reasoning about hard programs is possible. We prove a strong soundness result. All libraries written in our subset and proved correct with respect to their specifications will be wellbehaved, even when called by arbitrary JavaScript code.
p3592
aVFunctional reactive programming (FRP) is an elegant and successful approach to programming reactive systems declaratively. The high levels of abstraction and expressivity that make FRP attractive as a programming model do, however, often lead to programs whose resource usage is excessive and hard to predict. In this paper, we address the problem of space leaks in discretetime functional reactive programs. We present a functional reactive programming language that statically bounds the size of the dataflow graph a reactive program creates, while still permitting use of higherorder functions and highertype streams such as streams of streams. We achieve this with a novel linear type theory that both controls allocation and ensures that all recursive definitions are wellfounded. We also give a denotational semantics for our language by combining recent work on metric spaces for the interpretation of higherorder causal functions with lengthspace models of spacebounded computation. The resulting category is doubly closed and hence forms a model of the logic of bunched implications.
p3593
aVThere has been great progress in recent years on developing effective techniques for reasoning about program equivalence in MLlike languages that is, languages that combine features like higherorder functions, recursive types, abstract types, and general mutable references. Two of the most prominent types of techniques to have emerged are *bisimulations* and *Kripke logical relations (KLRs)*. While both approaches are powerful, their complementary advantages have led us and other researchers to wonder whether there is an essential tradeoff between them. Furthermore, both approaches seem to suffer from fundamental limitations if one is interested in scaling them to interlanguage reasoning. In this paper, we propose *relation transition systems (RTSs)*, which marry together some of the most appealing aspects of KLRs and bisimulations. In particular, RTSs show how bisimulations' support for reasoning about recursive features via *coinduction* can be synthesized with KLRs' support for reasoning about local state via *state transition systems*. Moreover, we have designed RTSs to avoid the limitations of KLRs and bisimulations that preclude their generalization to interlanguage reasoning. Notably, unlike KLRs, RTSs are transitively composable.
p3594
aVComputation is a physical process which, like all other physical processes, is fundamentally reversible. From the notion of type isomorphisms, we derive a typed, universal, and reversible computational model in which information is treated as a linear resource that can neither be duplicated nor erased. We use this model as a semantic foundation for computation and show that the "gap" between conventional irreversible computation and logically reversible computation can be captured by a typeandeffect system. Our typeandeffect system is structured as an arrow metalanguage that exposes creation and erasure of information as explicit effect operations. Irreversible computations arise from interactions with an implicit information environment, thus making them a derived notion, much like open systems in Physics. We sketch several applications which can benefit from an explicit treatment of information effects, such as quantitative informationflow security and differential privacy.
p3595
aVIt is becoming increasingly important for applications to protect sensitive data. With current techniques, the programmer bears the burden of ensuring that the application's behavior adheres to policies about where sensitive values may flow. Unfortunately, privacy policies are difficult to manage because their global nature requires coordinated reasoning and enforcement. To address this problem, we describe a programming model that makes the system responsible for ensuring adherence to privacy policies. The programming model has two components: 1) core programs describing functionality independent of privacy concerns and 2) declarative, decentralized policies controlling how sensitive values are disclosed. Each sensitive value encapsulates multiple views; policies describe which views are allowed based on the output context. The system is responsible for automatically ensuring that outputs are consistent with the policies. We have implemented this programming model in a new functional constraint language named Jeeves. In Jeeves, sensitive values are introduced as symbolic variables and policies correspond to constraints that are resolved at output channels. We have implemented Jeeves as a Scala library using an SMT solver as a model finder. In this paper we describe the dynamic and static semantics of Jeeves and the properties about policy enforcement that the semantics guarantees. We also describe our experience implementing a conference management system and a social network.
p3596
aVReynolds' relational parametricity provides a powerful way to reason about programs in terms of invariance under changes of data representation. A dazzling array of applications of Reynolds' theory exists, exploiting invariance to yield "free theorems", noninhabitation results, and encodings of algebraic datatypes. Outside computer science, invariance is a common theme running through many areas of mathematics and physics. For example, the area of a triangle is unaltered by rotation or flipping. If we scale a triangle, then we scale its area, maintaining an invariant relationship between the two. The transformations under which properties are invariant are often organised into groups, with the algebraic structure reflecting the composability and invertibility of transformations. In this paper, we investigate programming languages whose types are indexed by algebraic structures such as groups of geometric transformations. Other examples include types indexed by principals for information flow security and types indexed by distances for analysis of analytic uniform continuity properties. Following Reynolds, we prove a general Abstraction Theorem that covers all these instances. Consequences of our Abstraction Theorem include free theorems expressing invariance properties of programs, type isomorphisms based on invariance properties, and nondefinability results indicating when certain algebraically indexed types are uninhabited or only inhabited by trivial programs. We have fully formalised our framework and most examples in Coq.
p3597
aVWe present a calculus for processing semistructured data that spans differences of application area among several novel query languages, broadly categorized as "NoSQL". This calculus lets users define their own operators, capturing a wider range of data processing capabilities, whilst providing a typing precision so far typical only of primitive hardcoded operators. The type inference algorithm is based on semantic type checking, resulting in type information that is both precise, and flexible enough to handle structured and semistructured data. We illustrate the use of this calculus by encoding a large fragment of Jaql, including operations and iterators over JSON, embedded SQL expressions, and cogrouping, and show how the encoding directly yields a typing discipline for Jaql as it is, namely without the addition of any type definition or type annotation in the code.
p3598
aVWe propose a general framework for abstraction with respect to quantitative properties, such as worstcase execution time, or power consumption. Our framework provides a systematic way for counterexample guided abstraction refinement for quantitative properties. The salient aspect of the framework is that it allows anytime verification, that is, verification algorithms that can be stopped at any time (for example, due to exhaustion of memory), and report approximations that improve monotonically when the algorithms are given more time. We instantiate the framework with a number of quantitative abstractions and refinement schemes, which differ in terms of how much quantitative information they keep from the original system. We introduce both statebased and tracebased quantitative abstractions, and we describe conditions that define classes of quantitative properties for which the abstractions provide overapproximations. We give algorithms for evaluating the quantitative properties on the abstract systems. We present algorithms for counterexample based refinements for quantitative properties for both statebased and segmentbased abstractions. We perform a case study on worstcase execution time of executables to evaluate the anytime verification aspect and the quantitative abstractions we proposed.
p3599
aVThe correctness of a sequential program can be shown by the annotation of its control flow graph with inductive assertions. We propose inductive data flow graphs, data flow graphs with incorporated inductive assertions, as the basis of an approach to verifying concurrent programs. An inductive data flow graph accounts for a set of dependencies between program actions in interleaved thread executions, and therefore stands as a representation for the set of concurrent program traces which give rise to these dependencies. The approach first constructs an inductive data flow graph and then checks whether all program traces are represented. The size of the inductive data flow graph is polynomial in the number of data dependencies (in a sense that can be made formal); it does not grow exponentially in the number of threads unless the data dependencies do. The approach shifts the burden of the exponential explosion towards the check whether all program traces are represented, i.e., to a combinatorial problem (over finite graphs).
p3600
aVModern satisfiability solvers implement an algorithm, called Conflict Driven Clause Learning, which combines search for a model with analysis of conflicts. We show that this algorithm can be generalised to solve the latticetheoretic problem of determining if an additive transformer on a Boolean lattice is always bottom. Our generalised procedure combines overapproximations of greatest fixed points with underapproximation of least fixed points to obtain more precise results than computing fixed points in isolation. We generalise implication graphs used in satisfiability solvers to derive underapproximate transformers from overapproximate ones. Our generalisation provides a new method for static analysers that operate over nondistributive lattices to reason about properties that require disjunction.
p3601
aVWe present a calculus which combines a simple, CCSlike representation of finite behaviors, with two dual binders \u03bb and \u03bb. Infinite behaviors are obtained through a syntactical fixedpoint operator, which is used to give a translation of \u03bbterms. The duality of the calculus makes the roles of a function and its environment symmetrical. As usual, the environment is allowed to call a function at any given point, each time with a different argument. Dually, the function is allowed to answer any given call, each time with a different behavior. This grants terms in our language the power of functional references. The inspiration for this language comes from game semantics. Indeed, its normal forms give a simple concrete syntax for finite strategies, which are inherently noninnocent. This very direct correspondence allows us to describe, in syntactical terms, a number of features from game semantics. The fixedpoint expansion of translated \u03bbterms corresponds to the generation of infinite plays from the finite views of an innocent strategy. The syntactical duality between terms and coterms corresponds to the duality between Player and Opponent. This duality also gives rise to a Bhmout lemma. The paper is divided into two parts. The first one is purely syntactical, and requires no background in game semantics. The second describes the fully abstract game model.
p3602
aVWe show that time complexity analysis of higherorder functional programs can be effectively reduced to an arguably simpler (although computationally equivalent) verification problem, namely checking firstorder inequalities for validity. This is done by giving an efficient inference algorithm for linear dependent types which, given a PCF term, produces in output both a linear dependent type and a cost expression for the term, together with a set of proof obligations. Actually, the output type judgement is derivable iff all proof obligations are valid. This, coupled with the already known relative completeness of linear dependent types, ensures that no information is lost, i.e., that there are no false positives or negatives. Moreover, the procedure reflects the difficulty of the original problem: simple PCF terms give rise to sets of proof obligations which are easy to solve. The latter can then be put in a format suitable for automatic or semiautomatic verification by external solvers. Ongoing experimental evaluation has produced encouraging results, which are briefly presented in the paper.
p3603
aVWe investigate impure, callbyvalue programming languages. Our first language only has variables and letbinding. Its equational theory is a variant of Lambek's theory of multicategories that omits the commutativity axiom. We demonstrate that type constructions for impure languages   products, sums and functions   can be characterized by universal properties in the setting of 'premulticategories', multicategories where the commutativity law may fail. This leads us to new, universal characterizations of two earlier equational theories of impure programming languages: the premonoidal categories of Power and Robinson, and the monadbased models of Moggi. Our analysis thus puts these earlier abstract ideas on a canonical foundation, bringing them to a new, syntactic level.
p3604
aVCoinduction is one of the most basic concepts in computer science. It is therefore surprising that the commonlyknown latticetheoretic accounts of the principles underlying coinductive proofs are lacking in two key respects: they do not support compositional reasoning (i.e. breaking proofs into separate pieces that can be developed in isolation), and they do not support incremental reasoning (i.e. developing proofs interactively by starting from the goal and generalizing the coinduction hypothesis repeatedly as necessary). In this paper, we show how to support coinductive proofs that are both compositional and incremental, using a dead simple construction we call the parameterized greatest fixed point. The basic idea is to parameterize the greatest fixed point of interest over the accumulated knowledge of "the proof so far". While this idea has been proposed before, by Winskel in 1989 and by Moss in 2001, neither of the previous accounts suggests its general applicability to improving the state of the art in interactive coinductive proof. In addition to presenting the latticetheoretic foundations of parameterized coinduction, demonstrating its utility on representative examples, and studying its composition with "upto" techniques, we also explore its mechanization in proof assistants like Coq and Isabelle. Unlike traditional approaches to mechanizing coinduction (e.g. Coq's cofix), which employ syntactic "guardedness checking", parameterized coinduction offers a semantic account of guardedness. This leads to faster and more robust proof development, as we demonstrate using our new Coq library, Paco.
p3605
aVFormalizing metatheory, or proofs about programming languages, in a proof assistant has many wellknown benefits. Unfortunately, the considerable effort involved in mechanizing proofs has prevented it from becoming standard practice. This cost can be amortized by reusing as much of existing mechanized formalizations as possible when building a new language or extending an existing one. One important challenge in achieving reuse is that the inductive definitions and proofs used in these formalizations are closed to extension. This forces language designers to cut and paste existing definitions and proofs in an adhoc manner and to expend considerable effort to patch up the results. The key contribution of this paper is the development of an induction technique for extensible Church encodings using a novel reinterpretation of the universal property of folds. These encodings provide the foundation for a framework, formalized in Coq, which uses type classes to automate the composition of proofs from modular components. This framework enables a more structured approach to the reuse of metatheory formalizations through the composition of modular inductive definitions and proofs. Several interesting language features, including binders and general recursion, illustrate the capabilities of our framework. We reuse these features to build fully mechanized definitions and proofs for a number of languages, including a version of miniML. Bounded induction enables proofs of properties for noninductive semantic functions, and mediating type classes enable proof adaptation for more featurerich languages.
p3606
aVEven with the assistance of computer tools, the formalized description and verification of researchlevel mathematics remains a daunting task, not least because of the talent with which mathematicians combine diverse theories to achieve their ends. By combining tools and techniques from type theory, language design, and software engineering we have managed to capture enough of these practices to formalize the proof of the Odd Order theorem, a landmark result in Group Theory.
p3607
aVWhile separation logic is acknowledged as an enabling technology for largescale program verification, most of the existing verification tools use only a fragment of separation logic that excludes separating implication. As the first step towards a verification tool using full separation logic, we develop a nested sequent calculus for Boolean BI (Bunched Implications), the underlying theory of separation logic, as well as a theorem prover based on it. A salient feature of our nested sequent calculus is that its sequent may have not only smaller child sequents but also multiple parent sequents, thus producing a graph structure of sequents instead of a tree structure. Our theorem prover is based on backward search in a refinement of the nested sequent calculus in which weakening and contraction are built into all the inference rules. We explain the details of designing our theorem prover and provide empirical evidence of its practicality.
p3608
aVWhen constructing complex concurrent systems, abstraction is vital: programmers should be able to reason about concurrent libraries in terms of abstract specifications that hide the implementation details. Relaxed memory models present substantial challenges in this respect, as libraries need not provide sequentially consistent abstractions: to avoid unnecessary synchronisation, they may allow clients to observe relaxed memory effects, and library specifications must capture these. In this paper, we propose a criterion for sound library abstraction in the new C11 and C++11 memory model, generalising the standard sequentially consistent notion of linearizability. We prove that our criterion soundly captures all clientlibrary interactions, both through call and return values, and through the subtle synchronisation effects arising from the memory model. To illustrate our approach, we verify implementations against specifications for the lockfree Treiber stack and a producerconsumer queue. Ours is the first approach to compositional reasoning for concurrent C11/C++11 programs.
p3609
aVBuilding distributed services and applications is challenging due to the pitfalls of distribution such as process and communication failures. A natural solution to these problems is to detect potential failures, and retry the failed computation and/or resend messages. Ensuring correctness in such an environment requires distributed services and applications to be idempotent. In this paper, we study the interrelated aspects of process failures, duplicate messages, and idempotence. We first introduce a simple core language (based on lambda calculus inspired by modern distributed computing platforms. This language formalizes the notions of a service, duplicate requests, process failures, data partitioning, and local atomic transactions that are restricted to a single store. We then formalize a desired (generic) correctness criterion for applications written in this language, consisting of idempotence (which captures the desired safety properties) and failurefreedom (which captures the desired progress properties). We then propose language support in the form of a monad that automatically ensures failfree idempotence. A key characteristic of our implementation is that it is decentralized and does not require distributed coordination. We show that the language support can be enriched with other useful constructs, such as compensations, while retaining the coordinationfree decentralized nature of the implementation. We have implemented the idempotence monad (and its variants) in F# and C# and used our implementation to build realistic applications on Windows Azure. We find that the monad has low runtime overheads and leads to more declarative applications.
p3610
aVOver the last decade, global descriptions have been successfully employed for the verification and implementation of communicating systems, respectively as protocol specifications and choreographies. In this work, we bring these two practices together by proposing a purelyglobal programming model. We show a novel interpretation of asynchrony and parallelism in a global setting and develop a typing discipline that verifies choreographies against protocol specifications, based on multiparty sessions. Exploiting the nature of global descriptions, our type system defines a new class of deadlockfree concurrent systems (deadlockfreedombydesign), provides type inference, and supports session mobility. We give a notion of Endpoint Projection (EPP) which generates correct entity code (as picalculus terms) from a choreography. Finally, we evaluate our approach by providing a prototype implementation for a concrete programming language and by applying it to some examples from multicore and serviceoriented programming.
p3611
aVWe introduce the concept of behavioral separation as a general principle for disciplining interference in higherorder imperative concurrent programs, and present a typebased approach that systematically develops the concept in the context of an MLlike language extended with concurrency and synchronization primitives. Behavioral separation builds on notions originally introduced for behavioral type systems and separation logics, but shifts the focus from the separation of static program state properties towards the separation of dynamic usage behaviors of runtime values. Behavioral separation types specify how values may be safely used by client code, and can enforce finegrained interference control disciplines while preserving compositionality, information hiding, and flexibility. We illustrate how our type system, even if based on a small set of general primitives, is already able to tackle fairly challenging program idioms, involving aliasing at various types, concurrency with firstclass threads, manipulation of linked data structures, behavioral borrowing, and invariantbased separation.
p3612
aVCompositional abstractions underly many reasoning principles for concurrent programs: the concurrent environment is abstracted in order to reason about a thread in isolation; and these abstractions are composed to reason about a program consisting of many threads. For instance, separation logic uses formulae that describe part of the state, abstracting the rest; when two threads use disjoint state, their specifications can be composed with the separating conjunction. Type systems abstract the state to the types of variables; threads may be composed when they agree on the types of shared variables. In this paper, we present the "Concurrent Views Framework", a metatheory of concurrent reasoning principles. The theory is parameterised by an abstraction of state with a notion of composition, which we call views. The metatheory is remarkably simple, but highly applicable: the relyguarantee method, concurrent separation logic, concurrent abstract predicates, type systems for recursive references and for unique pointers, and even an adaptation of the OwickiGries method can all be seen as instances of the Concurrent Views Framework. Moreover, our metatheory proves each of these systems is sound without requiring induction on the operational semantics.
p3613
aVSeparation logic is a powerful tool for reasoning about structured, imperative programs that manipulate pointers. However, its application to unstructured, lowerlevel languages such as assembly language or machine code remains challenging. In this paper we describe a separation logic tailored for this purpose that we have applied to x86 machinecode programs. The logic is built from an assertion logic on machine states over which we construct a specification logic that encapsulates uses of frames and step indexing. The traditional notion of Hoare triple is not applicable directly to unstructured machine code, where code and data are mixed together and programs do not in general run to completion, so instead we adopt a continuationpassing style of specification with preconditions alone. Nevertheless, the range of primitives provided by the specification logic, which include a higherorder frame connective, a novel readonly frame connective, and a 'later' modality, support the definition of derived forms to support structuredprogrammingstyle reasoning for common cases, in which standard rules for Hoare triples are derived as lemmas. Furthermore, our encoding of scoped assemblylanguage labels lets us give definitions and proof rules for powerful assemblylanguage 'macros' such as while loops, conditionals and procedures. We have applied the framework to a model of sequential x86 machine code built entirely within the Coq proof assistant, including tactic support based on computational reflection.
p3614
aVThere is a tradeoff between performance and correctness in implementing concurrent data structures. Better performance may be achieved at the expense of relaxing correctness, by redefining the semantics of data structures. We address such a redefinition of data structure semantics and present a systematic and formal framework for obtaining new data structures by quantitatively relaxing existing ones. We view a data structure as a sequential specification S containing all "legal" sequences over an alphabet of method calls. Relaxing the data structure corresponds to defining a distance from any sequence over the alphabet to the sequential specification: the krelaxed sequential specification contains all sequences over the alphabet within distance k from the original specification. In contrast to other existing work, our relaxations are semantic (distance in terms of data structure states). As an instantiation of our framework, we present two simple yet generic relaxation schemes, called outoforder and stuttering relaxation, along with several ways of computing distances. We show that the outoforder relaxation, when further instantiated to stacks, queues, and priority queues, amounts to tolerating bounded outoforder behavior, which cannot be captured by a purely syntactic relaxation (distance in terms of sequence manipulation, e.g. edit distance). We give concurrent implementations of relaxed data structures and demonstrate that bounded relaxations provide the means for trading correctness for performance in a controlled way. The relaxations are monotonic which further highlights the tradeoff: increasing k increases the number of permitted sequences, which as we demonstrate can lead to better performance. Finally, since a relaxed stack or queue also implements a pool, we actually have new concurrent pool implementations that outperform the stateoftheart ones.
p3615
aVWe develop a domain theory within nominal sets and present programming language constructs and results that can be gained from this approach. The development is based on the concept of orbitfinite subset, that is, a subset of a nominal sets that is both finitely supported and contained in finitely many orbits. This concept appears prominently in the recent research programme of Bojanczyk et al. on automata over infinite languages, and our results establish a connection between their work and a characterisation of topological compactness discovered, in a quite different setting, by Winskel and Turner as part of a nominal domain theory for concurrency. We use this connection to derive a notion of Scott domain within nominal sets. The functionals for existential quantification over names and `definite description' over names turn out to be compact in the sense appropriate for nominal Scott domains. Adding them, together with parallelor, to a programming language for recursively defined higherorder functions with name abstraction and locally scoped names, we prove a full abstraction result for nominal Scott domains analogous to Plotkin's classic result about PCF and conventional Scott domains: two program phrases have the same observable operational behaviour in all contexts if and only if they denote equal elements of the nominal Scott domain model. This is the first full abstraction result we know of for higherorder functions with local names that uses a domain theory based on ordinary extensional functions, rather than using the more intensional approach of game semantics.
p3616
aVRecent advances in verification have made it possible to envision trusted implementations of realworld languages. Java with its typesafety and fully specified semantics would appear to be an ideal candidate; yet, the complexity of the translation steps used in production virtual machines have made it a challenging target for verifying compiler technology. One of Java's key innovations, its memory model, poses significant obstacles to such an endeavor. The Java Memory Model is an ambitious attempt at specifying the behavior of multithreaded programs in a portable, hardware agnostic, way. While experts have an intuitive grasp of the properties that the model should enjoy, the specification is complex and not wellsuited for integration within a verifying compiler infrastructure. Moreover, the specification is given in an axiomatic style that is distant from the intuitive reorderingbased reasonings traditionally used to justify or rule out behaviors, and ill suited to the kind of operational reasoning one would expect to employ in a compiler. This paper takes a step back, and introduces a Buffered Memory Model (BMM) for Java. We choose a pragmatic point in the design space sacrificing generality in favor of a model that is fully characterized in terms of the reorderings it allows, amenable to formal reasoning, and which can be efficiently applied to a specific hardware family, namely x86 multiprocessors. Although the BMM restricts the reorderings compilers are allowed to perform, it serves as the key enabling device to achieving a verification pathway from bytecode to machine instructions. Despite its restrictions, we show that it is backwards compatible with the Java Memory Model and that it does not cripple performance on TSO architectures.
p3617
aVFinegrained concurrent data structures (or FCDs) reduce the granularity of critical sections in both time and space, thus making it possible for clients to access different parts of a mutable data structure in parallel. However, the tradeoff is that the implementations of FCDs are very subtle and tricky to reason about directly. Consequently, they are carefully designed to be contextual refinements of their coarsegrained counterparts, meaning that their clients can reason about them as if all access to them were sequentialized. In this paper, we propose a new semantic model, based on Kripke logical relations, that supports direct proofs of contextual refinement in the setting of a typesafe highlevel language. The key idea behind our model is to provide a simple way of expressing the "local life stories" of individual pieces of an FCD's hidden state by means of protocols that the threads concurrently accessing that state must follow. By endowing these protocols with a simple yet powerful transition structure, as well as the ability to assert invariants on both heap states and specification code, we are able to support clean and intuitive refinement proofs for the most sophisticated types of FCDs, such as conditional compareandset (CCAS).
p3618
aVDifferential privacy offers a way to answer queries about sensitive information while providing strong, provable privacy guarantees, ensuring that the presence or absence of a single individual in the database has a negligible statistical effect on the query's result. Proving that a given query has this property involves establishing a bound on the query's sensitivity how much its result can change when a single record is added or removed. A variety of tools have been developed for certifying that a given query differentially private. In one approach, Reed and Pierce [34] proposed a functional programming language, Fuzz, for writing differentially private queries. Fuzz uses linear types to track sensitivity and a probability monad to express randomized computation; it guarantees that any program with a certain type is differentially private. Fuzz can successfully verify many useful queries. However, it fails when the sensitivity analysis depends on values that are not known statically. We present DFuzz, an extension of Fuzz with a combination of linear indexed types and lightweight dependent types. This combination allows a richer sensitivity analysis that is able to certify a larger class of queries as differentially private, including ones whose sensitivity depends on runtime information. As in Fuzz, the differential privacy guarantee follows directly from the soundness theorem of the type system. We demonstrate the enhanced expressivity of DFuzz by certifying differential privacy for a broad class of iterative algorithms that could not be typed previously.
p3619
aVMany tools allow programmers to develop applications in highlevel languages and deploy them in web browsers via compilation to JavaScript. While practical and widely used, these compilers are ad hoc: no guarantee is provided on their correctness for whole programs, nor their security for programs executed within arbitrary JavaScript contexts. This paper presents a compiler with such guarantees. We compile an MLlike language with higherorder functions and references to JavaScript, while preserving all source program properties. Relying on typebased invariants and applicative bisimilarity, we show full abstraction: two programs are equivalent in all source contexts if and only if their wrapped translations are equivalent in all JavaScript contexts. We evaluate our compiler on sample programs, including a series of secure libraries.
p3620
aVA great deal of research on sanitizer placement, sanitizer correctness, checking path validity, and policy inference, has been done in the last five to ten years, involving type systems, static analysis and runtime monitoring and enforcement. However, in pretty much all work thus far, the burden of sanitizer placement has fallen on the developer. However, sanitizer placement in largescale applications is difficult, and developers are likely to make errors, and thus create security vulnerabilities. This paper advocates a radically different approach: we aim to fully automate the placement of sanitizers by analyzing the ow of tainted data in the program. We argue that developers are better off leaving out sanitizers entirely instead of trying to place them. This paper proposes a fully automatic technique for sanitizer placement. Placement is static whenever possible, switching to run time when necessary. Runtime taint tracking techniques can be used to track the source of a value, and thus apply appropriate sanitization. However, due to the runtime overhead of runtime taint tracking, our technique avoids it wherever possible.
p3621
aVA Bayesian model is based on a pair of probability distributions, known as the prior and sampling distributions. A wide range of fundamental machine learning tasks, including regression, classification, clustering, and many others, can all be seen as Bayesian models. We propose a new probabilistic programming abstraction, a typed Bayesian model, which is based on a pair of probabilistic expressions for the prior and sampling distributions. A sampler for a model is an algorithm to compute synthetic data from its sampling distribution, while a learner for a model is an algorithm for probabilistic inference on the model. Models, samplers, and learners form a generic programming pattern for modelbased inference. They support the uniform expression of common tasks including model testing, and generic compositions such as mixture models, evidencebased model averaging, and mixtures of experts. A formal semantics supports reasoning about model equivalence and implementation correctness. By developing a series of examples and three learner implementations based on exact inference, factor graphs, and Markov chain Monte Carlo, we demonstrate the broad applicability of this new programming pattern.
p3622
aVWe exploit the apparent similarity between (discretetime) stream processing and (continuoustime) signal processing and transfer a deductive verification framework from the former to the latter. Our development is based on rigorous semantics that relies on nonstandard analysis (NSA). Specifically, we start with a discrete framework consisting of a Lustrelike stream processing language, its Kahnstyle fixed point semantics, and a program logic (in the form of a type system) for partial correctness guarantees. This stream framework is transferred as it is to one for hyperstreams streams of streams, that typically arise from sampling (continuoustime) signals with progressively smaller intervals via the logical infrastructure of NSA. Under a certain continuity assumption we identify hyperstreams with signals; our final outcome thus obtained is a deductive verification framework of signals. In it one verifies properties of signals using the (conventionally discrete) proof principles, like fixed point induction.
p3623
aVEven welltyped programs can go wrong in modern functional languages, by encountering a patternmatch failure, or simply returning the wrong answer. An increasinglypopular response is to allow programmers to write contracts that express semantic properties, such as crashfreedom or some useful postcondition. We study the static verification of such contracts. Our main contribution is a novel translation to firstorder logic of both Haskell programs, and contracts written in Haskell, all justified by denotational semantics. This translation enables us to prove that functions satisfy their contracts using an offtheshelf firstorder logic theorem prover.
p3624
aVWe present Sigma*, a novel technique for learning symbolic models of software behavior. Sigma* addresses the challenge of synthesizing models of software by using symbolic conjectures and abstraction. By combining dynamic symbolic execution to discover symbolic inputoutput steps of the programs and counterexample guided abstraction refinement to overapproximate program behavior, Sigma* transforms arbitrary source representation of programs into faithful inputoutput models. We define a class of stream filters programs that process streams of data items for which Sigma* converges to a complete model if abstraction refinement eventually builds up a sufficiently strong abstraction. In other words, Sigma* is complete relative to abstraction. To represent inferred symbolic models, we use a variant of symbolic transducers that can be effectively composed and equivalence checked. Thus, Sigma* enables fully automatic analysis of behavioral properties such as commutativity, reversibility and idempotence, which is useful for web sanitizer verification and stream programs compiler optimizations, as we show experimentally. We also show how models inferred by Sigma* can boost performance of stream programs by parallelized code generation.
p3625
aVEffects are fundamental to programming languages. Even the lambda calculus has effects, and consequently the two famous evaluation strategies produce different semantics. As such, much research has been done to improve our understanding of effects. Since Moggi introduced monads for his computational lambda calculus, further generalizations have been designed to formalize increasingly complex computational effects, such as indexed monads followed by layered monads followed by parameterized monads. This succession prompted us to determine the most general formalization possible. In searching for this formalization we came across many surprises, such as the insufficiencies of arrows, as well as many unexpected insights, such as the importance of considering an effect as a small component of a whole system rather than just an isolated feature. In this paper we present our semantic formalization for producer effect systems, which we call a productor, and prove its maximal generality by focusing on only sequential composition of effectful computations, consequently guaranteeing that the existing monadic techniques are specializations of productors.
p3626
aVWe introduce bisimulation up to congruence as a technique for proving language equivalence of nondeterministic finite automata. Exploiting this technique, we devise an optimisation of the classical algorithm by Hopcroft and Karp. We compare our approach to the recently introduced antichain algorithms, by analysing and relating the two underlying coinductive proof methods. We give concrete examples where we exponentially improve over antichains; experimental results moreover show non negligible improvements.
p3627
aVExecutable biology presents new challenges to formal methods. This paper addresses two problems that cell biologists face when developing formally analyzable models. First, we show how to automatically synthesize a concurrent insilico model for cell development given invivo experiments of how particular mutations influence the experiment outcome. The problem of synthesis under mutations is unique because mutations may produce nondeterministic outcomes (presumably by introducing races between competing signaling pathways in the cells) and the synthesized model must be able to replay all these outcomes in order to faithfully describe the modeled cellular processes. In contrast, a "regular" concurrent program is correct if it picks any outcome allowed by the nondeterministic specification. We developed synthesis algorithms and synthesized a model of cell fate determination of the earthworm C. elegans. A version of this model previously took systems biologists months to develop. Second, we address the problem of underconstrained specifications that arise due to incomplete sets of mutation experiments. Underconstrained specifications give rise to distinct models, each explaining the same phenomenon differently. Addressing the ambiguity of specifications corresponds to analyzing the space of plausible models. We develop algorithms for detecting ambiguity in specifications, i.e., whether there exist alternative models that would produce different fates on some unperformed experiment, and for removing redundancy from specifications, i.e., computing minimal nonambiguous specifications. Additionally, we develop a modeling language and embed it into Scala. We describe how this language design and embedding allows us to build an efficient synthesizer. For our C. elegans case study, we infer two observationally equivalent models expressing different biological hypotheses through different protein interactions. One of these hypotheses was previously unknown to biologists.
p3628
aVPolyhedral compilation has been successful in the design and implementation of complex loop nest optimizers and parallelizing compilers. The algorithmic complexity and scalability limitations remain one important weakness. We address it using subpolyhedral underaproximations of the systems of constraints resulting from affine scheduling problems. We propose a subpolyhedral scheduling technique using (Unit)TwoVariablePerInequality or (U)TVPI Polyhedra. This technique relies on simple polynomial time algorithms to underapproximate a general polyhedron into (U)TVPI polyhedra. We modify the stateoftheart PLuTo compiler using our scheduling technique, and show that for a majority of the Polybench (2.0) kernels, the above underapproximations yield polyhedra that are nonempty. Solving the underapproximated system leads to asymptotic gains in complexity, and shows practically significant improvements when compared to a traditional LP solver. We also verify that code generated by our subpolyhedral parallelization prototype matches the performance of PLuTooptimized code when the underapproximation preserves feasibility.
p3629
aVHigh level data structures are a cornerstone of modern programming and at the same time stand in the way of compiler optimizations. In order to reason about user or librarydefined data structures compilers need to be extensible. Common mechanisms to extend compilers fall into two categories. Frontend macros, staging or partial evaluation systems can be used to programmatically remove abstraction and specialize programs before they enter the compiler. Alternatively, some compilers allow extending the internal workings by adding new transformation passes at different points in the compile chain or adding new intermediate representation (IR) types. None of these mechanisms alone is sufficient to handle the challenges posed by high level data structures. This paper shows a novel way to combine them to yield benefits that are greater than the sum of the parts. Instead of using staging merely as a front end, we implement internal compiler passes using staging as well. These internal passes delegate back to program execution to construct the transformed IR. Staging is known to simplify program generation, and in the same way it can simplify program transformation. Defining a transformation as a staged IR interpreter is simpler than implementing a lowlevel IR to IR transformer. With custom IR nodes, many optimizations that are expressed as rewritings from IR nodes to staged program fragments can be combined into a single pass, mitigating phase ordering problems. Speculative rewriting can preserve optimistic assumptions around loops. We demonstrate several powerful program optimizations using this architecture that are particularly geared towards data structures: a novel loop fusion and deforestation algorithm, array of struct to struct of array conversion, object flattening and code generation for heterogeneous parallel devices. We validate our approach using several non trivial case studies that exhibit order of magnitude speedups in experiments.
p3630
aVSeveral popular languages, such as Haskell, Python, and F#, use the indentation and layout of code as part of their syntax. Because contextfree grammars cannot express the rules of indentation, parsers for these languages currently use ad hoc techniques to handle layout. These techniques tend to be lowlevel and operational in nature and forgo the advantages of more declarative specifications like contextfree grammars. For example, they are often coded by hand instead of being generated by a parser generator. This paper presents a simple extension to contextfree grammars that can express these layout rules, and derives GLR and LR(k) algorithms for parsing these grammars. These grammars are easy to write and can be parsed efficiently. Examples for several languages are presented, as are benchmarks showing the practical efficiency of these algorithms.
p3631
aVPrograms manipulating mutable data structures with intrinsic sharing present a challenge for modular verification. Deep aliasing inside data structures dramatically complicates reasoning in isolation over parts of these objects because changes to one part of the structure (say, the left child of a dag node) can affect other parts (the right child or some of its descendants) that may point into it. The result is that finding intuitive and compositional proofs of correctness is usually a struggle. We propose a compositional proof system that enables local reasoning in the presence of sharing. While the AI "frame problem" elegantly captures the reasoning required to verify programs without sharing, we contend that natural reasoning about programs with sharing instead requires an answer to a different and more challenging AI problem, the "ramification problem": reasoning about the indirect consequences of actions. Accordingly, we present a RAMIFY proof rule that attacks the ramification problem headon and show how to reason with it. Our framework is valid in any separation logic and permits sound compositional and local reasoning in the context of both specified and unspecified sharing. We verify the correctness of a number of examples, including programs that manipulate dags, graphs, and overlaid data structures in nontrivial ways.
p3632
aVCraig interpolation has been a valuable tool for formal methods with interesting applications in program analysis and verification. Modern SMT solvers implement interpolation procedures for the theories that are most commonly used in these applications. However, many applicationspecific theories remain unsupported, which limits the class of problems to which interpolationbased techniques apply. In this paper, we present a generic framework to build new interpolation procedures via reduction to existing interpolation procedures. We consider the case where an applicationspecific theory can be formalized as an extension of a base theory with additional symbols and axioms. Our technique uses finite instantiation of the extension axioms to reduce an interpolation problem in the theory extension to one in the base theory. We identify a modeltheoretic criterion that allows us to detect the cases where our technique is complete. We discuss specific theories that are relevant in program verification and that satisfy this criterion. In particular, we obtain complete interpolation procedures for theories of arrays and linked lists. The latter is the first complete interpolation procedure for a theory that supports reasoning about complex shape properties of heapallocated data structures. We have implemented this procedure in a prototype on top of existing SMT solvers and used it to automatically infer loop invariants of listmanipulating programs.
p3633
aVIt is wellknown that floatingpoint exceptions can be disastrous and writing exceptionfree numerical programs is very difficult. Thus, it is important to automatically detect such errors. In this paper, we present Ariadne, a practical symbolic execution system specifically designed and implemented for detecting floatingpoint exceptions. Ariadne systematically transforms a numerical program to explicitly check each exception triggering condition. Ariadne symbolically executes the transformed program using real arithmetic to find candidate realvalued inputs that can reach and trigger an exception. Ariadne converts each candidate input into a floatingpoint number, then tests it against the original program. In general, approximating floatingpoint arithmetic with real arithmetic can change paths from feasible to infeasible and vice versa. The key insight of this work is that, for the problem of detecting floatingpoint exceptions, this approximation works well in practice because, if one input reaches an exception, many are likely to, and at least one of them will do so over both floatingpoint and real arithmetic. To realize Ariadne, we also devised a novel, practical linearization technique to solve nonlinear constraints. We extensively evaluated Ariadne over 467 scalar functions in the widely used GNU Scientific Library (GSL). Our results show that Ariadne is practical and identifies a large number of real runtime exceptions in GSL. The GSL developers confirmed our preliminary findings and look forward to Ariadne's public release, which we plan to do in the near future.
p3634
aVInductive datatypes provide mechanisms to define finite data such as finite lists and trees via constructors and allow programmers to analyze and manipulate finite data via pattern matching. In this paper, we develop a dual approach for working with infinite data structures such as streams. Infinite data inhabits coinductive datatypes which denote greatest fixpoints. Unlike finite data which is defined by constructors we define infinite data by observations. Dual to pattern matching, a tool for analyzing finite data, we develop the concept of copattern matching, which allows us to synthesize infinite data. This leads to a symmetric language design where pattern matching on finite and infinite data can be mixed. We present a core language for programming with infinite structures by observations together with its operational semantics based on (co)pattern matching and describe coverage of copatterns. Our language naturally supports both callbyname and callbyvalue interpretations and can be seamlessly integrated into existing languages like Haskell and ML. We prove type soundness for our language and sketch how copatterns open new directions for solving problems in the interaction of coinductive and dependent types.
p3635
aVThe widely studied I/O and idealcache models were developed to account for the large difference in costs to access memory at different levels of the memory hierarchy. Both models are based on a two level memory hierarchy with a fixed size primary memory(cache) of size M, an unbounded secondary memory organized in blocks of size B. The cost measure is based purely on the number of block transfers between the primary and secondary memory. All other operations are free. Many algorithms have been analyzed in these models and indeed these models predict the relative performance of algorithms much more accurately than the standard RAM model. The models, however, require specifying algorithms at a very low level requiring the user to carefully lay out their data in arrays in memory and manage their own memory allocation. In this paper we present a cost model for analyzing the memory efficiency of algorithms expressed in a simple functional language. We show how some algorithms written in standard forms using just lists and trees (no arrays) and requiring no explicit memory layout or memory management are efficient in the model. We then describe an implementation of the language and show provable bounds for mapping the cost in our model to the cost in the idealcache model. These bound imply that purely functional programs based on lists and trees with no special attention to any details of memory layout can be as asymptotically as efficient as the carefully designed imperative I/O efficient algorithms. For example we describe an O(n_B logM/Bn_B)cost sorting algorithm, which is optimal in the ideal cache and I/O models.
p3636
aVIn this paper we study the complexity of the Linear Ranking problem: given a loop, described by linear constraints over a finite set of integer variables, is there a linear ranking function for this loop? While existence of such a function implies termination, this problem is not equivalent to termination. When the variables range over the rationals or reals, the Linear Ranking problem is known to be PTIME decidable. However, when they range over the integers, whether for singlepath or multipath loops, the complexity of the Linear Ranking problem has not yet been determined. We show that it is coNPcomplete. However, we point out some special cases of importance of PTIME complexity. We also present complete algorithms for synthesizing linear ranking functions, both for the general case and the special PTIME cases.
p3637
aVWe present an efficient algorithm to reduce the size of nondeterministic Buchi word automata, while retaining their language. Additionally, we describe methods to solve PSPACEcomplete automata problems like universality, equivalence and inclusion for much larger instances (13 orders of magnitude) than before. This can be used to scale up applications of automata in formal verification tools and decision procedures for logical theories. The algorithm is based on new transition pruning techniques. These use criteria based on combinations of backward and forward trace inclusions. Since these relations are themselves PSPACEcomplete, we describe methods to compute good approximations of them in polynomial time. Extensive experiments show that the averagecase complexity of our algorithm scales quadratically. The size reduction of the automata depends very much on the class of instances, but our algorithm consistently outperforms all previous techniques by a wide margin. We tested our algorithm on Buchi automata derived from LTLformulae, many classes of random automata and automata derived from mutual exclusion protocols, and compared its performance to the wellknown automata tool GOAL.
p3638
aVWe present an automated approach to relatively completely verifying safety (i.e., reachability) property of higherorder functional programs. Our contribution is twofold. First, we extend the refinement type system framework employed in the recent work on (incomplete) automated higherorder verification by drawing on the classical work on relatively complete "Hoare logic like" program logic for higherorder procedural languages. Then, by adopting the recently proposed techniques for solving constraints over quantified firstorder logic formulas, we develop an automated type inference method for the type system, thereby realizing an automated relatively complete verification of higherorder programs.
p3639
aVThe C11 standard of the C programming language does not specify the execution order of expressions. Besides, to make more effective optimizations possible (eg. delaying of sideeffects and interleaving), it gives compilers in certain cases the freedom to use even more behaviors than just those of all execution orders. Widely used C compilers actually exploit this freedom given by the C standard for optimizations, so it should be taken seriously in formal verification. This paper presents an operational and axiomatic semantics (based on separation logic) for nondeterminism and sequence points in C. We prove soundness of our axiomatic semantics with respect to our operational semantics. This proof has been fully formalized using the Coq proof assistant.
p3640
aVRecent years have seen growing interest in highlevel languages for programming networks. But the design of these languages has been largely ad hoc, driven more by the needs of applications and the capabilities of network hardware than by foundational principles. The lack of a semantic foundation has left language designers with little guidance in determining how to incorporate new features, and programmers without a means to reason precisely about their code. This paper presents NetKAT, a new network programming language that is based on a solid mathematical foundation and comes equipped with a sound and complete equational theory. We describe the design of NetKAT, including primitives for filtering, modifying, and transmitting packets; union and sequential composition operators; and a Kleene star operator that iterates programs. We show that NetKAT is an instance of a canonical and wellstudied mathematical structure called a Kleene algebra with tests (KAT) and prove that its equational theory is sound and complete with respect to its denotational semantics. Finally, we present practical applications of the equational theory including syntactic techniques for checking reachability, proving noninterference properties that ensure isolation between programs, and establishing the correctness of compilation algorithms.
p3641
aVIt is often the case that increasing the precision of a program analysis leads to worse results. It is our thesis that this phenomenon is the result of fundamental limits on the ability to use precise abstract domains as the basis for inferring strong invariants of programs. We show that biasvariance tradeoffs, an idea from learning theory, can be used to explain why more precise abstractions do not necessarily lead to better results and also provides practical techniques for coping with such limitations. Learning theory captures precision using a combinatorial quantity called the VC dimension. We compute the VC dimension for different abstractions and report on its usefulness as a precision metric for program analyses. We evaluate cross validation, a technique for addressing biasvariance tradeoffs, on an industrial strength program verification tool called YOGI. The tool produced using cross validation has significantly better running time, finds new defects, and has fewer timeouts than the current production version. Finally, we make some recommendations for tackling biasvariance tradeoffs in program analysis.
p3642
aVThis article introduces an abstract interpretation framework that codifies the operations in SAT and SMT solvers in terms of lattices, transformers and fixed points. We develop the idea that a formula denotes a set of models in a universe of structures. This set of models has characterizations as fixed points of deduction, abduction and quantification transformers. A wide range of satisfiability procedures can be understood as computing and refining approximations of such fixed points. These include procedures in the DPLL family, those for preprocessing and inprocessing in SAT solvers, decision procedures for equality logics, weak arithmetics, and procedures for approximate quantification. Our framework provides a unified, mathematical basis for studying and combining program analysis and satisfiability procedures. A practical benefit of our work is a new, logicagnostic architecture for implementing solvers.
p3643
aVCounting arguments are among the most basic proof methods in mathematics. Within the field of formal verification, they are useful for reasoning about programs with infinite control, such as programs with an unbounded number of threads, or (concurrent) programs with recursive procedures. While counting arguments are common in informal, handwritten proofs of such programs, there are no fully automated techniques to construct counting arguments. The key questions involved in automating counting arguments are: how to decide what should be counted?, and how to decide when a counting argument is valid? In this paper, we present a technique for automatically constructing and checking counting arguments, which includes novel solutions to these questions.
p3644
aVSAFE is a cleanslate design for a highly secure computer system, with pervasive mechanisms for tracking and limiting information flows. At the lowest level, the SAFE hardware supports finegrained programmable tags, with efficient and flexible propagation and combination of tags as instructions are executed. The operating system virtualizes these generic facilities to present an informationflow abstract machine that allows user programs to label sensitive data with rich confidentiality policies. We present a formal, machinechecked model of the key hardware and software mechanisms used to control information flow in SAFE and an endtoend proof of noninterference for this model.
p3645
aVWe have developed and mechanically verified an ML system called CakeML, which supports a substantial subset of Standard ML. CakeML is implemented as an interactive readevalprint loop (REPL) in x8664 machine code. Our correctness theorem ensures that this REPL implementation prints only those results permitted by the semantics of CakeML. Our verification effort touches on a breadth of topics including lexing, parsing, type checking, incremental and dynamic compilation, garbage collection, arbitraryprecision arithmetic, and compiler bootstrapping. Our contributions are twofold. The first is simply in building a system that is endtoend verified, demonstrating that each piece of such a verification effort can in practice be composed with the others, and ensuring that none of the pieces rely on any oversimplifying assumptions. The second is developing novel approaches to some of the more challenging aspects of the verification. In particular, our formally verified compiler can bootstrap itself: we apply the verified compiler to itself to produce a verified machinecode implementation of the compiler. Additionally, our compiler proof handles diverging input programs with a lightweight approach based on logical timeout exceptions. The entire development was carried out in the HOL4 theorem prover.
p3646
aVRelational program logics have been used for mechanizing formal proofs of various cryptographic constructions. With an eye towards scaling these successes towards endtoend security proofs for implementations of distributed systems, we present RF*, a relational extension of F*, a generalpurpose higherorder stateful programming language with a verification system based on refinement types. The distinguishing feature of F* is a relational Hoare logic for a higherorder, stateful, probabilistic language. Through careful language design, we adapt the F* typechecker to generate both classic and relational verification conditions, and to automatically discharge their proofs using an SMT solver. Thus, we are able to benefit from the existing features of F*, including its abstraction facilities for modular reasoning about program fragments. We evaluate RF* experimentally by programming a series of cryptographic constructions and protocols, and by verifying their security properties, ranging from information flow to unlinkability, integrity, and privacy. Moreover, we validate the design of RF* by formalizing in Coq a core probabilistic \u03bb calculus and a relational refinement type system and proving the soundness of the latter against a denotational semantics of the probabilistic lambda \u03bb calculus.
p3647
aVWe present a new technique for parameter synthesis under boolean and quantitative objectives. The input to the technique is a "sketch"   a program with missing numerical parameters   and a probabilistic assumption about the program's inputs. The goal is to automatically synthesize values for the parameters such that the resulting program satisfies: (1) a {boolean specification}, which states that the program must meet certain assertions, and (2) a {quantitative specification}, which assigns a real valued rating to every program and which the synthesizer is expected to optimize. Our method   called smoothed proof search   reduces this task to a sequence of unconstrained smooth optimization problems that are then solved numerically. By iteratively solving these problems, we obtain parameter values that get closer and closer to meeting the boolean specification; at the limit, we obtain values that provably meet the specification. The approximations are computed using a new notion of smoothing for program abstractions, where an abstract transformer is approximated by a function that is continuous according to a metric over abstract states. We present a prototype implementation of our synthesis procedure, and experimental results on two benchmarks from the embedded control domain. The experiments demonstrate the benefits of smoothed proof search over an approach that does not meet the boolean and quantitative synthesis goals simultaneously.
p3648
aVWe present a constraintbased approach to computing winning strategies in twoplayer graph games over the state space of infinitestate programs. Such games have numerous applications in program verification and synthesis, including the synthesis of infinitestate reactive programs and branchingtime verification of infinitestate programs. Our method handles games with winning conditions given by safety, reachability, and general Linear Temporal Logic (LTL) properties. For each property class, we give a deductive proof rule that   provided a symbolic representation of the game players   describes a winning strategy for a particular player. Our rules are sound and relatively complete. We show that these rules can be automated by using an offtheshelf Horn constraint solver that supports existential quantification in clause heads. The practical promise of the rules is demonstrated through several case studies, including a challenging "CinderellaStepmother game" that allows infinite alternation of discrete and continuous choices by two players, as well as examples derived from prior work on program repair and synthesis.
p3649
aVWe introduce a Galois connection calculus for language independent specification of abstract interpretations used in programming language semantics, formal verification, and static analysis. This Galois connection calculus and its type system are typed by abstract interpretation.
p3650
aVWriting accurate numerical software is hard because of many sources of unavoidable uncertainties, including finite numerical precision of implementations. We present a programming model where the user writes a program in a realvalued implementation and specification language that explicitly includes different types of uncertainties. We then present a compilation algorithm that generates a finiteprecision implementation that is guaranteed to meet the desired precision with respect to real numbers. Our compilation performs a number of verification steps for different candidate precisions. It generates verification conditions that treat all sources of uncertainties in a unified way and encode reasoning about finiteprecision roundoff errors into reasoning about real numbers. Such verification conditions can be used as a standardized format for verifying the precision and the correctness of numerical programs. Due to their nonlinear nature, precise reasoning about these verification conditions remains difficult and cannot be handled using stateofthe art SMT solvers alone. We therefore propose a new procedure that combines exact SMT solving over reals with approximate and sound affine and interval arithmetic. We show that this approach overcomes scalability limitations of SMT solvers while providing improved precision over affine and interval arithmetic. Our implementation gives promising results on several numerical models, including dynamical systems, transcendental functions, and controller implementations.
p3651
aVJohn Reynolds (19352013) was a pioneer of programming languages research. In this paper we pay tribute to the man, his ideas, and his influence.
p3652
aVDeterministicbyconstruction parallel programming models offer the advantages of parallel speedup while avoiding the nondeterministic, hardtoreproduce bugs that plague fully concurrent code. A principled approach to deterministicbyconstruction parallel programming with shared state is offered by LVars: shared memory locations whose semantics are defined in terms of an applicationspecific lattice. Writes to an LVar take the least upper bound of the old and new values with respect to the lattice, while reads from an LVar can observe only that its contents have crossed a specified threshold in the lattice. Although it guarantees determinism, this interface is quite limited. We extend LVars in two ways. First, we add the ability to "freeze" and then read the contents of an LVar directly. Second, we add the ability to attach event handlers to an LVar, triggering a callback when the LVar's value changes. Together, handlers and freezing enable an expressive and useful style of parallel programming. We prove that in a language where communication takes place through these extended LVars, programs are at worst quasideterministic: on every run, they either produce the same answer or raise an error. We demonstrate the viability of our approach by implementing a library for Haskell supporting a variety of LVarbased data structures, together with a case study that illustrates the programming model and yields promising parallel speedup.
p3653
aVGeographically distributed systems often rely on replicated eventually consistent data stores to achieve availability and performance. To resolve conflicting updates at different replicas, researchers and practitioners have proposed specialized consistency protocols, called replicated data types, that implement objects such as registers, counters, sets or lists. Reasoning about replicated data types has however not been on par with comparable work on abstract data types and concurrent data types, lacking specifications, correctness proofs, and optimality results. To fill in this gap, we propose a framework for specifying replicated data types using relations over events and verifying their implementations using replicationaware simulations. We apply it to 7 existing implementations of 4 data types with nontrivial conflictresolution strategies and optimizations (lastwriterwins register, counter, multivalue register and observedremove set). We also present a novel technique for obtaining lower bounds on the worstcase space overhead of data type implementations and use it to prove optimality of 4 implementations. Finally, we show how to specify consistency of replicated stores with multiple objects axiomatically, in analogy to prior work on weak memory models. Overall, our work provides foundational reasoning tools to support research on replicated eventually consistent stores.
p3654
aVWe address the verification problem of eventual consistency of optimistic replication systems. Such systems are typically used to implement distributed data structures over large scale networks. We introduce a formal definition of eventual consistency that applies to a wide class of existing implementations, including the ones using speculative executions. Then, we reduce the problem of checking eventual consistency to reachability and model checking problems. This reduction enables the use of existing verification tools for messagepassing programs in the context of verifying optimistic replication systems. Furthermore, we derive from these reductions decision procedures for checking eventual consistency of systems implemented as finitestate programs communicating through unbounded unordered channels.
p3655
aVWe study bisimulation and context equivalence in a probabilistic lambdacalculus. The contributions of this paper are threefold. Firstly we show a technique for proving congruence of probabilistic applicative bisimilarity. While the technique follows Howe's method, some of the technicalities are quite different, relying on nontrivial "disentangling" properties for sets of real numbers. Secondly we show that, while bisimilarity is in general strictly finer than context equivalence, coincidence between the two relations is attained on pure lambdaterms. The resulting equality is that induced by LevyLongo trees, generally accepted as the finest extensional equivalence on pure lambdaterms under a lazy regime. Finally, we derive a coinductive characterisation of context equivalence on the whole probabilistic language, via an extension in which terms akin to distributions may appear in redex position. Another motivation for the extension is that its operational semantics allows us to experiment with a different congruence technique, namely that of logical bisimilarity.
p3656
aVProbabilistic coherence spaces (PCoh) yield a semantics of higherorder probabilistic computation, interpreting types as convex sets and programs as power series. We prove that the equality of interpretations in Pcoh characterizes the operational indistinguishability of programs in PCF with a random primitive. This is the first result of full abstraction for a semantics of probabilistic PCF. The key ingredient relies on the regularity of power series. Along the way to the theorem, we design a weighted intersection type assignment system giving a logical presentation of PCoh.
p3657
aVWe propose a new kind of probabilistic programming language for machine learning. We write programs simply by annotating existing relational schemas with probabilistic model expressions. We describe a detailed design of our language, Tabular, complete with formal semantics and type system. A rich series of examples illustrates the expressiveness of Tabular. We report an implementation, and show evidence of the succinctness of our notation relative to current best practice. Finally, we describe and verify a transformation of Tabular schemas so as to predict missing values in a concrete database. The ability to query for missing values provides a uniform interface to a wide variety of tasks, including classification, clustering, recommendation, and ranking.
p3658
aVSince the mid '80s, compiler writers for functional languages (especially lazy ones) have been writing papers about identifying and exploiting thunks and lambdas that are used only once. However it has proved difficult to achieve both power and simplicity in practice. We describe a new, modular analysis for a higherorder language, which is both simple and effective, and present measurements of its use in a fullscale, state of the art optimising compiler. The analysis finds many singleentry thunks and oneshot lambdas and enables a number of program optimisations.
p3659
aVThis article is the first part of a two articles series about a calculus with higherorder polymorphic functions, recursive types with arrow and product type constructors and settheoretic type connectives (union, intersection, and negation). In this first part we define and study the explicitlytyped version of the calculus in which type instantiation is driven by explicit instantiation annotations. In particular, we define an explicitlytyped lambdacalculus with intersection types and an efficient evaluation model for it. In the second part, presented in a companion paper, we define a local type inference system that allows the programmer to omit explicit instantiation annotations, and a type reconstruction system that allows the programmer to omit explicit type annotations. The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higherorder polymorphic functional languages for semistructured data.
p3660
aVWhile many programmers appreciate the benefits of lazy programming at an abstract level, determining which parts of a concrete program to evaluate lazily poses a significant challenge for most of them. Over the past thirty years, experts have published numerous papers on the problem, but developing this level of expertise requires a significant amount of experience. We present a profilingbased technique that captures and automates this expertise for the insertion of laziness annotations into strict programs. To make this idea precise, we show how to equip a formal semantics with a metric that measures waste in an evaluation. Then we explain how to implement this metric as a dynamic profiling tool that suggests where to insert laziness into a program. Finally, we present evidence that our profiler's suggestions either match or improve on an expert's use of laziness in a range of realworld applications.
p3661
aVFunctional Reactive Programming (FRP) models reactive systems with events and signals, which have previously been observed to correspond to the "eventually" and "always" modalities of linear temporal logic (LTL). In this paper, we define a constructive variant of LTL with least fixed point and greatest fixed point operators in the spirit of the modal mucalculus, and give it a proofsasprograms interpretation as a foundational calculus for reactive programs. Previous work emphasized the propositionsastypes part of the correspondence between LTL and FRP; here we emphasize the proofsasprograms part by employing structural proof theory. We show that the type system is expressive enough to enforce liveness properties such as the fairness of schedulers and the eventual delivery of results. We illustrate programming in this calculus using (co)iteration operators. We prove type preservation of our operational semantics, which guarantees that our programs are causal. We give also a proof of strong normalization which provides justification that our programs are productive and that they satisfy liveness properties derived from their types.
p3662
aVStateless model checking is a powerful technique for program verification, which however suffers from an exponential growth in the number of explored executions. A successful technique for reducing this number, while still maintaining complete coverage, is Dynamic Partial Order Reduction (DPOR). We present a new DPOR algorithm, which is the first to be provably optimal in that it always explores the minimal number of executions. It is based on a novel class of sets, called source sets, which replace the role of persistent sets in previous algorithms. First, we show how to modify an existing DPOR algorithm to work with source sets, resulting in an efficient and simple to implement algorithm. Second, we extend this algorithm with a novel mechanism, called wakeup trees, that allows to achieve optimality. We have implemented both algorithms in a stateless model checking tool for Erlang programs. Experiments show that source sets significantly increase the performance and that wakeup trees incur only a small overhead in both time and space.
p3663
aVFirst order logic with transitive closure, and separation logic enable elegant interactive verification of heapmanipulating programs. However, undecidabilty results and high asymptotic complexity of checking validity preclude complete automatic verification of such programs, even when loop invariants and procedure contracts are specified as formulas in these logics. This paper tackles the problem of proceduremodular verification of reachability properties of heapmanipulating programs using efficient decision procedures that are complete: that is, a SAT solver must generate a counterexample whenever a program does not satisfy its specification. By (a) requiring each procedure modifies a fixed set of heap partitions and creates a bounded amount of heap sharing, and (b) restricting program contracts and loop invariants to use only deterministic paths in the heap, we show that heap reachability updates can be described in a simple manner. The restrictions force program specifications and verification conditions to lie within a fragment of firstorder logic with transitive closure that is reducible to effectively propositional logic, and hence facilitate sound, complete and efficient verification. We implemented a tool atop Z3 and report on preliminary experiments that establish the correctness of several programs that manipulate linked data structures.
p3664
aVPrefix sums are key building blocks in the implementation of many concurrent software applications, and recently much work has gone into efficiently implementing prefix sums to run on massively parallel graphics processing units (GPUs). Because they lie at the heart of many GPUaccelerated applications, the correctness of prefix sum implementations is of prime importance. We introduce a novel abstraction, the interval of summations, that allows scalable reasoning about implementations of prefix sums. We present this abstraction as a monoid, and prove a soundness and completeness result showing that a generic sequential prefix sum implementation is correct for an array of length $n$ if and only if it computes the correct result for a specific test case when instantiated with the interval of summations monoid. This allows correctness to be established by running a single test where the input and result require O(n lg(n)) space. This improves upon an existing result by Sheeran where the input requires O(n lg(n)) space and the result O(n2 \u005clg(n)) space, and is more feasible for large n than a method by Voigtlaender that uses O(n) space for the input and result but requires running O(n2) tests. We then extend our abstraction and results to the context of dataparallel programs, developing an automated verification method for GPU implementations of prefix sums. Our method uses static verification to prove that a generic prefix sum implementation is data racefree, after which functional correctness of the implementation can be determined by running a single test case under the interval of summations abstraction. We present an experimental evaluation using four different prefix sum algorithms, showing that our method is highly automatic, scales to large thread counts, and significantly outperforms Voigtlaender's method when applied to large arrays.
p3665
aVAn authenticated data structure (ADS) is a data structure whose operations can be carried out by an untrusted prover, the results of which a verifier can efficiently check as authentic. This is done by having the prover produce a compact proof that the verifier can check along with each operation's result. ADSs thus support outsourcing data maintenance and processing tasks to untrusted servers without loss of integrity. Past work on ADSs has focused on particular data structures (or limited classes of data structures), one at a time, often with support only for particular operations. This paper presents a generic method, using a simple extension to a MLlike functional programming language we call \u03bb\u2022 (lambdaauth), with which one can program authenticated operations over any data structure defined by standard type constructors, including recursive types, sums, and products. The programmer writes the data structure largely as usual and it is compiled to code to be run by the prover and verifier. Using a formalization of \u03bb\u2022 we prove that all welltyped \u03bb\u2022 programs result in code that is secure under the standard cryptographic assumption of collisionresistant hash functions. We have implemented \u03bb\u2022 as an extension to the OCaml compiler, and have used it to produce authenticated versions of many interesting data structures including binary search trees, redblack+ trees, skip lists, and more. Performance experiments show that our approach is efficient, giving up little compared to the handoptimized data structures developed previously.
p3666
aVJavaScript's flexible semantics makes writing correct code hard and writing secure code extremely difficult. To address the former problem, various forms of gradual typing have been proposed, such as Closure and TypeScript. However, supporting all common programming idioms is not easy; for example, TypeScript deliberately gives up type soundness for programming convenience. In this paper, we propose a gradual type system and implementation techniques that provide important safety and security guarantees. We present TS# , a gradual type system and sourcetosource compiler for JavaScript. In contrast to prior gradual type systems, TS# features full runtime reflection over three kinds of types: (1) simple types for higherorder functions, recursive datatypes and dictionarybased extensible records; (2) the type any, for dynamically typesafe TS# expressions; and (3) the type un, for untrusted, potentially malicious JavaScript contexts in which TS# is embedded. After typechecking, the compiler instruments the program with various checks to ensure the type safety of TS# despite its interactions with arbitrary JavaScript contexts, which are free to use eval, stack walks, prototype customizations, and other offensive features. The proof of our main theorem employs a form of typepreserving compilation, wherein we prove all the runtime invariants of the translation of TS# to JavaScript by showing that translated programs are welltyped in JS# , a previously proposed dependently typed language for proving functional correctness of JavaScript programs. We describe a prototype compiler, a secure runtime, and sample applications for TS#. Our examples illustrate how web security patterns that developers currently program in JavaScript (with much difficulty and still with dubious results) can instead be programmed naturally in TS#, retaining a flavor of idiomatic JavaScript, while providing strong safety guarantees by virtue of typing.
p3667
aVWe present a system, SIFT, for generating input filters that nullify integer overflow errors associated with critical program sites such as memory allocation or block copy sites. SIFT uses a static pro gram analysis to generate filters that discard inputs that may trigger integer overflow errors in the computations of the sizes of allocated memory blocks or the number of copied bytes in block copy operations. Unlike all previous techniques of which we are aware, SIFT is sound   if an input passes the filter, it will not trigger an integer overflow error at any analyzed site. Our results show that SIFT successfully analyzes (and therefore generates sound input filters for) 56 out of 58 memory allocation and block memory copy sites in analyzed input processing modules from five applications (VLC, Dillo, Swfdec, Swftools, and GIMP). These nullified errors include six known integer overflow vulnerabilities. Our results also show that applying these filters to 62895 realworld inputs produces no false positives. The analysis and filter generation times are all less than a second.
p3668
aVIn this paper, we close the logical gap between provability in the logic BBI, which is the propositional basis for separation logic, and validity in an intended class of separation models, as employed in applications of separation logic such as program verification. An intended class of separation models is usually specified by a collection of axioms describing the specific model properties that are expected to hold, which we call a separation theory. Our main contributions are as follows. First, we show that several typical properties of separation theories are not definable in BBI. Second, we show that these properties become definable in a suitable hybrid extension of BBI, obtained by adding a theory of naming to BBI in the same way that hybrid logic extends normal modal logic. The binderfree extension captures most of the properties we consider, and the full extension HyBBI(\u2193) with the usual \u2193 binder of hybrid logic covers all these properties. Third, we present an axiomatic proof system for our hybrid logic whose extension with any set of "pure" axioms is sound and complete with respect to the models satisfying those axioms. As a corollary of this general result, we obtain, in a parametric manner, a sound and complete axiomatic proof system for any separation theory from our considered class. To the best of our knowledge, this class includes all separation theories appearing in the published literature.
p3669
aVAbstract separation logics are a family of extensions of Hoare logic for reasoning about programs that mutate memory. These logics are "abstract" because they are independent of any particular concrete memory model. Their assertion languages, called propositional abstract separation logics, extend the logic of (Boolean) Bunched Implications (BBI) in various ways. We develop a modular proof theory for various propositional abstract separation logics using cutfree labelled sequent calculi. We first extend the cutfee labelled sequent calculus for BBI of Hou et al to handle Calcagno et al's original logic of separation algebras by adding sound rules for partialdeterminism and cancellativity, while preserving cutelimination. We prove the completeness of our calculus via a sound intermediate calculus that enables us to construct countermodels from the failure to find a proof. We then capture other propositional abstract separation logics by adding sound rules for indivisible unit and disjointness, while maintaining completeness and cutelimination. We present a theorem prover based on our labelled calculus for these logics.
p3670
aVModule systems like that of Haskell permit only a weak form of modularity in which module implementations depend directly on other implementations and must be processed in dependency order. Module systems like that of ML, on the other hand, permit a stronger form of modularity in which explicit interfaces express assumptions about dependencies, and each module can be typechecked and reasoned about independently. In this paper, we present Backpack, a new language for building separatelytypecheckable *packages* on top of a weak module system like Haskell's. The design of Backpack is inspired by the MixML module calculus of Rossberg and Dreyer, but differs significantly in detail. Like MixML, Backpack supports explicit interfaces and recursive linking. Unlike MixML, Backpack supports a more flexible applicative semantics of instantiation. Moreover, its design is motivated less by foundational concerns and more by the practical concern of integration into Haskell, which has led us to advocate simplicity in both the syntax and semantics of Backpack over raw expressive power. The semantics of Backpack packages is defined by elaboration to sets of Haskell modules and binary interface files, thus showing how Backpack maintains interoperability with Haskell while extending it with separate typechecking. Lastly, although Backpack is geared toward integration into Haskell, its design and semantics are largely agnostic with respect to the details of the underlying core language.
p3671
aVSeparation logic is an extension of Hoare logic which is acknowledged as an enabling technology for largescale program verification. It features two new logical connectives, separating conjunction and separating implication, but most of the applications of separation logic have exploited only separating conjunction without considering separating implication. Nevertheless the power of separating implication has been well recognized and there is a growing interest in its use for program verification. This paper develops a proof system for full separation logic which supports not only separating conjunction but also separating implication. The proof system is developed in the style of sequent calculus and satisfies the admissibility of cut. The key challenge in the development is to devise a set of inference rules for manipulating heap structures that ensure the completeness of the proof system with respect to separation logic. We show that our proof of completeness directly translates to a proof search strategy.
p3672
aVInvariance is of paramount importance in programming languages and in physics. In programming languages, John Reynolds' theory of relational parametricity demonstrates that parametric polymorphic programs are invariant under change of data representation, a property that yields "free" theorems about programs just from their types. In physics, Emmy Noether showed that if the action of a physical system is invariant under change of coordinates, then the physical system has a conserved quantity: a quantity that remains constant for all time. Knowledge of conserved quantities can reveal deep properties of physical systems. For example, the conservation of energy is by Noether's theorem a consequence of a system's invariance under timeshifting. In this paper, we link Reynolds' relational parametricity with Noether's theorem for deriving conserved quantities. We propose an extension of System F$\u005comega$ with new kinds, types and term constants for writing programs that describe classical mechanical systems in terms of their Lagrangians. We show, by constructing a relationally parametric model of our extension of F$\u005comega$, that relational parametricity is enough to satisfy the hypotheses of Noether's theorem, and so to derive conserved quantities for free, directly from the polymorphic types of Lagrangians expressed in our system.
p3673
aVReynolds' theory of relational parametricity captures the invariance of polymorphically typed programs under change of data representation. Reynolds' original work exploited the typing discipline of the polymorphically typed lambdacalculus System F, but there is now considerable interest in extending relational parametricity to type systems that are richer and more expressive than that of System F. This paper constructs parametric models of predicative and impredicative dependent type theory. The significance of our models is twofold. Firstly, in the impredicative variant we are able to deduce the existence of initial algebras for all indexed=functors. To our knowledge, ours is the first account of parametricity for dependent types that is able to lift the useful deduction of the existence of initial algebras in parametric models of System F to the dependently typed setting. Secondly, our models offer conceptual clarity by uniformly expressing relational parametricity for dependent types in terms of reflexive graphs, which allows us to unify the interpretations of types and kinds, instead of taking the relational interpretation of types as a primitive notion. Expressing our model in terms of reflexive graphs ensures that it has canonical choices for the interpretations of the standard type constructors of dependent type theory, except for the interpretation of the universe of small types, where we formulate a refined interpretation tailored for relational parametricity. Moreover, our reflexive graph model opens the door to generalisations of relational parametricity, for example to higherdimensional relational parametricity.
p3674
aVWe consider an object calculus in which open terms interact with the environment through interfaces. The calculus is intended to capture the essence of contextual interactions of Middleweight Java code. Using game semantics, we provide fully abstract models for the induced notions of contextual approximation and equivalence. These are the first denotational models of this kind.
p3675
aVWe present abstract acceleration techniques for computing loop invariants for numerical programs with linear assignments and conditionals. Whereas abstract interpretation techniques typically overapproximate the set of reachable states iteratively, abstract acceleration captures the effect of the loop with a single, noniterative transfer function applied to the initial states at the loop head. In contrast to previous acceleration techniques, our approach applies to any linear loop without restrictions. Its novelty lies in the use of the Jordan normal form decomposition of the loop body to derive symbolic expressions for the entries of the matrix modeling the effect of \u03b7 \u2265 \u039f iterations of the loop. The entries of such a matrix depend on \u03b7 through complex polynomial, exponential and trigonometric functions. Therefore, we introduces an abstract domain for matrices that captures the linear inequality relations between these complex expressions. This results in an abstract matrix for describing the fixpoint semantics of the loop. Our approach integrates smoothly into standard abstract interpreters and can handle programs with nested loops and loops containing conditional branches. We evaluate it over small but complex loops that are commonly found in control software, comparing it with other tools for computing linear loop invariants. The loops in our benchmarks typically exhibit polynomial, exponential and oscillatory behaviors that present challenges to existing approaches. Our approach finds nontrivial invariants to prove useful bounds on the values of variables for such loops, clearly outperforming the existing approaches in terms of precision while exhibiting good performance.
p3676
aVSymbolic Automata extend classical automata by using symbolic alphabets instead of finite ones. Most of the classical automata algorithms rely on the alphabet being finite, and generalizing them to the symbolic setting is not a trivial task. In this paper we study the problem of minimizing symbolic automata. We formally define and prove the basic properties of minimality in the symbolic setting, and lift classical minimization algorithms (HuffmanMoore's and Hopcroft's algorithms) to symbolic automata. While Hopcroft's algorithm is the fastest known algorithm for DFA minimization, we show how, in the presence of symbolic alphabets, it can incur an exponential blowup. To address this issue, we introduce a new algorithm that fully benefits from the symbolic representation of the alphabet and does not suffer from the exponential blowup. We provide comprehensive performance evaluation of all the algorithms over large benchmarks and against existing stateoftheart implementations. The experiments show how the new symbolic algorithm is faster than previous implementations.
p3677
aVApplications in many areas of computing make discrete decisions under uncertainty, for reasons such as limited numerical precision in calculations and errors in sensorderived inputs. As a result, individual decisions made by such programs may be nondeterministic, and lead to contradictory decisions at different points of an execution. This means that an otherwise correct program may execute along paths, that it would not follow under its ideal semantics, violating essential program invariants on the way. A program is said to be consistent if it does not suffer from this problem despite uncertainty in decisions. In this paper, we present a sound, automatic program analysis for verifying that a program is consistent in this sense. Our analysis proves that each decision made along a program execution is consistent with the decisions made earlier in the execution. The proof is done by generating an invariant that abstracts the set of all decisions made along executions that end at a program location l, then verifying, using a fixpoint constraintsolver, that no contradiction can be derived when these decisions are combined with new decisions made at l. We evaluate our analysis on a collection of programs implementing algorithms in computational geometry. Consistency is known to be a critical, frequentlyviolated, and thoroughly studied correctness property in geometry, but ours is the first attempt at automated verification of consistency of geometric algorithms. Our benchmark suite consists of implementations of convex hull computation, triangulation, and point location algorithms. On almost all examples that are not consistent (with two exceptions), our analysis is able to verify consistency within a few minutes.
p3678
aVWe introduce a general way to locate programmer mistakes that are detected by static analyses such as type checking. The program analysis is expressed in a constraint language in which mistakes result in unsatisfiable constraints. Given an unsatisfiable system of constraints, both satisfiable and unsatisfiable constraints are analyzed, to identify the program expressions most likely to be the cause of unsatisfiability. The likelihood of different error explanations is evaluated under the assumption that the programmer's code is mostly correct, so the simplest explanations are chosen, following Bayesian principles. For analyses that rely on programmerstated assumptions, the diagnosis also identifies assumptions likely to have been omitted. The new error diagnosis approach has been implemented for two very different program analyses: type inference in OCaml and information flow checking in Jif. The effectiveness of the approach is evaluated using previously collected programs containing errors. The results show that when compared to existing compilers and other tools, the general technique identifies the location of programmer errors significantly more accurately.
p3679
aVChanging a program in response to a type error plays an important part in modern software development. However, the generation of good type error messages remains a problem for highly expressive type systems. Existing approaches often suffer from a lack of precision in locating errors and proposing remedies. Specifically, they either fail to locate the source of the type error consistently, or they report too many potential error locations. Moreover, the change suggestions offered are often incorrect. This makes the debugging process tedious and ineffective. We present an approach to the problem of type debugging that is based on generating and filtering a comprehensive set of typechange suggestions. Specifically, we generate all (programstructurepreserving) type changes that can possibly fix the type error. These suggestions will be ranked and presented to the programmer in an iterative fashion. In some cases we also produce suggestions to change the program. In most situations, this strategy delivers the correct change suggestions quickly, and at the same time never misses any rare suggestions. The computation of the potentially huge set of typechange suggestions is efficient since it is based on a variational type inference algorithm that type checks a program with variations only once, efficiently reusing type information for shared parts. We have evaluated our method and compared it with previous approaches. Based on a large set of examples drawn from the literature, we have found that our method outperforms other approaches and provides a viable alternative.
p3680
aVThe analysis of the energy consumption of software is an important goal for quantitative formal methods. Current methods, using weighted transition systems or energy games, model the energy source as an ideal resource whose status is characterized by one number, namely the amount of remaining energy. Real batteries, however, exhibit behaviors that can deviate substantially from an ideal energy resource. Based on a discretization of a standard continuous battery model, we introduce {\u005cem battery transition systems}. In this model, a battery is viewed as consisting of two parts   the availablecharge tank and the boundcharge tank. Any charge or discharge is applied to the availablecharge tank. Over time, the energy from each tank diffuses to the other tank. Battery transition systems are infinite state systems that, being not wellstructured, fall into no decidable class that is known to us. Nonetheless, we are able to prove that the $\u005comega$regular modelchecking problem is decidable for battery transition systems. We also present a case study on the verification of control programs for energyconstrained semiautonomous robots.
p3681
aVMost dependentlytyped programming languages either require that all expressions terminate (e.g. Coq, Agda, and Epigram), or allow infinite loops but are inconsistent when viewed as logics (e.g. Haskell, ATS, \u03a9mega. Here, we combine these two approaches into a single dependentlytyped core language. The language is composed of two fragments that share a common syntax and overlapping semantics: a logic that guarantees total correctness, and a callbyvalue programming language that guarantees type safety but not termination. The two fragments may interact: logical expressions may be used as programs; the logic may soundly reason about potentially nonterminating programs; programs can require logical proofs as arguments; and "mobile" program values, including proofs computed at runtime, may be used as evidence by the logic. This language allows programmers to work with total and partial functions uniformly, providing a smooth path from functional programming to dependentlytyped programming.
p3682
aVThe rise in efficiency of Satisfiability Modulo Theories (SMT) solvers has created numerous uses for them in software verification, program synthesis, functional programming, refinement types, etc. In all of these applications, SMT solvers are used for generating satisfying assignments (e.g., a witness for a bug) or proving unsatisfiability/validity(e.g., proving that a subtyping relation holds). We are often interested in finding not just an arbitrary satisfying assignment, but one that optimizes (minimizes/maximizes) certain criteria. For example, we might be interested in detecting program executions that maximize energy usage (performance bugs), or synthesizing short programs that do not make expensive API calls. Unfortunately, none of the available SMT solvers offer such optimization capabilities. In this paper, we present SYMBA, an efficient SMTbased optimization algorithm for objective functions in the theory of linear real arithmetic (LRA). Given a formula \u03c6 and an objective function t, SYMBA finds a satisfying assignment of \u03c6that maximizes the value of t. SYMBA utilizes efficient SMT solvers as black boxes. As a result, it is easy to implement and it directly benefits from future advances in SMT solvers. Moreover, SYMBA can optimize a set of objective functions, reusing information between them to speed up the analysis. We have implemented SYMBA and evaluated it on a large number of optimization benchmarks drawn from program analysis tasks. Our results indicate the power and efficiency of SYMBA in comparison with competing approaches, and highlight the importance of its multiobjectivefunction feature.
p3683
aVWe give a denotational semantics for a regionbased effect system that supports type abstraction in the sense that only externally visible effects need to be tracked: nonobservable internal modifications, such as the reorganisation of a search tree or lazy initialisation, can count as 'pure' or 'read only'. This 'fictional purity' allows clients of a module to validate soundly more effectbased program equivalences than would be possible with previous semantics. Our semantics uses a novel variant of logical relations that maps types not merely to partial equivalence relations on values, as is commonly done, but rather to a proofrelevant generalisation thereof, namely setoids. The objects of a setoid establish that values inhabit semantic types, whilst its morphisms are understood as proofs of semantic equivalence. The transition to proofrelevance solves twoawkward problems caused by nave use of existential quantification in Kripke logical relations, namely failure of admissibility and spurious functional dependencies.
p3684
aVWe study fundamental properties of a generalisation of monad called parametric effect monad, and apply it to the interpretation of general effect systems whose effects have sequential composition operators. We show that parametric effect monads admit analogues of the structures and concepts that exist for monads, such as Kleisli triples, the state monad and the continuation monad, Plotkin and Power's algebraic operations, and the categorical \u252c\u252clifting. We also show a systematic method to generate both effects and a parametric effect monad from a monad morphism. Finally, we introduce two effect systems with explicit and implicit subeffecting, and discuss their denotational semantics and the soundness of effect systems.
p3685
aVFinding a denotational semantics for higher order quantum computation is a longstanding problem in the semantics of quantum programming languages. Most past approaches to this problem fell short in one way or another, either limiting the language to an unusably small finitary fragment, or giving up important features of quantum physics such as entanglement. In this paper, we propose a denotational semantics for a quantum lambda calculus with recursion and an infinite data type, using constructions from quantitative semantics of linear logic.
p3686
aVStandardization is a fundamental notion for connecting programming languages and rewriting calculi. Since both programming languages and calculi rely on substitution for defining their dynamics, explicit substitutions (ES) help further close the gap between theory and practice. This paper focuses on standardization for the linear substitution calculus, a calculus with ES capable of mimicking reduction in lambdacalculus and linear logic proofnets. For the latter, proofnets can be formalized by means of a simple equational theory over the linear substitution calculus. Contrary to other extant calculi with ES, our system can be equipped with a residual theory in the sense of Lvy, which is used to prove a lefttoright standardization theorem for the calculus with ES but without the equational theory. Such a theorem, however, does not lift from the calculus with ES to proofnets, because the notion of lefttoright derivation is not preserved by the equational theory. We then relax the notion of lefttoright standard derivation, based on a total order on redexes, to a more liberal notion of standard derivation based on partial orders. Our proofs rely on Gonthier, Lvy, and Mellis' axiomatic theory for standardization. However, we go beyond merely applying their framework, revisiting some of its key concepts: we obtain uniqueness (modulo) of standard derivations in an abstract way and we provide a coinductive characterization of their key abstract notion of external redex. This last point is then used to give a simple proof that linear head reduction  a nondeterministic strategy having a central role in the theory of linear logic  is standard.
p3687
aVTracing justintime compilation is a popular compilation schema for the efficient implementation of dynamic languages, which is commonly used for JavaScript, Python, and PHP. It relies on two key ideas. First, it monitors the execution of the program to detect socalled hot paths, i.e., the most frequently executed paths. Then, it uses some store information available at runtime to optimize hot paths. The result is a residual program where the optimized hot paths are guarded by sufficient conditions ensuring the equivalence of the optimized path and the original program. The residual program is persistently mutated during its execution, e.g., to add new optimized paths or to merge existing paths. Tracing compilation is thus fundamentally different than traditional static compilation. Nevertheless, despite the remarkable practical success of tracing compilation, very little is known about its theoretical foundations. We formalize tracing compilation of programs using abstract interpretation. The monitoring (viz., hot path detection) phase corresponds to an abstraction of the trace semantics that captures the most frequent occurrences of sequences of program points together with an abstraction of their corresponding stores, e.g., a type environment. The optimization (viz., residual program generation) phase corresponds to a transform of the original program that preserves its trace semantics up to a given observation as modeled by some abstraction. We provide a generic framework to express dynamic optimizations and to prove them correct. We instantiate it to prove the correctness of dynamic type specialization. We show that our framework is more general than a recent model of tracing compilation introduced in POPL~2011 by Guo and Palsberg (based on operational bisimulations). In our model we can naturally express hot path reentrance and common optimizations like deadstore elimination, which are either excluded or unsound in Guo and Palsberg's framework.
p3688
aVThe trivialautomaton model checking problem for higherorder recursion schemes has become a widely studied object in connection with the automatic verification of higherorder programs. The problem is formidably hard: despite considerable progress in recent years, no decision procedures have been demonstrated to scale robustly beyond recursion schemes that comprise more than a few hundred rewrite rules. We present a new, fixedparameter polynomial time algorithm, based on a novel, type directed form of abstraction refinement in which behaviours of a scheme are distinguished by the abstraction according to the intersection types that they inhabit (the properties that they satisfy). Unlike other intersection type approaches, our algorithm reasons both about acceptance by the property automaton and acceptance by its dual, simultaneously, in order to minimize the amount of work done by converging on the solution to a problem instance from both sides. We have constructed Preface, a prototype implementation of the algorithm, and assembled an extensive body of evidence to demonstrate empirically that the algorithm readily scales to recursion schemes of several thousand rules, well beyond the capabilities of current stateoftheart higherorder model checkers.
p3689
aVWe present a generic analysis approach to the imperative relationship update problem, in which destructive updates temporarily violate a global invariant of interest. Such invariants can be conveniently and concisely specified with dependent refinement types, which are efficient to check flowinsensitively. Unfortunately, while traditional flowinsensitive type checking is fast, it is inapplicable when the desired invariants can be temporarily broken. To overcome this limitation, past works have directly ratcheted up the complexity of the type analysis and associated type invariants, leading to inefficient analysis and verbose specifications. In contrast, we propose a generic lifting of modular refinement type analyses with a symbolic analysis to efficiently and effectively check concise invariants that hold almost everywhere. The result is an efficient, highly modular flowinsensitive type analysis to optimistically check the preservation of global relationship invariants that can fall back to a precise, disjunctive symbolic analysis when the optimistic assumption is violated. This technique permits programmers to temporarily break and then reestablish relationship invariants a flexibility that is crucial for checking relationships in realworld, imperative languages. A significant challenge is selectively violating the global type consistency invariant over heap locations, which we achieve via almost typeconsistent heaps. To evaluate our approach, we have encoded the problem of verifying the safety of reflective method calls in dynamic languages as a refinement type checking problem. Our analysis is capable of validating reflective call safety at interactive speeds on commonlyused ObjectiveC libraries and applications.
p3690
aVJavaScript is the most widely used web language for clientside applications. Whilst the development of JavaScript was initially just led by implementation, there is now increasing momentum behind the ECMA standardisation process. The time is ripe for a formal, mechanised specification of JavaScript, to clarify ambiguities in the ECMA standards, to serve as a trusted reference for highlevel language compilation and JavaScript implementations, and to provide a platform for highassurance proofs of language properties. We present JSCert, a formalisation of the current ECMA standard in the Coq proof assistant, and JSRef, a reference interpreter for JavaScript extracted from Coq to OCaml. We give a Coq proof that JSRef is correct with respect to JSCert and assess JSRef using test262, the ECMA conformance test suite. Our methodology ensures that JSCert is a comparatively accurate formulation of the English standard, which will only improve as time goes on. We have demonstrated that modern techniques of mechanised specification can handle the complexity of JavaScript.
p3691
aVThe programming languages (PL) research community has traditionally catered to the needs of professional programmers in the continuously evolving technical industry. However, there is a new opportunity that knocks our doors. The recent IT revolution has resulted in the masses having access to personal computing devices. More than 99% of these computer users are nonprogrammers and are today limited to being passive consumers of the software that is made available to them. Can we empower these users to more effectively leverage computers for their daily tasks? The formalisms, techniques, and tools developed in the PL and the formal methods research communities can play a pivotal role!
p3692
aVWe present a new approach for predicting program properties from massive codebases (aka "Big Code"). Our approach first learns a probabilistic model from existing data and then uses this model to predict properties of new, unseen programs. The key idea of our work is to transform the input program into a representation which allows us to phrase the problem of inferring program properties as structured prediction in machine learning. This formulation enables us to leverage powerful probabilistic graphical models such as conditional random fields (CRFs) in order to perform joint prediction of program properties. As an example of our approach, we built a scalable prediction engine called JSNice for solving two kinds of problems in the context of JavaScript: predicting (syntactic) names of identifiers and predicting (semantic) type annotations of variables. Experimentally, JSNice predicts correct names for 63% of name identifiers and its type annotation predictions are correct in 81% of the cases. In the first week since its release, JSNice was used by more than 30,000 developers and in only few months has become a popular tool in the JavaScript developer community. By formulating the problem of inferring program properties as structured prediction and showing how to perform both learning and inference in this context, our work opens up new possibilities for attacking a wide range of difficult problems in the context of "Big Code" including invariant generation, decompilation, synthesis and others.
p3693
aVWe present DReX, a declarative language that can express all regular stringto string transformations, and can still be efficiently evaluated. The class of regular string transformations has a robust theoretical foundation including multiple characterizations, closure properties, and decidable analysis questions, and admits a number of string operations such as insertion, deletion, substring swap, and reversal. Recent research has led to a characterization of regular string transformations using a primitive set of function combinators analogous to the definition of regular languages using regular expressions. While these combinators form the basis for the language DReX proposed in this paper, our main technical focus is on the complexity of evaluating the output of a DReX program on a given input string. It turns out that the natural evaluation algorithm involves dynamic programming, leading to complexity that is cubic in the length of the input string. Our main contribution is identifying a consistency restriction on the use of combinators in DReX programs, and a singlepass evaluation algorithm for consistent programs with time complexity that is linear in the length of the input string and polynomial in the size of the program. We show that the consistency restriction does not limit the expressiveness, and whether a DReX program is consistent can be checked efficiently. We report on a prototype implementation, and evaluate it using a representative set of text processing tasks.
p3694
aVStringmanipulating programs are an important class of programs with applications in malware detection, graphics, input sanitization for Web security, and largescale HTML processing. This paper extends prior work on BEK, an expressive domainspecific language for writing stringmanipulating programs, with algorithmic insights that make BEK both analyzable and dataparallel. By analyzable we mean that unlike most general purpose programming languages, many algebraic properties of a BEK program are decidable (i.e., one can check whether two programs commute or compute the inverse of a program). By dataparallel we mean that a BEK program can compute on arbitrary subsections of its input in parallel, thus exploiting parallel hardware. This latter requirement is particularly important for programs which operate on large data: without data parallelism, a programmer cannot hide the latency of reading data from various storage media (i.e., reading a terabyte of data from a modern hard drive takes about 3 hours). With a dataparallel approach, the system can split data across multiple disks and thus hide the latency of reading the data. A BEK program is expressive: a programmer can use conditionals, switch statements, and registers or local variables in order to implement common stringmanipulating programs. Unfortunately, this expressivity induces data dependencies, which are an obstacle to parallelism. The key contribution of this paper is an algorithm which automatically removes these data dependencies by mapping a B EK program into a intermediate format consisting of symbolic transducers, which extend classical transducers with symbolic predicates and symbolic assignments. We present a novel algorithm that we call exploration which performs symbolic loop unrolling of these transducers to obtain simplified versions of the original program. We show how these simplified versions can then be lifted to a stateless form, and from there compiled to dataparallel hardware. To evaluate the efficacy of our approach, we demonstrate up to 8x speedups for a number of realworld, BEK programs, (e.g., HTML encoder and decoder) on dataparallel hardware. To the best of our knowledge, these are the first data parallel implementation of these programs. To validate that our approach is correct, we use an automatic testing technique to compare our generated code to the original implementations and find no semantic deviations.
p3695
aVThe World Wide Web has evolved gradually from a document delivery platform to an architecture for distributed programming. This largely unplanned evolution is apparent in the set of interconnected languages and protocols that any Web application must manage. This paper presents Ur/Web, a domainspecific, statically typed functional programming language with a much simpler model for programming modern Web applications. Ur/Web's model is unified, where programs in a single programming language are compiled to other "Web standards" languages as needed; supports novel kinds of encapsulation of Webspecific state; and exposes simple concurrency, where programmers can reason about distributed, multithreaded applications via a mix of transactions and cooperative preemption. We give a tutorial introduction to the main features of Ur/Web and discuss the language implementation and the production Web applications that use it.
p3696
aVCurrent proposals for adding gradual typing to JavaScript, such as Closure, TypeScript and Dart, forgo soundness to deal with issues of scale, code reuse, and popular programming patterns. We show how to address these issues in practice while retaining soundness. We design and implement a new gradual type system, prototyped for expediency as a 'Safe' compilation mode for TypeScript. Our compiler achieves soundness by enforcing stricter static checks and embedding residual runtime checks in compiled code. It emits plain JavaScript that runs on stock virtual machines. Our main theorem is a simulation that ensures that the checks introduced by Safe TypeScript (1) catch any dynamic type error, and (2) do not alter the semantics of typesafe TypeScript code. Safe TypeScript is carefully designed to minimize the performance overhead of runtime checks. At its core, we rely on two new ideas: differential subtyping, a new form of coercive subtyping that computes the minimum amount of runtime type information that must be added to each object; and an erasure modality, which we use to safely and selectively erase type information. This allows us to scale our design to fullfledged TypeScript, including arrays, maps, classes, inheritance, overloading, and generic types. We validate the usability and performance of Safe TypeScript empirically by typechecking and compiling around 120,000 lines of existing TypeScript source code. Although runtime checks can be expensive, the endtoend overhead is small for code bases that already have type annotations. For instance, we bootstrap the Safe TypeScript compiler (90,000 lines including the base TypeScript compiler): we measure a 15% runtime overhead for type safety, and also uncover programming errors as type safety violations. We conclude that, at least during development and testing, subjecting JavaScript/TypeScript programs to safe gradual typing adds significant value to source type annotations at a modest cost.
p3697
aVThe standard algorithm for higherorder contract checking can lead to unbounded space consumption and can destroy tail recursion, altering a program's asymptotic space complexity. While space efficiency for gradual types contracts mediating untyped and typed code is well studied, sound space efficiency for manifest contracts contracts that check stronger properties than simple types, e.g., "is a natural'' instead of "is an integer'' remains an open problem. We show how to achieve sound space efficiency for manifest contracts with strong predicate contracts. The essential trick is breaking the contract checking down into coercions: structured, blameannotated lists of checks. By carefully preventing duplicate coercions from appearing, we can restore space efficiency while keeping the same observable behavior.
p3698
aVWe study algebraic data types in a manifest contract system, a software contract system where contract information occurs as refinement types. We first compare two simple approaches: refinements on type constructors and refinements on data constructors. For example, lists of positive integers can be described by {l:int list | for_all (lambda y. y > 0) l} in the former, whereas by a userdefined datatype pos_list with cons of type {x:int | x > 0} X pos_list > pos_list in the latter. The two approaches are complementary: the former makes it easier for a programmer to write types and the latter enables more efficient contract checking. To take the best of both worlds, we propose (1) a syntactic translation from refinements on type constructors to equivalent refinements on data constructors and (2) dynamically checked casts between different but compatible datatypes such as int list and pos_list. We define a manifest contract calculus to formalize the semantics of the casts and prove that the translation is correct.
p3699
aVWe show that the weak memory model introduced by the 2011 C and C++ standards does not permit many common sourcetosource program transformations (such as expression linearisation and "roach motel" reorderings) that modern compilers perform and that are deemed to be correct. As such it cannot be used to define the semantics of intermediate languages of compilers, as, for instance, LLVM aimed to. We consider a number of possible local fixes, some strengthening and some weakening the model. We evaluate the proposed fixes by determining which program transformations are valid with respect to each of the patched models. We provide formal Coq proofs of their correctness or counterexamples as appropriate.
p3700
aVGraphical choreographies, or global graphs, are general multiparty session specifications featuring expressive constructs such as forking, merging, and joining for representing applicationlevel protocols. Global graphs can be directly translated into modelling notations such as BPMN and UML. This paper presents an algorithm whereby a global graph can be constructed from asynchronous interactions represented by communicating finitestate machines (CFSMs). Our results include: a sound and complete characterisation of a subset of safe CFSMs from which global graphs can be constructed; an algorithm to translate CFSMs to global graphs; a time complexity analysis; and an implementation of our theory, as well as an experimental evaluation.
p3701
aVConcurrent datastructures, such as stacks, queues, and deques, often implicitly enforce a total order over elements in their underlying memory layout. However, much of this order is unnecessary: linearizability only requires that elements are ordered if the insert methods ran in sequence. We propose a new approach which uses timestamping to avoid unnecessary ordering. Pairs of elements can be left unordered if their associated insert operations ran concurrently, and order imposed as necessary at the eventual removal. We realise our approach in a new nonblocking datastructure, the TS (timestamped) stack. Using the same approach, we can define corresponding queue and deque datastructures. In experiments on x86, the TS stack outperforms and outscales all its competitors   for example, it outperforms the eliminationbackoff stack by factor of two. In our approach, more concurrency translates into less ordering, giving lesscontended removal and thus higher performance and scalability. Despite this, the TS stack is linearizable with respect to stack semantics. The weak internal ordering in the TS stack presents a challenge when establishing linearizability: standard techniques such as linearization points work well when there exists a total internal order. We present a new stack theorem, mechanised in Isabelle, which characterises the orderings sufficient to establish stack semantics. By applying our stack theorem, we show that the TS stack is indeed linearizable. Our theorem constitutes a new, generic proof technique for concurrent stacks, and it paves the way for future weakly ordered datastructure designs.
p3702
aVThe standard reading of type theory through the lens of category theory is based on the idea of viewing a type system as a category of welltyped terms. We propose a basic revision of this reading: rather than interpreting type systems as categories, we describe them as functors from a category of typing derivations to a category of underlying terms. Then, turning this around, we explain how in fact any functor gives rise to a generalized type system, with an abstract notion of typing judgment, typing derivations and typing rules. This leads to a purely categorical reformulation of various natural classes of type systems as natural classes of functors. The main purpose of this paper is to describe the general framework (which can also be seen as providing a categorical analysis of refinement types), and to present a few applications. As a larger case study, we revisit Reynolds' paper on ``The Meaning of Types'' (2000), showing how the paper's main results may be reconstructed along these lines.
p3703
aVThis paper reports on the design and soundness proof, using the Coq proof assistant, of Verasco, a static analyzer based on abstract interpretation for most of the ISO C 1999 language (excluding recursion and dynamic allocation). Verasco establishes the absence of runtime errors in the analyzed programs. It enjoys a modular architecture that supports the extensible combination of multiple abstract domains, both relational and nonrelational. Verasco integrates with the CompCert formallyverified C compiler so that not only the soundness of the analysis results is guaranteed with mathematical certitude, but also the fact that these guarantees carry over to the compiled code.
p3704
aVWe want to prove that a static analysis of a given program is complete, namely, no imprecision arises when asking some query on the program behavior in the concrete (ie, for its concrete semantics) or in the abstract (ie, for its abstract interpretation). Completeness proofs are therefore useful to assign confidence to alarms raised by static analyses. We introduce the completeness class of an abstraction as the set of all programs for which the abstraction is complete. Our first result shows that for any nontrivial abstraction, its completeness class is not recursively enumerable. We then introduce a stratified deductive system to prove the completeness of program analyses over an abstract domain A. We prove the soundness of the deductive system. We observe that the only sources of incompleteness are assignments and Boolean tests   unlikely a common belief in static analysis, joins do not induce incompleteness. The first layer of this proof system is generic, abstractionagnostic, and it deals with the standard constructs for program composition, that is, sequential composition, branching and guarded iteration. The second layer is instead abstractionspecific: the designer of an abstract domain A provides conditions for completeness in A of assignments and Boolean tests which have to be checked by a suitable static analysis or assumed in the completeness proof as hypotheses. We instantiate the second layer of this proof system first with a generic nonrelational abstraction in order to provide a sound rule for the completeness of assignments. Orthogonally, we instantiate it to the numerical abstract domains of Intervals and Octagons, providing necessary and sufficient conditions for the completeness of their Boolean tests and of assignments for Octagons.
p3705
aVThis paper reports on the development of Compositional CompCert, the first verified separate compiler for C. Specifying and proving separate compilation for C is made challenging by the coincidence of: compiler optimizations, such as register spilling, that introduce compilermanaged (private) memory regions into function stack frames, and C's stackallocated addressable local variables, which may leak portions of stack frames to other modules when their addresses are passed as arguments to external function calls. The CompCert compiler, as built/proved by Leroy etal 2006 2014, has proofs of correctness for whole programs, but its simulation relations are too weak to specify or prove separately compiled modules. Our technical contributions that make Compositional CompCert possible include: languageindependent linking, a new operational model of multilanguage linking that supports strong semantic contextual equivalences; and structured simulations, a refinement of Beringer etal logical simulation relations that enables expressive modulelocal invariants on the state communicated between compilation units at runtime. All the results in the paper have been formalized in Coq and are available for download together with the Compositional CompCert compiler.
p3706
aVThis article is the second part of a two articles series about the definition of higher order polymorphic functions in a type system with recursive types and settheoretic type connectives (unions, intersections, and negations). In the first part, presented in a companion paper, we defined and studied the syntax, semantics, and evaluation of the explicitlytyped version of a calculus, in which type instantiation is driven by explicit instantiation annotations. In this second part we present a local type inference system that allows the programmer to omit explicit instantiation annotations for function applications, and a type reconstruction system that allows the programmer to omit explicit type annotations for function definitions. The work presented in the two articles provides the theoretical foundations and technical machinery needed to design and implement higherorder polymorphic functional languages with union and intersection types and/or for semistructured data processing.
p3707
aVGradual typing is a discipline for integrating dynamic checking into a static type system. Since its introduction in functional languages, it has been adapted to a variety of type systems, including objectoriented, security, and substructural. This work studies its application to implicitly typed languages based on type inference. Siek and Vachharajani designed a gradual type inference system and algorithm that infers gradual types but still rejects illtyped static programs. However, the type system requires local reasoning about type substitutions, an imperative inference algorithm, and a subtle correctness statement. This paper introduces a new approach to gradual type inference, driven by the principle that gradual inference should only produce static types. We present a static implicitly typed language, its gradual counterpart, and a type inference procedure. The gradual system types the same programs as Siek and Vachharajani, but has a modular structure amenable to extension. The language admits letpolymorphism, and its dynamics are defined by translation to the Polymorphic Blame Calculus. The principal types produced by our initial type system mask the distinction between static parametric polymorphism and polymorphism that can be attributed to gradual typing. To expose this difference, we distinguish static type parameters from gradual type parameters and reinterpret gradual type consistency accordingly. The resulting extension enables programs to be interpreted using either the polymorphic or monomorphic Blame Calculi.
p3708
aVIn this paper, we develop a novel notion of dependent information flow types. Dependent information flow types fit within the standard framework of dependent type theory, but, unlike usual dependent types, crucially allow the security level of a type, rather than just the structural data type itself, to depend on runtime values.  Our dependent function and dependent sum information flow types provide a direct, natural and elegant way to express and enforce fine grained security policies on programs, including programs that manipulate structured data types in which the security level of a structure field may depend on values dynamically stored in other fields, still considered a challenge to security enforcement in software systems such as datacentric webbased applications. We base our development on the very general setting of a minimal lambdacalculus with references and collections. We illustrate its expressiveness, showing how secure operations on relevant scenarios can be modelled and analysed using our dependent information flow type system, which is also shown to be amenable to algorithmic type checking. Our main results include typesafety and noninterference theorems ensuring that welltyped programs do not violate prescribed security policies.
p3709
aVWe introduce a model for mixed syntactic/semantic approximation of programs based on symbolic finite automata (SFA). The edges of SFA are labeled by predicates whose semantics specifies the denotations that are allowed by the edge. We introduce the notion of abstract symbolic finite automaton (ASFA) where approximation is made by abstract interpretation of symbolic finite automata, acting both at syntactic (predicate) and semantic (denotation) level. We investigate in the details how the syntactic and semantic abstractions of SFA relate to each other and contribute to the determination of the recognized language. Then we introduce a family of transformations for simplifying ASFA. We apply this model to prove properties of commonly used tools for similarity analysis of binary executables. Following the structure of their control flow graphs, disassembled binary executables are represented as (concrete) SFA, where states are program points and predicates represent the (possibly infinite) I/O semantics of each basic block in a constraint form. Known tools for binary code analysis are viewed as specific choices of symbolic and semantic abstractions in our framework, making symbolic finite automata and their abstract interpretations a unifying model for comparing and reasoning about soundness and completeness of analyses of lowlevel code.
p3710
aVNetKAT is a domainspecific language and logic for specifying and verifying network packetprocessing functions. It consists of Kleene algebra with tests (KAT) augmented with primitives for testing and modifying packet headers and encoding network topologies. Previous work developed the design of the language and its standard semantics, proved the soundness and completeness of the logic, defined a PSPACE algorithm for deciding equivalence, and presented several practical applications. This paper develops the coalgebraic theory of NetKAT, including a specialized version of the Brzozowski derivative, and presents a new efficient algorithm for deciding the equational theory using bisimulation. The coalgebraic structure admits an efficient sparse representation that results in a significant reduction in the size of the state space. We discuss the details of our implementation and optimizations that exploit NetKAT's equational axioms and coalgebraic structure to yield significantly improved performance. We present results from experiments demonstrating that our tool is competitive with stateoftheart tools on several benchmarks including allpairs connectivity, loopfreedom, and translation validation.
p3711
aVWe propose algorithms for checking language equivalence of finite automata over a large alphabet. We use symbolic automata, where the transition function is compactly represented using (multiterminal) binary decision diagrams (BDD). The key idea consists in computing a bisimulation by exploring reachable pairs symbolically, so as to avoid redundancies. This idea can be combined with already existing optimisations, and we show in particular a nice integration with the disjoint sets forest datastructure from Hopcroft and Karp's standard algorithm. Then we consider Kleene algebra with tests (KAT), an algebraic theory that can be used for verification in various domains ranging from compiler optimisation to network programming analysis. This theory is decidable by reduction to language equivalence of automata on guarded strings, a particular kind of automata that have exponentially large alphabets. We propose several methods allowing to construct symbolic automata out of KAT expressions, based either on Brzozowski's derivatives or on standard automata constructions. All in all, this results in efficient algorithms for deciding equivalence of KAT expressions.
p3712
aVThis paper presents the design of Zombie, a dependentlytyped programming language that uses an adaptation of a congruence closure algorithm for proof and type inference. This algorithm allows the type checker to automatically use equality assumptions from the context when reasoning about equality. Most dependentlytyped languages automatically use equalities that follow from betareduction during type checking; however, such reasoning is incompatible with congruence closure. In contrast, Zombie does not use automatic betareduction because types may contain potentially diverging terms. Therefore Zombie provides a unique opportunity to explore an alternative definition of equivalence in dependentlytyped language design. Our work includes the specification of the language via a bidirectional type system, which works "uptocongruence,'' and an algorithm for elaborating expressions in this language to an explicitly typed core language. We prove that our elaboration algorithm is complete with respect to the source type system, and always produces well typed terms in the core language. This algorithm has been implemented in the Zombie language, which includes general recursion, irrelevant arguments, heterogeneous equality and datatypes.
p3713
aVIn this paper, we show how to integrate linear types with type dependency, by extending the linear/nonlinear calculus of Benton to support type dependency. Next, we give an application of this calculus by giving a prooftheoretic account of imperative programming, which requires extending the calculus with computationally irrelevant quantification, proof irrelevance, and a monad of computations. We show the soundness of our theory by giving a realizability model in the style of Nuprl, which permits us to validate not only the betalaws for each type, but also the etalaws. These extensions permit us to decompose Hoare triples into a collection of simpler typetheoretic connectives, yielding a rich equational theory for dependentlytyped higherorder imperative programs. Furthermore, both the type theory and its model are relatively simple, even when all of the extensions are considered.
p3714
aVWe propose meta lambda calculus Lambda* as a basic model of textual substitution via metavariables. The most important feature of the calculus is that every betaredex can be reduced regardless of whether the betaredex contains metalevel variables or not. Such a meta lambda calculus has never been achieved before due to difficulty to manage binding structure consistently with alpharenaming in the presence of metalevel variables. We overcome the difficulty by introducing a new mechanism to deal with substitution and binding structure in a systematic way without the notion of free variables and alpharenaming. Calculus Lambda* enables us to investigate crosslevel terms that include a certain type of level mismatch. Crosslevel terms have been regarded as meaningless terms and left out of consideration thus far. We find that some crosslevel terms behave as quotes and `eval' command in programming languages. With these terms, we show a procedural language as an application of the calculus, which sheds new light on the notions of stores and recursion via metalevel variables.
p3715
aVWe develop a new framework of algebraic theories with linear parameters, and use it to analyze the equational reasoning principles of quantum computing and quantum programming languages. We use the framework as follows: we present a new elementary algebraic theory of quantum computation, built from unitary gates and measurement; we provide a completeness theorem or the elementary algebraic theory by relating it with a model from operator algebra;  we extract an equational theory for a quantum programming language from the algebraic theory; we compare quantum computation with other local notions of computation by investigating variations on the algebraic theory.
p3716
aVIn this paper, we present a new approach to automatically verify multithreaded programs which are executed by an unbounded number of threads running in parallel. The starting point for our work is the problem of how we can leverage existing automated verification technology for sequential programs (abstract interpretation, Craig interpolation, constraint solving, etc.) for multithreaded programs. Suppose that we are given a correctness proof for a trace of a program (or for some other program fragment). We observe that the proof can always be decomposed into a finite set of Hoare triples, and we ask what can be proved from the finite set of Hoare triples using only simple combinatorial inference rules  (without access to a theorem prover and without the possibility to infer genuinely new Hoare triples)? We introduce a proof system where one proves the correctness of a multithreaded program by showing that for each trace of the program, there exists  a correctness proof in the space of proofs that are derivable from a finite set of axioms using simple combinatorial inference rules. This proof system is complete with respect to the classical proof method of establishing an inductive invariant (which uses thread quantification and control predicates). Moreover, it is possible to algorithmically check whether a given set of axioms is sufficient to prove the correctness of a multithreaded program, using ideas from wellstructured transition systems.
p3717
aVOne of the most studied behavioural equivalences is bisimilarity. Its success is much due to the associated bisimulation proof method, which can be further enhanced by means of "upto bisimulation" techniques such as "upto context". A different proof method is discussed, based on unique solution of special forms of inequations called contractions, and inspired by Milner's theorem on unique solution of equations. The method is as powerful as the bisimulation proof method and its "upto context" enhancements. The definition of contraction can be transferred onto other behavioural equivalences, possibly contextual and noncoinductive. This enables a coinductive reasoning style on such equivalences, either by applying the method based on unique solution of contractions, or by injecting appropriate contraction preorders into the bisimulation game. The techniques are illustrated on CCSlike languages; an example dealing with higherorder languages is also shown.
p3718
aVWe present a method and a tool for generating succinct representations of sets of concurrent traces. We focus on trace sets that contain all correct or all incorrect permutations of events from a given trace. We represent trace sets as HBFormulas that are Boolean combinations of happensbefore constraints between events. To generate a representation of incorrect interleavings, our method iteratively explores interleavings that violate the specification and gathers generalizations of the discovered interleavings into an HBFormula; its complement yields a representation of correct interleavings. We claim that our trace set representations can drive diverse verification, fault localization, repair, and synthesis techniques for concurrent programs. We demonstrate this by using our tool in three case studies involving synchronization synthesis, bug summarization, and abstraction refinement based verification. In each case study, our initial experimental results have been promising. In the first case study, we present an algorithm for inferring missing synchronization from an HBFormula representing correct interleavings of a given trace. The algorithm applies rules to rewrite specific patterns in the HBFormula into locks, barriers, and waitnotify constructs. In the second case study, we use an HBFormula representing incorrect interleavings for bug summarization. While the HBFormula itself is a concise counterexample summary, we present additional inference rules to help identify specific concurrency bugs such as data races, defineuse order violations, and twostage access bugs. In the final case study, we present a novel predicate learning procedure that uses HBFormulas representing abstract counterexamples to accelerate counterexampleguided abstraction refinement (CEGAR). In each iteration of the CEGAR loop, the procedure refines the abstraction to eliminate multiple spurious abstract counterexamples drawn from the HBFormula.
p3719
aVThis paper presents KJava, a complete executable formal semantics of Java 1.4. KJava was extensively tested with a test suite developed alongside the project, following the Test Driven Development methodology. In order to maintain clarity while handling the great size of Java, the semantics was split into two separate definitions   a static semantics and a dynamic semantics. The output of the static semantics is a preprocessed Java program, which is passed as input to the dynamic semantics for execution. The preprocessed program is a valid Java program, which uses a subset of the features of Java. The semantics is applied to modelcheck multithreaded programs. Both the test suite and the static semantics are generic and ready to be used in other Javarelated projects.
p3720
aVHygiene is an essential aspect of Scheme's macro system that prevents unintended variable capture. However, previous work on hygiene has focused on algorithmic implementation rather than precise, mathematical definition of what constitutes hygiene. This is in stark contrast with lexical scope, alphaequivalence and captureavoiding substitution, which also deal with preventing unintended variable capture but have widely applicable and wellunderstood mathematical definitions. This paper presents such a precise, mathematical definition of hygiene. It reviews various kinds of hygiene violation and presents examples of how they occur. From these examples, we develop a practical algorithm for hygienic macro expansion. We then present algorithmindependent, mathematical criteria for whether a macro expansion algorithm is hygienic. This characterization corresponds closely to existing hygiene algorithms and sheds light on aspects of hygiene that are usually overlooked in informal definitions.
p3721
aVIn 1991, Pfenning and Lee studied whether System F could support a typed selfinterpreter. They concluded that typed selfrepresentation for System F "seems to be impossible", but were able to represent System F in F\u03c9. Further, they found that the representation of F\u03c9 requires kind polymorphism, which is outside F\u03c9. In 2009, Rendel, Ostermann and Hofer conjectured that the representation of kindpolymorphic terms would require another, higher form of polymorphism. Is this a case of infinite regress? We show that it is not and present a typed selfrepresentation for Girard's System U, the first for a \u03bbcalculus with decidable type checking. System U extends System F\u03c9 with kind polymorphic terms and types. We show that kind polymorphic types (i.e. types that depend on kinds) are sufficient to "tie the knot"   they enable representations of kind polymorphic terms without introducing another form of polymorphism. Our selfrepresentation supports operations that iterate over a term, each of which can be applied to a representation of itself. We present three typed selfapplicable operations: a selfinterpreter that recovers a term from its representation, a predicate that tests the intensional structure of a term, and a typed continuationpassingstyle (CPS) transformation   the first typed selfapplicable CPS transformation. Our techniques could have applications from verifiably typepreserving metaprograms, to growable typed languages, to more efficient selfinterpreters.
p3722
aVIn recent years, advances in machine learning and related fields have led to significant advances in a range of userinterface technologies, including audio processing, speech recognition, and natural language processing. These advances in turn have enabled speechbased digital assistants and speechtospeech translation systems to become practical to deploy on a large scale. In essence, machines are becoming capable of hearing what we are saying. But will they understand what we want them to do when we talk to them? What are the prospects for getting useful work done in essence, by synthesizing programs   through the act of having a conversation with a computer? In this lecture, I will speculate on the central role that programminglanguage design and program synthesis may have in this possible   and I will argue, likely   future of computing, one in which every user writes programs, every day, by conversing with a computing system.
p3723
aVThe 1990s saw a hugely productive interaction between database and programming language research. Ideas about type systems from programming languages played a central role in generalizing and adapting relational database systems to new data models. At the same time databases provided some of the best concrete examples of the application of concurrency theory and of the benefits of highlevel optimization in functional programming languages. One of the driving ambitions behind this research was the idea that database access should be properly embedded in programming languages: one should not have to be bilingual in order to use a database from a programming language; and that goal has to some extent been realized. In the past fifteen years, new data models, both for data storage and for data exchange have appeared with depressing regularity and with each such model, the inevitable query language. Does programming language research have anything to contribute to these new languages? Should we take the time to to worry about embedding these models in conventional languages? Over the same period, some interesting new connections between databases and programming languages have emerged, notably in the areas of scientific databases, annotation and provenance. Will this provide new opportunities for crossfertilization?
p3724
aVHomotopy Type Theory is a new field of mathematics based on the recentlydiscovered correspondence between MartinLf's constructive type theory and abstract homotopy theory. We have a powerful interplay between these disciplines  we can use geometric intuition to formulate new concepts in type theory and, conversely, use typetheoretic machinery to verify and often simplify existing mathematical proofs. Higher inductive types form a crucial part of this new system since they allow us to represent mathematical objects, such as spheres, tori, pushouts, and quotients, in the type theory. We investigate a class of higher inductive types called Wsuspensions which generalize MartinLf's wellfounded trees. We show that a propositional variant of Wsuspensions, whose computational behavior is determined up to a higher path, is characterized by the universal property of being a homotopyinitial algebra. As a corollary we get that Wsuspensions in the strict form are homotopyinitial.
p3725
aVWe propose a framework to prove almost sure termination for probabilistic programs with real valued variables. It is based on ranking supermartingales, a notion analogous to ranking functions on nonprobabilistic programs. The framework is proven sound and complete for a meaningful class of programs involving randomization and bounded nondeterminism. We complement this foundational insigh by a practical proof methodology, based on sound conditions that enable compositional reasoning and are amenable to a direct implementation using modern theorem provers. This is integrated in a small dependent type system, to overcome the problem that lexicographic ranking functions fail when combined with randomization. Among others, this compositional methodology enables the verification of probabilistic programs outside the complete class that admits ranking supermartingales.
p3726
aVWe propose the first sound and complete learningbased compositional verification technique for probabilistic safety properties on concurrent systems where each component is an Markov decision process. Different from previous works, weighted assumptions are introduced to attain completeness of our framework. Since weighted assumptions can be implicitly represented by multiterminal binary decision diagrams (MTBDD's), we give an L*based learning algorithm for MTBDD's to infer weighted assumptions. Experimental results suggest promising outlooks for our compositional technique.
p3727
aVNetwork theory uses the string diagrammatic language of monoidal categories to study graphical structures formally, eschewing specialised translations into intermediate formalisms. Recently, there has been a concerted research focus on developing a network theoretic approach to signal flow graphs, which are classical structures in control theory, signal processing and a cornerstone in the study of feedback. In this approach, signal flow graphs are given a relational denotational semantics in terms of formal power series. Thus far, the operational behaviour of such signal flow graphs has only been discussed at an intuitive level. In this paper we equip them with a structural operational semantics. As is typically the case, the purely operational picture is too concrete   two graphs that are denotationally equal may exhibit different operational behaviour. We classify the ways in which this can occur and show that any graph can be realised   rewritten, using the graphical theory, into an executable form where the operational behavior and the denotation coincides.
p3728
aVThe past decades have witnessed an extensive study of structured recursion schemes. A general scheme is the hylomorphism, which captures the essence of divideandconquer: a problem is broken into subproblems by a coalgebra; subproblems are solved recursively; the subsolutions are combined by an algebra to form a solution. In this paper we develop a simple toolbox for assembling recursive coalgebras, which by definition ensure that their hylo equations have unique solutions, whatever the algebra. Our main tool is the conjugate rule, a generic rule parametrized by an adjunction and a conjugate pair of natural transformations. We show that many basic adjunctions induce useful recursion schemes. In fact, almost every structured recursion scheme seems to arise as an instance of the conjugate rule. Further, we adapt our toolbox to the more expressive setting of parametrically recursive coalgebras, where the original input is also passed to the algebra. The formal development is complemented by a series of workedout examples in Haskell.
p3729
aVWe consider the quantitative analysis problem for interprocedural controlflow graphs (ICFGs). The input consists of an ICFG, a positive weight function that assigns every transition a positive integervalued number, and a labelling of the transitions (events) as good, bad, and neutral events. The weight function assigns to each transition a numerical value that represents a measure of how good or bad an event is. The quantitative analysis problem asks whether there is a run of the ICFG where the ratio of the sum of the numerical weights of good events versus the sum of weights of bad events in the longrun is at least a given threshold (or equivalently, to compute the maximal ratio among all valid paths in the ICFG). The quantitative analysis problem for ICFGs can be solved in polynomial time, and we present an efficient and practical algorithm for the problem. We show that several problems relevant for static program analysis, such as estimating the worstcase execution time of a program or the average energy consumption of a mobile application, can be modeled in our framework. We have implemented our algorithm as a tool in the Java Soot framework. We demonstrate the effectiveness of our approach with two case studies. First, we show that our framework provides a sound approach (no false positives) for the analysis of inefficientlyused containers. Second, we show that our approach can also be used for static profiling of programs which reasons about methods that are frequently invoked. Our experimental results show that our tool scales to relatively large benchmarks, and discovers relevant and useful information that can be used to optimize performance of the programs.
p3730
aVWe present a framework for computing contextfree language reachability properties when parts of the program are missing. Our framework infers candidate specifications for missing program pieces that are needed for verifying a property of interest, and presents these specifications to a human auditor for validation. We have implemented this framework for a taint analysis of Android apps that relies on specifications for Android library methods. In an extensive experimental study on 179 apps, our tool performs verification with only a small number of queries to a human auditor.
p3731
aVTechnology trends will cause data movement to account for the majority of energy expenditure and execution time on emerging computers. Therefore, computational complexity will no longer be a sufficient metric for comparing algorithms, and a fundamental characterization of data access complexity will be increasingly important. The problem of developing lower bounds for data access complexity has been modeled using the formalism of Hong and Kung's red/blue pebble game for computational directed acyclic graphs (CDAGs). However, previously developed approaches to lower bounds analysis for the red/blue pebble game are very limited in effectiveness when applied to CDAGs of real programs, with computations comprised of multiple subcomputations with differing DAG structure. We address this problem by developing an approach for effectively composing lower bounds based on graph decomposition. We also develop a static analysis algorithm to derive the asymptotic dataaccess lower bounds of programs, as a function of the problem size and cache size.
p3732
aVOver the past decade, great progress has been made in the static modular verification of C code by means of separation logicbased program logics. However, the runtime guarantees offered by such verification are relatively limited when the verified modules are part of a whole program that also contains unverified modules. In particular, a memory safety error in an unverified module can corrupt the runtime state, leading to assertion failures or invalid memory accesses in the verified modules. This paper develops runtime checks to be inserted at the boundary between the verified and the unverified part of a program, to guarantee that no assertion failures or invalid memory accesses can occur at runtime in any verified module. One of the key challenges is enforcing the separation logic frame rule, which we achieve by checking the integrity of the footprint of the verified part of the program on each control flow transition from the unverified to the verified part. This in turn requires the presence of some support for moduleprivate memory at runtime. We formalize our approach and prove soundness. We implement the necessary runtime checks by means of a program transformation that translates C code with separation logic annotations into plain C, and that relies on a protected module architecture for providing moduleprivate memory and restricted module entry points. Benchmarks show the performance impact of this transformation depends on the choice of boundary between the verified and unverified parts of the program, but is below 4% for realworld applications.
p3733
aVModern computer systems consist of a multitude of abstraction layers (e.g., OS kernels, hypervisors, device drivers, network protocols), each of which defines an interface that hides the implementation details of a particular set of functionality. Client programs built on top of each layer can be understood solely based on the interface, independent of the layer implementation. Despite their obvious importance, abstraction layers have mostly been treated as a system concept; they have almost never been formally specified or verified. This makes it difficult to establish strong correctness properties, and to scale program verification across multiple layers. In this paper, we present a novel languagebased account of abstraction layers and show that they correspond to a strong form of abstraction over a particularly rich class of specifications which we call deep specifications. Just as data abstraction in typed functional languages leads to the important representation independence property, abstraction over deep specification is characterized by an important implementation independence property: any two implementations of the same deep specification must have contextually equivalent behaviors. We present a new layer calculus showing how to formally specify, program, verify, and compose abstraction layers. We show how to instantiate the layer calculus in realistic programming languages such as C and assembly, and how to adapt the CompCert verified compiler to compile certified C layers such that they can be linked with assembly layers. Using these new languages and tools, we have successfully developed multiple certified OS kernels in the Coq proof assistant, the most realistic of which consists of 37 abstraction layers, took less than one person year to develop, and can boot a version of Linux as a guest.
p3734
aVMany verifications of realistic software systems are monolithic, in the sense that they define single global invariants over complete system state. More modular proof techniques promise to support reuse of component proofs and even reduce the effort required to verify one concrete system, just as modularity simplifies standard software development. This paper reports on one case study applying modular proof techniques in the Coq proof assistant. To our knowledge, it is the first modular verification certifying a system that combines infrastructure with an application of interest to end users. We assume a nonblocking API for managing TCP networking streams, and on top of that we work our way up to certifying multithreaded, databasebacked Web applications. Key verified components include a cooperative threading library and an implementation of a domainspecific language for XML processing. We have deployed our casestudy system on mobile robots, where it interfaces with offtheshelf components for sensing, actuation, and control.
p3735
aVSecurity enforcement mechanisms like execution monitors are used to make sure that some untrusted program complies with a policy. Different enforcement mechanisms have different strengths and weaknesses and hence it is important to understand the qualities of various enforcement mechanisms. This paper studies runtime enforcement mechanisms for reactive programs. We study the impact of two important constraints that many practical enforcement mechanisms satisfy: (1) the enforcement mechanism must handle each input/output event in finite time and on occurrence of the event (as opposed to for instance Ligatti's edit automata that have the power to buffer events for an arbitrary amount of time), and (2) the enforcement mechanism treats the untrusted program as a  black box: it can monitor and/or edit the input/output events that the program exhibits on execution and it can explore alternative executions of the program by running additional copies of the program and providing these different inputs. It can  not inspect the source or machine code of the untrusted program. Such enforcement mechanisms are important in practice: they include for instance many execution monitors, virtual machine monitors, and secure multiexecution or shadow executions. We establish upper and lower bounds for the class of policies that are enforceable by such black box mechanisms, and we propose a generic enforcement mechanism that works for a wide range of policies. We also show how our generic enforcement mechanism can be instantiated to enforce specific classes of policies, at the same time showing that many existing enforcement mechanisms are optimized instances of our construction.
p3736
aVWe propose a new approach to programming multicore, relaxedmemory architectures in imperative, portable programming languages. Our memory model is based on explicit, programmerspecified requirements for order of execution and the visibility of writes. The compiler then realizes those requirements in the most efficient manner it can. This is in contrast to existing memory models, which if they allow programmer control over synchronization at all are based on inferring the execution and visibility consequences of synchronization operations or annotations in the code. We formalize our memory model in a core calculus called RMC\u005c@. Outside of the programmer's specified requirements, RMC is designed to be strictly more relaxed than existing architectures. It employs an aggressively nondeterministic semantics for expressions, in which actions can be executed in nearly any order, and a store semantics that generalizes Sarkar et al.'s and Alglave et al.'s models of the Power architecture. We establish several results for RMC, including sequential consistency for two programming disciplines, and an appropriate notion of type safety. All our results are formalized in Coq.
p3737
aVWe present Iris, a concurrent separation logic with a simple premise: monoids and invariants are all you need. Partial commutative monoids enable us to express and invariants enable us to enforce userdefined *protocols* on shared state, which are at the conceptual core of most recent program logics for concurrency. Furthermore, through a novel extension of the concept of a *view shift*, Iris supports the encoding of *logically atomic specifications*, i.e., Hoarestyle specs that permit the client of an operation to treat the operation essentially as if it were atomic, even if it is not.
p3738
aVEfficient implementations of concurrent objects such as semaphores, locks, and atomic collections are essential to modern computing. Yet programming such objects is error prone: in minimizing the synchronization overhead between concurrent object invocations, one risks the conformance to reference implementations   or in formal terms, one risks violating observational refinement. Testing this refinement even within a single execution is intractable, limiting existing approaches to executions with very few object invocations. We develop a polynomialtime (per execution) approximation to refinement checking. The approximation is parameterized by an accuracy k\u2208N representing the degree to which refinement violations are visible. In principle, more violations are detectable as k increases, and in the limit, all are detectable. Our insight for this approximation arises from foundational properties on the partial orders characterizing the happensbefore relations between object invocations: they are interval orders, with a well defined measure of complexity, i.e., their length. Approximating the happensbefore relation with a possiblyweaker interval order of bounded length can be efficiently implemented by maintaining a bounded number of integer counters. In practice, we find that refinement violations can be detected with very small values of k, and that our approach scales far beyond existing refinementchecking approaches.
p3739
aVSoftwaredefined networking (SDN) is a new paradigm for operating and managing computer networks. SDN enables logicallycentralized control over network devices through a "controller"   software that operates independently of the network hardware. Network operators can run both inhouse and thirdparty SDN programs on top of the controller, e.g., to specify routing and access control policies. In practice, having the controller handle events limits the network scalability. Therefore, the feasibility of SDN depends on the ability to efficiently decentralize network eventhandling by installing forwarding rules on the switches. However, installing a rule too early or too late may lead to incorrect behavior, e.g., (1) packets may be forwarded to the wrong destination or incorrectly dropped; (2) packets handled by the switch may hide vital information from the controller, leading to incorrect forwarding behavior. The second issue is subtle and sometimes missed even by experienced programmers. The contributions of this paper are two fold. First, we formalize the correctness and optimality requirements for decentralizing network policies. Second, we identify a useful class of network policies which permits automatic synthesis of a controller which performs optimal forwarding rule installation.
p3740
aVIn this paper, we investigate an approach to program synthesis that is based on crowdsourcing. With the help of crowdsourcing, we aim to capture the "wisdom of the crowds" to find good if not perfect solutions to inherently tricky programming tasks, which elude even expert developers and lack an easytoformalize specification. We propose an approach we call program boosting, which involves crowdsourcing imperfect solutions to a difficult programming problem from developers and then blending these programs together in a way that improves their correctness. We implement this approach in a system called CROWDBOOST and show in our experiments that interesting and highly nontrivial tasks such as writing regular expressions for URLs or email addresses can be effectively crowdsourced. We demonstrate that carefully blending the crowdsourced results together consistently produces a boost, yielding results that are better than any of the starting programs. Our experiments on 465 program pairs show consistent boosts in accuracy and demonstrate that program boosting can be performed at a relatively modest monetary cost.
p3741
aVMechanism design is the study of algorithm design where the inputs to the algorithm are controlled by strategic agents, who must be incentivized to faithfully report them. Unlike typical programmatic properties, it is not sufficient for algorithms to merely satisfy the property, incentive properties are only useful if the strategic agents also believe this fact. Verification is an attractive way to convince agents that the incentive properties actually hold, but mechanism design poses several unique challenges: interesting properties can be sophisticated relational properties of probabilistic computations involving expected values, and mechanisms may rely on other probabilistic properties, like differential privacy, to achieve their goals. We introduce a relational refinement type system, called HOARe2, for verifying mechanism design and differential privacy. We show that HOARe2 is sound w.r.t. a denotational semantics, and correctly models (epsilon,delta)differential privacy; moreover, we show that it subsumes DFuzz, an existing linear dependent type system for differential privacy. Finally, we develop an SMTbased implementation of HOARe2 and use it to verify challenging examples of mechanism design, including auctions and aggregative games, and new proposed examples from differential privacy.
p3742
aVDifferential privacy provides a way to get useful information about sensitive data without revealing much about any one individual. It enjoys many nice compositionality properties not shared by other approaches to privacy, including, in particular, robustness against sideknowledge. Designing differentially private mechanisms from scratch can be a challenging task. One way to make it easier to construct new differential private mechanisms is to design a system which allows more complex mechanisms (programs) to be built from differentially private building blocks in principled way, so that the resulting programs are guaranteed to be differentially private by construction. This paper is about a new accounting principle for building differentially private programs. It is based on a simple generalisation of classic differential privacy which we call Personalised Differential Privacy (PDP). In PDP each individual has its own personal privacy level. We describe ProPer, a interactive system for implementing PDP which maintains a privacy budget for each individual. When a primitive query is made on data derived from individuals, the provenance of the involved records determines how the privacy budget of an individual is affected: the number of records derived from Alice determines the multiplier for the privacy decrease in Alice's budget. This offers some advantages over previous systems, in particular its finegrained character allows better utilisation of the privacy budget than mechanisms based purely on the concept of global sensitivity, and it applies naturally to the case of a live database where new individuals are added over time. We provide a formal model of the ProPer approach, prove that it provides personalised differential privacy, and describe a prototype implementation based on McSherry's PINQ system.
p3743
aVBuilding a summary for library code is a common approach to speeding up the analysis of client code. In presence of callbacks, some reachability relationships between library nodes cannot be obtained during librarycode summarization. Thus, the library code may have to be analyzed again during the analysis of the client code with the library summary. In this paper, we propose to summarize library code with treeadjoininglanguage (TAL) reachability. Compared with the summary built with contextfreelanguage (CFL) reachability, the summary built with TAL reachability further contains conditional reachability relationships. The conditional reachability relationships can lead to much lighter analysis of the library code during the client code analysis with the TALreachabilitybased library summary. We also performed an experimental comparison of contextsensitive datadependence analysis with the TALreachabilitybased library summary and contextsensitive datadependence analysis with the CFLreachabilitybased library summary using 15 benchmark subjects. Our experimental results demonstrate that the former has an 8X speedup over the latter on average.
p3744
aVInterprocedural analysis is at the heart of numerous applications in programming languages, such as alias analysis, constant propagation, etc. Recursive state machines (RSMs) are standard models for interprocedural analysis. We consider a general framework with RSMs where the transitions are labeled from a semiring, and path properties are algebraic with semiring operations. RSMs with algebraic path properties can model interprocedural dataflow analysis problems, the shortest path problem, the most probable path problem, etc. The traditional algorithms for interprocedural analysis focus on path properties where the starting point is fixed as the entry point of a specific method. In this work, we consider possible multiple queries as required in many applications such as in alias analysis. The study of multiple queries allows us to bring in a very important algorithmic distinction between the resource usage of the onetime preprocessing vs for each individual query. The second aspect that we consider is that the control flow graphs for most programs have constant treewidth. Our main contributions are simple and implementable algorithms that support multiple queries for algebraic path properties for RSMs that have constant treewidth. Our theoretical results show that our algorithms have small additional onetime preprocessing, but can answer subsequent queries significantly faster as compared to the current bestknown solutions for several important problems, such as interprocedural reachability and shortest path. We provide a prototype implementation for interprocedural reachability and intraprocedural shortest path that gives a significant speedup on several benchmarks.
p3745
a.